Sélection variables supervisée contraintes hiérarchiques Quang Hanane Azzag Mustapha Lebbah Université Paris Sorbonne Paris Laboratoire Informatique Paris avenue Baptiste Clément 93430 Villetaneuse France prénom paris13 Résumé sélection variables important fouille données lorsqu grand nombre variables disponible Ainsi certaines variables peuvent significatives corrélées pertinentes thode sélection objectif mesurer pertinence ensemble utili principalement critère évaluation présentons article critère supervisé permettant mesurer pertinence ensemble variables dernier repose utilisation score Laplacien auquel avons ajouté contraintes hiérarchiques Travailler cadre supervisé challenge domaine absence étiquettes classes résultats obtenus plusieurs bases tests encourageants metteurs Introduction sélection variables important classification lorsqu grand nombre variables disponibles Ainsi certaines variables peuvent significa tives corrélées pertinentes sélection variables permet également accélérer étape apprentissage réduire complexité algorithmes méthode sélection repose principalement algorithme recherche critère évaluation mesurer pertinence ensembles potentiels variables apprentissage supervisé sélection variables largement étudiée connu sélection variables améliorer qualité classificateur Zhang Parmi méthodes supervisées citons coefficient corrélation Pearson Rodgers Nicewander score Fisher information Thomas sélection variables attention apprentissage supervisé comparaison supervisé problème devient difficile raison absence étiquettes classes guider sélection Ainsi question impor tante comment évaluer pertinence ensemble fonctionnalités avoir recours étiquettes classe littérature approches souvent utilisées pertinence ensemble variables sélectionnées Kohavi Sélection variables supervisée approche filtrage filter approach celle enveloppante approach approches enveloppantes évaluent variables utilisant algorithme apprentissage finalement utilisé processus classement Cependant thodes enveloppantes généralement coûteuses temps peuvent appliqués grandes masses données Kohavi méthodes trage intéressés elles beaucoup efficaces critères évaluation totalement indépendants discriminateur utilisé variables alors traitées avant processus apprentissage travaux Caruana Freitag Sahami sélection variables montrent différentes approches traitant problème optimisation Parmi méthodes sélection attributs contexte supervisé sommes intéressés principalement score Laplacien critère évaluation utilisé littérature Plusieurs travaux tenté exploiter principe Score Laplacien Benab deslem Hindawi auteurs proposent variante utilise types contraintes supervisé données contraintes contraintes Cannot score calcule variance entre données étiquette auteurs proposé nouveau score appelé Cette méthode sélectionner variables manière conserver structure multi cluster données mesure corrélations entre variables manière supervisée méthode efficace traiter grande dimension limitée choix nombre classes Zhang Songcan auteurs utilisent principe proposant nouvelle méthode appelée supervised dimensionality reduction Cette approche préserve structure données utilise contraintes supervisés finies utilisateurs autres auteurs proposent méthode sélection variables supervisé combinant scores calculés données étiquetées quetés Kalakech combinaison simple considérablement biaiser résultat variables ayant meilleur score supervisé celles ayant mauvais scores partie supervisée versa hypothèse jacente score Laplacien structure données espace attributs localement préservée espace attributs sortie représentant cette structure graphes similarité distance données similaires espace entrée doivent aussi quand elles projetées vecteur attributs pertinents Inspirés travaux récents classification supervisée hiérarchique aussi modèle classification hiérarchique AntTree Azzag sommes intéressés étude score laplacien auquel avons intégré nouvelles contraintes supervisées hiérarchiques score définissons appelé Score laplacien hiérarchique principale contribution proposons utiliser approche construction graphe autre celle priori notre approche utilisons algorithme classification hiérarchique autonome structure arbre fournie permet définir nouveau score intégrant contraintes supervisées basées arborescence Score Laplacien contraintes hiérarchiques score Laplacien ensemble observations observation vecteur dimensions variables désigne échantillon variable Ainsi définissons variable score Laplacien sélec tionne variables pertinentes préservent mieux structure locale produisent grandes valeurs variances supposons données appartenant classe soient proches autres variable ainsi minimisé fonction suivante voisins sinon matrice diagonale matrice Laplacienne score Laplacien Hiérarchique notre approche sélection variables utiliser principe proches voisins fournis structure arbre AntTree Azzag littéra nombreux algorithmes apprentissage proposés découvrir structures adjacentes données construisant graphe voisinage effectuer analyse spectrale Belkin Niyogi Roweis Tenenbaum algorithme AntTree avantage complètement autonome avoir complexité faible modèle AntTree chaque arbre interne feuille représente donnée Ainsi nœuds arbre seront successivement ajoutés niveau niveaux inférieurs figure Toutes données doivent passer similarité propriété voisinage vérifiée observation connecter arbre seulement cette action augmente valeur TDist TDist valeur maximale distance distance euclidienne entre nœuds règle consiste comparer proche nœuds suffisamment éloignés TDist alors connecte Sinon déplace Ainsi TDist augmente localement chaque connecte arbre nouveau score souhaitons définir algorithme complètement sélection variables algorithme essentiellement score Laplacien auquel avons ajoutés contraintes hiérarchiques Ainsi utiliser graphe proches voisins proposons utiliser AntTree structure rarchique fournira automatiquement chaque observation AntTree voisins voisins représentent nœuds niveau inférieur directement Sélection variables supervisée Exemple structure topologique arbre connectés figure montre exemple observation quatre voisins seulement voisins niveau inférieur utilisant cette topologie nouvelle matrice adjacence définie comme partage direct sinon Ainsi critère score Laplacien contraintes hiérarchiques défini comme existe voisins niveau inférieur sinon sinon minimisant donnons avantage attributs respectant struc hiérarchique maximisant score sélectionne variables ayant grandes valeurs variance locale représentatives topologie arbre construit processus démonstration celui présenté Algorithme présente trois étapes nécessaires sélection variables Expérimentations cette section plusieurs expérimentations réalisées plusieurs bases réelles expérimentations présentées parties qualité clustering qualité classification supervisée utilisant algorithme proche voisin caractéristiques bases données résumées tableau comparons Algorithm Require Ensemble données Construire structure hiérarchique utilisant algorithme AntTtree calculer chaque variable suivant Equation Trier ordre croissant correspondant score obtenu données Taille Variables Classes ARP10P Coil20 Isolet Sonar Frank Asuncion Soybean Frank Asuncion Propriété bases données ARP10P disponible tureselection datasets performances notre approche score Laplacien classique algorithme MaxVa riance permet sélectionner attributs maximisent variance Comparaison cadre supervisé évaluer qualité clustering utilisons mesures pureté Infor mation Mutuelle Normalisée Normalized Mutual Information Strehl chacune maximisée faciliter comparaison entre méthodes appli quons algorithme means données prenant compte variables sélectionnées expérimentations construire graphe fixons mètre évaluons ensuite qualité clustering différentes valeurs nombre clusters manière suivante AR10P Coil20 Isolet Sonar Soybean chaque valeur exécutons différents critères sélectionner variables AR10P Coil20 Isolet Sonar Soybean figures jusqu montrent courbes performances clustering Pureté rapport nombre variables sélectionnées manière générale notre approche obtient meilleurs résultats rapport autres méthodes figure observons algorithme fournit résultats raisonnables rapport critères Pureté données Coil20 meilleur terme Pureté lorsque nombre variables alentour trois expérimentations notons notre algorithme nettement meilleur autres figure pureté augmente manière constante mêmes remarques observées Sonar Soybean atteint valeur Pureté plupart utilisant seulement variables Sélection variables supervisée tables jusqu résumons résultats clustering obtenus toutes bases testées résultats numériques obtenus AR10P montrent amélioration performances Pureté qualité MaxVariance tableau remarquons résultats fournis coil20 clusters variables sélectionnées donnent valeur mieux avait utilisé variables tableau résultats Pureté utilisant variables meilleurs proches utilisant variables Sonar seule méthode permet obtenir meilleurs résultats Clusters Clusters Clusters Clusters Clusters Clusters Performance clustering Pureté nombre variables sélectionnées AR10P Pureté Clusters MaxVariance Toutes variables Qualité clustering utilisant variables sélectionnées AR10P dernière ligne représente performance obtenue toutes variables Clusters Clusters Clusters Clusters Clusters Clusters Performance clustering Pureté nombre variables sélectionnées Coil20 Pureté Clusters MaxVariance Toutes variables Qualité clustering utilisant variables sélectionnées Coil20 dernière ligne représente performance obtenue toutes variables Pureté Clusters MaxVariance Toutes variables Qualité clustering utilisant variables sélectionnées Isolet dernière ligne présente performance utilisant toutes variables Sélection variables supervisée Clusters Clusters Clusters Clusters Clusters Clusters Performance clustering Pureté nombre variables sélectionnées Isolet Clusters Clusters Clusters Clusters Clusters Clusters Performance clustering Pureté nombre variables sélectionnées Sonar Pureté Clusters MaxVariance Toutes variables Qualité clustering utilisant variables sélectionnées Sonar dernière ligne présente performance utilisant toutes variables Clusters Clusters Clusters Clusters Clusters Clusters Performance clustering pureté nombre variables sélectionnées Soybean Pureté Clusters MaxVariance Toutes variables Qualité clustering utilisant variables sélectionnées Soybean dernière ligne présente performance utilisant toutes variables Sélection variables supervisée Comparaison cadre supervisé cette partie souhaitons évaluer différents critères sélection variables utilisant classifieur chaque donnée cherchons proche voisin argmin classe erreur classification calculée comme suivant Erreur autrement figure montre résultats erreur classification bases données sélectionnées manière générale erreurs classification meilleures erreurs MaxVariance AR10P meilleure classification obtenue utilisant seulement variables Erreur également intéressant noter Coil20 obtient bonne classification Erreur utilisant variables Isolet converge alentours variables Sonar Soybeans obtient résultats utilisant uniquement variables tableau présente erreurs classification obtenues différentes valeurs variables chaque remarque produit résultats comparables obtenus classification toutes variables AR10P Coil20 Isolet Sonar Soybean Erreur classification nombre variables sélectionnées AR10P Coil20 Isolet Sonar Soybean Variables MaxVariance Toutes variables Erreur classification utilisant nombre variables dernière ligne sente performance utilisant toutes variables Conclusions perspectives Etudier sélection variables supervisé challenge munauté scientifique raison manque informations labels données relever avons proposé approche autonome sélection variables nommée variante score Laplacien utilise structure topologie arbre défini AntTree Notre algorithme autonome nécessite aucun paramètre sultats expérimentaux plusieurs données montrent algorithme réalise performances élevées supervisé supervisé Comme perspective sommes fixés comme objectif introduire nouvelles contraintes supervisées rarchiques sélection variables utiliser autre graphe représenterait liens faibles Références Azzag Monmarché Slimane Venturini Guinot Anttree model clustering artificial Canberra Australia Belkin Niyogi Laplacian eigenmaps spectral techniques embedding clustering Advances Neural Information Processing Systems Press Benabdeslem Hindawi Constrained laplacian score supervised selection Proceedings European conference Machine learning knowledge discovery databases Volume Berlin Heidelberg Springer Verlag Huang Graph regularized nonnegative matrix factori zation representation Trans Pattern Intell Zhang Unsupervised feature selection multi cluster SIGKDD Conference Knowledge Discovery Mining Caruana Freitag Greedy attribute selection Cover Thomas Elements Information Theory Wiley Series communications Signal Processing Wiley Interscience Sélection variables supervisée Stork Pattern Classification Edition Wiley Interscience Frank Asuncion machine learning repository Niyogi Laplacian score feature selection Advances Neural Information Processing Systems Kohavi Pfleger Irrelevant Features Subset Selection Problem International Conference Machine Learning Kalakech Biela Macaire Hamad Constraint scores supervised feature selection comparative study Pattern Recogn Kohavi Wrappers Feature Subset Selection Artificial Intelli gence Koller Sahami Toward optimal feature selection Rodgers Nicewander Thirteen Correlation Coeffi cient American Statistician Roweis Nonlinear dimensionality reduction locally linear bedding SCIENCE Strehl Ghosh Cardie Cluster ensembles knowledge reuse framework combining multiple partitions Journal Machine Learning Research Tenenbaum Silva Langford global geometric framework nonlinear dimensionality reduction Science Feature selection dimensional correlation based filter solution Zhang Songcan supervised dimensionality reduction Proceedings International Conference Mining Zhang Robles Feature selection multi label naive bayes classification Summary paper address problem unsupervised feature selection which portant challenge absence class labels would guide search relevant information define Laplacian score constraining Laplacian score using topology structure interest structure automatically discover local structure local neighbors Experimental results comparison studies demonstrated effectiveness proposed algorithm clustering classification applications