Microsoft do_et_al docAlgorithmes rapides boosting Thanh Daniel Fekete François Poulet Equipe INRIA Futurs Université Paris 91405 Orsay Cedex dtnghi Daniel Fekete inria dtnghi fekete IRISA TexMex Université Rennes Campus Beaulieu 35042 Rennes Cedex francois poulet irisa irisa texmex people poulet index_fr Résumé algorithmes boosting Newton Support Vector Machine Proximal Support Vector Machine Least Squares Support Vector Machine présentons visent classification grands ensembles données machines standard présentons extension algorithmes construire algorithmes boosting cette avons utilisé terme régularisation Tikhonov théorème Sherman Morrison Woodbury adapter algorithmes traitement ensembles données ayant grand nombre dimensions avons ensuite étendus construction algorithmes boosting traiter données ayant simultanément grand nombre individus dimensions performances algorithmes évaluées grands ensembles données comme Adult KDDCup Forest Covertype Reuters 21578 binary machine standard Introduction algorithmes Séparateurs Vaste Marge proposés Vapnik méthodes noyaux permettent construire modèles précis deviennent outils classification données populaires trouver nombreuses applications clopinet isabelle Projects applist comme reconnaissance visages catégorisation textes bioinformatique Cependant demandent résolution programme quadratique calcul moins complexité égale carré nombre individus ensemble apprentissage quantité mémoire nécessaire impossible utiliser grands ensembles données heure actuelle Lyman besoin permettre passage échelle traiter grands ensembles données machines standard heuristique possible améliorer apprentissage décomposer programme quadratique série petits problèmes Boser Chang Osuna Platt niveau œuvre Algorithmes rapides boosting méthodes apprentissage incrémental Cauwenberghs Poulet Poulet Mangasarian Poulet permettent traiter grands ensembles données solutions partielles chargeant consécutivement ensembles apprentissage mémoire avoir charger ensemble total algorithmes parallèles distribués Poulet Poulet utilisent machines réseaux améliorer temps exécution apprentissage algorithmes apprentissage actif Poulet Koller choisissent ensemble individus ensemble actif représentatif construction modèle article continuons développer algorithmes boosting Poulet Poulet Fekete classifier simultanément grand nombre individus dimensions présentons classe algorithmes boosting basant Mangasarian Mangasarian Suykens Vandewalle classification grands ensembles données machines standard algorithmes obtiennent solution résolution système équations linéaires programme quadratique permettent classifier beaucoup rapidement ensembles ayant grands nombres individus avons reformulés utilisant terme régularisation Tikhonov théorème Sherman Morrison Woodbury Golub traiter ensembles données ayant nombre dimensions Enfin avons construit algorithmes boosting Adaboost Freund Schapire Breiman classification ensembles données ayant simultanément grand nombre individus dimensions performances algorithmes évaluées grands ensembles Blake comme Adult KDDCup Forest Covertype Reuters 21578 binary résultats comparés obtenus LibSVM Chang Joachims paragraphe présente brièvement algorithmes paragraphe décrit extensions classification ensembles données ayant grand nombre dimensions paragraphe présentons algorithmes boosting traitement grands ensembles données simultanément nombre individus dimensions quelques résultats paragraphe avant conclusion travaux futurs Quelques notations utilisées article vecteurs représentés matrices colonne produit scalaire vecteurs norme vecteur matrice inverse notée vecteur colonne représente matrice identité Algorithmes considère tâche classification binaire linéaire individus dimensions representés matrice leurs classes stockées matrice diagonale algorithme cherche meilleur hyperplan séparation données ramène maximiser marge distance entre plans supports classes support classe sépare individus classe autres autre minimiser erreurs individus mauvais support distances erreurs notées variables individu support alors algorithme revient programme quadratique marge optimal marge optimal linéaire classification données classes constante utilisée contrôler marge erreurs optimal obtenu résolution programme quadratique œuvre coûteuse temps mémoire Algorithme algorithme generalized Mangasarian modifie algorithme maximisant marge minimisant erreurs substituant fonction objectif programme quadratique obtient problème optimisation contrainte obtenu remplaçant valeurs négatives notant réécrire problème Mangasarian proposé utiliser méthode itérative Newton résoudre efficacement problème optimisation principe méthode Newton minimiser successivement approximations second ordre fonction objectif basant développement Taylor second ordre voisinage problème Algorithmes rapides boosting optimisation dérivée première ensuite Hessien dérivée seconde matrice diagonale élément diagonal gradient Algorithme Newton algorithme itératif Newton tableau converge solution après relativement faible nombre itérations nécessite résolutions système linéaire inconnues programme quadratique nombre dimensions ensemble données inférieur algorithme capable classifier rapidement grand nombre individus algorithme classifie million points dimension secondes Algorithme algorithme proximal Mangasarian modifie aussi algorithme maximisant marge minimisant erreurs contrainte substituant fonction objectif programme quadratique obtenons alors problème optimisation calculons dérivées partielles donne système linéaire inconnues suivant algorithme demande résolution système linéaire inconnues programme quadratique capable traiter grand nombre individus temps restreint machine standard exemple classification million points dimension effectuée secondes 512Mo répéter calculer dérivée première calculer Hessien jusqu obtenir coordonnées scalaire partir solution Algorithme algorithme proposé Suykens Vandewalle modifie également algorithme maximisant marge minimisant erreurs contrainte substituant fonction objectif programme quadratique obtenons alors problème optimisation calculons également dérivées partielles donne système linéaire inconnues suivant matrice diagonale identité dernier élément algorithme complexité celle demande résolution système linéaire inconnues programme quadratique capable traiter grand nombre individus temps restreint machine standard Extensions grand nombre dimensions Certaines applications comme bioinformatique fouille textes nécessitent traiter données ayant nombre important dimensions nombre individus réduit matrice taille importante résolution système inconnues nécessite temps calcul élevé adapter algorithme données avons appliqué théorème Sherman Morrison Woodbury systèmes équations algorithmes algorithme iDEXec inversion matrice formule écrite Ensuite appliquons formule Sherman Morrison Woodbury partie droite formule obtenons inversion matrice taille ramenée inverser matrice taille Cette nouvelle formulation permet algorithme traiter facilement ensembles données ayant nombre dimensions important Algorithmes rapides boosting algorithme appliquons directement théorème Sherman Morrison Woodbury obtenons nouveau système nécessite inversion matrice taille matrice taille système Cette nouvelle formulation algorithme classifier facilement ensembles données ayant nombre dimensions important adapter algorithme grand nombre dimensions applique théorème Sherman Morrison Woodbury système équations faisant avions matrice singulière inverser avons ajouté terme régularisation Tikhonov méthode couramment utilisée résoudre genre problème terme Tikhonov ajouté obtenons alors représente matrice diagonale terme autres termes valent Ensuite pouvons utiliser théorème Sherman Morrison Woodbury obtenons nouveau système aussi inversion matrice taille matrice taille système Cette nouvelle formulation permet algorithme classifier facilement ensemble données ayant nombre dimensions important Boosting avons décrit versions permettant traiter ensembles données ayant grand nombre individus grand nombre dimensions ensemble données simultanément grand nombre individus dimensions traité aucune versions parce algorithmes demandent inversion matrice taille carré nombre individus carré nombre dimensions nécessite alors énormément mémoire temps exécution remédier problème proposons construire algorithmes boosting algorithme Adaboost développé Freund Schapire années méthode mettant œuvre ensemble classifieurs basiques principale répéter apprentissage classifieur basique plusieurs reprises chaque étape boosting classifieur basique concentre individus classifiés précédente Enfin combine modèles obtenus algorithme Adaboost simple implémenter donne résultats pratique Reyzin Schapire démontré cohérence Adaboost utiliser algorithmes comme classifieurs basiques chaque étape boosting Adaboost construction échantillon ensemble données taille inférieure ensemble total partir poids assignés individus classifiés précédente données grand nombre individus Adaboost utilise algorithmes échantillons données chaque étape boosting ensemble données grand nombre dimensions simultanément grand nombre individus dimensions Adaboost utilise algorithmes décrits paragraphe algorithme Adaboost présenté tableau Algorithme Adaboost Remarquons utilisation algorithmes algorithme Adaboost intéressante parce algorithmes traitent beaucoup rapidement ensembles échantillons données résolution programme quadratique standard Entrée individus apprentissage nombre itérations Apprentissage initialisation poids individus faire boucle étapes boosting faire créer échantillon individus basant poids créer modèle partir échantillon calculer erreur apparente iWiit choisir coefficient calculer facteur normalisation mettre poids individus alors sinon finpour retourner Classification nouvel individu basée xhsigne Algorithmes rapides boosting proposons également utiliser algorithme Breiman algorithmes manière similaire algorithme Adaboost principe algorithme semblable Adaboost utilise mécanisme majoritaire poids également manière différente prendre échantillon concentrant erreurs toutes étapes précédentes algorithme donne bonnes performances classification grands ensembles données Quelques résultats avons développé programme Linux utilisant librairie Lapack lapack bénéficier bonnes performances calcul matriciel avons aussi développé algorithmes spécifiques matrices creuses allons présenter évaluation prenant compte précision temps apprentissage avons sélectionné grands ensembles données Blake ensemble données textuelles binary cjlin libsvmtools datasets ensembles décrits tableau Classes Individus Dimensions Protocole Adult 48842 32561 16281 Forest cover 581012 Reuters 21578 10789 29406 binary Description ensembles données avons utilisé algorithmes boosting AdaNSVM Arcx4NSVM AdaPSVM Arcx4PSVM AdaLSSVM Arcx4LSSVM effectuer classification Pentium 1024Mo résultats comparés obtenus algorithme standard LibSVM Chang nouvelles versions Joachims mettons tableaux meilleurs résultats caractères deuxièmes souligné faisons comparer premiers résultats expérimentaux algorithmes boosting LibSVM parce algorithme semblable algorithme ensembles données Reuters 21578 binary utilisés évaluer performances classification ensembles ayant simultanément grand nombre individus dimensions avons utilisé mccallum prétraitement données Reuters 21578 Chaque document comme vecteur avons obtenu 29406 dimensions sélection dimensions avons effectué classification classes nombreuses ensemble données ayant classes avons utilisé approche contre reste résultats présentés tableau moyenne précision rappel breakeven point grandes catégories remarque algorithmes donnent toujours résultats meilleurs concerne précision jusqu amélioration détriment certains rapidité calcul Précision AdaLSSVM ArcLSSVM AdaNSVM ArcNSVM LibSVM Money Grain Crude Trade Interest Wheat Résultats précision catégories ensemble données Reuters 21578 Temps AdaLSSVM ArcLSSVM AdaNSVM ArcNSVM LibSVM Money Grain Crude Trade Interest Wheat Temps classification catégories ensemble Reuters 21578 ensemble données avons utilisé prétraitement Chang formant classe positive négative enlevant instances apparaissant simultanément classes positive négative résultats montrent algorithmes permettent petite amélioration précision étant significativement rapide jusqu rapide facteur encore significatif ensemble données Adult rapide précision équivalente ensemble données Forest Cover algorithmes effectué classification classes nombreuses moins secondes LibSVM donné aucun résultat après jours calcul Cependant travaux récents montré algorithme Joachims effectue classification ensemble données secondes Intel tenant compte différences rapidité processeurs pouvons estimer raisonnablement algorithmes rapides Algorithmes rapides boosting Enfin ensemble données effectué classification secondes 800MHz précision algorithmes obtiennent précision seulement secondes rapidement Précision AdaLSSVM ArcLSSVM AdaNSVM ArcNSVM LibSVM Adult Forest covertype KddCup Performances précision algorithmes grands ensembles données Temps AdaLSSVM ArcLSSVM AdaNSVM ArcNSVM LibSVM Adult Forest covertype KddCup Performances temps exécution grands ensembles données Conclusion perspectives avons présenté classe algorithmes boosting classification grands ensembles données machines standard principale étendre algorithmes récents Mangasarian Suykens construire algorithmes boosting avons utilisé terme régularisation Tikhonov théorème Sherman Morrison Woodbury adapter algorithmes traiter ensembles données ayant grand nombre dimensions avons ensuite étendus construction algorithmes boosting Adaboost traiter données ayant simultanément grand nombre individus dimensions performances algorithmes évaluées grands ensembles données comme Adult KDDCup Forest Covertype Reuters 21578 binary machine standard résultats expérimentaux montrent algorithmes boosting rapides bonne précision permettent passage échelle obtiennent précision comparaison LibSVM algorithmes efficaces autres nouveaux algorithmes ensembles données grandes tailles nombre dimensions individus également montré bonne rapidité exécution précision première extension travaux consister étendre algorithmes faire versions parallèles distribuées ensemble machines Cette extension permettra améliorer temps tâche apprentissage seconde proposer nouvelle approche classification linéaire Références Blake Repository Machine Learning Databases mlearn MLRepository Boser Guyon Vapnik Training Algorithm Optimal Margin Classifiers Annual Workshop Computational Learning Theory Pittsburgh Pennsylvania Breiman Arcing classifiers annals statistics Cauwenberghs Poggio Incremental Decremental Support Vector Machine Learning Advances Neural Information Processing Systems Press Chang LIBSVM Library Support Vector Machines cjlin libsvm Fekete Large Scale Classification Support Vector Machine Algorithms appear ICMLA Poulet Classification grands ensembles données nouvel algorithme Actes Série Extraction Gestion Connaissances Cépaduès Editions Poulet Classifying billion distributed algorithm Vietnam Poulet Towards Dimensional Mining Boosting Visualization Tools ICEIS Entreprise Information Systems Porto Portugal Poulet Mining Large Datasets Visualization ICEIS Entreprise Information Systems Miami Freund Schapire decision theoretic generalization learning application boosting EuroCOLT Mangasarian Incremental Support Vector Machine Classification Mining Arlington Virginia Golub Matrix Computations Hopkins University Press Balti Maryland Joachims Training Linear Linear SIGKDD Lyman Varian Swearingen Charles Jordan information berkeley research projects Algorithmes rapides boosting Mangasarian Generalized Support Vector Machines Mining Institute Technical Report Computer Sciences Department University Wisconsin Madison Mangasarian finite newton method classification problems Mining Institute Technical Report Computer Sciences Department University Wisconsin Platt Training Support Vector Machines Using Sequential Minimal Optimization Advances Kernel Methods Support Vector Learning Schoelkopf Burges Smola Poulet Mining Large Datasets Support Vector Machine Algorithms Enterprise Information Systems Filipe Hammoudi Piattini Kluwer Academic Publishers Reyzin Schapire boosting margin boost classifier complexity Machine Learning Pittsburgh Pennsylvania Suykens Vandewalle Least Squares Support Vector Machines Classifiers Neural Processing Letters Incremental Learning Support Vector Machines SIGKDD Diego Koller Support Vector Machine Active Learning Applications Classification Machine Learning Stanford Vapnik Nature Statistical Learning Theory Springer Verlag Classifying large using hierarchical clusters SIGKDD Summary Boosting algorithms classifying large datasets standard personal computers extend several efficiently classify large datasets developed incremental version datasets billions points adding Tikhonov regularization using Sherman Morrison Woodbury formula developed column incremental version process datasets small number points dimensionality Finally applying boosting including AdaBoost Arcx4 these incremental algorithms developed classification algorithms massive dimensional datasets Numerical results binary Reuters 21578 Forest cover datasets showed algorithms often significantly faster accurate state algorithms LibSVM