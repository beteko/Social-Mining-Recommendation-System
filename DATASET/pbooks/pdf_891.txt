nouvelle approche boosting données bruitées Bahri Mondher Maddouri Laboratoire Université avenue Pierre Mendes France 69676 Cedex Bahri lyon2 lyon2 INSAT urbaine charguia Tunis Tunisie Mondher Maddouri insatech Résumé réduction erreur généralisation principales motivations recherche apprentissage automatique grand nombre travaux menés méthodes agrégation classifieurs améliorer techniques performances classifieur unique Parmi méthodes agrégation boosting doute formant grâce adaptative distribution exemples visant augmenter façon exponentielle poids exemples classés Cepen données fortement bruitées cette méthode sensible apprentissage vitesse convergence affectée article proposons nouvelle approche basée modifications exemples calcul erreur apparente effectuées algorithme classique AdaBoost étude expérimentale montre intérêt cette velle approche appelée Approche Hybride AdaBoost BrownBoost version AdaBoost adaptée données bruitées Introduction Générale émergence bases données modernes présentent énormes capacités ckage gestion associée évolution systèmes transmission techniques acquisition automatique données contribuent construction masse données dépasse capacités humaines traiter données sources infor mations pertinentes nécessitent outils synthèse interprétation recherches orientées systèmes intelligence artificielle puissants permettant extraction informations utiles aidant prise décisions meilleure synthèse terprétation fouille données mining puisant outils statistique intelligence artificielle bases données méthodologie offre possibilité construire modèle prédiction phénomène partir autres phénomènes facilement accessibles basant processus traction connaissances partir données processus classification intelligente données Cependant modèle construit parfois engendrer erreurs nouvelle approche boosting classification classification aléatoire aurait engendrée réduire erreurs nombreux travaux mining spécifiquement apprentissage automa tique porté méthodes agrégation classifieurs suprême améliorer techniques performances classifieur unique méthodes agréga efficaces point compromis Biais variance aussi grâce trois raisons fondamentales raison statistique raison informatique raison représentation pliquées étude menée Dietterich méthodes agrégation classifieurs peuvent regroupées catégories celles fusionnent classifieurs prédéfinis trouve exemple simple Bauer Kohavi pondéré Bauer Kohavi majorité pondérée Littlestone Warmuth celles fusionnent classifieurs selon données durant apprentissage trouve stratégies adaptatives boosting algorithme AdaBoost Shapire stratégies aléatoires bagging Breiman intéresserons suite boosting mesure étude comparative menée Dietterich montre données faiblement bruitées AdaBoost performant bagging semblant immunisé contre apprentissage effet AdaBoost essaye directement optimiser votes pondérés Cette constatation traduite seulement erreur empirique échantillon apprentissage décroît exponentiellement nombre itérations également erreur généralisation baisse aussi Cepen cette étude montre données fortement bruitées AdaBoost présente erreur parfois important bagging raison mauvaises performances évidence Dietterich vient boosting augmenter poids exemples bruités travers itérations conséquence immédiate apprentissage exemples bruités vitesse convergence boosting trouve également pénalisée données surmonter difficultés rencontrées boosting données bruitées proposons nouvelle approche associe hypothèses construites hypothèse courante définir erreurs itération courante nature cette approche appelée approche Hybride suite article organisée comme section consacrée synthétique principaux travaux visant amélio boosting section présentons amélioration boosting proposons section effectuons large étude expérimentale visant comparer nombreuses bases données réelles performances AdaBoost AdaBoost Hybride termes erreur rappel vitesse convergence Enfin section terminons conclusion perspectives présence données bruitées boosting présente différentes faiblesses telles apprentissage dégradation vitesse apprentissage Diverses améliorations proposées opèrent poids exemples parfois principe boosting allons présenter principales méthodes ayant comme objectif amélioration boosting rapport faiblesses effectué selon recherches premier regroupe approches abordant problème gestion données bruitées laquelle phénomènes apprentissage peuvent Bahri survenir deuxième regroupe approches portant problèmes vitesse convergence apprentissage émergence évolution bases données modernes contraignent aujourd chercheurs étudier améliorer capacités tolérance bruit boosting effet bases données fortement bruitées raison nouvelles technologies acqui sition données telles parallèle études telles celles Dietterich Rätsch Schapire Singer montrent AdaBoost apprendre données lorsqu elles bruitées certain nombre travaux récents tenté limiter risques apprentissage améliorations proposées fondent essentiellement AdaBoost augmenter poids exemples manière exponentielle solutions présentent réduire données bruitées données détectées supprimées avant apprentissage heuristiques ficaces Brodley Friedl Wilson Martinez données détectées processus boosting parle alors bonne gestion bruit faire chercheurs orientés amélioration points forts boosting exemples classés maximisation marge signification poids Adaboost associe hypothèses enfin choix apprenant faible Modification poids exemples adaptative distribution exemples visant augmenter poids appris classifieur précédent permet améliorer performances importe algorithme apprentissage effet chaque itération distribution courante favorise exemples ayant classés hypothèse précédente plusieurs chercheurs proposé tégies portant modification poids exemples apprentissage Madaboost Domingo Watanabe principe borner poids exemples suspects probabilité initiale ainsi croissance incontrôlée poids exemples bruités origine problèmes AdaBoost autre approche algorithme boosting résistant bruit BrownBoost McDonald algorithme Boost Majority incorporant paramètre temporel Ainsi bonne évaluation paramètre BrownBoost capable éviter apprentissage citera encore Logitboost Schapire Singer adapte principe AdaBoost modèle régression logistique LogitBoost réduit minimum critère ployant étapes Newton adapter modèle régression logistique optimisant logarithme vraisemblance autre approche produit moins erreur généralisation comparée approche classique erreur apprentissage légèrement élevée celle Vladimir Vezhnevets diminution contribution classifieurs fonctionne données correctement classifiées pourquoi méthode appelée Modest AdaBoost force classifieurs modestes travaille seulement domaine défini distribution terminée critère arrêt normal SmoothBoost Servedio essaye réduire effet apprentissage limites imposées distribution produite pendant processus boosting particulier chaque itération limite nouvelle approche boosting poids exemple poids dépasse cette limite considéré comme bruité retiré ensemble apprentissage dernière approche IAdaBoost Sebban Suchier construire autour chacun exemples mesure information locale permettant évaluer risques surapprentissage utilisant graphe voisinage permet mesurer information autour chaque exemple Grâce mesures calcule fonction évalue nécessité mettre exemple AdaBoost Cette fonction permet gérer outliers recoupements classes centres clusters Modification marge certaines études après observation algorithmes boosting montré erreur généralisation décroît encore apprentissage stable nulle explication exemples apprentissage classés boosting maximiser vantage marges Servedio performances boosting suite cette explication certains cherché modifier marge explicitement maximisant minimisant améliorer performances boosting contre apprentissage Plusieurs approches succédées telles AdaBoos Rätsch essaye identifier enlever exemples étiquetés ensemble apprentissage appliquer contrainte marge maximale exemples supposés étiquetés utilisant Margin moins sensible apprentissage rapport marge Adaboost algorithme proposé Friedman auteurs utilisent schéma pondération exploite fonction marges croît moins fonction exponentielle Modification poids classifieurs évaluation performances chercheurs également interrogés signification poids AdaBoost associe hypothèses produites poids valeur déterminée fonction échecs réussites classification échantillon déterminé Cependant expériences données simples erreur généralisation diminuait encore alors apprenant faible avait fourni toutes hypothèses possibles Autrement lorsqu hypothèse apparaît plusieurs finalement poids cumul caractère absolu plusieurs auteurs espéré approcher valeurs processus adaptatif Locboost alternative construction ensemble représentation hypothèses permet coefficients pendre données poids locaux attribués chaque exemple Choix apprenant faible Plusieurs auteurs intéressés choix classi fieur boosting GloBoost Torre utilise apprenant faible produit hypothèses correctes Celles peuvent abstenir partie exemples aucun tromper exemple moindres généralisés malement corrects RankBoost Dietterich apprenant faible accepte comme données entrées attributs préférences fonctions Bahri Certes méthodes améliorer façon autre performance contre bruit Toutefois autres paramètres boosting trouvent pénalisés erreur apprentissage temps détection bruit vitesse convergence vitesse convergence problème apprentissage rencontré boosting bases modernes évoqué précédemment existe autre problème celui vitesse convergence algorithmes boosting spécialement Adaboost effet présence données fortement bruitées erreur optimale algorithme apprentissage utilisé atteinte tardivement autres termes Adaboost temps itérations pondérer exemples méritent aucune attention puisqu bruit recherches menées détecter données bruitées améliorer ainsi perfor mances boosting termes convergence iBoost Nguyen spécialiser hypothèses faibles exemples elles supposées correctement classer iAdaboost aussi approche contribue améliorer Adaboost contre conver gence amélioration modification théorème Schapire Singer Cette modification réalisée intégrer risque Bayes mettre exergue situations certains exemples classes différentes partagent représen tation effets cette modification convergence rapide risque optimal réduction nombre hypothèses faibles construire Enfin RegionBoost Maclin nouvelle stratégie pondération classifieurs Cette pondération évaluée moment technique basée proches voisins exemple étiqueter Cette approche permet spécialiser chaque classifieur régions ensemble apprentissage Amélioration proposée Adaboost Hybride améliorer performances Adaboost éviter forcer apprendre exemples priori bruités exemples deviendraient difficiles apprendre durant processus boosting proposons nouvelle approche inspire Adaboost construit chaque itération hypothèses échantillon défini calcul erreur apprentissage faits partir résultats seules hypothèses exploitent résultats fournis hypothèses construites itérations antérieures autres échantillons Cette approche appelée approche Hybride hypothèses antérieures exemples itération courante tiendra compte seulement résultats itération courante aussi itérations antérieures Pseudo Adaboost Hybride classe prévoir échantillon faire Initialiser poids nouvelle approche boosting faire Tirer échantillon apprentissage selon probabilités Construire hypothèse algorithme apprentissage erreur apparente poids exemples argmax Calculer faire argmax classé argmax classé valeur normalisation telle Fournir sortie hypothèse finale argmax modification algorithme porte prise compte ensemble itérations passées effectuer prédiction courante modifie notamment poids exemples calcul erreur Modification poids exemples chaque itération appel experts utilisés hypothèses itérations antérieures mettre poids exemples effet compare seulement classe prédite hypothèse itération courante classe réelle somme hypothèses pondérées depuis première itération jusqu itération courante cette somme classe différente classe réelle alors exponentielle semblable celle effectuée Adaboost appliquée exemple classé Ainsi cette modification concerne exemples classés encore classés logique attendre amélioration vitesse convergence réduction erreur généralisation étant donnée lissage hypothèses chaque itération Modification calcul erreur hypothèse itération méthode proposons prend considération hypothèses antérieures itération courante former hypothèse courante chaque itération erreur apparente poids exemples prédits façon erronée moyenne pondérée thèses itérations antérieures coefficient attribué hypothèse courante aussi modifié puisque coefficient dépend calcul erreur apparente Cette modification effet lissage laisse algorithme chaque itération dépendant autres itérations résultats améliorant surtout erreur généralisa attendus puisque chaque hypothèse coefficient calculé partir autres hypothèses Expérimentations cette section allons comparer résultats notre algorithme AdaBoostHyb fournis AdaBoost algorithme référence BrownBoost algorithme connu résistant données bruitées BrownBoost utilise fonction pondération suivante autre probabilité variable binomiale Bahri dépend nombre itérations finales temps total exécution itération courante nombre exemple correctement étiqueté enfin probabilité succès imposée toute hypothèse faible avantage cette approche données bruitées seront probablement détectées certain moment poids cessera augmenter cours expérimentations comparaison travers erreur généralisation rappel vitesse convergence apprenant faible utilisé algorithme choisi suite étude Dietterich montré sensible bruit estimer biais succès théorique avons appel procédure validation croisée parties évaluer comportement AdaBoostHyb performances tesse convergence avons séparé expérimentations plusieurs parties première avons travaillé bases Newman rapportons valeur erreur généralisation rappel choisis comme critère perfor mance deuxième partie avons bruité aléatoirement bases données bruit analyser comportement trois algorithmes retenus choisi référence étude Dietterich signalait résultats décevants AdaBoost données bruitées dernière partie avons établi nostic convergence différents algorithmes fondant nombre itérations effectuées avons choisi retenir bases données diverses effet bases données choisies contiennent bases ayant valeurs manquantes HEPATI HYPOTHYROID autres ayant classe prédire comporte plusieurs modalités classe modalités DIABETES modalités classe modalités classe modalités autres ayant plusieurs attributs attributs tableau décrit bases données utilisées expérimentations Bases données Attrib numeric numeric symbolic boolean valued WEATHER numeric symbolic CREDIT 16numeric symbolic TITANIC symbolic DIABETES numeric HYPOTHYROID numeric symbolic HEPATITIS numeric symbolic CONTACT LENSES nominal numeric boolean STRAIGHT numeric numeric symbolic LYMPH numeric BREAST CANCER numeric symbolic Caractéristiques bases données Comparaison termes erreur généralisation tableau présente résultats obtenus cette partie ayant choisi chacun algorithmes effectuer itérations choix nombre itérations expliqué dernière partie expériences avons indiqué erreur généralisation nouvelle approche boosting chacun algorithmes AdaBoostM1 BrownBoost AdaBoostHyb avons ailleurs utilisé mêmes échantillons validation croisée différents algorithmes avoir comparaison observation résultats montre effets positifs approche hybride effet bases algorithme AdaBoostHyb présente erreur inférieur celui AdaBoostM1 seulement LYMPH notre approche donne erreur généralisation élevée approche classique remarquons aussi améliorations significatives erreur généralisation correspondant bases données CONTACT BREAST CANCER exemple erreur généralisation BREAST CANCER passe comparons approche proposée BrownBoost remarquons bases données algorithme AdaBoostHyb présente erreur inférieur BrownBoost faveur AdaBoostHyb montre exploitant hypothèses générées itérations antérieures corriger poids exemples possible améliorer performances boosting expliqué calcul erreur apparente conséquent calcul coefficient classifieur ainsi hybridation hypothèse courante hypothèses antérieures Databases AdaBoost BrownBoost AdaBoostHyb WEATHER CREDIT TITANIC DIABETES HYPOTHYROID HEPATITIS CONTACT LENSES STRAIGHT LYMPH BREAST CANCER erreurs généralisation Comparaison terme rappel résultats encourageants auxquels sommes parvenus mènent approfondir étude cette nouvelle approche cette partie essayons connaître impact notre approche rappel puisque celle améliore effectivement boosting positivement rappel tableau présente résultats obtenus cette partie ayant choisi chacun gorithmes effectuer itérations comme précédemment avons indiqué rappel chacun algorithmes AdaBoostM1 BrownBoost AdaBoostHyb résultats obtenus confirment précédents effet AdaBoostHyb augmente rappel bases données ayant erreur moins important rappel algorithmes erreur bases données égaux considérons BrownBoost remarquons dernier améliore rappel AdaBoostM1 Bahri chaque données données TITANIC Cependant rappel obtenu notre approche meilleur celui obtenu BrownBoost données constatons aussi notre approche améliore rappel LYMPH erreur était importante notons alors nouvelle approche tivement rappel améliore lorsque erreur généralisation importante Databases AdaBoost BrownBoost AdaBoostHyb WEATHER CREDIT TITANIC DIABETES HYPOTHYROID HEPATITIS CONTACT LENSES STRAIGHT LYMPH BREAST CANCER Rappel Comparaison données bruitées cette partie étude faite Dietterich Dietterich ajoutant bruit aléatoire données ajout bruit effectué chacune bases changeant aléatoirement valeur classe prédite programme autre valeur possible cette classe tableau montre comportement algorithmes bruit remarquons approche hybride sensible aussi bruit puisque erreur généralisation augmenté toute bases données Cependant cette augmentation reste toujours inférieure celle approche sique bases données telles CREDIT HEPATITIS HYPOTHYROID avons étudié bases données avons point commun valeurs manquantes CREDIT HEPATITIS HYPOTHYROID possèdent respec tivement valeurs manquantes constatons alors notre amélioration effet accumulation types bruit valeurs manquantes bruit artificiel algorithme AdaBoos améliore performances AdaBoost contre bruit reste bases données Considérant BrownBoost remarquons améliore erreur généralisation toute bases données comparaison AdaBoostM1 Cependant BrownBoost donne résultats meilleurs approche hybride seulement bases données Notre approche donne meilleurs résultats autres bases données résultats encouragent étudier détails comportement notre approche bruit nouvelle approche boosting Bases Données AdaBoost BrownBoost AdaBoostHyb WEATHER CREDIT TITANIC DIABETES HYPOTHYROID HEPATITIS CONTACT LENSES STRAIGHT LYMPH BREAST CANCER Erreur généralisation données bruités Comparaison vitesse convergence cette partie intéresser nombre itérations partir duquel algorithmes convergent erreur stabilise tableau montre approche hybride permet AdaBoost converger effet erreur AdaBoostM1 stabilise 1000ie itération alors AdaBoostHyb converge itération avant cette raison choisi première partie itérations effectuer comparaison termes erreur rappel résultats aussi valables données HEPATITIS cette données riche valeurs manquantes valeurs manquantes présentent toujours problème convergence algorithmes apprentissage résultats manifestent bases données différents types plusieurs attributs classe prédire ayant modalités tailles importantes laisse penser grâce façon calculer erreur apparente tenant compte hypothèses antérieures algorithme atteint rapidement stabilité Finalement remarquons BrownBoost converge pratiquement après itérations confirme problème vitesse convergence BrownBoost AdaBoostM1 BrownBoost AdaBoosthyb iterations Weather Credit Titanic Diabetes Hypothyroid Hepatitis Contact Lenses Straight Lymph Breast Cancer Comparaison vitesse convergence Bahri Conclusion article avons proposé amélioration AdaBoost fonde ploitation hypothèses construites itérations précédentes expérimentations résultats trouvés montrent cette approche améliore performances AdaBoost erreur rappel vitesse convergence Cependant avéré cette approche sensible aussi bruit avons effectué étude comparative notre approche Hybride BrownBoost approche connue amélioration AbaBoost bruit résultats trouvés montrent approche Hybride donne résultats meilleurs BrownBoost termes rappel vitesse convergence bases données montrent aussi BrownBoost donne erreur meilleurs certains bases données données bruitées arrive conclusion résultats encourageants étude théorique convergence envisagée justifier résultats expérimentaux autre perspective semble importante consiste améliorer cette approche contre données bruitées basant graphes voisinage paramètres efficaces Enfin dernière perspective travail consiste étudier boosting apprenant générant plusieurs règles chaque itération effet problème apprenant production chaque itération règles peuvent conflictuelles Références Bauer Kohavi empirical comparison voting classification algorithms Bagging boosting variants Machine Learning Breiman Bagging predictors Machine Learning Brodley Friedl Identifying eliminating mislabeled training tances Dietterich experimental comparison three methods constructing sembles decision trees bagging boosting randomization Machine Learning Dietterich Ensemble methodes machine learning First International Multiple ClassifierSystems Newman Hettich repository machine learning bases Domingo Watanabe Madaboost modification adaboost Conference Comput Learning Theory Morgan Kaufmann Francisco Friedman Hastie Tibshirani Additive logistic regression statistical boosting Statistics Stanford University Technical Report Nguyen iboost Boosting using instance based exponential weigh scheme hirteenth European Conference Machine Learning Littlestone Warmuth weighted majority algorithm Information computation Volume nouvelle approche boosting Maclin Boosting classifiers regionally McDonald Eckley empirical comparison three boosting algorithms artificial class noise Fourth International Workshop Multiple Classifier Systems Yaniv David Localized boosting Conference Comput Learning Theory Morgan Kaufmann Francisco Rätsch Ensemble learning methods classification Master thesis puter science University Potsdam Rätsch Onoda Müller margins adaboost Learn Schapire Singer Improved boosting algorithms using confedence rated predictions Machine Learning Sebban Suchier Étude amélioration boosting réduction erreur accélération convergence Journal électronique intelligence artificielle submitted Servedio Smooth boosting learning malicious noise Annual Conference Computational Learning Theory European Conference Computational Learning Theory EuroCOLT Amsterdam Netherlands Proceedings Volume Springer Berlin Shapire strength learnability Machine Learning Torre Globoost Boosting moindres généralisés Technical report GRAppA Université Charles Gaulle Lille Vladimir Vezhnevets Modest adaboost Teaching adaboost generalize better Moscow State University Wilson Martinez Reduction techniques instance based learning algorithms Machine Learning Summary reduction error generalization principal motivations research machine learning important induced classification great number carried methods aggregation classifiers order improve techniques performances single classifier Among these aggregation boosting which practical thanks adaptive update distribution examples aiming increasing exponential weight badly classified examples However method blamed following training speed convergence especially noise study propose approach modifications carried algorithm AdaBoost demonstrate exploiting assumptions generated former iterations correct weights examples possible improve performances boosting experimental study shows interest approach called hybrid approach