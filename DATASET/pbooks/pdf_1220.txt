Microsoft do_poulet_egc04 docFouille Grands Ensembles Données Boosting Proximal Thanh François Poulet ESIEA Recherche Docteurs Calmette Guérin Universitaire Laval Changé 53000 Laval dothanh poulet esiea ouest Résumé support vector machines montré efficacité plusieurs domaines application apprentissage ramène résoudre programme quadratique œuvre général coûteuse temps reformulation récente proximal proposée Mangasarian nécessite résolution système linéaire algorithme efficace permet traiter données nombre individus important nombre attributs restreint proposons utiliser formule Sherman Morrison Woodbury adapter fouille ensembles données nombre attributs important nombre individus restreint matériel standard présentons algorithme boosting classifier données grandes tailles nombre individus attributs évaluons performances nouvel algorithme ensembles données Twonorm Ringnorm Reuters 21578 Introduction fouille données domaine récent informatique développement masses données importantes stockées heure actuelle pouvoir extraire informations intéressantes bases données nombreux algorithmes venant différents domaines recherche utilisés extraction connaissances intelligence artificielle statistique analyse données bases données Parmi trouve arbres décision règles association intéressons particulièrement classe algorithmes apprentissage supervisé Séparateurs Vaste Marge fournissant outils performants classification régression détection nouveauté Bennett Campbell algorithmes utilisés plusieurs applications comme reconnaissance visages reconnaissance chiffres manuscrits classification textes bioinformatique Guyon général donnent précision apprentissage ramène résoudre programme quadratique œuvre algorithme coûteuse temps Récemment Mangasarian proposé algorithme proximal besoin résolution système linéaire offre particularités intéressantes Fouille grands ensembles données BoostPSVM algorithme incrémental traiter difficulté fichiers grandes tailles machines standard autres algorithmes nécessiteront capacités mémoire beaucoup importantes machines spécifiques genre serveurs rapide ordre plusieurs dizaines plusieurs centaines rapide standard suivant types fichiers précision équivalent autres algorithmes facilement parallélisable Poulet traités fichiers milliard individus moins minutes standard complexité algorithme varie linéairement nombre individus carré nombre attributs occupation mémoire quant proportionnelle carré nombre attributs certaines applications comme exemple classification textes gènes traitent ensembles données ayant grand nombre attributs ordre nombre restreint individus ordre cadre utilisation algorithme rapidement montrer limites remédier problème avons appliqué théorème Sherman Morrison Woodbury Golub algorithme linéaire obtient alors complexité fonction nombre attributs carré nombre individus occupation mémoire fonction carré nombre individus Cette nouvelle écriture algorithme permet traiter données grand nombre attributs nombre faible individus performances algorithme Poulet évaluées ensembles données médicales pouvoir traiter ensembles données grande taille nombre individus nombre attributs avons étendu algorithme utilisant algorithme boosting avons évalué performances nouvel algorithme ensembles données Blake Twonorm Ringnorm Breiman Reuters 21578 Lewis Musicant avons également comparé précision temps exécution boosting algorithme standard LibSVM Chang paragraphe présente principe algorithme linéaire Ensuite décrivons versions incrémentales paragraphe présentons construction boosting paragraphe résultats numériques présentés paragraphe avant conclure travaux utilisons quelques notations article vecteurs vecteurs colonnes norme vecteur notée transposée matrice représente individus attributs matrice diagonale représente classes individus matrice identité vecteur colonne constante positive variable ressort slack coefficients scalaire hyperplan Proximal support vector machine linéaire algorithmes objectif rechercher meilleur hyperplan permettant séparer données classes individus espace dimension représentés matrice leurs classes représentées matrice Thanh François Poulet diagonale séparation réalisée plans supports individus correspondant individus correspondant Séparation linéaire données classes rassembler formules formule algorithme standard maximisation marge minimisation erreurs solution programme quadratique variable ressort slack constante utilisée contrôler marge erreurs hyperplan obtenu résolution classification nouvel individu calculée peuvent utiliser autres fonctions noyau fonction polynômiale degré fonction Radial Basis Function fonction sigmoïdale lecteur intéressé détails consulter Cristianini Shawe Taylor explication complète algorithme proximal Mangasarian modifie formule algorithme standard maximisant marge minimisant erreurs marge Fouille grands ensembles données BoostPSVM remplaçant inégalité égalité substituant fonction objectif condition minimale dérivées premières nulles forment système linéaire inconnues coordonnées scalaire réécrire DeEEEI matrice juxtaposition colonne matrice identité algorithme besoin seule résolution système linéaire complexité varie linéairement nombre individus carré nombre attributs classifie 2x106 individus attributs classes secondes incrémental ensemble données grand nombre individus ordre nombre restreint attributs ordre Mangasarian proposé découper horizontalement lignes ensemble données attributs individus classes individus blocs principale calculer manière incrémentale Mangasarian parallélisée Poulet valeurs EiTEi EiTDie matrices EiTEi taille vecteurs EiTDie taille calcul incrémental terminé reste inverser matrice Théoriquement algorithme toujours valide œuvre cependant poser problèmes certains limites venir dimensions ensemble données traité nombre lignes individus nombre colonnes attributs place données telles nombre individus beaucoup important nombre attributs matrice conserve taille raisonnable seulement fonction nombre attributs explique partie bonnes performances algorithme exemple milliard individus attributs classifiés classes quart heure Pentium Cependant inverse nombre individus beaucoup faible nombre attributs comme exemple analyse expressions gènes traite ensembles données ayant grand nombre attributs ordre nombre restreint individus ordre performances algorithme deviennent alors catastrophiques matrice tenir mémoire machine obliger utiliser mémoire secondaire disque Thanh François Poulet ralentit manière significative vitesse exécution limite système incapable traiter telle matrice calcul inverse matrice aussi poser problème cause taille quantité données égales algorithme rapide pourra traiter problème données ayant grand nombre milliard individus machine standard inverse algorithme pourra stocker matrices permettant résolution problème pouvoir traiter données ayant nombre attributs beaucoup important nombre individus avons appliqué formule Sherman Morrison Woodbury partie droite équation permettant résolution problème obtenons DeTEETEEIDeTE DeTEETEEIDeTE DeTEETEEIDeTE DeTEETEEIDeTE DeTEETEEITEDeTE DeTEIETEIEITEII DeTEETEI formulation équation permet avoir traiter matrices matrices équation passe ainsi matrice taille carré nombre attributs matrice taille carré nombre Fouille grands ensembles données BoostPSVM individus ainsi matrice taille raisonnable manipuler assurant ainsi bonnes performances vitesse algorithme nécessiter machines spécifiques énormément mémoire alors augmenter pratiquement volonté nombre attributs utilisant technique incrémentale celle décrite algorithme incrémental ligne découpage données cette effectuer colonne ligne découpe verticalement colonne ensemble données ensemble blocs Ensuite algorithme calcule manière incrémentale valeurs EiEiT taille matrices EiEiT ordre carré nombre individus calcul incrémental terminé reste calculer inverse matrice Cette nouvelle formulation algorithme incrémental permet traiter données nombre attributs supérieur nombre individus machines standard temps restreint Boosting Malheureusement ensemble données simultanément grand nombre individus attributs traité aucune versions incrémentales exemple ensemble données individus attributs occupe mémoire chargent petit données chaque étape gardent mémoire matrice taille carré nombre individus carré nombre attributs ensuite inversent cette matrice dernière étape tâche apprentissage nécessite alors énormément mémoire coûteuse temps exécution remédier problème proposons extension algorithme construction boosting Cette solution apporte seulement bonnes performances temps exécution occupation mémoire aussi précision commençons présenter sommairement construction boosting comme boosting développé Freund collègues années méthode mettant œuvre ensemble classifieurs faibles learner améliorer performances classifieurs principale répéter apprentissage classifieur faible plusieurs reprises chaque étape boosting classifieur faible concentre individus classifiés précédente assigne poids chaque individu poids augmentés individus classifiés après chaque tâche apprentissage Initialement poids égaux classifieur faible construit modèle partir échantillon ensuite poids assignés individus augmenter poids classifiés modèle boucle combine modèles obtenus boosting simple implémenter donne résultats pratique lecteur intéressé pourra trouver études détaillées boosting Freund Schapire focalisons construction boosting considérer algorithme comme classifieur faible créons échantillon ensemble données construire modèle basant poids assignés individus algorithme ainsi traiter grands ensembles données données grand nombre individus algorithme incrémental ligne utilisé comme classifieur faible ensemble données grand nombre attributs Thanh François Poulet simultanément grand nombre individus attributs algorithme incrémental colonne utilisé comme classifieur faible nouvel algorithme bonnes performances temps exécution précision occupation mémoire ensembles données Twonorm Ringnorm Reuters 21578 avons comparé performances algorithme LibSVM Résultats ensemble programme écrit station Linux intéressons évaluer performances temps exécution bonne classification occupation mémoire nouvel algorithme boosting linéaire algorithme LibSVM noyau linéaire utilisé comparer notre algorithme tests réalisés Pentium Linux Redhat description ensembles données avons utilisés fournit tableau Classes Individus Attributs Protocole Twonorm Ringnorm Ionosphere Mushroom Adult 48842 32561 16281 Reuters 21578 10789 29406 Forest cover 581012 55000 20000 50000 Description ensembles données Temps Précision BoostPSVM LibSVM BoostSVM LibSVM Twonorm Ringnorm Ionosphere Mushroom Adult Performances terme temps exécution précision Fouille grands ensembles données BoostPSVM avons classifié premiers ensembles données utilisant notre algorithme LibSVM avons transformé attributs nominaux ensembles données Adult Mushroom attributs binaires résultats présentés tableau temps exécution tableau moyenne exécution ensemble apprentissage meilleurs résultats notés boosting meilleur terme précision ensembles données temps exécution boosting meilleur essais Remarquons ensembles données ayant grand nombre individus programme quadratique résolu LibSVM nécessite beaucoup temps calcul exemple Adult temps exécution LibSVM augmente manière significative boosting alors rapide LibSVM données Reuters 21578 utilisé évaluer algorithmes classification textes donnent résultats données intéressons comparer performances nouvel algorithme celles LibSVM données Reuters 21578 avons utilisé programme McCallum extraire termes partir documents Aucune sélection termes nécessaire traitons 29406 termes obtenus document représenté vecteur fréquences termes document vecteur considéré comme individu entrée algorithmes apprentissage Enfin avons classifié catégories ensemble données Reuters 21578 Précision Rappel Temps Boost Boost Boost Money Grain Crude Trade Interest Wheat Résultats catégories ensemble données Reuters 21578 ensemble données Reuters 21578 classification multicatégorie traiter avons utilisé approche contre reste avons classes classification consiste construire modèles chaque modèle sépare classe autres tableau présente résultats concernant précision rappel obtenus BoostPSVM LibSVM boosting toujours meilleur terme précision meilleur catégories terme rappel concerne temps exécution LibSVM rapide boosting expliquer boosting nécessité plusieurs étapes boosting atteindre précision rappel présentés Thanh François Poulet avons également mesuré performances algorithmes grands ensembles données résultats présentés tableau LibSVM charge ensemble données mémoire avons augmenté capacité mémoire LibSVM puisse traiter grands ensembles données avons utilisé classes grandes nombre individus classe Spruce 211840 nombre individus classe Lorgepole 283301 ensemble données Forest cover avons lancé LibSVM après jours avait toujours résultat Ensuite avons généré grand ensemble données mémoire individus attributs classes utilisant programme Musicant Encore LibSVM traiter ensemble données parce nécessite stocker ensemble données mémoire cette raison avons obtenir résultat LibSVM Mémoire Précision Temps minutes Boost Boost Boost Forest cover 31202 Performances algorithmes grands ensembles données Quant notre algorithme aucune augmentation mémoire nécessaire contre données chaque étape boosting temps lecture occupe temps exécution algorithme algorithme boosting démontré capacité pouvoir traiter ensembles données ayant simultanément grand nombre individus attributs ordre standard temps raisonnable Conclusion avons présenté article nouvel algorithme boosting linéaire permettant traiter grands ensembles données matériel standard Pentium partir algorithme initial avons utilisé théorème Sherman Morrison Woodbury adapter fouille ensembles données nombre attributs important nombre individus restreint inverse matériel standard Ensuite avons construit boosting permettant traiter données ayant simultanément grand nombre individus attributs ordre machines standard temps restreint nouvel algorithme bonnes performances temps exécution précision occupation mémoire résultats comparés obtenus LibSVM extension immédiate travail parallélisation algorithme gagner temps exécution basant réseau ordinateurs Ensuite essayerons boosting linéaire Enfin résultat sortie algorithmes boosting compréhensible étudierons également possibilités utiliser méthodes graphiques interactives permettant améliorer compréhensibilité résultat obtenu algorithme Fouille grands ensembles données BoostPSVM Remerciements remercions vivement Jason Rennie prétraitement ensemble données Reuters 21578 Références Bennett Campbell Bennett Campbell Support Vector Machines Hallelujah SIGKDD Explorations Blake Blake Repository Machine Learning Databases mlearn MLRepository Breiman Breiman variance arcing classifiers Technical Report Statistics Department University California Chang Chang LIBSVM Library Support Vector Machines cjlin libsvm Cristianini Shawe Taylor Cristianini Shawe Taylor Introduction Support Vector Machines Other Kernel based Learning Methods Cambridge University Press Poulet Poulet Incremental Visualization Tools medical Mining Proceedings Workshop Mining Mining Bioinformatics Cavtat Dubrovnik Freund Schapire Freund Schapire Short Introduction Boosting Journal Japanese Society Artificial Intelligence Mangasarian Mangasarian Proximal Support Vector Machine Classifiers Proceedings SIGKDD International Conference Knowledge Discovery Mining Francisco Mangasarian Mangasarian Incremental Support Vector Machine Classification Proceedings International Conference Mining Virginia Golub Golub Matrix Computations Hopkins University Press Balti Maryland edition Guyon Guyon Application clopinet isabelle Projects applist Lewis Lewis Reuters 21578 Classification Collection daviddlewis resources testcollections reuters21578 McCallum McCallum Toolkit Statistical Language Modeling Retrieval Classification Clustering mccallum Musicant Musicant Normally Distributed Clustered Datasets musicant Pavlov Pavlov Scaling Support Vector Machines Using Boosting Algorithm Proceedings International Conference Pattern Recognition Barcelona Poulet Poulet Mining Large Datasets Support Vector Machine Algorithms Proceedings International Conference Enterprise Information Systems Angers Thanh François Poulet Tresp Tresp Scaling Kernel based Systems Large Mining Knowledge Discovery Vapnik Vapnik Nature Statistical Learning Theory Springer Verlag Summary recent years support vector machines successfully applied large number applications Training usually needs quadratic programming learning large requires large memory capacity Proximal proposed Mangasarian formulation train because requires solution linear system Sherman Morrison Woodbury formula adapt process large number attributes extended applying boosting mining massive simultaneously large number points attributes evaluated performance Twonorm Ringnorm Reuters 21578