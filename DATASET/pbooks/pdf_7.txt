Apport modèles locaux moyennes prédictives Vincent Lemaire Oumaima Alaoui Ismaili Avenue Pierre Marzin 22300 Lannion vincent oumaima orange Résumé cadre clustering prédictif attribuer classe formés phase apprentissage majoritaire méthode communément utilisée Cependant cette approche comporte certaines limita tions influent directement qualité résultats obtenus termes prédiction surmonter problème proposons incorporer dèles prédictifs localement clusters formés améliorer qualité prédictive modèle global résultats expérimentaux montrent cette corporation permet obtenir résultats termes prédiction significati vement meilleurs rapport obtenus utilisant majoritaire ainsi résultats compétitifs obtenus algorithmes perfor mants apprentissage supervisé similaires effectué dégrader pouvoir descriptif explicatif modèle global Introduction Objectif clustering prédictif algorithme moyennes prédictives Harbi Rayward Smith Alaoui Ismaili version modifiée algorithme moyennes standard décrire prédire nière simultanée générer phase apprentissage nombre minimal clusters compacts instances doivent appartenir classe clusters suite décrire données prédire classe nouvelles instances figure méthode communément utilisée littérature permettant attribuer classe clusters formés gorithme moyennes prédictives majori taire cette approche parvienne obtenir résultats celle également certaines limites citera example bonne classification cluster contient instances classe instances classe alors utilisation majoritaire produire mauvaise classification important présence modèle local cluster devrait permettre mieux discriminer exemples selon classe appartenance visible clusters figure clustering prédictif modèles locaux courbe baser classe majoritai rement présente cluster produit courbe ayant allure courbe rouge figure classement exemples cluster courbe comporte segments courbe optimale figure Illustration courbes surmonter problèmes propose ticle entrainer modèle apprentissage localement chaque cluster formé phase apprentissage gorithme moyennes prédictives modèle global cette phase suite utilisé prédire classe nouvelles instances reste papier organisé comme présente différentes étapes algorithme moyennes prédictives utilisé cette étude ainsi méthode proposée surmonter problèmes cités dessus section présente brève description classifieurs seront comparés approche proposée protocole expérimental résultats obtenus présente partie analyse descriptive modèle global obtenu Finalement section conclue article présente pistes améliorations futur moyennes prédictives choix modèles locaux moyennes prédictives algorithme moyennes utilisé cette étude algorithme proposé laoui Ismaili algorithme fourni meilleurs résultats termes prédiction rapport algorithmes clustering prédictif répandus littérature gorithme algorithme Harbi Harbi Rayward Smith présenté succinctement dessous Algorithme correspond rithme moyennes classique supervision incorporée certaines étapes section donnera détails chacune étapes supervisées algorithme objectif étude menée cours présent article mesurer changer majoritaire étape incorporant modèles locaux clusters obtenus convergence algorithme moyennes prédictives Choix modèles locaux surmonter problèmes rencontrés majoritaire attribution classes clusters formés algorithme moyennes prédictives pensons ajout modèles locaux ajout classifieur localement chaque cluster solution Cependant nécessaire modèles locaux puissent entrainés éventuellement données soient robustes ratio performances train Lemaire idéalement puissent comporter aucun paramètre utilisateur avoir réaliser cross validation localement cluster aient complexité algorithmique linéaire apprentissage soient créés localement cluster information suffisante création modèle local cluster majoritaire serait conservé conservent voire améliorent qualités interprétation initiales modèle global individualisent prédictions cluster donné améliorer courbe bleue courbe rouge figure large étude réalisée Salperwyck Lemaire étudier tesse apprentissage classifieurs couramment utilisés Vitesse nombre exemples utilisés versus performances classification Cette étude montre capacité classifieur Bayes apprendre données confirmant Bouchard Triggs version standard version variables çoivent poids parle alors Averaging Naive Bayes Selective Naive Bayes Langley trouve travaux décrits Boullé 2007a critère analytique famille Bondu gorithme optimisation permettant apprendre Selective Naive Bayes mètre utilisateur régularisé Cette régularisation permet exhiber bonnes performances assurant bonne robustesse ratio performances entre prentissage proche prétraitements calcul poids variables basés proche Boullé 2007b trouve point intéressant approche découvre assez information données alors taire aucune variable informative majoritaire alors consi clustering prédictif modèles locaux comme meilleur autre choix notre applicatif intérêt retournera localement cluster seulement informatif majoritaire contraire majoritaire conservé autre intérêt proche régularisée besoin faire cross validation trouver paramètres modèle termes interprétation interprète aisément Lemaire exemple poids variables utilisé crire importance respective décidons famille entrainé localement chacun clusters formés étape algorithme chaque cluster clusterl classifieur entrainé notons modèle obtenu Expérimentation Classifieurs comparaison étudier impact utilisation modèles locaux qualité résultats issus algorithme moyennes prédictives allons comparer performances prédic tives celles obtenues méthode utilisant majoritaire celles obtenues trois autres algorithmes performants cadre classification supervisée premier modèle combine arbres décision régression logistique Logis Model Landwehr deuxième modèle combine arbres décision classifieurs naïfs Bayes Naives Bayes Kohavi modèles choisis leurs bonnes performances aussi proximité approche proposée consiste avoir modèles locaux entrainés partie données Enfin comme notre modèle hybride utilise localement clusters ajoutons global entrainé ensemble données avons décidé centrer analyse algorithme moyennes termes classification comparer méthode proposée algorithmes clustering concurrents lesquels parfaitement possible faire prédiction concentre comparaison article autres méthodes hybrides telles contenant modèles locaux comparaison autres méthodes clustering néanmoins partiellement réalisée Alaoui Ismaili algorithme moyennes prédictives majoritaire était positionné donne dessous brève description Naive Bayes arbre Naive Bayes algorithme induction arbres décision classifieurs naïfs Bayésiens feuilles Kohavi arbre induit nière descendante segmentations univariées basant formation élagage utilisé phase entrainement arbre décider partitionné terminal modèle Bayésien entrainé localement instances feuille classifieur bonnes formances comparaison arbres décision classifieur Bayes Lemaire Logistic Model arbre régression logistique Landwehr modèle classifi cation algorithme apprentissage supervisé combine régression logistique arbres décision objectif améliorer performances classification obtenues arbres décision cette associer chaque feuille label vecteur probabilité piecewise constant model modèle régression logistique entrainé feuilles estimer chaque exemple vecteur probabilités adapté piecewise linear regression model algorithme LogitBoost employé ajuster modèle régression logistique chaque ensuite partitionné utilisant information comme fonction impureté appel algorithme Logit Boost chaque utilise comme point initial modèle obtenu parent Finalement arbre élagué moyen algorithme élagage nombre rations LogitBoost chaque déterminé validation croisée éviter apprentissage Selective Naive Bayes classifieur Bayésien outil largement utilisé problèmes classifi cation supervisée avantage montrer efficace nombreux réels Cependant hypothèse naïve indépendance variables certains dégrader performances classifieur Aussi méthodes posant réaliser sélection variables Langley Elles consistent place heuristiques ajout suppression variables sélectionner meilleur ensemble variables maximisant critère formances classifieur selon approche wrapper Guyon Elisseeff montré Boullé Boullé 2007a moyenner grand nombre classifieurs Bayésiens naïfs sélectifs réalisés différents ensembles variables revenait considérer modèle pondération variables formule Bayes pothèse indépendance variables conditionnellement classes devient représente poids variable classe prédite celle maximise probabilité conditionnelle probabilités peuvent estimées intervalle discrétisation variables numériques variables catégorielles cette estimation faire directement variable prend modalités différentes après groupage contraire moyennes prédictives KMSNB algorithme présenté brièvement section correspond algorithme moyennes classique étapes modifiées incorporer supervision Prétraitement données Généralement tâche clustering nécessite étape prétraitement supervisé fournir clusters intéressants algorithme moyennes exemple Milligan Cooper Celebi Cette étape prétraitement empêcher certaines variables dominer calcul distances inspirant résultat Alaoui montré Ismaili utilisation clustering prédictif modèles locaux prétraitement supervisé aider algorithme moyennes standard atteindre bonne performance prédictive utilisons méthode proposée nommée Conditional avantage prétraitement supervisé introduire distance sienne donne garanties termes proximité instances cluster connaissant classe appartenance Alaoui Ismaili chapitre Initialisation centres utilisons méthode supervisée initialisation moyennes décrite Lemaire Cette méthode basée composition classes Vilalta chaque centre initial correspond centre gravité classe centre suivants initialisés méthode kmeans Arthur Vassilvitskii méthode étant déterministe algorithme convergence moyennes réalisé seule Prédiction classe nouvelle instance abord affectée cluster proche distingue classe prédite classe majoritaire présente cluster prédiction classe effectue selon règle décision modèle argmax1 PSNBl existe modèle local sinon majoritaire conservé KMSNB approches seront comparées article Nombre clusters article présente première expérimentation moyennes prédictives munies modèles locaux avons arbitrairement décidé fixer cadre évident intérêt travailler contexte nombre clusters nombre classes prédire limite sérieusement apport clustering prédictif lativement méthode concurrente classification appliquée mêmes données ploitation directe classifieur sûrement meilleure contexte étant pourrons néanmoins évaluer utilisation modèles locaux meilleure celle majoritaire résultats proches méthodes concurrentes alors travaux futurs réglant valeur seront surement intéressant mener Protocole expérimental données utilisées évaluer comparer différents algorithmes allons effectuer tests différents données Lichman données choisis avoir bases données diverses termes nombre classes variables continues catégorielles instances Tableau Elles choisies Adult liste comparaison article comparant Landwehr lecteur pourra noter petites bases Adult Entrainement modèles éléments reproductibilité codes utilisés apprentissage contenus logiciel Hornik utilise wrappers Lemaire Données Instances Classes Glass Vehicle Segmentation Waveform Mushroom Pendigits 10992 Adult 48842 données utilisés Variables numériques Variables catégorielles avons obtenu licence provisoire logiciel Khiops Boullé permis produire globaux Boullé 2007a KMSNB pouvoir réaliser éléments supervision prentissage algorithme moyennes décrits section avons aussi utilisé logiciel Khiops produire locaux clusters méthodes prétraitement contenues Ismaili algorithme Moyennes classique étant réalisé Matlab MATLAB Paramétrage classifieurs paramétrage défaut action données quantes control Weka_control passage options KMSNB paramètre utilisateur nombre clusters limitons cette étude nombre clusters égale nombre classes Section Evaluation performances manière pouvoir comparer résultats modèles KMSNB folds train utilisés performances prédictives présentées article dessous données folds cross validation stratifié résultats ainsi obtenus permettent calcul résultat moyen écart logiciel logiciel Khiops ayant mêmes façons calculer avons recodé calcul manière produire valeurs comparables donnons dessous résultats proposés valeurs aires courbes espérance désigne valeur classe contre toutes autres désigne prior classe fréquence éléments classe calcul réalisé vecteur probabilités uniquement classe prédite aucune façon biais faveur autre méthodes clustering prédictif modèles locaux Résultats tableau présente performances prédictives termes accuracy termes présenté obtenues algorithmes variante proposée article KMSNB Résultats accuracy Données KMSNB Glass Vehicle Segment Waveform Mushroom PenDigits Adult Moyenne Résultats Données KMSNB Glass Vehicle Segment Waveform Mushroom Pendigits Adult Moyenne Performances tests 10x10 folds cross validation résultats montrent algorithme moyennes prédictives suivi insertion modèles locaux KMSNB exhibe résultats significativement meilleurs algorithme utilisant majoritaire difficile comparer résultats moyens notons néanmoins bases données testées moyen accuracy comparaisons entre modèles ayant classifieurs Bayes modèles locaux montrent résultats KMSNB légèrement meilleurs notera KMSNB contient nombre modèles locaux inférieurs Landwehr détails taille arbres produits contre notons avantage bases testées Adult peuplée Enfin comparaison entre KMSNB modèle global indique méthode proposée donne résultats légèrement supérieures notamment bases données variables explicatives corrélées hypothèse indépendance affaiblie comme PenDigits retrouvons expériences résultats Landwehr hormis Glass sultats avons trouvés significativement meilleurs conservons néanmoins résultats trouvés favorisent Lemaire compléter comparaison donnons dessous tableau quelques ments supplémentaires comparaison Classifieur KMSNB Modèle hybride global hybride hybride global global hybride Gestion valeurs manquantes arbre Codage variables catégorielles codage disjonctif codage disjonctif groupage modèles locaux complet complet supervisé Cross validation requise modèles locaux Méthode sélection variables modèles locaux Elements comparaison après Landwehr Kohavi Boullé 2007a Alaoui Ismaili observons méthode proposée assez placée nécessite cross validation valeurs manquantes nativement opère sélection variables étape clustering construction modèles locaux Enfin réalise groupage supervisé valeurs variables catégorielles évitant ainsi passer codage disjonctif complet entraine création vecteur entrée souvent grand compliquant ensuite interprétation modèle obtenu autres comparaison existent comme complexité algorithmique robustesse nombre modèles locaux produits place manquant pouvons aborder détails mentionnerons juste modèle KMSNB compétitif notamment présence variables catégorielles ayant beaucoup modalités points Synthèse introduction modèles locaux obtenir KMSNB répond objectifs étions fixés termes performance facilité œuvre méthode estimons performance détruit caractère interprétable Moyennes prédictives principal article discuter pouvoir interprétable moyennes prédictives pouvoir prédictif essayons illustrer point cours section suivante limite considérations place Méthodologie analyse résultats avoir capacité notre algorithme moyennes prédictives résultats performants termes prédiction faciles interpréter donnée Vehicle utilisée comme exemple illustratif Cette données consti exemples variables descriptives variable prédire contenant classes cette étude illustrative utilisons ensemble données apprendre modèle nombre classes détaillons signification variables lecteur pourra trouver bases Lichman raisons place limitons utiliser variables informatives clustering initial change méthodolo analyse clustering prédictif modèles locaux Analyse niveaux interprétation KMSNB réalisée simplement méthodologie niveaux figure première analyse intéresse profil clusters présentant Figure histogrammes profil moyen population globale chaque bâton repré sente pourcentage individus possédant valeur intervalle considéré intervalles issus phase prétraitement ainsi profil moyen individus chaque cluster Cette visualisation permet diffé renciation comprendre pourquoi individus regroupés titre exemple aperçoit variable discriminante individus cluster lesquels entre Histogramme profils moyens individus global chaque clusters Histogramme poids variables dèles locaux Enfin deuxième niveau fournit histogramme cluster Figure donne poids variables locaux permettant connaitre pourquoi classification locale exemple aperçoit poids indiquant classifieur joritaire conservé cluster variables poids proches tandis cluster variable importante deuxième niveau analyse contient éléments interprétation disponibles avantages incorporer modèles locaux notre algorithme moyennes prédictives améliorer pouvoir descriptif interprétation résultats effet grâce modèles locaux capable seulement connaître variables discriminantes Lemaire modèle global également connaître celles contribuent construction chaque groupe phase apprentissage conséquent arrivée nouvelle instance capable connaître facilement différentes raisons déterminent diction classe Conclusion perspectives introduction modèles locaux obtenir KMSNB répond objec étions fixés termes performances facilité œuvre méthode conservant aspect interprétable modèle global hybride obtenu résultats expérimentaux placent honorablement aspects malgré fixer nombre clusters comme étant nombre classes futurs travaux étudierons possibilité trouver automatiquement nombre clusters exemple clustering hiérarchique descendant plaçant alors méthode manière champ clustering prédictif alors nouveau termes performances Enfin pensons développer outil visualisation résultats permettant naviguer clusters observer aisément profils moyens importances variables localement clusters Références Alaoui Ismaili Clustering prédictif Décrire Prédire simultanément thesis University Paris Saclay Paris Harbi Rayward Smith Adapting means supervised clustering Applied Intelligence Arthur Vassilvitskii means advantages careful seeding Proceedings Eighteenth Annual Symposium Discrete Algorithms Banerjee Mooney supervised clustering seeding Proceedings Nineteenth International Conference Machine Learning Bondu Boullé models Slides tutorial given Toulouse France boulle publications TutorialEGC13 Bouchard Triggs tradeoff between generative discriminative sifiers International Symposium Computational Statistics COMPSTAT Boullé 2007a Compression based averaging selective naive Bayes classifiers Journal Machine Learning Research Boullé 2007b Recherche sentation données efficace fouille grandes bases thesis Boullé Khiops outil apprentissage supervisé automatique fouille grandes bases données multi tables 16éme Journées Francophones Extraction Gestion Connaissances clustering prédictif modèles locaux Celebi Kingravi comparative study efficient initializa methods means clustering algorithm Expert Zeidat Supervised clustering algorithms benefits International Conference Tools Artificial Intelligence Guyon Elisseeff introduction variable feature selection Learn Frank Holmes Pfahringer Reutemann Witten mining software update SIGKDD Explor Newsl Idiot bayes stupid after International Statistical Review Hornik Ismaili Lemaire Cornuejols Supervised processings useful supervised clustering Springer International Publishing Kohavi Scaling accuracy naive bayes classifiers decision hybrid International Conference Mining Press Landwehr Frank Logistic model trees Learn Langley Induction selective bayesian classifiers Proceedings Tenth International Conference Uncertainty Artificial Intelligence Francisco Morgan Kaufmann Publishers Lemaire Alaoui Ismaili Cornuejols initialization scheme supervi means International Joint Conference Neural Networks Lemaire Bernier Correlation explorations classification model Workshop Mining Studies Practice Prize Lichman machine learning repository MATLAB version R2010a Natick Massachusetts MathWorks Milligan Cooper study standardization variables cluster analysis Journal Classification Salperwyck Lemaire Learning examples empirical study leading classifiers International Joint Conference Neural Networks Vilalta Achari Class decomposition clustering mework variance classifiers International conference Mining Summary majority commonly method predictive clustering context signing class resulting clusters training phase However method limits which could influence results quality obtained overcome these problems proposed incorporate local model inside every cluster improve predictive performance global model Experimental results incorporation local models allow better results those obtained using majority keeping descriptive aspect global model