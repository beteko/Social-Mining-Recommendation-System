untitledAccélération méthode proches voisins catégorisation textes Fatiha Barigou Baghdad Atmani Youcef Bouziane Naouel Barigou Département Informatique Université Naouer Senia Algérie Laboratoire informatique fatbarigou atmani baghdad gamil youcefbouzianemi barigounaouel gmail Résumé Parmi panoplie classificateurs utilisés catégorisation textes intéressons algorithme voisins proches performances situent parmi meilleures méthodes catégorisation textes Toutefois présente certaines limites mémoire stocker semble apprentissage entier élevé calcul explorer ensemble apprentissage classer nouveau document papier proposons nouvelle démarche réduire temps classification dégrader performances classification Introduction Catégorisation textes important recherche information fouille textes Cette tâche couronnée succès faisant grande variété applications succès principalement participation croissante munauté apprentissage machine travail intéressons algorithme proches voisins Cover dernier développé abord Hodges devenu algorithmes populaires catégorisation textes robuste placé parmi meilleurs algorithmes Sebastiani Toutefois présente certaines limites stockage mémoire énorme stocker ensemble complet apprentissage élevé calcul explorer ensemble apprentissage entier pouvoir classer nouveau document solution intéressante automate cellulaire appelée CAkNN Cellular Automaton combined proposée Barigou réduire temps classification cadre filtrage expériences réalisées corpus LingSpam montré méthode CAkNN permet atteindre meilleures performances classification comparée autres travaux publiés domaine filtrage papier allons reprendre cette solution catégorisation textes allons montrer travers ensemble expériences CAkNN permet réduire temps classification sélection minimum tances apprentissage classification nouveau document perfor mance prédictive affectée papier organisé comme section dédiée Accélération méthode travaux connexes section décrivons principe méthode décrit notre contribution améliorer cette méthode expériences résultats présentés section conclusion donnée section Travaux connexes Différentes solutions proposées réduire complexité calcul Comme souligne Bhatia distinguons méthodes sélection instances méthodes réduction temps calcul premières visent réduction nombre exemples apprentissage certaines techniques édition éliminant tains exemples redondant certain Gates deuxièmes méthodes accélèrent procédure recherche classification structures ganisées ensemble apprentissage Cependant dimensions importantes espace requis croit manière exponentielle algorithme proches voisins algorithme proches voisins méthode apprentissage instances comporte phase apprentissage telle documents faisant partie ensemble apprentissage seulement enregistrés Lorsqu nouveau document classer arrive comparé documents apprentissage similarité proches voisins alors considérés observe catégorie celle revient parmi voisins affectée document classer méthode paramètres nombre fonction similarité mesure similarité utilisée avons adoptée papier similarité cosinus équation consiste quantifier similarité entre documents calculant cosinus angle entre leurs vecteurs nouveau document ensemble appren tissage étiquetés ensemble classes représentés vocabulaire définissons comme étant fonction attribue classe nouvelle tance notre Cette fonction utilise majoritaire pondéré donné équation poids terme poids classe sinon Méthode proposée cette section reprenons méthode étudiée Barigou cette catégorisation textes proposons solution originale permettant Barigou surmonter inconvénients majeurs méthode classification tâche comme catégorisation textes manipulons milliers ments voire milliers milliers principe cette méthode comme faire participer toutes instances apprentissage recherche voisins augmenter temps calcul sélection ensemble réduit instances abord réalisée Cette opération sélection comme conséquence réduction significative temps classification approche proposée utilise machine cellulaire Atmani Beldjilali représenter instances apprentissage extraire cuments pertinents autre Représentation instances apprentissage proposons nouvelle stratégie représentation documents apprentissage derniers encodés structure cellulaire ensemble apprentissage abord traité construire index distinguons trois étapes établir liste initiale termes effectuant segmentation texte éliminer inutiles utilisant liste prédéfini vides enfin utiliser variante algorithme Porter effectuer racinisation différents retenus Puisque termes généralement extraits certains entre devraient sélec tionnés comme caractéristiques représentatives travail meilleurs termes sélectionnés informationnel Trois couches automates états finis définies représenter apprentissage couche CelTerm composée cellules représente vocabulaire états cellules composent trois parties étant entrée interne sortie couche CelDoc composée cellules représente ensemble documents prentissage états cellules composent trois parties étant entrée interne sortie couche CelRule composée règles index chaque terme appartenant associons règle indique document terme trouve états cellules composent trois parties étant entrée interne sortie termes leurs documents matrices voisinage CelRule prémisse Alors Sinon CELDOC CELRULE conclusion Alors Sinon Sélection instances processus sélection permet déterminer contribution chaque document prentissage documents ayant grand nombre termes communs instance Accélération méthode classer seront sélectionnés participer classification allons abord définir concepts suivants Nombre Total Termes vocabulaire trouvés seuil défini pertinent processus sélection instances passe trois étapes Initialisation couche CelTerm Recherche ensemble documents Recherche ensemble vérifiant condition donnée inéquation recherche instances réalisée application fonction booléenne globale δfact δrule récupérer documents partageant moins terme ensemble fonctions booléennes définies comme δfact δfact δrule δrule chaque document ensemble attribuer valeur Cette valeur correspond nombre total cellules actives obtenues produit booléen vecteur couche CelTerm vecteur seuil ensuite appliqué réduire davantage données apprentissage obtenir ensemble utilisé méthode AavecNTC Étude expérimentale Résultats évaluons solution proposée comparons méthode Corpus mesures performances utilisons papier corpus NewsGroups corpus traite catégories chaque catégorie représente corpus contient total 18828 documents avons utilisé corpus apprentissage corpus évaluer performances méthodes CAkNN calculons chaque catégo mesure Sebastiani donnée formule suivante avecπ précision rappel mesure globale toutes classes calculée moyenne résultats obtenus chaque catégorie temps nécessaire classifier nouvelle instance calculé comme méthode temps considère parcours ensemble apprentissage entier contre CAkNN temps classification calculé sommant temps sélection ensemble temps classification considérant seulement ensemble Barigou Résultats expérimentaux figures regroupent résultats obtenus corpus 20Newsgroups partir figures observons contribution méthode CAkNN consomme moins temps restant performante méthode Performance classifica corpus fonction seuil Temps classification corpus fonction seuil dégageons résultats intéressants premier concerne efficacité proche qualité prédiction CAkNN meilleure celle classifieur deuxième concerne réduction temps classification obtenue grâce réduction tique instances apprentissage résultats indiquent écart entre résultats avant après application méthode CAkNN suffisamment significatifs Conclusion papier avons proposé nouvelle solution améliorer temps sification méthode avons besoin ensemble apprentissage classifier nouvelle instance travail proposons sélectionner ensemble réduit documents cette tâche catégorisation problème sélection traduit problème manipulation opérations booléennes utilisation automate cellulaire machine automate premièrement censé filtrer instances produire bruit deuxièmement assurer convergence algorithme temps calcul intéressant Comme perspective prévoyons étude comparative entre méthode CAkNN autres solutions proposées Bhatia Gates Wilson Martinez Références Atmani Beldjilali Knowledge discovery database Induction graph cellular automaton Computing Informatics Journal Accélération méthode Barigou Beldjilali Atmani Improving based filter Mediter ranean Journal Computers Networks Bhatia Survey nearest neighbor techniques International Journal computer science information security Cover Nearest neighbor pattern classification Transactions Infor mation Theory Hodges Discriminatory analysis nonparametric discrimination Consis tency properties International Statistical Review Gates Reduced nearest neighbor Transactions Information condensed nearest neighbour Transactions Information Theory Moore algorithms efficient dimensional parametric classification Journal Machine Learning Research Sebastiani Machine learning automated categorization Computing Surveys Wilson Martinez Reduction techniques instance based learning algorithms Machine Learning Summary Among panoply classifiers categorization concerned nearest neighbors algorithm performances allow among categorization methods However limitations memory because store entire training classifying document computing because explore training classify document paper propose approach reduce classification without degrading performance classification