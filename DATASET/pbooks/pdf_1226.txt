dviintégration efficace méthodes fouille données cédric udréa fadila bentayeb jérôme darmont boussaid université lumière avenue pierre mendès france 69676 cedex france cudrea bentayeb jdarmont boussaid lyon2 résumé article présente nouvelle approche permettant appli algorithmes fouille particulier apprentissage supervisé grandes bases données temps traitement acceptables objectif atteint intégrant algorithmes ainsi sommes limités taille disque celle mémoire cependant entrées sorties nécessaires accéder engendrent temps traitement longs proposons article méthode originale réduire taille apprentissage construisant table contingence algorithmes apprentissage alors adaptés appliquer table contin gence valider notre approche avons implémenté méthode construction arbre décision montré utilisation table contingence permet obtenir temps traitements équiva lents logiciels classiques intégration bases données fouille données arbres décision relationnelles table contingence apprentissage super performance introduction application opérateurs fouille données grandes bases données enjeu intéressant cependant algorithmes fouille données peuvent opérer structures mémoire tableau attributs valeurs limite taille bases traiter méthodes classiques fouille données utilisent méthodes traitement données telles sélection variables motoda échantillonnage chauchat appliquer algorithmes fouille données grandes bases nouvelles voies recherche apparues dernières années elles consistent intégrer méthodes fouille systèmes gestion bases données chaudhuri premières avancées domaine intégra méthodes analyse données amorcée avènement entrepôts données analyse ligne particulier autres travaux recherche concerné intégration méthodes règles ciation généralisation sarawagi revanche existe travaux intégration méthodes classiques fouille intégration efficace méthodes fouille données analyse données telles regroupement classification effet domaine plupart travaux concernent plutôt application méthodes fouille grandes bases données agrawal gehrke néanmoins plupart éditeurs logiciels leurs méthodes fouille données microsoft oracle pendant cette intégration présente forme boîtes noires réalisées grâce extensions nécessitant utilisation interfaces programmations pourquoi avons proposé bentayeb darmont approche intégration méthodes fouille données arbre décision lisant outils offerts précisément avons implémenté méthode oracle forme procédure stockée utilisant relationnelles cette approche avons montré possible traiter grandes bases données limitation taille cependant temps traitement demeurent longs raison accès multiples améliorer temps traitements préparation données processus fouille devient cruciale proposons article méthode originale atteindre objectif construire table contingence table populations agrégées taille beaucoup réduite appren tissage départ avons ensuite adapté méthodes fouille données elles puissent appliquer table contingence obtenue actuel connaissances aucune méthode fouille données utilise telle méthode préparation données illustrer valider notre approche avons adapté implémentation appliquer table contingence obtenue partir apprentis départ montrons utilisation table contingence permet nettes améliorations termes temps traitement rapport notre première implémentation temps traitement équivalents logiciels classiques article organisé façon suivante section présente principe notre approche intégration méthodes fouille données arbres décision particulier intégration méthode oracle section expose principe notre approche utilisant phase préparation données basée construction table contingence section présente résultats expérimentaux étude complexité valident notre approche section conclut article expose perspectives recherche intégration méthodes construction arbres décision principe arbres décision outils apprentissage produisent règles alors zighed rakotomalala utilisent entrée ensemble objets uplets décrits variables attributs chaque objet appartient classe classes étant mutuellement exclusives construire arbre décision udrea nécessaire disposer population apprentissage table consti objets classe connue processus apprentissage consiste ensuite déterminer classe objet quelconque après valeur variables méthodes construction arbres décision segmentent population prentissage obtenir groupes desquels effectif classe cette segmentation ensuite réappliquée façon récursive partitions obtenues recherche meilleure partition segmentation noeud revient rechercher variable discriminante classes ainsi arbre généralement graphe constitué finalement règles décision obtenues suivant chemins partant racine arbre population entière jusqu feuilles populations desquelles classe représente majorité objets figure montre exemple arbre décision ainsi règles associées classe représente probabilité objet appartenir classe numéro classe classe classe classe classe classe classe classe classe classe règle alors classe règle alors classe règle alors classe noeud noeud noeud noeud noeud exemple arbre décision pouvoir appliquer méthodes fouille données bases volumi neuses nouvelles voies recherche consiste intégrer méthodes étant utiliser uniquement outils offerts derniers approche présentée bentayeb darmont racine arbre décision représentée relationnelle correspond population apprentissage entière comme chaque noeud arbre décision représente population noeud parent associons chaque noeud construite partir parente ensuite utilisées dénombrer effectifs chaque classe noeud comptages servent finalement déterminer critère partition nement noeuds partitions conclure noeud feuille figure présente titre exemple commandes permettant créer associées arbre décision figure entropie information algorithme induction decision quinlan pouvoir discri minant variable segmentation noeud exprimé variation entropie entropie noeud présisément entropie shannon intégration efficace méthodes fouille données noeud create select class training_set noeud create select class where noeud create select class where noeud create select class where noeud create select class where relationnelles associées arbre décision exemple effectif nombre objets appartiennent classe cardinalité classe information portée partition noeuds alors moyenne pondérée entropies effectif noeud segmenté finalement informationnel associé comme toujours positif processus construction arbre décision revient heuristique maximisation chaque itération sélection variable correspondante segmenter noeud donné rithme arrête lorsque devient inférieur seuil minimum défini utilisateur discussion avantage intégrer méthodes fouille données bénéficier puissance niveau accès données persistantes effet logiciels fouille classiques nécessitent charger données mémoire traiter limités niveau quantité données analysables illustré figure représente temps construction arbre décision taille augmente logiciels fouille classiques occurrence sipina zighed rakotomalala configuré utiliser méthode autre notre implémentation oracle baptisée buildtree tests effectués ordinateur disposant mémoire cette configuration sipina traiter bases taille dépasse résultat montre travaux permettent continuer traiter bases données grande taille logiciels travaillant mémoire peuvent udrea taille sipina buildtree temps traitement fonction taille opérer cependant résultats terme taille bases traiter promet teurs temps traitements demeurent longs temps calcul généralement considéré comme point critique processus fouille données néanmoins nécessaire réduire maximum ailleurs périence figure oeuvre données nombre uplets augmente schématisant complexité algorithmes construction arbres décision linéaire selon nombre objets uplets exponentielle selon nombre variables attributs primordial optimiser temps traitement algorithmes fouille fonctionnant obtenir temps réponse acceptables utilisation table contingence principe améliorer temps traitement méthodes fouille données grées proposons réduire taille apprentissage calculant table contingence algorithmes fouille données seront ainsi appliqués cette dernière algorithmes construction arbres décision procèdent manière récursive chaque noeud déterminer variable prédictive apportant meilleur information ainsi chaque noeud mêmes opérations noeud effectuées collection uplets restreinte chaque opérations consiste déterminer chaque valeur classe nombre total uplets chacune valeurs chaque attribut prédictif cette façon procéder entraîne plusieurs lectures chaque uplet puisque ensemble uplets noeuds correspond uplets chaque compte uplets ensemble collection associée conséquent lectures multiples induisent supplémentaire alors définitive nombre uplets similaires significatif uplets mêmes construction noeud consiste écrire population uplets intégration efficace méthodes fouille données correspondant diminuée attribut écrire nouveau uplets construction table contingence contexte méthodes fouille intégrées table contingence obtenue simple requête utilisant regroupement fonction agrégat count appliquée table départ attribut supplé mentaire appelé population naturellement obtenu représenter effectif obtenu chaque uplets table contingence nombre uplets table contingence inférieur nombre uplets départ terme temps traitement important manière générale apprentissage définie attributs prédictifs attribut prédire table contingence obtenue simple requête figure create contingence select count population group relationnelle associée table contingence exemple titanic illustrons notre approche utilisant apprentissage titanic comporte trois attributs prédictifs classe 2ième 3ième equipage adulte enfant homme femme ainsi classe prédire survivant population totale individus titre exemple bentayeb darmont application méthode intégrée oracle buildtree cette apprentissage consiste créer succession partitions départ individus formeront arbre décision construisant table contingence titanic méthode appliquer nombre individus beaucoup réduit effet table contingence obtenue contient uplets figure requête correspondant présentée figure create contingence select classe survivant count population titanic group classe survivant relationnelle associée table contingence exemple titanic calcul information entropie démontrer pertinence efficacité notre approche avons implé menté nouveau méthode tenant compte phase préparation udrea classe survivant population adulte homme adulte homme adulte femme adulte femme enfant homme enfant femme 2ième adulte homme 2ième adulte homme 2ième adulte femme 2ième adulte femme 2ième enfant homme 2ième enfant femme 3ième adulte homme 3ième adulte homme 3ième adulte femme 3ième adulte femme 3ième enfant homme 3ième enfant homme 3ième enfant femme 3ième enfant femme equipage adulte homme equipage adulte homme equipage adulte femme equipage adulte femme table contingence titanic données véritable différence situe calcul information chaque attribut prédictif conséquent calcul entropie implémentations algorithme standard contraintes calculer information attribut prédictif uplets partie départ correspondant noeud courant arbre induction déterminer répartition uplets fonction chaque valeur attribut prédictif chaque valeur classe notre approche connaître effectif population noeud obtenu partir ensemble critères enfant femme exemple suffit effectuer somme valeurs attribut population table contingence uplets satisfaisant cette technique beaucoup rapide applique nombre restreint uplets enfin modifiant manière calculer devient possible effectuer seule lecture connaître information attribut prédictif effet comme avons section calcul normal attribut intégration efficace méthodes fouille données ayant valeurs possibles classe ayant valeurs possibles entropie noeud effectif noeud ayant valeur attribut prédictif effectif population noeud effectif noeud ayant valeur attribut prédictif valeur classe développant obtient entropie noeud étant log2a log2b obtient après entropie noeud log2nik log2nk développant obtient entropie noeud log2nik log2nk log2nk log2nk après entropie noeud log2nik log2nk appliquant formule table contingence lisons seule unique obtenons facilement effet cette formule nécessaire connaître moment différents effectifs obtient somme somme implémentation implémentation méthode table contingence effectuée compatible oracle oracle forme procédure stockée nommée tc_id3 package procédures nommé decision_tree adaptation algorithme calcul entropie infor mation nécessaire puisse appliquer table contingence lyon2 download decision_tree udrea résultats complexité résultats expérimentaux valider notre approche comparer performances aussi méthodes fouille classiques méthode intégrée buildtree basée relationnelles avons effectué tests titanic schéma donné section comporte uplets trois attributs prédictifs attribut prédire avons comparé temps traitements méthode appliquée titanic préparation données buildtree utilisant relationnelles nouvelle version tc_id3 appliquée table contingence tests effectués environnement matériel ordinateur disposant mémoire version oracle oracle essai titanic résultats obtenus montrent efficacité notre approche rapport méthode buildtree puisque temps traitement divisé avons comparé temps traitements tc_id3 pouvons observer grâce application table contin gence associée titanic temps traitement notre approche laire celui méthodes classiques figure temps traitement fonction taille apportons preuve efficacité notre approche puisque façon nérale celle réduit considérablement taille apprentissage néanmoins extrêmes arriver taille table contingence proche celle départ devient infime restant toutefois pratique rares utilisation table contingence cadre intégration méthodes fouille améliore considérablement temps traitement lyon2 ricco sipina intégration efficace méthodes fouille données etude complexité étude complexité suivante permet appuyer résultats expérimentaux obtenus soient nombre uplets départ nombre attri prédictifs taille table contigence notre objectif comparer complexité entre algorithme relationnelles buildtree celui utilisant table contingence tc_id3 considérons algorithmes optimisés implémentation telle sorte seuls uplets nécessaires cette étude intéressons temps passé lecture écriture données opérations coûteuses considérons uplet écrit unité temps enfin considérons arbre décision obtenu équilibré complet chaque niveau arbre union populations différents noeuds niveau équivaut toute entière algorithme relationnelles niveau quelconque arbre aboutir niveau chaque noeud autant existe tributs prédictifs niveau comme somme populations noeuds niveau correspond population départ nécessaire uplets autrement nombre uplets nombre attri candidats temps total lecture niveau obtenir niveau fallu écrire uplets correspondants temps écriture rappelant obtenons alors complexité finale racine jusqu feuilles niveau égale lecture unités temps écriture unités temps notre approche convient abord créer table contingence temps écriture obtenir convient intégralement départ première temps lecture chaque niveau aboutir niveau intégralité uplets temps chaque niveau création table contingence résultats lecture unités temps écriture unités temps ainsi temps traitement notre approche apporte amélioration lecture écriture comme normalement supérieur cette amélioration importante augmente nombre attributs conclusion perspectives avantage majeur intégrer méthodes fouille données pouvoir traiter grandes bases données grâce puissance niveau accès données persistantes cependant processus fouille données udrea engendre nombreuses entrées sorties ralentissent considérablement temps traitement remédier problème avons proposé article méthode originale préparation données construire table contingence table populations agrégées biais requête réduit considérablement taille table apprentissage départ valider notre approche avons implémenté méthode construction arbre décision algorithme adapté table contingence forme procédure stockée nommée tc_id3 ainsi avons montré utilisation table contingence permet nettes améliorations termes temps traitement rapport méthodes fouille classiques méthode intégrée buildtree tc_id3 présente résultats similaires terme temps traitement comparée méthodes fouille classiques opérant mémoire enrichir notre package decision_tree sommes train implémen autres méthodes fouille données telles utilisant table contingence ailleurs envisageons comparer performances tc_id3 sipina grandes bases données réelles classiquement employées communauté apprentissage autres tests autres bases actuelle cours tests objectif déterminer empiriquement temps quand nombre attributs augmente quand nombre uplets mente prévoyons intégration package decision_tree autres procédures compléter outils fouille données offerts comme échantillon gestion valeurs manquantes validation références agrawal agrawal mannila srikant toivonen verkamo discovery association rules advances kowledge discovery mining pages bentayeb darmont bentayeb darmont decision modeling relational views xiiith international symposium methodologies intelli systems ismis france volume pages heidelberg germany springer verlag chauchat chauchat echantillonnage validation généralisation traction connaissances partir données mémoire habilitation diriger recherches université lumière france janvier chaudhuri chaudhuri mining database systems where intersection engineering bulletin providing analytical processing analysts mandate technical report associates gehrke gehrke ramakrishnan ganti rainforest mework decision construction large datasets international conference large bases pages morgan kaufmann intégration efficace méthodes fouille données intelligent miner scoring software iminer scoring motoda motoda feature selection knowledge discovery mining kluwer academic publishers psaila operator mining association rules international conference large bases mumbai india pages morgan kaufmann microsoft microsoft introduction mining microsoft oledb oracle oracle oracle mining white paper quinlan quinlan induction decision trees machine learning sarawagi sarawagi thomas agrawal integrating mining relational database systems alternatives implications sigmod ternational conference management sigmod seattle pages press performance study microsoft mining algorithms technical report microsoft zighed rakotomalala zighed rakotomalala sipina guide technical report laboratory university france zighed rakotomalala zighed rakotomalala graphes induc apprentissage mining summary propose paper approach applying mining algorithms particularly supervised machine learning algorithms large databases acceptable response times achieved integrating these algorithms within limited capacity available memory however necessary access database induce response times hence propose paper original method reduce learning building contingency table machine learning algorithms adapted operate contingency table order validate approach implemented decision construction method showed using contingency table helped obtaining response times equivalent those classical software keywords integration databases mining decision trees relational views contingency table supervised machine learning performance