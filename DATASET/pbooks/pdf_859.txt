Optimisation incrémentale réseaux neurones régression algorithme évolutionnaire Virginie LEFORT Guillaume BESLON Laboratoire Université Lumière avenue Pierre Mendès France 69676 Cedex France virginie lefort lyon2 Laboratoire LIRIS Bâtiment Blaise Pascal 69621 Villeurbanne Cedex FRANCE guillaume beslon liris Résumé réseaux neurones excellents régresseurs cependant difficiles utiliser raison nombre paramètres libres nombre neurones poids connexions algorithmes évolutionnaires permettent optimiser nombreux complexes proposons nouvel algorithme permet optimiser structure poids réseau grâce inspiration biologique compétitif autres techniques régression surtout évolution choisir dynamiquement nombre neurones précision différents paramètres Introduction réseaux neurones supervisés excellents régresseurs permettant classer données trouver relations entre entrées sorties régression Cependant souvent mettre œuvre nombre important mètres utilisation algorithmes évolutionnaires particulier algorithmes génétiques permet faciliter œuvre réseaux définissant structure poids connexions Grâce inspiration fortement biologique proposons nouvel gorithme permet optimisation incrémentale réseau structure connexions manière efficace Algorithmes évolutionnaires réseaux neurones réseau neurones chaque neurone réalise traitement simple nombre neurones connectivité faire toute puissance réseau réseaux neurones couche réseau constitué trois ensembles neurones neurones entrée neurones cachés complètement connectés neurones entrée réseau possède couche cachée couche précédente neurones sortie connectés dernière couche neurones cachés réseaux Poggio Girosi Radial Basis Function réseaux couche cachée neurones cachés utilisent fonction transfert gaussienne tandis neurones sortie réalisent simple somme pondérée réponses Optimisation réseaux régression algorithme évolutionnaire neurones cachés réseaux approximateurs universels Sandberg efficaces tâches classification régression utilisation passe choix grand nombre paramètres libres nombre neurones cachés leurs mètres moyenne écart poids combinaison linéaire paramètres interdépendants détermination difficile algorithmes évolutionnaires souvent employés paramétrer seaux neurones recouvrent différentes techniques inspirées évolution biologique algorithmes génétiques programmation génétique stratégies évolution évolution gramma ticale Cependant principe général toujours population solutions générée aléatoirement successivement subir phase sélection repro duction jusqu critère terminaison classer permettant optimisation réseaux neurones trois gories optimiseurs poids Blanco permettent optimiser poids réseau structure fixée préalablement optimiseurs structure MacLeod Maxwell Barrios permettent tester différentes structures réseau neurones second algorithme étant utilisé déterminer poids Enfin optimi seurs structure poids Arotaritei Negoita Thornton permettent optimiser simultanément structure réseau poids connexions derniers algorithmes représentation choisie souvent taille variable partie correspondant structure réseau nombre neurones cachés placement autre poids connexions entre neurones cachés sorties structure réseau forte influence nombre connexions poids Toute mutation partie structure entraîner modification taille partie connexions évolution alors faire temps abord évolution première partie décodage évolution seconde fonction informations première processus spécifiques doivent œuvre maintenir cohérence génomes particulier opérations croisement crossover modèle proposons nouvel algorithme permettant optimiser struc réseau connexions inspiré mécanismes biologiques traduction particulier existence niveau organisation intermédiaire entre notype phénotype protéome génétique permettant calcul niveau intermédiaire analogie entre biologie notre algorithme simple voulons obtenir phénotype solution constitué assemblage unités gènes protéines neurones cachés doivent nombre variable lesquelles ordre place correspondant génome importance structure réseau nombre neurones déduite protéome pouvons augmenter diminuer facilement nombre neurones cours évolution toujours obtenir réseaux valides autre avantage présence protéome permet diminuer contraintes structure génome gènes peuvent dépla dupliquer disparaître ordre gènes modifier cours temps rapprocher gènes taille séquences codantes entre gènes adapter dynamiquement opérateurs Lefort Beslon individus possèdent trois niveaux organisation génome composé ensemble gènes détectés localement protéome composé ensemble protéines virtuelles neurones cachés entièrement définis phénotype réseau construit partir protéome passage protéome phénotype trivial réseau construit simplement assemblage différents neurones cachés intéresserons surtout passage génome protéome différents opérateurs génétiques présentation détaillée algorithme trouvée Lefort génétique mapping génotype protéome Chacun gènes neurone caché séquence définir totalement neurone façon détermine totalement séquence protéine rendu possible utilisation génétique transformer contenu suite acides aminés celle étant ensuite décodée calculer paramètres neurone moyenne vecteur dimension celle entrées écart vecteur poids sortie avons choisi coder chacun paramètres binaire longueur pourra évoluer manière pouvoir avoir valeurs grossières début évolution gènes courts nécessaire précises évolution gènes longs Symbole Paramètre Valeur Start Moyenne Moyenne Moyenne Moyenne Hauteur Hauteur Ecart Ecart génétique Génome Moyenne Moyenne Hauteur Ecart 01011 10101 Exemple mapping génome protéome entrées sortie avons besoin bases différentes gènes délimités start Ensuite partir génétique reconstitue binaire chaque caractéristique neurone caché exemple bases permettent déduire première moyenne binaire 01011 Notre génome constitué suite lettres bases lettres particulières start permettant délimiter gènes autres étant utilisées paires coder différents paramètres nombre lettres nécessaires dépend blème outre start lettres chaque caractéristique neurone représenter binaire Ainsi problème entrée sortie avons besoin lettres Optimisation réseaux régression algorithme évolutionnaire reconstituer valeur paramètre suffira rechercher bases corres pondantes extraire différents paramètre pouvant mixés transformer suite suffit ensuite normaliser valeur binaire obtenir valeur réelle figure Opérateurs Notre algorithme conserve boucle générationnelle algorithmes évolutionnaires pendant structure génétique autorise élargir répertoire opérateurs mutation puisque peuvent modifier gènes structure génome avons opérateurs mutations Trois opérateurs locaux agissent seule principalement modifier valeurs paramètres switch remplace autre délétion ponctuelle supprime insertion rajoute Trois opérateurs globaux agissant segment génétiques contienne gènes permettent remaniements structure génétique translocation déplace autre endroit génome duplication insère copie génome délétion large supprime génome Enfin avons crossover point effectué après alignement gauche génomes permettre échange gènes Étude régression avons étudié notre algorithme benchmarks régression vérifier abord facilité paramétrage résultats convergence enfin déroulement évolution benchmark utilisé Boston Housing Automatic Knowledge Miner Server possède entrées sortie constitué points apprentissage validation algorithme testé autres bench marks Abalone résultats convergence pouvant trouvés Lefort paramétrage détaillé utilisons réglages Lefort notera notre algorithme possède paramètres soient classiques algorithmes évolutionnaires mutations larges taille initiale génomes comparaison notre algorithme autres algorithmes régression travail notre algorithme montré était compétitif principaux algorithmes régression utilisés Madigan Ridgeway détails compa raisons trouvent Lefort solutions terme évolution compétitives autres algorithmes gression Notre algorithme possède cependant nombreux degrés liberté nombre rones taille gènes taille génomes intéressant étudier allons intéresser indicateurs taille génome taille moyenne gènes évolutions indicateurs représentées figure remarque toute première phase taille génome augmente manière exponentielle Cependant malgré absence processus limitant taille celle stabi 15000 bases valeur stabilisation dépendant problème mutation Lefort Beslon Générations Générations Évolution taille génomes gauche taille gènes droite moyennes effectuées meilleures individus simulations Contrairement autres algorithmes programmation génétique avons explosion taille Cette augmentation taille facteurs premier augmentation nombre gènes génome permet résoudre efficacement problème Boston Housing deuxième correspond taille gènes reste stable toutes premières générations augmente fortement stabiliser bases Cette mentation correspond amélioration précision codage convergence chaque paramètre moyenne Notre algorithme façon dynamique nombre gènes comme taille nombre neurones cachés précision paramètres notre précision comme notre nombre neurones restant cohérents problème Conclusion inspirant biologie avons proposé algorithme évolutionnaire optimiser réseaux régression Notre génome homogène composé gènes indépendants autres pouvant déplacer dupliquer changer taille Chacun gènes représente neurone caché complètement défini mètres seront issus uniquement contenu transformé génétique artificiel binaire taille variable évolution artificielle pouvoir optimiser valeurs différents paramètres aussi précision chacun entre nombre gènes disposition génome résultats compétitifs autres algorithmes dédiés régression surtout notre algorithme permet choix dynamique taille génome celle gènes stabilisent toutes serait maintenant intéressant appliquer problèmes réels complexes tester limites Optimisation réseaux régression algorithme évolutionnaire Références Arotaritei Negoita Optimization recurrent variable length genotype Springer Verlag Automatic Knowledge Miner Server mining analysis request abalone Technical report University Waikato Hamilton Zealand Barrios Manrique Plaza algebraic model generating adapting neural networks means optimization methods Annals Mathematics Artificial Intelligence Blanco Delgado Pegalajar coded genetic algorithm training recurrent neural networks Neural Networks Thornton Design artificial neural networks using genetic algorithms review prospect Technical Report Cognitive Computing Sciences University Sussex Falmer Brighton Sussex Lefort Evolution second ordre algorithmes évolutionaires algorithme thesis Institut National Sciences Appliquées leurbanne France MacLeod Maxwell Incremental evolution Neural which Artificial Intelligence Review Madigan Ridgeway Discussion least angle regression efron Annals Statistics Sandberg Universal approximation using radial basis function networks Neural Computation Poggio Girosi theory networks approximation learning Techni Report Massachusetts Institute Technology Artificial Intelligence Laboratory Cambridge Summary neural networks powerfull regression often difficult because choices neurons number their center standard deviation weight connections Evolutionary algorithms optimise these networks complex propose algorithm optimise structure weights networks thanks biological analogy genome genes encoding complete hidden neuron variable order place whole constitute network algorithm competitive other regression techniques evolution choose dynamically neurons number precision parameters