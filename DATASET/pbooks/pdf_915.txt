Choix conclusions validation règles issues arbres classification Vincent Pisetta Gilbert Ritschard Djamel Zighed Université Lumière Laboratoire pisett lyon2 abdelkader zighed lyon2 lyon2 Université Genève Département économétrie Suisse gilbert ritschard unige Résumé article traite validation règles contexte blage déterminer profils différentes valeurs variable prédire concepts analyse statistique implicative fondée différence entre nombre observé contre exemples nombre moyen produirait hasard avèrent particulièrement adaptés contexte papier montre comment notions indice intensité implication appliquent règles produites arbres décision présente ternatives inspirées résidus utilisés modélisation tables contingence discutons ensuite données réelles usages indica teurs force implication règles issues arbres évaluation individuelle règles autre utilisation comme critère choix conclusion règle Introduction arbres décision utilisés depuis longtemps statistique Morgan Sonquist devenus suite ouvrages Breiman Quinlan outils populaires générer règles classification généralement règles prédictions parle ainsi arbre classification lorsque variable prédire catégo rielle valeurs représentent classes Cependant contrairement expression arbre classification laisser entendre classification térêt arbres décisions exemple sciences sociales comprendre comment prédicteurs peuvent affecter valeurs prises variable prédire classer individus peuvent avoir intérêt descriptif encore comme marketing notamment utiliser optique ciblage dernier plutôt prédire valeur réponse repérer profils typiques individus apparte chacune classes variable prédire inverse quelque sorte problème cherchant caractériser profils propres classe plutôt classe partir profil évaluation qualité arbre fonde souvent erreur sification classés règles calculé données prentissage données encore validation croisée évidemment pertinent comme Choix conclusions validation règles issues arbres mesure qualité règles quand objectif classification proprement cependant lorsque utilise arbre autres alors exploiter autres mesures mieux adaptées Ritschard Zighed Ritschard avons exemple proposés mesures déviance permettent juger qualité descrip arbre évaluant aptitude prédire distribution variable réponse profil donné intéressons ciblage Quelle information donne règle typicité profil prémisse règle conclusion proposons mesurer cette typicité concepts analyse statistique implicative analyse statistique implicative introduite Régis Larher comme outil analyse données connu dernières années essor remarquable cadre fouille règles association observe alors devrait aussi observer Suzuki Kodratoff principe fondamental consiste juger pertinence relation dépendance fonction fréquence contre exemples règle contre exemples considérée comme implicative règle laquelle contre exemples quents Curieusement ayons montré Ritschard appliquait difficulté règles issues arbres cette force implication guère exploi contexte apprentissage supervisé force implication règle évaluée écart entre nombre observé contre exemples nombre moyen générerait hasard correspond précisément notion typicalité profil conclusion intéresse article organisé comme section rappelons concepts indice intensité implication utilisation associée arbre classification discutons ensuite toujours section analogie entre indice implication résidus issus modélisation tables contingence intérêt résidus comme mesures alternatives force implication section illustrons exemple utilisations intensité implication évaluer posteriori règles issues arbre effectuer choix conclusion règle Enfin présentons remarques conclusives perspectives développement section Arbres indice implication arbres classification outils classification supervisés déterminent règles classification temps première étape partition espace prédicteurs déterminée telle distribution variable discrète prédire diffère possible classe autre partition chaque classe possible partition successivement selon valeurs prédicteurs commence partitionner données selon modalités attribut discriminant répète opération localement chaque ainsi obtenu jusqu réalisation critère arrêt second temps après arbre généré dérive règles classification choisissant valeur variable prédire pertinente général simplement fréquente chaque feuille terminal arbre Pratiquement relève chaque feuille nombre Ainsi récapituler distributions feuilles forme table contingence croisant états variable feuilles Tableau Pisetta noter marge droite tableau donne total lignes correspond distribution initial arbre feuille feuille feuille Total Total Table contingence croisant états réponse feuilles arbre Contre exemples indice implication indice implication exemple règle définit partir contre exemples notre chaque feuille colonne tableau nombre catégorie majoritaire vérifient effet prémisse règle conclusion notant conclusion ligne tableau règle maximum colonne nombre contre exemples indice implication forme standardisée écart entre nombre nombre espéré contre exemples seraient générés répartition entre valeurs réponse indépendante condition règle Formellement hypothèse répartition indépendante condition notons postule nombre contre exemples règle résulte tirage aléatoire indépendant groupe vérifiant prémisse règle autre vérifient conclusion règle conditionnellement nombre aléatoire contre exemples réputé Lerman suivre Poisson paramètre paramètre espérance mathématique variance nombre contre exemples correspond nombre feuille seraient contre exemples répartissait selon distribution marginale celle initial arbre marge droite tableau indice implication écart entre nombres contre exemples observés attendus hypothèse standardisé écart termes vérifiant condition indice écrit encore 1Notons ligne contenant maximum évidemment varier selon colonne Choix conclusions validation règles issues arbres cpred feuille feuille feuille contre exemple exemple Total Table nombres observés exemples contre exemples cpred feuille feuille feuille contre exemple exemple Total Table nombres attendus exemples contre exemples expliciter calcul indice considère variable classe prédite prend valeur chaque exemple appartenant classe majoritaire feuille apparte nance autres contre exemples cette variable cpred croisant cette variable conditions règles obtient tableau première ligne donne chaque règle nombre contre exemples seconde ligne nombre vérifiant règle tableau donne nombres espérés exemples contre exemples répartition couverts chaque règle selon tribution marginale important noter effectifs attendus déduisent marges tableau obtiennent répartissant abord selon distribution marginale tableau procédant ensuite regroupements selon classe majoritaire observée chaque colonne tableau Indice implication résidus formulation indice implication apparence résidu standardisé racine signée contribution Pearson exemple Agresti contribution mesurant distance entre bleaux effet suffit remarquer ainsi défini écrire reconnaît alors premier signe sommation expression carré indice implication Cette interprétation indice implication termes résidu ajustement nombre contre exemples modèle indépendance suggère autres formes résidus utilisés contexte modélisation tables contingence puissent également avérer intéressantes mesurer force implication règle particulier citer Pisetta classement règle Deviance ajusté Valeurs indices selon biens classés règle résidu déviance resdev signe racine signée contribution valeur absolue rapport vraisem blance Bishop résidu ajusté Haberman resadj résidu standardisé Pearson divisé erreur standard Agresti résidu Freeman Tukey resFT résulte transformation stabilisation variance Bishop résidu standardisé correspond indice implication connu avoir variance inférieure problème pratique nombres pendent échantillon considéré mêmes aléatoires Ainsi estimation paramètre Poisson alors tenir compte formule dénominateur estimation écart résidus déviance Freeman Tukey ajusté mieux adaptés cette situation réputés avoir pratique distribution proche normale simple résidu standar dernier conséquent indice implication estimer force implication figure montre valeurs résidus ordonnée fonction proportion abscisse feuille vérifient conclusion règle courbes représentées règle associée feuille couvrant proportion marginale vérifiant conclusion gauche seuil indices prennent valeur positive indiquant règle moins hasard relever comportement curieux résidu déviance valeur lorsque nombre contre exemples suggère règle devient implicative quand nombre contre exemples devient évidemment tisfaisant indice résidu ajusté évoluent manière linéaire proportion Choix conclusions validation règles issues arbres Poisson Normale Normale correction Distributions normale correction continuité Poisson résidu ajusté prennant valeurs étendue importante Quant résidu Freeman Tukey relève variation accélère lorsque biens classés règle approche Intensité implication valeur naturel intéresser valeur degré signification indices impli cation observés Cette valeur correspond probabilité Quand petit calcul faire conditionnellement Poisson mètre grand normale donne bonne approximation condition toutefois procéder correction continuité différence pouvant atteindre encore points pourcentage figure montre fonctions répartition Poisson normale correction continuité relever approximation normale particulier correction continuité reste bonne relativement petit Ainsi notant fonction distribution normale standardisée appelle intensité implication complémentaire cette valeur définissent termes approximation normale correction continuité notre calculerons règle comme IntImp cette intensité interprète comme probabilité obtenir hypothèse nombre contre exemples supérieur celui observé règle Pisetta Validation règles indice implication variantes proposées section précédente avèrent naturellement utiles juger pertinence individuelle règles classification Cette information vient enrichir critères usuels évaluation globale classifieur proposons illustrer usage données réelles considérons données collectées cadre étude STULONG Tomec effets pathologiques consommation alcool réalisée conjointement Université Charles Hôpital universitaire Charles Prague portent patients intéresse prédire habitudes consommation jamais occasionnellement régulièrement partir prédicteurs quantitatifs répertoriés tableau syst1 pression artérielle systolique mesure syst2 pression artérielle systolique mesure indice masse corporel chlst cholestérol cigarettes nombre cigarettes différents prédicteurs figure montre arbre obtenu méthode CHAID seuil significativité taille minimale nœuds induit règles classification Celles correspondent feuilles nœuds terminaux conclusions étant données classe Arbre induit données STULONG méthode CHAID Choix conclusions validation règles issues arbres syst2 syst1 buveur occasionnel syst2 syst1 buveur occasionnel syst2 syst1 chlst buveur occasionnel syst2 syst1 chlst buveur occasionnel syst2 chlst buveur occasionnel syst2 chlst syst1 buveur occasionnel syst2 chlst syst1 buveur régulier règles induites majoritaire leurs prémisses définies chemins mutuellement exclusifs mènent feuilles règles explicitées tableau Remarquons premier toutes règles concluent buveur occasionnel modalité majoritaire initial arbre typique situations déséquilibre répartition modalités éloignée situation équiprobabilité algorithmes apprentissage parfois discriminer différentes classes relève également aucune règle conclut jamais faible représentativité initial arbre induit difficulté trouver règles isolant individus tableau présente classique matrice confusion associée arbre défauts cités jusqu apparaissent façon flagrantes particulier erreur associée modalité jamais prédiction jamais occasionnellement régulièrement jamais occasionnellement régulièrement Matrice confusion règle majoritaire erreur valeurs résidus définis section présentées chacune règles tableau valeurs négatives indiquent nombre contre exemples observé inférieur celui attendu condition indépendance entre prémisse conclusion règle valeurs négatives synonymes qualité règle syst2 chlst syst1 buveur occasionnel laquelle résidus positifs règle moins indépendance nombre contre exemples observé supérieur nombre moyen générerait hasard règle considérée comme pertinente intéressant faire comparaison qualité implicative communément utilisé évaluation règles classification nombre contre exemples considérés précisément nombre erreurs produites règle échan tillon apprentissage erreur correspond ainsi pourcentage contre exemples parmi couverts règle règle encore complé mentaire confiance erreur souffre mêmes inconvénients confiance particulier règle apporte Pisetta Standardisé Deviance Freeman Tukey Ajusté Valeurs résidus chacune règles arbre sification indépendante toute condition notre règle exemple confiance contre classifieur consistant classer monde comme buveur occasionnel classe fréquente initial question évidemment savoir faire règle pertinente point implicatif décider conserver qualité globale classification contraire privilégier force implicative chaque règle solutions envisageables fusionner règle règles sœurs changer conclusion règle fusionnant règles revient élaguer branche pertinente arbre obtient nouvelle règle syst2 chlst buveur occasionnel valeurs résidus cette nouvelle règle resstd resdev resFT resadj positifs indiquent clairement détérioration rapport situation précédente observer arbre figure remontant branche partir feuille correspondant règle rencontre nœuds classe majoritaire occasionnellement fréquence inférieure celle relevée initial fusion solution particulier garde principe classe majoritaire choix conclusion amène discuter autre solution consistant changer conclusion règle choisissant modalité maximise intensité implication Choix conclusion règles objectif maximiser intensité implication règles parti culier déterminer profils caractéristiques chaque variable semble naturel choisir conclusion règle maximise cette intensité plutôt classe majoritaire choisir ainsi classe maximisant intensité implication minimisant résidu notamment exploitée Zighed Rakotomalala titre exemple donnons tableau conclusion sélectionnée cette procédure chacune règles selon résidu utilisé comme critère choix observe conclusions restent celles classe majoritaire règle principe maximisation implication donne conclusions différentes Choix conclusions validation règles issues arbres Déviance Freeman Tukey Ajusté Majorité jamais occasionnellement régulièrement Conclusion selon résidu utilisé comme critère quatre autres règles règle conclusion varie entre occasionnellement fréquemment selon critère implicatif retenu règles quatre indices implication conduisent résultat intéressant relever également critère implicatif chacune trois modalités variable prédire retenue comme conclusion moins règle souligner indices implication valeurs montrées restent négatifs expérience intéressante consiste recalculer matrice confusion nouvellement obtenue tableau montre cette dernière utilise résidu standardisé indice erreur global évidemment élevé tableau surprenant puisqu minimiser erreur classification tableau fournit cependant enseignements utiles plans Premièrement observer maximisation intensité implication améliore considérablement valeur mesures rappel intra classe modalités faiblement représentées également confirmation contrairement tableau puisse prédit moins règle Ensuite intérêt principal choix conclusion selon principe maximisation intensité matrice ressortir règles discriminantes rapport répartition initial arbre Ainsi arbre généré comme représentation typologie modalités variable prédire interprétation règles alors règles classification revue terme typicité condition conclusion choisie Ainsi personnes ayant pression artérielle systolique inférieure pression artérielle systolique inférieure indice masse corporelle inférieur caractéristiques jamais buveurs contraire prédiction jamais occasionnellement régulièrement jamais occasionnellement régulièrement Matrice confusion maximisation intensité implicative erreur Pisetta buveurs réguliers caractérisés pression artérielle systolique élevée cholestérol également élevé Enfin buveurs occasionnels clairement caractérisés existe circonstances typiques comme cholestérol élevé allié également élevé toutefois connaître problème niveau pression artérielle difficulté discriminer buveurs occasionnels autres également venir différents types buveurs définition occasionnellement étant subjective Conclusion avons montré papier intérêt statistique implicative arbres classification avons abord défini rappelé concept force implication réalisé parallèle résidus utilisés modélisation tables contingence lisation concepts objectifs intéressants statistique implicative fournit outils juger manière pertinente intérêt règles classification optique ciblage lorsqu cherche caractériser profils chaque valeur variable réponse autre offre critères utiles sélectionner conclusions règles contexte ciblage règles obtenues selon critères force implication règles classification permettent effectuer typologie différentes modalités variable réponse perspective intéressante contexte ciblage alors utiliser intensité plication construction arbre évaluer uniquement posteriori règles penser utiliser critères réalisant compromis entre classement tenir meilleur erreur possible validation statistique intensité implication utilisant mesures comme intensité implication entropique Etant nature statistique permet effectuer directement élagage arbre offre critère arrêt naturel questions réflexions méritent toutefois étude approfondie Références Agresti Categorical Analysis Wiley Bishop Fienberg Holland Discrete Multivariate Analysis Cambridge Press Breiman Friedman Olshen Stone Classification Regression Trees Chapman Contribution étude expérimentale analyse certaines acquisitions cognitives certains objectifs didactiques Thèse Université Rennes France Almouloud Bailleul Laher Ratsimba Rajohn hasina implication statistique Nouvelle méthode exploratoire données cherches didactique mathématiques Grenoble pensée sauvage Couturier Blanchard Briand Kuntz Peter Quelques critères mesure qualité règles association Revue nouvelles technologies information Choix conclusions validation règles issues arbres Kuntz Couturier Guillet version entropique intensité implication corpus volumineux Extraction Connaissances Apprentissage Larher implication statistique nouvelle méthode analyse données Mathématique Informatique Sciences Humaines exploratory technique investigating large quantities categorical Applied Statistics Lerman Rostam Elaboration indice implication données binaires Mathématiques sciences humaines Morgan Sonquist Problems analysis survey proposal Journal American Statistical Association Quinlan Programs Machine Learning Mateo Morgan Kaufmann Ritschard usage statistique implicative arbres classification Spagnolo David Actes Troisièmes Rencontres Internationale Analyse Statistique Implicative Volume Secondo supplemento Quaderni Ricerca Didattica Palermo Università degli Studi Palermo Ritschard Computing using deviance classification trees Rizzi Vichi COMPSTAT Proceedings Computational Statistics Berlin Springer Ritschard Zighed Qualité ajustement arbres induction Revue nouvelles technologies information Suzuki Kodratoff Discovery surprising exception rules based inten implication Zytkow Quafafou Principles Mining Knowledge Discovery Second European Symposium Nantes France Septem Proceedings Berlin Springer Tomec Rauch Berka longitudinal study atherosclerosis factors Berka Discovery Challenge Workshop Notes Helsinki Zighed Rakotomalala Graphes induction apprentissage mining Paris Hermes Science Publications Summary paper deals validation targeting framework where acterize typical profiles outcome classes Implicative statistic analysis which founded difference between observed number counter examples number expect hazard suited issue notions implication index intensities applied rules derived trees propose alternatives index based residuals modeling contingency tables using world discuss usages these measures plication strength first individual validation rules second concerns criterion selecting conclusion