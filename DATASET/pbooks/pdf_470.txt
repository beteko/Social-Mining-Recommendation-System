moyennes contraintes classifieur Application personnalisation scores campagnes Vincent Lemaire Nicolas Creff Fabrice Clérot Orange avenue Pierre Marzin 22300 Lannion Epita Voltaire 94276 Kremlin Bicêtre Cedex Résumé Lorsqu désire contacter client proposer produit calcule préalable probabilité achètera produit Cette probabilité calculée modèle prédictif ensemble clients marketing contacte ensuite ayant forte probabilité acheter produit parallèle avant contact commercial intéressant réaliser typologie clients seront contactés étant proposer campagnes différenciées groupe clients article montre comment possible contraindre typologie réalisée moyennes respecter proximité clients score appétence Introduction Problématique industrielle mining consiste ensemble méthodes techniques permettent extraire informations partir grande masse données utilisation permet établir relations entre données exemple définir comportements clients cadre gestion relation client Lorsqu désire contacter client proposer produit calcule préalable appétence produit calculer probabilité achètera produit Cette probabilité encore appelée score calculée modèle prédictif ensemble clients périmètre campagne calcul score exploite grand nombre variables explicatives issues système information clients ensuite triés ordre décroissant probabilité appétence service marketing contacte ensuite appétents nommés scores ayant forte probabilité acheter produit parallèle avant contact commercial intéressant réaliser typologie clients seront contactés étant proposer campagnes différenciées segment argumentaire commercial construit chaque groupe clients après analyse caractéristiques groupe offres actuellement détenues fréquent raisons pratiques temps analyse analyse groupe résume analyse centre représentant groupe Aujourd cette typologie souvent réalisée manière supervisée technique partitionnement moyennes valeur prédéfinie Personnalisation score campagnes métrique utilisée tient compte prédicteur modèle délivrant probabilité tence problèmes clients clusters probabilité appétence cluster contenir clients appétents clients appétents analyse centre groupe argumentaire commercial erroné segments créés stables temps lorsque classifieur déployé plusieurs consécutivement périmètre campagne tères section pourrait essayer alors poser problème comme problème supervisé essayant construire second modèle classification régression variable cible scores issus premier classifieur obtiendrait alors exemple arbre décision raisons impactent scores Cette intéressante correspond notre souhait découvrir structure scores grouper clients pureté scores effet mêmes scores peuvent avoir obtenus voies différentes voies raisons cherchons découvrir Aussi résoudre problèmes cités dessus article propose réaliser logie algorithme partitionnement algorithme contraint connais sance issue classifieur calcule scores appétence construire clustering conserve proximité clients ayant mêmes scores appétence section article décrit processus conduit choisir algorithme moyennes comme algorithme partitionnement choix algorithme section détaille comment possible utiliser métrique cours calcul partitionnement dépende classifieur utilisé calculer scores appétence section présentera résultats obtenus avant conclure cours dernière section Choix technique parmi différentes méthodes clustering basées partitionnement clustering processus partitionnement ensemble données semble significatif groupes appelés clusters regroupement trouver groupes éléments similaires mesure similarité donnée éléments principaux choisir méthode création groupes métrique utilisée création groupes notations seront utilisées suite article suivantes apprentissage comportant instances attributs variable prédire comportant modalités classes prédire notées chaque instance données vecteur valeurs continues catégorielles utilisé désigner nombre classes souhaitées Lemaire Introduction existe grandes techniques partitionnement peuvent utilisées regrouper éléments ensemble donnée autour centre gravité moyenne empirique moyennes MacQueen médiane géométrique médianes Bradley centre contenant modes fréquents modes Huang medoid élément ensemble minimise somme distances entre chacun autres éléments ensemble medoids Kaufman Rousseeuw choix algorithmes dépend nature données lesquelles devra appliqué résultat souhaité moyenne medoid temps disponible complexité algorithme chacun algorithmes dépend représentants initialement choisis valeur indicateur mesure évaluera qualité partition cohésion clusters obtenus distance mesure similarité utilisée représentation données présentées entrée algorithme différents points discutés dessous régulièrement relation contexte industriel étude Influence nature données initiales contexte industriel précis données proviennent système information Orange variables explicatives placées entrée classifieur servant calculer probabilités appétences numériques catégorielles grand nombre modalités existe valeurs manquantes lecteur pourra trouver description données Guyon reste telle représenta données choix technique partition devrait tourner technique prototypes Huang mixage moyennes modes données utilisées peuvent aussi contenir certains nombre clients atypiques données erronées amènerait choix medoid nature moins sensibles valeurs aberrantes Influence résultat souhaité résultat partitionnement permettre construire argumentaire commercial cluster argumentaire vente ensemble structuré arguments présente caractéristiques produit service comme autant avantages client suppose connaissance approfondie produit caractéristiques aussi besoins motiva tions client adapté client conséquent souhaiterait centre clusters formés client client moyen effet difficile savoir exemple moyenne offres commerciales desideratum pencher choix technique partition medoid Influence métrique certain nombre éléments prendre compte choix métrique forme clusters obtenus dépend norme utilisée autre chacun algorithmes décrits dessus moyennes minimiser norme particulier Personnalisation score campagnes moyennes norme médianes norme Jajuga rithmes clustering basés partitionnement fonctionnent importe fonction distance mesure similarité obtient mêmes garanties métrique utilise exemple théorème Huygens montre somme inertie intraclusters inertie interclusters constante valable utilise distance euclidienne notre désire adapter métrique celle naturellement cluse classifieur servant calculer probabilités appétences Cette adaptation décrite cours section dessous mentionne uniquement moment compréhension suite cette section utilisera norme pondérée Influence complexité algorithmique complexités algorithmiques différentes techniques partitionnement varient mément selon technique partitionnement aussi selon implémentation faite trouve peled Mazumdar différentes implémentations dianes Kaufman Rousseeuw différentes implémentations medoids Partitioning Around Medoids CLARA Clustering LARge Applications CLARANS Clustering Large Applications based RANdomized Search classe rithmes complexité faible élevée trouve moyennes modes medoids finalement médianes campagnes marketing concernées cette étude utilisent bases données portant centaines milliers clients chacun décrit potentiellement plusieurs zaines milliers variables explicatives Après construction classifieur réalise sélection variables retenant clients ayant fortes probabilités appétences obtient bases données quelques dizaines milliers clients décrits plusieurs centaines variables explicatives bases données utilisées construire partitionnement aperçoit certains algorithmes seront difficilement utilisables certaine volumétrie Influence prétraitement classifieur utilisé Orange cadre cette étude calculer probabi lités appétences KhiopsTM plateforme Féraud Khiops incorpore classifieur Bayes Langley construit après étape prétraitement variables Khiops discrétiser variables numériques faire modalités variables catégorielles processus prétraitement variables numériques catégorielles recodées chaque attribut recodé attribut qualitatif contenant valeurs recodage Chaque instance données alors recodée forme vecteur modalités discrètes représente valeur recodage attribut modalité discrète indice Ainsi variables départ alors toutes représentées forme numérique vecteur composantes prétraitement inutile choix algorithme comme celui modes puisque toutes variables après étape prétraitement numérique atténue également intérêt médianes medoid outliers après prétraitement valeurs aberrantes données Lemaire Discussion éléments présentés dessus montrent nombreuses contraintes influent choix algorithme partitionnement adapté problématique industrielle titre exemple complexité algorithmique nature prétraitements effectués rithme moyennes adapté notre problématique industrielle algorithme moins adapté usage norme souhait avoir vrais clients comme centres cluster algorithme médianes adapté norme utilisée nature données après prétraitement complexité algorithmique inutilisable données algorithme medoid algorithme vient naturellement ensuite comme choix complexité algorithmique faible celle médianes diffi cilement exploitable reste élevée plusieurs heures calcul petites bases données algorithmes CLARANS autres algorithmes modifient légèrement algorithme medoid rendre proche celui moyennes terme complexité nécessitent mettre mémoire matrice distances entre clients Décision prise alors utiliser algorithme médianes prenant approxima médiane comme prototype hypothèse indépendance variables ajoutant étape finale après convergence hypothèse indépendance variables permet utiliser version rapide calcul médiane appelée component median Kashima étape réalisée après convergence algorithme consiste placer chaque prototype client cluster proche prototype proximité entre client prototype cluster calculée distance norme Cette étape légèrement dégrader résultats partitionnement permet répondre ensemble souhaits énoncés section dessus moyennes basées connaissance classifieur Introduction montre cette section possible insérer métrique servira construction moyennes connaissance issue classifieur naïve Bayes moyenné logiciel Khiops construire nouvelle représentation supervisée permet construire métrique pondérée telle instances proches cette représentation supervisée scores proches section suivante décrit comment définit distance dépendante classe classifieur Bayes section présentera quant comment variables explicatives voient attribuer poids comment poids pondèrent distance Distance dépendante classe Reprenant notations introduites possible écrire distance Bayesienne dépendant classe prédire prédicteur Bayesien passe Personnalisation score campagnes chaque classe cible instance rappelle décision Bayesienne correspond classe cible maximisant formule précédente définit distance entre instances façon suivante alors coder chaque instance vecteur composantes comme illustré distance proposée correspond norme codage distance entre instances définie fonction recodages instances proches recodage supervisé seront proches comportement classe prédire effet définit distance entre distributions classes prédites façon suivante majoration suivante instances probabilité globale proches seront proches prédiction probabilités classe cible instances recodages proches espace supervisé auront probabilités proches avoir générées modèle recodage Cette majoration vraie aussi cadre régression linéaire Pondération distance étape construction poids variables utilisées classifieur Bayes totalement décrit Boullé comprend étapes clefs étape sélection variable décrite section article étape moyennage modèle décrite section article étape sélection variable permet classifieur éviter avoir variables explicatives inutiles liées problème classification étape moyennage modèle permet pondérer variable telle sorte équation devient Wmlog Lemaire poids variable quelle classe cible Chaque instance alors recodée vecteur composantes chaque composante pondérée poids distance équation pondérée poids variables majoration présentée équation restant vraie Discussion Algorithme modifié moyennes appellera représentation supervisée représentation issue passage données apprentissage initiale représentation chaque instance représentée vecteur composantes comme illustré équation classifieur Bayes Chaque variable pondérée poids résultat dessus équation donne garantie utilise algorithme moyennes représentation supervisée norme obtiendra clusters individus proches distance seront proches probabilité appartenance classe cible algorithme moyennes suite article appelé modifié utilise représentation supervisée données norme approximation médiane étape traitement désignation vrais clients comme centres modifications devraient permettre atteindre objectifs initiaux étude présentés introduction article Résultats expérimentaux Préambule Initialisation ensemble méthodes initialisation mentionnées Meila ckerman testées avons notre prétraitements supervisés norme mesuré différences significatives entre résultats obtenus résultats présentés article obtenus initialisation aléatoire proto types Validation croisée chacune phases expérimentales toutes valeurs bases données découpées réaliser validation croisée résultats moyens Under Curve indiqués score appartenance classe cible exemple défini comme proportion éléments classe cible cluster exemple nombre classes cibles supérieur donne espérance Première phase expérimentale première phase expérimentale menée manière mesurer impact représentation supervisée moyennes mesurer écart entre résultats entre algorithme medoid travaille directement vrais clients étape désignation incluse algorithme modifié moyennes logiciel Khiops testé données natives données mises représentation supervisée ensemble valeurs testées Personnalisation score campagnes doublant ensuite valeur pouvoir comparer résultats obtenus volumétrie limitée petites bases données provenant Blake somme erreurs carrés utilisée évaluer résultats obtenus inappro priée travaille représentations différentes native supervisée critère alors choisi donne indication pureté clusters classe cible tableau compare entre résultats obtenus représentation supervisée rapport représentation native algorithme modifié moyennes bases Phonème bases Letter Shuttle ayant abouti temps acceptable différentes valeurs testées cross validation seuls résultats moyennes présentés tableau présente résultats résultats moyens calculés valeurs individuelles obtenues fonction cross validation présente quelques résultats représentatifs croissants taille tests effectués lecteur intéressé pourra trouver détails Creff observons tableau utilisation représentation supervisée dégrade résultats voire permet amélioration critère supervisé Kmeans Kmeans supervisés Phonème Shuttle 58000 Letter 20000 Phase Résultats moyens entre représentation native représentation supervisée figures illustrent résultats obtenus bases Abalone Titactoe utilisant uniquement représentation supervisée figures courbe Rouge correspond courbe correspond algorithme modifié moyennes métrique issue naïve Bayes calculé Khiops enfin courbe Noire correspond classifieur Bayes calculé Khiops résultats illustratifs ainsi présentés Creff montrent rithme modifié moyennes utilisant représentation issue naïve Bayes Khiops compétitif observe aussi valeurs élevées algorithme modifié moyennes utilisé classifieur atteindre performances supérieures naïve Bayes Deuxième phase Plusieurs bases données mises notre disposition cette phase Trois bases clients datant problème churn produits Orange utilisées bases constituées environ variables données utilisée construire classifieur Ensuite scores utilisés réaliser partition groupes algorithme Lemaire Abalone Titactoe modifié moyennes bases correspondront ensembles tests critères évaluation calculés chacun notre contexte industriel utilisateurs algorithme clustering partitionnement souhaitent pouvoir choisir mêmes valeur appliqueront ensuite certains groupes campagne téléphonique autres campagne mailing autres campagne courrier contexte préférable laisser choix utilisateur basera expertise connaissance priori données nombre argumentaires commerciaux maximal établi Après consultation entité concernée valeurs testées considérations place seuls résultats présentés dessous sachant conclusions énoncées restent valides consultables Creff moment tests effectués existait solution logicielle treprise réaliser campagne cette solution était quasiment utilisée groupes obtenus deviennent différents algorithme modifié moyennes proposé article évalué critère stabilité temps clusters trouvés critère comprend premier évolution pourcentage appartenance cluster observe pourcentage éléments ensemble données appartenant cluster recommence opération suivants autres ensembles données autre proportions éléments appartenant cluster devraient rester mêmes considère solution comme stable rapport critère deuxième évolution répartition classes valeurs cibles clusters chaque client associé classe classes utilisées réaliser clustering observe clusters répartition clients appartenant classe recommence opération suivants répartition clients reste autre alors pourra considérer méthode clustering comme stable cours temps résultats obtenus stabilité présentés figures cisses représente ordonnées centage figures pourcentages somment correspondent scores contre figures pourcentage somme puisqu représente Personnalisation score campagnes Pourcentage éléments solution actuelle Proportion éléments churn solution actuelle Pourcentage éléments moyennes supervisé Proportion éléments churn moyennes pervisé proportion éléments chaque cluster ayant étiquette churn observe figures atteint clusters trouvés représentation supervisée dépendent classifieur construit beaucoup stables temps Figures comparaison Figures ailleurs clients présents cluster scores churn proches Discussion clustering contraint proximité entre scores utilisation représentation supervisée issue discrétisation groupage super permet majoration établie équation Cette majoration donne garantie utilise algorithme moyennes norme obtiendra clusters individus proches représentation supervisée seront proches babilité appartenance classe cible Cependant cette majoration indique seulement instances éloignées espace supervisé uniquement garantie distance entre leurs scores petite distance entre scores instances éloignées représentation supervisée grande serait intéressant représentation supervisée contraindre algorithme moyennes regrouper instances soient éloignées valeur seuil Lemaire notée algorithme Xmeans Pelleg Moore pourrait utilisé contrainte recouper cluster distance maximale entre instances supérieure Cette contrainte garantirait avoir aucun cluster diamètre supérieur instance score supérieur score autre instance Cette garantie permettrait améliorer algorithme modifié moyennes proposé article pourra aussi noter représentation supervisée construite avant étape clustering pourrait utilisée autres méthodes clustering cartes Kohonen propriété conservation proximité données espace initial pourraient utilisées Conclusion article montré comment possible réaliser typologie nique partitionnement contrainte connaissance issue classi fieur montré possible construire représentation supervisée classifieur naïve Bayes régression linéaire régression logistique Cette représentation supervisée permet créer partitionnement conserve proximité exemples ayant mêmes probabilités appartenance classes cibles Cette technique utilisée succès cadre applicatif scoring clients résultats expéri mentaux montrent comportement termes mesure aussi critère imposé stabilité temps Références Blake Repository machine learning databases archive visité dernière Boullé Compression based averaging selective naive Bayes classifiers Journal Machine Learning Research Bradley Mangasarian Street Clustering concave minimization Advances Neural Information Processing Systems Press Creff Clustering représentation supervisée Master thesis Epita Voltaire 94276 Kremlin Bicêtre Cedex Féraud Boullé Clérot Fessant Lemaire orange customer lysis platform Proceedings Industrial Conference Mining Berlin Germany Springer Verlag Guyon Lemaire Boullé Vogel Analysis scoring large orange customer database Workshop Conference Proceedings available kddcup orange peled Mazumdar Coresets means median clustering their applications Sympos Theory Comput Chicago Illinois Personnalisation score campagnes Huang Clustering large mixed numeric categorical values Pacific Knowledge Discovery Mining Conference Singapore World Scientific Huang Extensions means algorithm clustering large categorical values Knowl Discov Jajuga clustering method based Computational Statistics Analysis Kashima Singh means clustering proportional using distance Pattern Recognition International Conference Kaufman Rousseeuw Finding Groups Introduction Cluster Analysis Wiley Langley Thompson analysis Bayesian classifiers Procee dings tenth National Conference Artificial Intelligence California Press MacQueen methods classification analysis multivariate observa tions Berkeley Symposium Mathematical Statistics Probability Volume Meila Heckerman experimental comparison several clustering initialization methods Machine Learning simple algorithm medoids clustering Expert Pelleg Moore means Extending means efficient estimation number clusters Proceedings Seventeenth International Conference Machine Learning Francisco California Morgan Publishers Summary marketing service contact customers propose product ability these customers product calculated beforehand probability calculated using predictive model marketing service contacts those having highest probability buying product strongest appetency parallel before commercial contact interesting realize typology customers contacted propose differentiated campaigns group customers article shows possible force typology realized using means algorithm respect nearness customers refers their appetency score