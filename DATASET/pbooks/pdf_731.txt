actes_non_num 351rotes Caractérisation automatique classes découvertes classification supervisée Nistor Grozavu Younès Bennani Mustapha Lebbah Université Paris avenue Baptiste Clément 93430 Villetaneuse Prénom paris13 Résumé article proposons nouvelle approche classifi cation pondération variables durant processus apprentissage supervisé Cette approche basée modèle cartes organisatrices apprentissage cartes topologiques combiné mécanisme mation pertinences différentes variables forme poids influence qualité classification proposons types pondérations adaptatives pondération observations pondération distances entre observations apprentissage simultané pondérations prototypes utilisés partition observations permet obtenir classification timisée données statistique ensuite utilisé pondérations élaguer variables pertinentes processus sélection variables permet enfin grâce localité pondérations exhiber ensemble variables propre chaque groupe cluster offrant ainsi caractérisation approche proposée validé plusieurs bases données résultats expérimentaux montré performances prometteuses Introduction classification automatique clustering étape importante processus extraction connaissances partir données découvrir structure intrinsèque semble objets formant regroupements clusters partagent caractéristiques similaires Fisher Cheeseman complexité cette tâche forte accrue dernières décennies lorsque masses données disponibles volume exploser effet nombre objets présents bases données forte augmenté également taille description augmentation dimension données conséquences négligeables traitements classiquement oeuvre outre augmentation naturelle temps traitements approches classiques avèrent parfois inadaptées présence bruit redondance taille données mesurée selon dimensions nombre variables nombre observations dimensions peuvent prendre valeurs élevées poser problème exploration analyse données Caractérisation automatique groupes classification supervisée fondamental mettre place outils traitement données permettant meilleure compréhension données effet nombre dimensions données important données dispersées espace représentation férences entre données similaires données moins similaires réduit Ainsi espace grande dimensions difficile algorithme classification détecter variations similarité définissent regroupements données appelle fléau dimension contourner cette difficulté utilise souvent techniques réduction dimensions faciliter processus réduction dimensions permet éliminer informations pertinentes redondantes selon critère utilisé Cette réduction permet rendre ensemble données représentatif phénomène étudié problème complexe permet optimiser volume informations traiter faciliter processus apprentissage effet principaux objectifs réduction dimensions faciliter visualisation compréhension données réduire espace stockage nécessaire réduire temps apprentissage utilisation identifier facteurs pertinents réduction nombre observations faire quantification travers classifi cation supervisée sélection exemples cadre cette étude procé derons classification supervisée permettant ainsi calculer prototypes rents moyennes locales représentant ensemble données algorithmes apprentissage artificiel requièrent typiquement traits riables attributs significatifs caractérisant phénomène étudié problématique classification supervisée pourrait encore bénéfique incorporer module réduction nombre variables système global comme objectif enlever toute information inconséquente redondante effet important qualité sification effet nombre caractéristiques utilisées directement erreur finale importance chaque caractéristique dépend taille apprentissage échantillon petite taille élimination caractéristique importante diminuer aussi noter caractéristiques individuellement pertinentes peuvent informatives utilise conjointement réduction nombre variables pouvons procéder plusieurs manières sélection consiste choisir ensemble caractéristiques initiales espace mesure transformation construire nouvelles caractéristiques espace transformé espace projection cette étude intéressons réduction dimension espace description cadre classification supervisée sélection travers pondération locale variables approches différentes technique pondération locale proche utilisation structure carte seront présentées papier réduction dimension espace description apprentissage supervisé contributions plutôt moins conséquentes Lange Guyon littérature trouvons généralement approches basées pondération comme travaux Huang Blansche Guérif Bennani 1Extraction Connaissance partir Données Grozavu Frigui Nasraoui Grozavu approches sélection caractéris tiques comme méthodes proposées Basak Bassiouny Questier trouvons aussi méthodes permettant réduction simultanée dimensions données exemples variables méthodes souvent appelées techniques classification classification croisée encore Subspace clustering Parsons approches séduisantes pratique elles permettent grâce classification simultanée observations riables caractériser groupes identifiés approches proposons article peuvent comme proches identiques dernières techniques classification croisée approches sociées algorithmes apprentissage supervisé simultané observations variables première approche technique complètement nouvelle pondérer variables Cette technique amont pondérant caracteristiques observations cours apprentissage déduire pondérations locales deuxième approche extension reformulation stochastique approche pondération locale proposée Grozavu pondération distances permet déduire pondérations locales associées chaque groupe clusters cadre notre étude formalismes pondérations proposés associés modèle cartes organisatrices pondérations locales estimées utilisées caractérisation groupes partition obtenue carte topologique effet contrairement pondé ration globale estime vecteur pondérations ensemble référents toute carte pondération locale associe vecteur pondérations chaque référent carte topologique valeurs simultanément estimées cours prentissage partitionnement observations conséquent pouvons utiliser pertinences apprentissage regrouper caractériser prototypes reste article organisé comme Section présente brièvement modèle cartes organisatrices suivi détail approches proposées pondé ration locale adaptative pondération distance pondération observations technique caracteriser automatiquement groupes utilisant pondérations présentée section section présentons résultats expérimentations conclusion perspectives données section Cartes organisatrices traditionnelles cartes organisatrices présentées Kohonen largement lisées classification visualisation bases données multidimensionnelles trouve grande variété algorithmes cartes topologiques dérivée premier modèle original proposé Kohonen Bishop Cottrell Lebbah modèles différents autres partagent présenter données grande dimension simple relation géométrique topologie réduite modèle consiste recherche classification supervisée apprentissage individu dimension 2local weight distance using Organizing 3local weight observation using Organizing 4Self Organizing Caractérisation automatique groupes classification supervisée modèle classique présente forme carte possédant ordre topologique cellules cellules réparties nœuds maillage prise compte carte taille notion proximité impose définir relation voisinage logique modéliser notion influence cellule cellule dépend proximité utilise fonction noyau influence mutuelle entre cellules définie fonction chaque cellule grille associée vecteur référent dimension suite ensemble référents associés carte phases principales algorithme apprentissage associé cartes organisatrices définies littérature consistent minimiser fonction suivante Kohonen fonction affectation apprentissage carte organisatrice détermine partition données groupes associés chaque référent prototype représentant carte Apprentissage supervisé pondération observa tions variables inconvénients cartes organisatrices elles traitent égalité toutes variables souhaitable nombreuses applications partitionne observations décrites grand nombre variables groupes fournis caractérisent souvent ensemble variables plutôt ensemble variables définies conséquent certaine variables peuvent occulter découverte struc spécifique groupe cluster pertinence chaque variable change groupe autre pondération variables extension procédure sélection variables variables associées vecteur poids peuvent considérés comme degrés pertinence démarche proposée réaliser simultanément regroupement caractérisa groupes conçue telle manière estimer meilleurs prototypes semble optimaux poids cours phase apprentissage Chaque prototype associé vecteur poids notons suite ensemble vecteurs poids présentons versions pondération locale variables cartes topologiques nouvelle approche pondération observations reformulation stochastique pondération distances indique nombre éléments ensemble Grozavu Pondération locale observations Cette technique pondération estime vecteur poids pondérer filtrer servations adaptant processus apprentissage architecture proposée similaire architecture supervisée associons chaque référent vecteur pondérations Ainsi proposons minimiser nouvelle fonction suivante minimization itérativement descente gradient trois étapes jusqu stabilisation Après étape initialisation ensemble prototypes ensemble pondérations associés chaque étape apprentissage appliquons étapes suivantes Minimiser rapport fixant Chaque observation ponde affectée referent proche distance euclidienne Minimiser rapport fixant vecteurs référents expression suivante Minimiser rapport fixant expression vecteur pondérátions Comme algorithme stochastique classique Kohonen prentissage temps apprentissage généralement réalisé phases mière phase grand apprentissage initial grand rayon voisinage utilisés Pendant deuxième phase décroient cours temps pondération locale distance partir version analytique Grozavu avons developé version stochastique pondération distance fonction décrite formule suivante coefficient discrimination Comme algorithme minimisation fonction trois étapes Caractérisation automatique groupes classification supervisée Minimiser rapport fixant expression affectation suivante Minimiser rapport fixant vecteurs prototypes utilisant expression suivante Minimiser rapport fixant teurs pondérations après expression manière version décroître rayon apprentis constituer phases phase organisation associée grandes valeurs parametres phase quantification associée petites valeurs Caracterisation automatique groupes procédure sélection variables comporte trois éléments essentiels mesure pertinence procédure recherche critère arrêt distinguons trois types méthodologie approches filtres mesure pertinence indépendante algorithme utilise ensuite données approches symbioses évaluent pertinence ensembles variables performances système construit approches intégrées lesquelles mesure pertinence directement incluse fonction optimisée système approches intégrées types globale mesure pertinence calculée globalement individus locale quelle chaque référent propre vecteur mesures pertinence approches partie catégories approches integrées mesure localement adaptative pertinence Plusieurs critères arrêt introduit littéra souvent inconvénient critères fixation seuil dépend données notre détecter variables pertinentes avons utilisé statistique proposé Cattell Cattell appelé Scree permettre sélection variables pertinentes manière automatique définir seuil arrêt priori Critère sélection Scree Acceleration utilisation initiale Scree Cattell était détermination visuelle nombre valeurs propres prendre compte analyse composantes principales représenter graphiquement valeurs propres trouver partir quelle valeur graphique semble présenter changement brutal Selon Cattell devons Grozavu trouver représente ligne changement brutal Scree nombre composantes garder correspond nombre valeurs propres précédant Scree Fréquement Scree apparait pente graphe change radicalement Ainsi trouver décélération maximale graphique analogie utilisation modèles pondérations consiste tecter exemple changement brutal graphique pertinences faudrait détecter forte décélération procédure sélection ainsi composée étapes suivantes Ordonner vecteur pondérations suivant ordre décroissant nouveau vecteur ordonné exposant pondération indique ordre Calculer premières différences Calculer deuxièmes différences accélération Chercher changement brutal scree fonction suivante processus permet sélectionner toutes variables trouvant avant changement brutal Validation approches proposées avons utilisé différents données disponibles Asuncion Newman taille complexité variable évaluer approches pondération locale adaptative sélection particulier partie validation allons donner détails vagues Breiman bases utilisées Vagues Breiman bruitées composée exemples divisés classes originale comportait variables variables additionnelles tribuées selon normale rajoutées forme bruit Chaque observation généré comme combinaison vagues cancers Wisconsin Diagnostic Breast Cancer données contient individus décrit variables individus atteint cancer bénigne autres cancers malignes variables décrivent caractéristiques noyaux cellules présentes image numériques données Isolet données généré comme sujets prononcent chaque lettre alphabet reprises Ainsi avons exemples formation chaque locuteur données constituées individus variables Toutes variables continues Madelon données posent problème classes proposé origine pendant compétition sélection variables organisée conférence Guyon exemples situés sommets hyper dimension variables redondantes dimensions bruitées ajoutés données original était séparé trois parties apprentissage validation avons utilisé observations ensemble apprentis validation lesquels classes étaient connues SpamBase données composé observations décrites variables chacune decrivant categorie attributs Caractérisation automatique groupes classification supervisée descriptifs mails frequences apparition certains caractères ainsi informations quantité caractères capitale expérimentations comparaison différents résultats mesurée critères externes utiliser indices lorsque segmentation souhaitée connue particulier données comparaison entre segmentation proposée segmentation souhaitée Ainsi avons utilisé pureté indice calcule pourcentage nombre couples observations ayant classe retrouvant ensemble après segmentation carte Saporta avons lancé apprentissage algorithmes sélection variables bases décrites dessus calculons suite valeurs indices qualité cartes Carte topologique taille neurones graphiques indiquent ensemble référents issus respectivement classique graphiques représentent pondérations locales estimées respectivement Déroulement approche exemple Vagues Breiman utilisons données montrer intégralité processus permettant ractérisation groupes partir apprentissage approches Grozavu passant détection sélection variables pertinentes apprentissage carte dimension toutes observations permet fournir chaque vecteur référent vecteur pondérations dimension figure montre visualisation ensemble référents issus apprentissage trois algorithmes classique pondéra tions locales preséntées figure issues apprentissage respectivement indiquent respectivement variables dices référents amplitude indique valeur estimée rappelons algorithme référents calculés représentent observations pondérées observant trois graphiques constatons bruit represente variables visible faibles amplitudes Cette analyse visuelle résultats claire version nouvelle avons proposée graphiques representant référents pondérations montrent variables bruit pertinantes Après analyse types pondérations figure constatons pondérations issues corespondent structure données pertinence variables pondérations phénomène pondérations observations réprésentent filtre données estima référents tient compte filtrage vérifier possible sélectionner variables manière automatique algorithmes avons appliqué phase sélection ensemble référents version après segmentation carte Cette phase consiste détecter variations brutales chacun vecteurs entrée phase segmentation avons utilisé classification rarchique Vesanto Alhoniemi utilisant version segmentation carte référents résultats séléction groupe vagues Breiman intervalle indique variables groupes réels Classification croisée Pureté pondérés permet obtenir premier groupes contre version segmentation carte utilisant référent fournie groupes clusters segmentation partir produit permet oboutir groupes significatif notre exemple vagues Breiman caractérisation groupes algorithme ScreeTest fourni Table chaque technique montrons variables sélectionnées associées chaque groupe observons techniques fournissent groupes caractérisés variables Caractérisation automatique groupes classification supervisée différentes recouvrent constatons groupes riables détectées carte inclues ensemble variables détectées carte observons aussi aucune variable bruit sélection méthode contraire technique détecte seule variable bruit réduit qualité segmentation puisque calcul partitions confirme meilleur segmentation méthode Table Cette augmentation faible significatif puisqu acune connaissance priori utilisée cette tâche comparant résultats approches classification croisée constatons selectionne mêmes variables Résultats autres bases données concerne autres bases données allons contenter cette indiquer résultats obtenus après phase sélection variables application approches permis obtenir variables comme variables pertinentes forte importance premier groupe 9ieme groupe acune variable selectionées groupes concerne Isolet avons constaté accord sélection variables perti nentes algorithmes associés sélection fournissent variables pertinentes indices appartiennent intervalle comparant indices qualité partionement Indice pureté constatons amélioration approche rapport Détéction variables pertinantes groupes madelon isolet spambase intervalle indique variables groupe Classification sélection Pureté sélection Pureté croisée Isolet Après analyse expérimentation pouvons déduire quelques caractéristiques ticulières quelques comparaisons entre approches itératives pondérations donne aucune importance variables pertinentes opposé classique avons leurs élevées variables pertinentes méthodes stochastiques rapides comparant particulièrement version analytique batch pondération proposé Anonyme Grozavu algorithmes Scree fournissent caractérisation groupes adapté grâce utilisation pondérations locales indices qualité segmentation Indice Pureté meilleurs approches avantage algorithme pondérations après apprentissage apporte information celles fournis adaptation pondérations observations distances avantage méthode obtenu grâce pondération amont observations Conclusion papier avons introduit approches caractériser groupes clusters utilisant cartes organisatrices première nouvelle technique caractéri sation pondérant observations deuxième reformulation stochas tique pondération classique distances avons utilisé statis tique original Scree permis détecter automatiquement variables pertinentes avons montré travers différents exemples intérêt estimation pertinences variables visualisation sélection variables avons aussi approches peuvent considérées comme pseudo classification croisée simultanée observations variables Enfin contrairement classification croisée méthode proposée article permet caractériser groupes manière automatique estimation nombre correct groupes relation stabilité segmentation validité groupes générés perspective allons mesurer cette stabilité algorithmes techniques échantillonnage Références Asuncion Newman machine learning repository Basak Unsupervised feature selection using neuro fuzzy approach Pattern Recogn Bassiouny Hussein Feature subset selection based categorization Bishop Svensén Williams generative topographic mapping Neural Comput Blansche Gancarski Korczak Maclaw modular approach clustering local attribute weighting Pattern Recognition Letters Cattell scree number factors Cheeseman Kelly Stutz Taylor Freeman Autoclass bayesian classification system Fifth International Workshop Machine learning Cottrell Ibbou Letrémy based algorithms qualitative variables Neural Caractérisation automatique groupes classification supervisée Fisher Iterative optimization simplification hierarchical clusterings Journal Artificial Intelligence Research Frigui Nasraoui Unsupervised learning prototypes attribute weights Pattern Recognition Grozavu Bennani Lebbah Pondération locale variables apprentis numérique supervisé Extraction Gestion Connaissances Guérif Bennani Dimensionality reduction trough unsupervised features selec International Conference Engineering Applications Neural Networks Guyon Nikravesh Zadeh Found Springer Huang Automated variable weighting means clustering Transactions Pattern Analysis Machine Intelligence Kohonen organizing Springer Berlin Lebbah Rogovschi Bennani Besom Bernoulli organizing International Joint Conferences Neural Networks IJCNN Orlando Florida hybrid method unsupervised feature selection based ranking comparative study unsupervised feature selection methods clustering Parsons Haque Subspace clustering dimensional review SIGKDD Explor Newsl Questier Coomans Walczak Heyden multivariate regression trees supervised unsupervised feature selection Lange Feature selection clustering problems Thrun Schöl Advances Neural Information Processing Systems Saporta Probabilités analyse données statistiques Editions Technip Vesanto Alhoniemi Clustering organizing Neural Networks Transactions Summary introduce approach which provide simultaneously Organizing local weight vector cluster proposed approach computationally simple learns different feature vector weights cluster Clustering feature weighting offers advantages First guide process cluster meaningful clusters Second characterize cluster using feature selec method Based Organizing approach present simultaneously clustering weighting algorithms called respectively These algorithms achieve however minimize different objective functions estimates feature vector weights weighting observations while mates feature vector weights weighting distance between observations prototypes illustrate performance proposed approach using different