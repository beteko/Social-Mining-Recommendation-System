incrémental parallèle distribué traitement grandes quantités données Thanh François Poulet College Information Technology Cantho University Trong street Cantho Vietnam dtnghi ESIEA Docteurs Calmette Guérin 53000 Laval France poulet esiea ouest Résumé présentons nouvel algorithme Support Vector Machine Séparateur Vaste Marge linéaire linéaire parallèle distribué permettant traitement grands ensembles données temps restreint matériel standard partir algorithme Newton proposé Mangasarian avons construit algorithme incrémental parallèle distribué permettant améliorer performances temps exécution mémoire exécutant groupe ordinateurs nouvel algorithme capacité classifier million individus dimensions classes quelques secondes ensemble Introduction heure actuelle données arrivent capacité traitement algorithmes fouille données permet traiter amélioration performances algorithmes fouille données indispensable traiter grands ensembles données intéressons classification supervisée particulièrement classe algorithmes Vapnik règle générale donnent précision apprentissage ramène résoudre programme quadratique coûteux temps mémoire remédier problème méthodes décomposition Platt Chang travaillent ensembles arbitraires données utilise alors heuristiques Poulet permettant choisir ensembles données autres travaux visent construire algorithmes incrémentaux Mangasarian principe charger petit données mémoire construire modèle partiel mettre chargeant consécutivement blocs données parallèles distribués Poulet utilisent réseau machines améliorer performances présentons nouvel algorithme linéaire linéaire traiter grands ensembles données temps restreint matériel standard partir algorithme Newton Mangasarian avons construit algorithme incrémental parallèle distribué permettant améliorer performances temps exécution mémoire exécutant groupe ordinateurs résultats incrémental parallèle distribué expérimentaux montrent nouvel algorithme capacité traiter difficulté grandes quantités données utilisons quelques notations article vecteurs vecteurs colonnes norme vecteur notée transposée matrice représente individus notés espace dimension matrice diagonale représente classes individus matrice identité vecteur colonne constante positive variable ressort slack coefficients scalaire hyperplan paragraphe présente principe algorithme Newton Ensuite décrivons version incrémentale algorithme Newton paragraphe construction algorithme parallèle distribué Newton résultats numériques présentés paragraphe avant conclure travaux Méthode Newton individus espace dimension représentés matrice leurs classes représentées matrice diagonale recherchent hyperplan permettant séparer individus classes considère individus linéairement séparables séparation réalisée plans support exprimée vecteur colonne distances erreurs notées variables ressort individu support alors recherche hyperplan optimal ramène simultanément maximiser marge minimiser erreurs formulation primale problème exprimée programme quadratique constante utilisée contrôler marge erreurs optimal obtenu résolution programme quadratique œuvre coûteuse temps mémoire algorithme generalized Mangasarian modifie algorithme maximisant marge minimisant erreurs obtient alors formule primale constante utilisée contrôler marge erreurs contraintes réécrites manière suivante remplaçant valeurs négatives substituant formule fonction objectif programme quadratique obtient problème optimisation contrainte notant réécrire problème Mangasarian proposé utiliser méthode itérative Newton résoudre efficacement problème optimisation principe méthode Newton Poulet minimiser successivement approximations second ordre fonction objectif basant développement Taylor second ordre voisinage minimise fonction quadratique fournit dérivée première égale itération construit approximation quadratique voisinage minimise obtenir défini minimiser fonction problème optimisation calcule abord dérivée première ensuite Hessien dérivée seconde Tdiag matrice diagonale élément diagonal gradient algorithme itératif Newton construit partir calculs dérivée première Hessien algorithme converge solution après relativement faible nombre itérations efficace problèmes nombre restreint dimensions ordre lorsque calcul Hessien facile pouvoir classifier données linéairement séparables algorithme Newton utilise matrice noyau matrice représentant données construire matrice noyau entrée Newton utilisant ensemble individus toutes données utilisées comme vecteurs support taille cette matrice varie carré nombre individus œuvre linéaire coûteuse temps mémoire remédier problème algorithme Reduced Mangasarian utilise échantillon aléatoire taille comme ensemble vecteurs support créer matrice noyau réduit taille problème donne également résultats précision Algorithme incrémental Newton algorithme Newton classifie efficacement ensembles données ayant grand nombre individus nombre restreint dimensions beaucoup rapide algorithmes standard version incrémentale ligne consiste calculer manière incrémentale dérivée première Hessien ensemble données grand nombre individus ordre nombre restreint dimensions ordre découpe horizontalement lignes ensemble données individus classes individus petits blocs dérivée première XEDeED Hessien EDXEDediagED algorithme incrémental ligne Newton linéaire traiter grands ensembles données difficulté Entre étapes incrémentales conservées incrémental parallèle distribué mémoire dérivée première vecteur colonne taille Hessien matrice taille place données telles nombre individus beaucoup important nombre dimensions matrice conserve taille raisonnable seulement fonction nombre dimensions explique bonnes performances algorithme Newton linéaire incrémental cadre utilisation Parallélisation distribution algorithme incrémental Newton quantité données stockées cesse croître heure actuelle dépasse parfois possibilités traitement pouvoir faire afflux solution paralléliser distribuer processus fouille avons parallélisé algorithme incrémental Newton caractéristiques incrémentale parallèle algorithme permettent optimiser mieux utilisation mémoire grâce aspect incrémental temps exécution grâce aspect exécution parallèle grand ensemble données découpé blocs lignes distribué plusieurs machines distantes chaque machine distante données traité seule encore décomposé blocs lignes aspect intéressant œuvre calcul ensemble machines disparates adapte données capacité mémoire disponible calcul modèle partiel effectué chaque machine distante résultat envoyé serveur effectue solution chaque itération algorithme résultat final absolument identique aurait obtenu utilisant algorithme séquentiel ensemble données chaque itération calcule parallèlement indépendamment machines distantes DjEjXi Tdiag DjEjXi résultats envoyés serveur solution renvoie résultat machines distantes calcul prochaine itération condition arrêt vérifiée avons choisi solution simple basée mécanisme pouvoir travailler directement entrepôts données hétérogènes distribués lancer portions calcul sites distants système exploitation architecture machines Résultats ensemble programme écrit Linux librairie Lapack intéressons évaluer performances temps exécution bonne classification fouille grands ensembles données effectués ensembles grandes tailles avons utilisé Ringnorm Delve générer ensembles données mille milliard individus dimension classes classe moyenne égale variance égale classe moyenne égale variance égale description ensembles données fournit tableau tests réalisés Pentium avons découpé ensemble données blocs égaux chaque machine Ensuite avons varier taille blocs données machine peuvent Poulet traitées morceaux pratique taille blocs grande alors saturation mémoire système exploitation passe temps swapper taille blocs forte influence vitesse exécution algorithmes caractéristiques matérielles machine utilisée classification linéaire données Ringnorm Poulet trouvé environ individus vecteurs support permettaient obtenir résultats avons obtenu résultats échantillon aléatoire environ individus utilisés comme vecteurs support représentant ensemble données matrice noyau construite partir ensemble données vecteurs support algorithme incrémental parallèle distribué capacité traiter linéairement linéairement grands ensembles données avons aussi varier nombre machines taille ensembles données temps exécution algorithme varie linéairement taille ensembles données nombre machines utilisées carré nombre dimensions tableau présente résultats classification données Pentium Linux algorithme incrémental parallèle distribué Newton classifier linéairement milliard individus dimensions secondes minutes linéairement 90100 secondes heures 10PCs résultats illustrent notre algorithme capacité traiter efficacement grandes quantités linéairement linéairement séparable temps restreint machines standard individus individus Linéaire linéaire apprentissage 90100 Description ensembles données résultats classification Conclusion perspectives avons présenté nouvel algorithme incrémental parallèle distribué Newton traiter linéairement linéairement grands ensembles données temps restreint matériel standard milliard individus dimensions classifiés linéairement classes minutes linéairement heures Pentium apprentissage incrémental permet traiter grandes quantités données difficulté mémoire machine standard traitement parallèle distribué utilise groupe machines standard améliorer performances temps exécution complexité algorithme varie linéairement nombre individus ensemble données nombre machines utilisées carré nombre dimensions tâche classification linéaire milliard individus voire classifiés difficulté linéaire algorithme prend matrice noyau entrée matrice données complexité algorithme qualité modèle dépendent ensemble vecteurs support entrée rechercher bonnes incrémental parallèle distribué heuristiques sélection vecteurs support obtenir bonnes performances terme complexité préserver aussi bonnes qualités modèle Références Bennett Campbell Support vector machines hallelujah SIGKDD Explorations Chang LIBSVM Library Support Vector Machines cjlin libsvm Delve evaluating learning valid experiments toronto delve Poulet Towards Dimensional Mining Boosting Visualization Tools ICEIS Entreprise Information Systems Porto Portugal Poulet Mining Large Datasets Visualization ICEIS Entreprise Information Systems Miami Mangasarian Incremental Support Vector Machine Classification Mining Arlington Virginia Mangasarian Reduced Support Vector Machines Mining Institute Computer Sciences Department Wisconsin Madison Mangasarian Generalized Support Vector Machines Mining Institute Technical Report Computer Sciences Department Wisconsin Mangasarian Finite Newton Method Classification Problems Mining Institute Technical Report Computer Sciences Department University Wisconsin Madison Mangasarian Musicant Lagrangian Support Vector Machines Journal Machine Learning Research Platt Training Support Vector Machines Using Sequential Minimal Optimization Advances Kernel Methods Support Vector Learning Schoelkopf Burges Smola Poulet Mining large datasets support vector machine algorithms Enterprise Information Systems Filipe Hammoudi Piattini Kluwer Academic Publishers Vapnik nature statistical learning theory Springer Verlag Summary incremental parallel distributed algorithm using linear linear kernels proposed paper classifying large datasets standard personal computers extend recent finite Newton classifier building incremental parallel distributed algorithm algorithm handle large datasets linear linear classification tasks example effectiveness given linear classification classes million points dimensional input space seconds personal computers