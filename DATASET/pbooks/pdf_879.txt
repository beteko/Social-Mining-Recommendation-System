algorithme classification topographique supervisée niveaux simultanés Guénaël Cabanes Younès Bennani Université Paris Clément 93430 Villetaneuse cabanes younes paris13 Résumé questions importantes plupart applica tions réelles classification déterminer nombre approprié clusters Déterminer nombre optimal groupes problème diffi puisqu moyen simple connaître nombre connais sance priori article proposons nouvel algorithme classi fication supervisée niveaux appelé Simultaneous level Clustering Organizing permet déterminer automati quement nombre optimal groupes pendant apprentissage carte organisatrice estimation nombre correct groupes relation stabilité segmentation validité groupes générés mesurer cette stabilité utilisons méthode échantillonnage principal avantage algorithme proposé comparé méthodes classiques classification limité détection groupes convexes capable détecter groupes formes arbitraires validation expérimentale algorithme ensemble problèmes fondamentaux classification montre supériorité méthodes standards sification niveaux comme Moyennes Hierarchical Agglomerative Clustering Introduction classification supervisée clustering outil performant détec automatique groupes pertinents clusters données lorsqu connaissances priori structure interne données membres cluster doivent êtres similaires entre contrairement membres groupes diffé rents homogénéité interne séparation externe classification supervisée indispensable compréhension phénomènes variés décrits bases données problème regroupement défini comme tâche partitionnement semble items ensemble ensembles mutuellement disjoints classification problème regroupement considéré comme compétitifs prentissage supervisé nombreuses approches proposées Dubes approches classiques méthodes hiérarchiques méthodes partitives Classification niveaux simultanés méthodes classification hiérarchiques agglomératives utilisent arbre hiérar chique dendrogramme construit partir ensembles classifier nœuds arbre issus parent forment groupe homogène alors méthodes partitives regroupent données structure hiérarchique méthode efficace utilisée classification carte organisatrice Organizing Kohonen algorithme neuro inspiré processus supervisé compétitif capable projeter données grandes dimensions espace dimensions algorithme apprentissage supervisé technique linéaire populaire réduction dimensions visualisa données cette approche détection regroupements général obtenue utilisant autres techniques classification telles Moyennes méthodes rarchiques première phase processus standard utilisée estimer référents moyennes locales deuxième phase partitions associées chaque référent utilisées former classification finale données utilisant méthode classification traditionnelle telle approche appelée méthode niveaux article intéressons particulièrement algorithmes classification niveaux questions importantes plupart applications réelles aussi connue comme problème sélection modèle déterminer nombre approprié groupes connaissances priori moyen simple déterminer nombre objectif travail fournir approche classification niveaux simul tanés utilisant appliquée grandes bases données méthode proposée regroupe automatiquement données nombre groupes terminé automatiquement pendant processus apprentissage aucune hypothèse priori nombre groupes exigée Cette approche évaluée problèmes fondamentaux classification montre excellents résultats comparés approches classiques reste article organisé comme Section présente algorithme sification niveaux simultanés Section décrit bases données utilisées validation ainsi protocole expérimental section présentons résultats validation interprétation conclusion perspectives données section Algorithme classification niveaux simultanés carte organisatrice espace grande dimension données peuvent fortement dispersées difficile algorithme classification recherche structures données réponse problème grand nombre approches basées réduction dimen sions développées testées différents domaines application principale approches projeter données espace faible dimension conser topologie nouvelles coordonnées données ainsi projetées peuvent alors efficacement utilisées algorithme classification appelle classifica niveaux nombreuses approches proposées résoudre problèmes Cabanes Bennani classification niveaux Bohez Hussin Ultsch Guérif Bennani Korkmaz approches basées apprentissage carte organisatrice particulièrement efficaces vitesse apprentissage performances réduction dimensions linéaire Hussin Ultsch Guérif Bennani méthodes niveaux soient intéressantes méthodes classiques particulier réduisant temps calcul permettant interprétation visuelle analyse Vesanto Alhoniemi classification obtenue partir référents optimale puisqu partie information perdue première étape cette séparation étapes adaptée classification dynamique données évoluent temps malgré besoins importants outils analyse données proposons nouvel algorithme classification supervisée Simultaneous Level apprend simultanément prototypes référents carte organisatrice segmentation Principe algorithme apprentissage compétitif supervisé partir réseau neurones artificiels Kohonen Lorsqu observation reconnue activa neurone réseau sélectionné compétition entre neurones effet renforcement neurone inhibition autres règle Winner Takes Chaque neurone spécialise cours apprentissage reconnaissance certain observations carte organisatrice composée ensemble rones connectés entre liens topologiques forment grille dimensionnelle Chaque neurone connecté entrées correspondant dimensions espace représentation selon pondérations forment vecteur prototype neurone neurones aussi connectés leurs voisins liens topologiques données utilisé organiser carte selon contraintes topologiques espace entrée Ainsi configuration entre espace entrée espace réseau construite observa tions proches espace entrée activent unités proches carte organisa spatiale optimale déterminée partir données quand dimension espace entrée inférieure trois aussi position vecteurs poids relations voisinage directes entre neurones peuvent représentées visuellement neurone gagnant vecteur prototype façon devenir sensible sentation future donnée permet différents neurones entrainés différents types données façon assurer conservation topologie carte voisins neurone gagnant peuvent aussi ajuster vecteur prototype vecteur senté degré moindre fonction leurs distances prototype gagnant Ainsi prototypes proches donnée correspondent neurones voisins carte général utilise fonction voisinage gaussienne symétrie radiale algorithme proposons associer chaque connexion voisinage valeur réelle indique pertinence neurones connectés Étant donné contrainte organisation carte meilleurs représentants chaque donnée reliés connexion topologique Cette connexion récompensée augmentation valeur alors toutes autres connexions issues meilleur représentant seront diminution leurs valeurs récompenses punitions autant Classification niveaux simultanés importantes apprentissage avancé structure carte représentative structure donnés Ainsi apprentissage ensemble prototypes interconnectés représen tatif groupe pertinent ensemble données cluster Algorithme apprentissage connexionniste souvent présenté comme minimisation notre correspond minimisation distance entre données prototypes carte pondérée fonction voisinage Kohonen faire utilisons algorithme gradient fonction minimiser définie nombre données nombre neurones carte neurone vecteur prototype proche donnée fonction symétrique positive noyau fonction voisinage importance relative neurone comparé neurone pondérée valeur définie ainsi fonction température contrôle étendue voisinage diminue temps exemple nombre maximum itérations autorisé apprentissage tance Manhattan définie entre neurones coordonnée coordonnée grille carte processus apprentissage proche Competitive Hebbian Martinetz différence essentielle change férences prototypes cours apprentissage seuls neurones voisins carte peuvent connectés conserve topologie dimensions carte permet réduction dimensions visualisation simple structure données ailleurs utilisation valeur récompense associée connexions donne information représentativité locale neurones connectés Martinetz montré graphe généré cette manière préserve topologie façon optimale particulier chaque graphe triangulation Delaunay correspondant vecteurs référence Cabanes Bennani algorithme procède essentiellement trois phases Phase initialisation Définir topologie carte Initialiser aléatoirement prototypesw chaque neurone Initialiser connexions entre chaque couple neurones Phase compétition Présenter donnée choisie aléatoirement Parmi neurones choisir meilleurs repré senter cette donnée Argmin Argmin Augmenter valeur connexion entre diminuer leurs autres connexions issues νN1N2 νN1N2 νN1Ni νN1Ni voisin nombre connexions punies constante exemple Phase adaptation Mettre prototypes chaque neurone selon règle adaptation vante gradient Répéter phases jusqu apprentissage chaque ensemble neurones connectés entre connexions valeurs positives representatif groupe homogène données rithme attribue numero chacun ensemble nombre groupe ainsi obtenu automatiquement Classification niveaux simultanés Validation Description bases données utilisées façon démontrer performances algorithme classification proposé bases données présentant différentes difficultés classification utilisées bases données Hepta Chainlink TwoDiamonds proviennent Fundamental Clustering Problem Suite Ultsch avons généré aussi quatre autres bases données intéressantes Rings Spirals HighDim Random Rings composée groupes dimensions linéairement séparables densité riance différentes anneau rayon points forte densité anneau rayon points faible densité anneau rayon point densité moyenne HighDim constitué groupes points chacun séparés espace dimensions Random tirage aléatoire points espace dimen sions Enfin Spirals constitué spirales parallèles points chacune anneaux points densité points spirales diminue rayon Bases Difficultés Hepta Densité différentes Chainlink linéairement séparable linéairement séparable différentes densités variances TwoDiamonds Groupes contact Rings linéairement séparable différentes densités variances Spirals linéairement séparable différentes densités variances HighDim Grandes dimensions Random structure Description bases données validation nombre données nombre dimensions nombre groupes Données Rings Spirals Cabanes Bennani Données Chainlink Données TwoDiamonds Hepta Protocole expérimental avons comparé performances terme qualité segmenta stabilité rapport performances méthodes classiques niveaux algorithmes comparaison choisis Moyennes SingleLinkage appliqués données prototypes carte après apprentissage indice Davies Davies Bouldin utilisé déterminer meilleur découpage arbres SingleLink nombre optimal centroïdes Moyennes détermine automatiquement nombre classes besoin utiliser indice article qualité segmentation évaluée partir indices externes indices Jaccard fréquemment utilisés Halkidi effet catégo indépendantes données connues demander classification obtenue correspond catégories Classification niveaux simultanés Jaccard nombre paires objets éléments catégorie cluster nombre paires objets éléments catégorie cluster alors nombre paires objets éléments cluster catégorie finir nombre paires objets éléments cluster catégorie avons aussi utilisé travaux indices internes Davies Bouldin Calinski Harabasz principale question estimer point segmen tation données correspond structure interne absence connaissances priori cette segmentation toujours évaluée critères internes comme homogénéité intra groupes séparation entre groupes valeurs indices normalisés entre chaque meilleure lisibilité indice suggéré Davies Bouldin différentes valeurs nombre cluster typiquement introduit comme évaluer concepts séparation entre groupes dénominateur homogénéité intra groupes numérateur racine carrée erreur standard variance intra groupes groupe centroïde indice Calinski Harabasz largement utilisé méthodes classification classiques défini comme trace trace nombre points données nombre groupes trace somme distances carrées inter groupes alors trace somme distances carrées intra groupes indice présente valeur élevée lorsque nombre groupes optimum concept stabilité aussi utilisé estimer validité segmentation stabilité différents algorithmes utilisons méthode échantillonnage chaque donnée chaque échantillon segmenté algorithme classification comparons indice Jaccard différences entre segmentations obtenues processus répété grand nombre moyenne indice considérée comme estimation fiable stabilité classification Cabanes Bennani Résultats résultats indices externes montrent données pable retrouver aucune erreur segmentation attendue autres algorithmes particulier lorsque groupes formes arbitraires lorsqu structure données figure segmentations obtenues excellentes qualités selon indices internes lorsque données regroupées compacts moins hypersphériques Hepta HighDim exemple contre indices adaptés groupes formes arbitraires Rings Spirals Chainlink figure explique mauvaises performances données noter aussi pouvons évaluer cette façon segmentation groupe comme segmentation données Random Indice externe Jaccard Indice interne Calinski Harabasz Valeur indices qualité segmentation chaque algorithme chaque données concerne stabilité figure montre résultats excellents données regroupées hypersphères quelle dimension Hepta HighDim aussi groupes formes arbitraires dimensions Rings Spirals lorsque données structurées Random remarque dernier segmentation obtenue méthodes classiques extrêmement instable Lorsque données linéairement séparables dimensions supérieures Chainlink algorithme limité contrainte topologique dimensions carte organisatrice stabilité segmentation maximale cependant noter reste stable quasi méthodes classiques contre présentant stabilité relativement élevée moins stable plupart méthodes classiques lorsque groupes sentent point contact Diamonds effet point contact favorise création augmentation algorithme valeur connexions entre groupes Classification niveaux simultanés Valeur indice stabilité segmentation chaque algorithme chaque données visualisation groupes obtenus confirme résultats effet algorithme puissant outil visualisation dimensions segmentation obtenue groupes aisément clairement identifiables ainsi zones données figure résultats obtenus algorithme proches réalité obtenus méthodes classiques Données Chainlink Données Spirals Données Rings Visualisation groupes obtenus gauche droite Moyennes Cabanes Bennani Conclusions perspectives article proposons méthode classification niveaux simultanés utilise comme technique réduction dimensions effectue parallèle classification optimisée performances cette méthode évaluées partir tests série problèmes fondamentaux classification comparées méthodes niveaux classiques appuyant Moyennes résultats expérimentaux démontrent algorithme proposé produit classification meilleure qualité approches classiques montrent aussi grand avantage algorithme limité groupes formes convexes capable identifier groupes formes arbitraires finir nombre groupes déterminé automatiquement notre approche pendant apprentissage aucun priori nombre requis Cependant cette méthode fonctionner clusters suffisamment séparés espace données effet groupes touchent définis diminution densité contact détecté futur prévoyons utiliser informations densité données améliorer performances algorithme prévoyons aussi incorporer plasticité algorithme rendre modèle incrémental évolutif Remerciements travail soutenu partie projet Sillage financé Agence Nationale Recherche Références Elisseeff Guyon stability based method discovering struc clustered Pacific Symposium Biocomputing Bohez level cluster analysis based fractal dimension iteratedfunc systems speech signal recognition Pacific Conference Circuits Systems Calinski Harabasz Dendrite method cluster analysis Communications Statistics Davies Bouldin cluster separation measure Transactions Pattern Recognition Machine Intelligence Guérif Bennani Selection clusters number features subset during levels clustering Proceeding International Conference Artificial intelligence Computing Palma Mallorca Spain Halkidi Batistakis Vazirgiannis Clustering Validation Techniques Journal Intelligent Information Systems Halkidi Batistakis Vazirgiannis Cluster Validity Methods SIGMOD Record Hussin Kamel efficient level SOMART document clustering through dimensionality reduction ICONIP Classification niveaux simultanés Dubes Algorithms clustering Upper Saddle River Prentice Kohonen Organization Associative Memory Berlin Springer Verlag Kohonen Organizing Berlin Springer Verlag Korkmaz level clustering method using linear linkage encoding Inter national Conference Parallel Problem Solving Nature Lecture Notes Computer Science Martinetz Competitive hebbian learning forms perfectly topology preserving Gielen Kappen Proceedings International Conference Artificial Neural Networks ICANN Amsterdam Heidelberg Springer Ultsch Clustering Proceedings Workshop Organizing Vesanto Alhoniemi Clustering organizing transactions neural networks Hierarchical grouping optimize objective function Journal American Statistical Association Summary crucial questions world cluster applications determining suitable number clusters Determining optimum number clusters posed problem which there simple knowing number without priori knowledge paper propose level clustering algorithm based organizing called which allows automatic determination number clusters during learning Estimating numbers clusters related cluster stability which involved validity generated clusters measure stability sampling method great advantage proposed algorithm compared common partitional clustering methods restricted convex clusters recognize arbitrarily shaped validity algorithm superior standard level clustering methods means Hierarchical Agglomerative Clustering demonstrated critical clustering problems