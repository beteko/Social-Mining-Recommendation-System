nouvelle approche sélection variables basée métrique estimation qualité Charles Lamirel Pascal Cuxac Kafil Hajlaoui SYNALP LORIA INRIA Nancy Grand Vandoeuvre Nancy France charles lamirel loria loria Inist Vandoeuvre Nancy France pascal cuxac inist inist Résumé maximisation étiquetage métrique biaisée estimation qualité classification supervisée clustering vorise clusters ayant valeur maximale mesure étiquetage article montrons adaptation cette métrique cadre classification supervisée permet réaliser sélection variables calculer chacune elles fonction contraste méthode périmentée différents types données textuelles contexte montrons cette technique améliore performances méthodes sification façon significative rapport techniques sélection variables notamment classification données textuelles déséquilibrées fortement multidimensionnelles bruitées Introduction Depuis années progrès informatique capacités stockage mettent manipulation volumes données avoir espaces description plusieurs milliers voire dizaines milliers variables pourrait penser algorithmes classification efficaces grand nombre riables situation aussi simple premier problème augmentation temps calcul outre nombre important variables redondant pertinent tâche classification perturbe considérablement tionnement classifieurs plupart algorithmes apprentissage exploitent probabilités distributions peuvent difficiles estimer présence grand nombre variables intégration processus sélection variables cadre classification données grande dimension devient enjeu central térature trois types approches sélection variables principalement proposés approches directement intégrées méthodes classification dites embedded thodes basées techniques optimisation dites wrapper finalement approches filtrage états exhaustifs réalisés nombreux auteurs comme Ladha nouvelle approche sélection variables contraste Ladha Deepa Bolón Canedo Guyon Elisseeff faisons après rapide horizon approches existantes approches embedded intègrent sélection variables processus appren tissage Breiman méthodes populaires cette catégorie méthodes basées méthodes fondées réseaux neurones titre exemple Recursive Feature Elimination Support Vector Machines Guyon processus intégré effectue sélection variables façon itérative utilisant classificateur supprimant variables éloignées frontière décision méthodes wrappers utilisent critère performance recherche ensemble prédicteurs pertinents Kohavi souvent erreur prédiction courbe titre exemple méthode WrapperSubsetEval commence ensemble variables poursuit jusqu ajout nouvelles variables améliore performances exploitant validation croisée estimer précision apprentissage ensemble donné variables Witten Frank comparaisons entre méthodes comme celle Forman Forman mettent clairement évidence tenir compte cacité principaux inconvénients catégories méthodes elles gourmandes temps calcul proscrit utilisation données forte multidimensionnelles contexte alternative possible alors exploiter méthodes filtrage approches filtrage méthodes sélection utilisées amont pendamment algorithme apprentissage Basées tests statistiques elles légères termes temps calcul autres approches méthode carré exploite statistique courant mesure écart distribu attendue supposant variables indépendantes étiquettes classe Ladha Deepa information également méthodes courantes évaluation variables filtre univarié fournit classification ordonnée toutes variables cette approche variables retenues celles obtiennent valeur sitive information Smith méthode Mutual Information Feature Selection variable ajoutée ensemble variables sélectionnées classe cible surpasse connexion moyenne prédicteurs sélectionnés méthode prend compte pertinence redondance Smith méthode Consistency based Filter évalue pertinence ensemble riables niveau cohérence classes lorsque échantillons apprentissage projetés ensemble méthode MODTREE procédé filtrage repose principe calcul corrélation paire fonctionne espace paires individus décrits dicateurs étiquetage attachés chaque variable origine coefficient corrélation paire représente corrélation linéaire entre éléments utilisé calcul coefficients corrélation partiels permet alors effectuer sélection variables Lallich Rakotomalala hypothèse méthode Relief inspiration principe proches voisins considérer variable pertinente discrimine objet classe Lamirel positive rapport voisin proche classe négative score variables cumulatif calculé grâce tirage aléatoire données échantillons ReliefF tension Relief ajoute capacité résoudre problèmes multi classes Cette variante aussi robuste capable traiter données incomplètes bruitées Konokenko ReliefF considérée comme méthodes sélection filtres efficaces Comme statistique approches filtrage connues avoir compor tement erratique variables faibles fréquences représente situation habituelle classification texte Ladha Deepa montrons lement article malgré diversité toutes approches filtrages existantes avérent inopérantes néfastes données déséquilibrées fortement tidimensionnelles bruitées degré similitude élevé entre classes proposons comme alternative nouvelle méthode sélection variables contraste basée métrique maximisation étiquetage récemment développée comparons perfor mances techniques classiques contexte validation brevets étendons ensuite portée notre étude données textuelles référence habituellement utilisées suite document structurée comme section présente notre nouvelle approche sélection variables section détaille données utilisées section compare résultats classification utilisation approche proposée différents corpus données section présente conclusions perspectives Maximisation étiquetage sélection variables maximisation étiquetage métrique biaisée estimation qualité classification supervisée exploite propriétés données associées chaque cluster examen préalable profils clusters Lamirel princi avantage indépendante méthodes classification opératoire Lorsqu utilisée après apprentissage exploitée établir indices globaux qualité clustering Lamirel étiquetage Lamirel Considérons ensemble clusters résultant méthode clustering appliquée ensemble données représentées ensemble variables métrique maximisation étiquetage favorise clusters valeur maximale mesure étiquetage mesure étiquetage variable associée cluster définie comme moyenne harmonique rappel étiquetage précision étiquetage mêmes définis comme représente poids variable donnée représente ensemble variables représentées données associées cluster nouvelle approche sélection variables contraste Tenant compte définition métrique maximisation étiquetage ploitation tâche sélection variables contexte apprentissage supervisé devient processus simple cette métrique générique appliquer données associées classe aussi celles associées cluster proces sélection défini comme processus paramétré classes lequel variable classe caractérisée utilisant capacité discrimi classe donnée index capacité représenter fidèlement données classe index ensemble variables caractéristiques classe donnée appartenant ensemble classes traduit représente ensemble lequel variable représentée Enfin ensemble toutes variables sélectionnées ensemble défini comme autres termes variables jugées pertinentes classe donnée variables représentations meilleures cette classe leurs représentations moyennes toutes classes meilleures représentation moyenne toutes variables termes mesure étiquetage cadre spécifique processus maximisation étiquetage étape amélioration contraste exploitée complément première étape sélection cette étape adapter description chaque donnée caractéristiques spécifiques leurs classes associées consiste modifier schéma pondération données chaque classe prenant considération information fourni mesure étiquetage variables localement cette classe information proportionnel rapport entre valeur mesure variable classe valeur moyenne mesure cette variable ensemble partition donnée variable décrivant cette donnée résultant comme facteur contraste modulant poids existant cette variable profil donnée établi auparavant variable appartenant ensemble variables sélectionnées classe exprimé comme facteur amplification optimisé fonction précision obtenue variables actives classe celles lesquelles information supérieur celles Etant donné méthode proposée méthode sélection contraste basée classes nombre moyen variables actives classe comparable nombre total variables sélectionnées méthodes sélection usuelles Lamirel Données expérimentales poursuivi projet QUAERO celui exploiter informations bliographiques aider experts juger antériorité brevets premier temps prouver possible associer informations manière tinente classes brevets autrement classifier correctement classes données source expérimentales principales contiennent brevets format domaine pharmacologique regroupés classes classe préparation dicale citations bibliographiques brevets extraites données Medline 25887 citations extraites partir brevets interrogation données Medline citations extraites permet récupérer notices bibliogra phiques articles Chaque notice ensuite marquée premier classement brevet citant Hajlaoui résumé chaque notice traité transformé Salton utilisant outil TreeTagger Schmid réduire bruit généré outil seuil fréquence seuil moyen classe appliqué descripteurs extraits résulte espace description seuillé dimen dernière étape pondération Salton appliquée série notices étiquetées ainsi traitées représente corpus final lequel apprentissage effectué dernier corpus fortement déséquilibré petite classe contenant articles classe A61K41 grande contenant classe A61K31 simila inter classes calculée utilisant corrélation cosinus indique couples classes similitude comprise entre Ainsi capacité modèle classification détecter précisément bonne classe fortement réduite solution habituellement utilisée faire déséquilibre données classes échantillonnage grosses classes échantillonnage petites classes Chawla Toutefois échantillonnage introduit redondance données améliore performances ensemble données comme montré Hajlaoui proposons après solution alternative élaguer variables jugées pertinentes contraster celles jugées fiables titre complémentaire ensembles données textuelles référence également lisés expériences corpus corpus obtenus Cardoso Cachopo partir sembles données issus collection Reuters 21578 adaptations retenir données ayant seule étiquette Considérant uniquement documents monothématiques classes encore moins exemple apprentissage exemple réduction classes classes fréquentes réduction classes corpus classes corpus Amazontm ensemble données Bache Lichman dérivé clients Amazon exploitable identification teurs évaluer robustesse algorithmes classification grand nombre classes cibles utilisateurs actifs fréquemment postés pubmed acardoso datasets research lewis reuters21578 nouvelle approche sélection variables contraste mentaires newsgroups identifiés nombre messages collectés chacun entre Chaque message comprend style linguistique auteurs utilisation chiffres ponctuation phrases fréquentes Expériences résultats Expériences effectuer expériences prenons abord considération différents rithmes classification oeuvre boite outils arbres cision Quinlan forêts aléatoires Breiman proches voisins algorithmes bayésiens usuels savoir bayésien multino réseau bayésien enfin algorithme Platt paramètres défaut utilisés exécution algorithmes exception lequel nombre voisins optimisé précision résultante mettons ensuite particulièrement accent tests efficacité approches sélection variables compris notre nouvelle proposition incluons notre panel approches filtrage applicables données grande dimen exploitant nouvelle plateforme ensemble méthodes testées comprend carré information incertitude symétrique ReliefF Analyse Composantes Principales Pearson paramètres défaut utilisés plupart méthodes lequel pourcentage variance expliquée accordé fonction précision obtenue premier temps expérimentons méthodes séparément deuxième phase combinons sélection variables fournies différentes méthodes méthode contraste avons proposée validation croisée feuillets cross validation utilisée ensemble expériences Résultats différents résultats présentés tableaux basent sures performance standard positif Rappel positifs Précision mesure pondérées taille classes moyennés toutes classes chaque table chaque combinaison méthodes sélection classification indicateur perte performance calculé utilisant données originales comme référence Enfin comme résultats avèrent identiques carré information incertitude symétrique figurent seule tableaux comme résultats carré notés notre collection principale brevets tableau évidence performances toutes méthodes classification faibles ensemble données considéré processus sélection variables exécuté confirme également contexte supériorité méthodes bayésiennes autres méthodes basées arbres décision outre fournit meilleure performance globale termes discrimination comme montre valeur élevée Toutefois méthode waikato Lamirel RandomForest Résultats classification données initiales Résultats classification après sélection variables classifieur clairement exploitable contexte opérationnel évaluation brevets comme celui QUAERO raison grande confusion entre classes mettant ainsi évidence incapacité intrinsèque faire effet attraction grandes classes Chaque méthode usuelle sélection variables appliquée association thodes classification meilleures notre contexte exploitation altère légèrement qualité résultats comme indiqué tableau tableau souligne également réduction nombre variables méthode similaire termes variables actives section détails exploitation stimule performances méthodes classification particulier celles méthodes bayésiennes tableau conduisant résultats classification impressionnants contexte classification complexe précision données classées parmi total méthode résultats présentés tableau illustrent précisé RandomForest Résultats classification après sélection variables efficacité procédure contraste descriptions données expériences relatives tableau contraste appliqué individuellement variables extraites chaque méthode sélection deuxième étape nouvelle approche sélection variables contraste classifieur appliqué données résultantes contrastées résultats montrent méthode sélection variable utilisé performances classifica résultent renforcées chaque contraste appliqué sélection augmentation moyenne performance tableau illustre finale capacités approche faire efficacement problèmes déséquilibre similitude classes examen variations surtout petites classes dernier tableau montre effet attraction données grandes classes produit niveau élevé exploitation données originales pratiquement systématiquement surmonté chaque approche exploitée capacité approche corriger déséquilibre classes également clairement évidence répartition homogène variables actives classes malgré tailles hétérogènes classe résumé résultats ensembles données Résultats classification différentes méthodes sélection variables contraste classifieur complémentaires présenté tableaux soulignent méthode améliorer significativement performances classifieurs différents types Comme contexte notre expérience précédente brevets meilleure performances obtenues exploitation méthode combinaison classifieurs siens tableau présente résultats comparatifs telle combinaison évidence méthode particulièrement efficace augmenter performances classifieurs complexité tâche classification devient élevée raison nombre croissant classes corpus tableau fournit informations générales données comportement méthode sélec illustre diminution significative complexité classification obtenue raison réduction nombre variables gérer ainsi diminution conco mitante données classées souligne également temps calcul modéré méthode calcul effectué Linux ordinateur portable équipé processeur Intel Pentium mémoire ensembles données remarques similaires celles mentionnées ensemble données brevets peuvent faites sujet faible efficacité méthodes usuelles sélection variables méthodes échantillonnage tableau montre également valeur facteur plification contraste exploité obtenir meilleures performances varier expériences dernier contexte Cependant observer prenant valeur facteur exemple élevée dégrade résultats choix représente bonne alternative faire problème paramétrage Lamirel Etiquette classe Taille sélect avant a61k31 a61k33 a61k35 a61k36 a61k38 a61k39 a61k41 a61k45 a61k47 a61k48 a61k49 a61k51 a61k6 a61k8 a61k9 Caractéristiques classe avant après sélection classifieur Trade Grain tariff agricultur common trade farmer strike complet practic winter worker impos certif handl subject sanction tender Learn Money Interest Crude currenc dollar prime profit germani point prior shortag percentag quota split stabil surpris crude Liste variables lemmes contraste élevé classes corpus Reuters8 Reuters52 Amazon Résultats classifications après sélection variables classifieur nouvelle approche sélection variables contraste variables lemmes contrastées classes issues corpus Reuter8 présentées tableau grandes lignes thématiques couvertes classes puissent clairement mises évidence cette manière illustre capacités extraction sujets méthode Enfin obtention bonnes performances combinant approche sélection variables méthode classification comme avantage exploitation grande échelle sachant méthode capacités incrémentales méthodes temps calcul faibles classes données variables 10000 sélect activ classe Facteur amplification classés classés Temps calcul Informations données résultats complémentaires après sélection riables classifieur Conclusion Notre objectif principal était développer méthode efficace sélection contraste variables pourrait permettre surmonter problèmes habituels classification supervisée volumes données textuelles problèmes classes déséquilibrées degré élevé similitude entre elles hébergeant données fortement multidimensionnelles bruitées faire avons proposé adapter métrique développée récemment cadre supervisé contexte classification supervisée Grâce diverses expériences grands ensembles données textuelles avons illustré nombreux avantages notre approche surtout grande efficacité améliorer performances classifieurs contexte mettant accent classifieurs flexibles moins gourmands temps calcul comme classifieurs bayésiens autre avantage cette méthode approche paramètre appuie schéma simple extraction variables utilisée nombreux contextes comme apprentissage incrémental supervisé encore celui apprentissage numérique général autre perspective intéres sante serait adapter cette technique domaine exploration textes enrichir ontologies lexiques grâce exploitation grande échelle corpus existants Remerciements travail réalisé cadre programme QUAERO soutenu Agence française développement recherche quaero Lamirel Références Kibler Albert Instance based learning algorithms Machine learn Bache Lichman machine learning repository archive University california school information computer science irvine Bolón Canedo Sánchez Maroño Alonso Betanzos review feature selection methods synthetic Knowledge Information Systems Breiman Random forests Machine learning Breiman Friedman Olshen Stone Classification regression trees Technical report Wadsworth International Group Belmont Chawla Bowyer Kegelmeyer Synthetic minority oversampling technique Journal Artificial Intelligence Research Consistency based search feature selection Artificial Intelli gence Forman extensive empirical study feature selection metrics classifi cation Journal Machine Learning Research Resampling methods Birkhauser Guyon Elisseeff introduction variable feature selection Journal Machine Learning Research Guyon Weston Barnhill Vapnik selection cancer classification using support vector machines Machine Learning Hajlaoui Cuxac Lamirel FranÃ Enhancing patent expertise through automatic matching scientific papers Ganascia Lenca Petit Discovery Science Volume Lecture Notes Computer Science Springer Berlin Heidelberg Smith Feature selection machine learning Comparing correlation based filter approach wrapper Proceedings Twelfth International Florida Artificial Intelligence Research Society Conference Kohavi Wrappers feature subset selection Artificial Intelli gence Konokenko Learning filter netnews Proceedings European Conference Machine Learning Ladha Deepa Feature selection methods algorithms International Journal Computer Science Engineering Lallich Rakotomalala feature selection using partial correlation multi valued attributes Zighed Komorowski ytkow Principles Mining Knowledge Discovery Number Lecture Notes Computer Science Springer Berlin Heidelberg Lamirel Shehabi François Hoffmann classification quality nouvelle approche sélection variables contraste estimators analysis documentary information application patent analysis mapping Scientometrics Lamirel Ghribi Cuxac Unsupervised recall precision measures towards efficient clustering quality indexes Proceedings International Conference Computational Statistics COMPSTAT Paris France Lamirel Combination hyperbolic visualization graph based approach organizing analysis results application social network analysis Proceedings ofthe International Conference Webometrics Informetrics Scientometrics COLLNET Meetings Berlin Germany Pearson lines planes closetst systems points space Philosoph Magazine Platt Advances kernel methods Chapter training support vector machines using sequential minimal optimization Cambridge Press Quinlan programs machine learning Francisco Morgan Kaufmann Publishers Salton Automatic processing foreign language documents Englewood Clifs Prentice Schmid Probabilistic speech tagging using decision trees Proceedings International Conference Methods Language Processing Witten Frank Mining Practical machine learning tools techniques Morgan Kaufmann Feature selection dimensional correlation based filter solution Proceedings Washington Summary Feature maximization cluster quality metric which favors clusters maximum representation regard their associated paper further showing straightforward adaptation metric provide highly efficient feature selection feature contrasting model context supervised classification method experienced different types textual datasets paper illustrates proposed method provides clear performance increase studied cases single words model exploited description especially technique enhance performance classification methods whilst signifcantly outperform state feature selection techniques classification unbalanced highly multidimensional noisy textual degree similarity between classes