actes_non_num 351rotes approach handling uncertainty multiarmed bandit problems Stefano Perabò Fabrice Clerot France Télécom Division Recherche Développement avenue Pierre Marzin 22307 Lannion Cedex stefano perabo orange fabrice clerot orange ftgroup Abstract approach presented multiarmed bandit Specifically known exploration exploitation dilemma solved point maximizing utility function which measures decision maker attitude towards uncertain outcomes preference theory established Simulations results provided order support ideas compare approach existing methods emphasis short small sample behavior proposed method Introduction multiarmed bandit problem formulated follows given sequence dimensional random vectors called rewards whose probability distribution known priori objective determine quence actions called strategy policy where discrete random defined maximizes expectation cumulative observing realization difficulty problem consists objective function known advance means available strategy would obviously action Hence instant choice action result compromise trying estimate learn objective function exploring actions whose rewards determined enough confi dence maximize exploiting those which based preceding observations estimated provide rewards represents prototype decision problem where decision maker faced called exploration exploitation dilemma while pursuing second objective exploitation using unavoidably suboptimal strategy might incur losses could avoided better mates rewards means available contrary while pursuing first objective exploration using other suboptimal strategy might renounce supposed Italic characters represent realizations corresponding random variables which denoted using roman characters Handling bandit problems action found problem along variants model basic decision tasks commonly encountered resources allocation marketing popular example borrowed internet advertising community decision maker periodically display action choosing known objective being maximiza number visitors clicks displayed rewards which translates maximization decision maker revenues exploration exploitation dilemma comes users interests known advance order judge which attracts largest number clicks displayed tested certain number times realistic scenarios before taking decision decision maker could access information often called context above example profile currently visiting keywords extracted information could order better choose display represents extension bandit problem which usually called contextual bandit problem formalized introducing additional sequence random quantities contexts assuming correlation exists between contexts rewards ploration issue consists estimating correlation order better predict which action perform conditionally given context paper approach proposed exploration exploitation dilemma multiarmed bandit problems extension interesting contextual bandit problems pursued However should clear readers interested extension reasoning could applied without changes presence context difficulties being technical rather conceptual nature these difficulties arise particular dealing requiring simultaneously application classification techniques Moreover focus classic framework which rewards modeled independent identically distributed random variables independent decision maker strategy Besides these assumptions bounded rewards considered without generality assumption really necessary method stated comparing other existing Relation state literature bandit problems since seminal Robbins abundant recent contributions solution multiarmed bandit problem bounded rewards notably strategies Audibert guarantee expected cumulative lower bounded function where constant maximum value objective function shown Robbins logarithmic called regret cannot improved further following sense possible strategy suffers which lower bounded function another point strategy SUCCESSIVEELIMINATION proved optimal action Perabò Clerot example lower bounds average expected cumulative rithms solid dashed These bounds apply where rewards distributed described section second example wards support arbitrary prespecified probability finite number steps these strategies derived handling considerable mathematical insight concentration inequalities random variables precisely inequalities state following action played times empirical defined within distance probability greater equal certain threshold value functional depends inequality however generally fixed decreases increasing while fixed increases increasing Hence playing action increase confidence which known strategy design consists finding trade between competing objectives sampling order maximize sampling order increase confidence rewards means regret strategies mentioned above proved close arbitrary rewards distributions because concentration inequalities valid arbitrary probability distributions however drawback these approaches because confidence regions derived inequalities practical conservative resulting large constants front regret consequence lower bounds become useful quantity positive sufficiently large horizon example Figure plots average lower bounds quantity algorithms particular rewards distributed described section where results numerical simulations presented Clearly nothing criticism addressed current approaches solve bandit problems concentrate effort upper bounding regret algorithm tightly possi which leaves account preferences constraints Consider Handling bandit problems particular happens during first period following start bandit algorithm where exploration predominant activity question becomes switch exploration exploitation given information provided finite number samples answer clearly depends factors influential maybe being available horizon samples drawn decision maker attitude towards uncertainty risky choices objective paper propose strategy design procedure allows erences constraints impact algorithm behaviour taken account somehow automatically particular exploration exploitation dilemma reformu lated problem maximizing utility function which quantifies decision maker preferences appropriate confidence intervals precisely alternative confidence interval associated estimation future could obtained playing certain strategy utility function measure attitude towards uncertainty attitude drives choice confidence interval specific utility function which designed express aversion risky strategies provided related algorithm called results numerical simulations presented order compare short small horizon performance algorithm algorithms cited above Description approach Pretend while rewards probability distributions known possible problem maximizing finite horizon planning problem degenerate markov decision process problem determining optimal strategy often called policy consisting deterministic state known Puterman optimal strategy found backward induction algorithm applied under independence assumptions stated introduction simply amounts instant functions called optimal action value functions jargon solve optimal action value function equals maximum expected tained interval conditioned action played above formula follows order maximize future interval sufficient action maximizing expected reward follow strategy interval other words strategy probability action solution backward induction gives obviously where confirms above interpretation there stochastic process defined transition probabilities giving probability reaching possible state given current state action vector rewards depend current state through conditional probability framework considered obtained allowed transition deterministic state itself irrespective which action taken Perabò Clerot assume decision maker allowed randomized strategy action drawn random variable taking values Define probability taking action quantity where Clearly represents maximum expected obtained interval whenever randomized strategy played Moreover choice equality holding which quantity often called optimal value function Hence planning problem rewards distribution known there advantage adopting randomized strategy respect optimal determin istic obtained backward induction outlined above Consider prior knowledge about rewards distribution Neither functions evaluated because means known However suppose given confidence interval computed based observation interval meaning confidence interval interval whose extremes functions empirical contains probability value varying different confidence intervals obtained Hence reasonable define optimal strat strategy confidence interval preferred intervals obtained example interval should preferred whenever because maximum expected corresponding strategy likely larger necessary these inter constructed practice define preference relation confidence intervals Concerning first issue heuristic approach adopted known empirical means tributed random variable gaussian whose variance respectively variance divided number variance replaced unbiased empirical variance though these asymptotic sults pretend represent acceptable approximations finite number samples Hence approximately distributed where largest empirical empirical variance number samples corresponding action numerical example Figure extremes confidence interval Handling bandit problems crosses indicate couples obtained varying action selection probability range steps under straint using following circles represent values tained where quantile standard gaussian preference relation needed represent decision maker attitude towards uncertainty represented given strategy corresponding evaluated exactly preference relation binary relation denoted interval preferred interval writes classical result under appropriate assumptions there exists quantitative representation preference relation terms utility function defined intervals Besides technical assumptions which necessary account countable example Fishburn details proved above resentation holds relation order means transitive irreflexive never holds relation defined neither transitive called normative approach preference modelling Tsoukiàs followed existence preference relation above properties postulated decision maker adhering relation termed rational amounts practice utility function appropriate depending application choosing perhaps possible choices other words existence optimal utility function stated different forms might reasonable behave equally practice paper following simple utility function proposed tested numerical simulation Perabò Clerot parameters distributions model rewards outcomes numerical simulations along corresponding expectations deviations parametric function confidence intervals through which quantities define interval itself rationale behind choice following tangent point level curve equal everywhere plane Hence decision maker indifferent between confidence interval specified couple another specified where increments related Since lower bounds corresponding confidence intervals equal means decision maker accepts trade strategy another other words utility confidence intervals share lower bound Higher lower bounds preferred while upper bounds ignored indicating aversion risky strategies conclusion proposed strategy called Lower Confidence Bound following sample twice action unbiased empirical variances their initial value choose action probability where utility function numerical simulations section results numerical simulations provided rewards distributions considered belong family distributions parameters plots corresponding probability densities shown Table Figure respectively horizon tests which gives confidence interval standard gaussian random variable first corresponding rewards distributions Figure plots probability distribution obtained after steps algorithms black proposed algorithm These distributions obtained running these algorithms different realizations rewards sequences There remarkable differences between three algorithms reward means comparable reduction could obtained sampling action often counterbalanced reduction Handling bandit problems Plots rewards probability densities green lines respectively right magenta lines respectively expected second actions added actions first conducted described above Plots probability distribution shown Figure Contrary considered first performance superior checked Figure inferior actions faster other algorithms Discussion approach solution multiarmed bandit problem framework proposed Contrary known existing methods approach somewhat heuristic based rigorous convergence proofs However numerical simulations performance short least cases considered motivat further analyses practical applications short behavior particular important because assumption identical distribution might violated margins improvements worth indicating confidence intervals computed better established finite sample statistical techniques bootstrap example might provide correct asymmetric intervals other utility functions might exist would represent different balances between avoiding seeking behaviors different managing exploration exploitation dilemma contribution paper however established theory preferences which currently large research subfield economics approach presented shares similarities basic portfolio selection methods Markowitz possibility introducing advanced techniques called sensitive averse context bandit problems should considered future investigations worth mentioning possibility extending framework allow discounted rewards solve exploration exploitation dilemma where Perabò Clerot after steps after steps after steps after steps results first rewards distributions distribution obtained algorithm black after steps pendent objective function maximized where numbers given weights discussed paper decisions involve often tradeoffs among uncertain costs benefits occurring different points under constraint different horizons cases known Frederick choice weighting sequence related decision maker preferences different preferences demand different forms weighting should clear whenever these preferences known along utility function confidence intervals presented approach applied quite straightforwardly References Audibert Munos Szepesvári Variance estimates exploration function multi armed bandit Technical Report CERTIS France Bianchi Fischer Finite analysis multiarmed bandit problem Machine Learning Handling bandit problems after steps after steps after steps after steps results second rewards distributions distribution obtained algorithm black after steps Mannor Mansour Action elimination stopping condi tions multi armed bandit reinforcement learning problems Journal Machine Learning Research Fishburn Preference structures their numerical representation Theoretical Computer Science Frederick Loewenstein donoghue discounting prefer critical review Journal Economic Literature Robbins Asymptotically efficient adaptive allocation rules Markowitz Portfolio selection Journal Finance Puterman Dynamic programming Meyers Encyclopedia Physical Science Technology Volume Academic Press Robbins aspects sequential design experiments Tsoukiàs decision theory decision aiding methodology European Journal Operational Research Perabò Clerot Typical behaviours algorithms bottom obtained second rewards distributions symbol indicates corresponding action selected Résumé approche présentée prise considération risque problème bandit manchot précisément dilemme exploration exploitation reformulée comme problème maximisation fonction utilité mesure attitude décideur envers risque incertitude théorie préférence établi méthode proposée testée comparée simulation numérique autres connues littérature intérêt particulier comportement court terme petit nombre échantillons