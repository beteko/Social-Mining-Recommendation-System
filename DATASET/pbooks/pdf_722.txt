Actes_non_num 351rotes approche gestion risques incertitude problèmes bandit manchot Stefano Perabo Fabrice Clérot France Télécom Division Recherche Développement avenue Pierre Marzin 22307 Lannion Cedex stefano perabo orange fabrice clerot orange ftgroup Résumé approche présentée faire risques multiarmed bandit problè précisément dilemme exploration exploitation connu résolu partir point maximiser fonction utilité mesure attitude décideur envers risque résultats incertains théorie préférence ainsi établie résultats simulations prévues soutenir principales idées comparer approche méthodes existantes mettant accent comportement court terme petite taille échantillon méthode proposée Introduction problème bandit manchot formulé comme donnée séquence vecteurs aléatoires dimensions appelés récompenses distribution probabilité inconnue priori objectif déterminer ligne quence actions également appelée stratégie politique chacun mesure discrète aléatoire définie ensemble maximise espérance cumulatif observant chaque seulement réalisation principale difficulté problème réside fonction objective avance moyens étaient disponibles meilleure stratégie serait évidemment jouer action conséquent chaque instant choix action résultat compromis tenter estimer apprendre fonction objective explorant actions récompenses moyen encore déterminée confiance fiance assez temps maximiser exploitant observations précédentes estimés fournir meilleures récompenses représente problème décision prototype décideur faire appelle dilemme exploration exploitation poursuivant deuxième objectif exploitation utilisant inéluctablement stratégie suboptimale pourrait subir pertes pourraient évités meilleure accouple moyens récompenses étaient disponibles contraire poursuivant premier objectif exploration utilisant autre stratégie suboptimale pourrait renoncer jouer supposés caractères italiques comme représentent réalisations variables aléatoires correspondantes désignés utilisant caractères romains comme risque manipulation meilleure action problèmes bandit trouvé jusqu présent problème nombreuses variantes modéliser tâches décision communément rencontrées allocation ressources commercialisation exemple populaire emprunté communauté publicité Internet décideur afficher périodiquement annonce action choisissant partir ensemble connu annonces objectif étant maximisation nombre visiteurs clique annonce affichée récompenses traduit maximisation revenus décideur dilemme exploration exploitation provient utilisateurs intérêts connus avance juger quelle annonce attire grand nombre clics chaque annonce affichée testée certain nombre scénarios réalistes avant prendre décision décideur pourrait avoir accès sorte information latérale contexte souvent appelé comme exemple dessus profil utilisateur actuellement visite liste extrait Comment informations pourraient utilisées mieux choisir annonce affichage représente extension problème bandit généralement appelé problème contextuel bandit formalisé introduction séquence supplémentaire quantités aléatoires contextes supposant existe corrélation entre contextes récompenses question ploration consiste estimer cette corrélation mieux prédire quelle meilleure action effectuer conditionnellement contexte donné article approche proposée faire dilemme exploration exploitation problèmes bandit manchot extension problèmes bandit contextuel intéressant poursuivi Cependant devrait clair lecteurs intéressés telle extension raisonnement pourrait appliqué changements aussi présence contexte principales difficultés étant technique plutôt nature conceptuelle difficultés notamment traitement énormes ensembles données nécessitant simultanément application techniques classification accent cadre classique récompenses modélisés comme indépendants identiquement distribuées variables aléatoires indépendamment stratégie décideur dehors hypothèses considérées comme récompenses bornées perte généralité cette hypothèse vraiment nécessaire méthode travail intérêt comparer autres existants rapport littérature problèmes banditisme depuis travaux fondateurs Robbins abondante principales contributions récentes solution problème bandit manchot récompenses bornées notamment stratégies Audibert garantie cumulé attendu délimitée inférieure fonction forme constante valeur maximale fonction objective Comme montre Robbins expression logarithmique appelé regret améliorée suivant toute stratégie possible subit regret délimitée inférieure fonction autre point stratégie SUCCESSIVEELIMINATION prouvé trouver action optimale Perabo Clérot exemple limites inférieures cumulatif moyen attendu algorithmes trait plein ligne pointillés limites applicables points distribués comme décrit section deuxième exemple récompenses support probabilité prédéfinie arbitraire nombre étapes Toutes stratégies obtenues traitement aperçu mathématique considérable inégalités concentration sommes variables aléatoires précisément inégalités indiquent instant action moyenne empirique définie distance partir moyenne vraie grande probabilité égale certaine valeur seuil forme fonctionnelle dépend nature inégalité utilisée cependant généralement maintenue alors diminue augmenter alors maintenu augmente augmenter conséquent action augmenter confiance laquelle connu moyenne conception stratégie consiste trouver meilleur compromis entre objectifs contradictoires échantillonnage maximiser échantillonnage augmenter confiance moyens récompenses regret stratégies mentionnées dessus prouvé proche distributions récompenses bitrary parce comptent inégalités concentration valables distributions probabilité arbitraires cependant aussi inconvénient principal approches parce régions confiance provenant inégalités peuvent certains pratique conservatrice grandes constantes devant journal terme regret conséquence bornes inférieures deviennent utiles seulement terme quantité seulement positif horizon temps suffisamment grand titre exemple figure représente graphiquement moyennes bornes inférieures quantité algorithmes particulier récompenses répartis comme décrit section seront présentés résultats certaines simulations numériques clair principale critique adressée approches actuelles résoudre problèmes bandit concentrer efforts limite supérieure regret algorithme aussi serré possi feuilles place prendre compte préférences utilisateur contraintes Tenez compte risque manipulation problèmes bandit particulier passe cours première période suivant début algorithme bandit exploration activité principale question savoir quand comment passer exploration exploitation compte informations fournies nombre échantillons réponse dépend clairement nombreux facteurs influents horizon temps disponible combien autres échantillons peuvent tirer attitude décideur incertitude choix risqués objectif article proposer procédure conception stratégie permet Ørences contraintes préférences avoir impact comportement algorithme prendre compte quelque sorte automatiquement particulier dilemme exploration exploitation lated refor problème maximiser fonction utilité quantifie préférences décideur ensemble intervalles confiance appropriés précisément chaque intervalle confiance remplacement associée estimation futur pourrait obtenu jouant certaine stratégie fonction utilité mesurer attitude risque incertitude attitude entraîne choix intervalle confiance fonction utilité spécifique conçu exprimer aversion stratégies risque prévue algorithme connexe appelé résultats certaines simulations numériques présentées comparer court terme petit horizon performance nouvel algorithme celui algorithmes précité Description approche Pretend pendant certain temps récompenses distributions probabilités connues possible problème maximiser comme problème planification horizon processus décision dégénéré MARKOV problème détermination stratégie optimale souvent appelée politique consistant déterministe connu Puterman stratégie optimale trouvée algorithme induction arrière appliquée hypothèses indépendance énoncés introduction revient simplement trouver chaque instant fonctions appelées fonctions action valeur optimale jargon permettent résoudre fonctionnement optimal valeur action maximal attendu contenue intervalle conditionnée action temps formule dessus comme maximiser futur intervalle suffit jouer instant maximiser récompense attendue suivre meilleure stratégie intervalle autres termes meilleure stratégie consiste jouer probabilité action solution induction arrière donne évidemment confirme interprétation dessus existe processus stochastique défini probabilité transition donne probabilité atteindre suivant possible actuel action vecteur récompenses dépendent actuel probabilité conditionnelle cadre considéré obtenue lorsque transition autorisée seulement forme déterministe quelle mesure prise Perabo Clérot Maintenant supposons décideur autorisé jouer stratégie aléatoire action variable aléatoire prenant valeurs ensemble Définir probabilité prendre mesures instant ainsi quantité clair représente maximum attendu obtenir intervalle chaque stratégie randomisée instant outre choix maintien égalité lorsque quantité appelle souvent fonction valeur optimale conséquent problème planification lorsque distribution récompenses connus aucun avantage adopter stratégie aléatoire rapport valeur optimale obtenue istic détermi induction arrière décrit dessus Considérons alors aucune connaissance préalable distribution récompenses fonctions peuvent évalués parce moyens connus Cependant suppose donnée chaque intervalle confiance exemple jusqu calculée partir ensemble observation intervalle appeler intervalle confiance existe intervalle extrémités fonctions données empiriques contient probabilité valeur réelle faisant varier différents intervalles confiance obtenus raisonnable définir stratégie optimale stratégie telle intervalle confiance préférable intervalles obtenu exemple intervalle préféré chaque parce maximal prévu stratégie correspondante susceptible grande nécessaire montrer comment intervalles peuvent construits pratique définir relation préférence intervalles confiance concerne première question approche heuristique adoptée comme moyens empiriques tendance tribué variable aléatoire gaussienne moyenne variance respectivement moyenne variance vraie divisée nombre échan variance réelle aussi remplacé empirique variance biaisée sultats asymptotiques prétendre représentent approximations acceptables aussi nombre échantillons conséquent sensiblement distribué comme grand empirique variance empirique nombre échantillons action correspondante exemple numérique figure extrémités intervalle confiance jusqu risques manipulation problèmes bandits moyenne FIGUE graphique croix indiquent couples obtenus faisant varier chaque probabilité sélection action intervalle étapes straint utilisant données suivantes cercles représentent valeurs CONTENUES lorsque certain quantile gaussienne standard Ensuite relation préférence nécessaire mesure représenter attitude décideur incertitude représentée stratégie donnée correspondant évalué précision relation préférence relation binaire notée sorte intervalle préféré intervalle écrit résultat classique hypothèses appropriées existe représentation quantitative relation préférence termes fonction utilité réelle définie ensemble intervalles exemple Outre hypothèses techniques nécessaires tenir compte dénombrable exemple Fishburn détails prouvé précède repré sentation seulement relation ordre faible signifie transitif irréflexive détient relation définie aussi transitive disant approche normative modélisation préférences Tsoukias suivie existence relation préférence propriétés dessus postulée décideur adhérant telle relation appelée rationnelle montant pratique définir fonction utilité forme appropriée fonction application portée choisissant ensemble plusieurs choix possibles autres termes existence fonction utilité optimale indiqué différentes formes pourraient raisonnable comporter aussi pratique article suite fonction utilité simple proposé testé simulation numérique perabo CLEROT paramètres distributions utilisées modéliser résultats récompenses simulations numériques ainsi attentes correspondantes écarts Notez fonction paramétrique intervalles confiance quantités utilisées définir intervalle raison choix suivant tangente point courbe niveau partout conséquent décideur indifférent entre intervalle confiance spécifié couple autre spécifié incréments Étant donné limites inférieures intervalles confiance égaux signifie décideur accepte négocier stratégie autre autrement utilité intervalles confiance partagent limite inférieure limites préférés tandis limites supérieures ignorées indiquant ainsi aversion stratégies risquées conclusion Propo stratégie appelée Lower Confidence Bound suivant échantillon chaque action sorte variances empiriques authentiques peuvent réglées valeur initiale chaque temps choisir action probabilité fonction utilité Quelques simulations numériques cette section résultats simulations numériques prévues considérés comme distributions récompenses appartiennent famille distributions paramètres tracés densités probabilité correspondants présentés tableau figure respectivement horizon temps tests donne intervalle confiance variable aléatoire gaussienne norme premier correspond récompense distributions figure emplacements exposition distribution probabilité obtenu après algorithmes ligne bleue ligne noire algorithme proposé ligne rouge distributions obtenues exécution algorithmes réalisations différentes séquences récompenses différences remarquables entre trois algorithmes moyens récompense comparables réduction risque pourrait obtenu action échantillonnage souvent contrebalancés réduction risque manipulation problèmes bandits Emplacements récompenses densités probabilité gauche lignes bleues vertes rouges respectivement droite lignes magenta respectivement attendu second actions ajoutés actions premier effectué manière décrit dessus Parcelles distribution probabilité présentés figure Contrairement considéré premier performance supérieure comme vérifié figure débarrasse actions inférieures rapide autres algorithmes Discussion approche solution problème bandit manchot cadre proposé Contrairement méthodes existantes connues approche quelque heuristique repose preuves convergence rigoureuses Cependant simulations numériques montrent bonne performance court terme moins considérés ainsi analyses poussées motivat applications pratiques comportement court terme particulièrement important parce hypothèse répartition identique temps violé terme marges améliorations convient indiquer intervalles confiance manière calculés mieux établis échantillons techniques statistiques finies telles bootstrap exemple pourraient prévoir intervalles corrects asymétriques autre fonctions utilité peuvent exister représenterait différents équilibres entre risque éviter comportements recherche risques différentes façons gérer dilemme exploration exploitation principale contribution article cependant établi théorie préférences actuellement grand champ recherche économie approche présentée partage nombreuses similitudes méthodes sélection portefeuille Markowitz possibilité introduire techniques avancées disant aversion risque sensible risque contexte problèmes bandit devraient considérés enquêtes futures convient également mentionner possibilité étendre cadre permettre récompenses réduit résoudre dilemme exploration exploitation chaque instant temps Perabo CLEROT après étapes après après marches après résultats premier récompenses distributions distribution obtenu algorithme ligne bleue ligne noire ligne rouge après temps pendants fonction objective forme maximisée nombre poids donné discuté présent document décisions impliquent souvent compromis entre coûts avantages incertains produisent différents points temps contrainte horizons temporels différents connu Frederick choix séquence pondération préférences temporelles décideur différentes préférences exigent différentes formes pondération devrait clair chaque préférences connus ainsi fonction utilité intervalles confiance approche présentée appliquée détour Références Audibert Munos Szepesvári Estimations variance fonction exploration bandit plusieurs Rapport technique CERTIS France Bianchi Fischer Analyse finie temps problème bandit manchot Machine Learning risque manutention problèmes bandits après étapes après après marches après résultats deuxième récompenses distributions répartition obtenu algorithme ligne bleue ligne noire ligne rouge après temps Mannor Mansour élimination action arrêter condi tions problèmes apprentissage bandit plusieurs renforcement Journal Machine Learning Research Fishburn structures préférence représentation numérique Theoretical Computer Science Frederick Loewenstein donoghue actualisation temps temps préférence examen critique Journal Economic Literature Robbins règles allocation adaptative asymptotiquement efficace Markowitz sélection portefeuille Journal Finances Puterman Programmation dynamique affaire Meyers Encyclopédie sciences physiques technologie volume Academic Press Robbins Certains aspects conception séquentielle expériences Taureau Tsoukias théorie décision décision méthodologie Complicité Revue européenne recherche opérationnelle perabo CLEROT Temps comportements typiques algorithmes obtenu deuxième distributions récompenses symbole indic action correspondante sélectionnée instant approach résumé présentéisme prise considération Erotisme Problème bandit manchot précisement exploration Dilemme exploitation reformulée Comme maximisation Problème fonction utilité attitude mesure Décideur Envers incertitude théorie ÉTABLI Préférence méthode proposed Testée simulation numérique comparée Autres connues littérature interest Particulier comportement terme petit Échantillons Nombre