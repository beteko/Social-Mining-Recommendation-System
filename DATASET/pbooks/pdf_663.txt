articles assemblage pdfIntégration Connaissances Priori Principe Maximum Entropie Chakik Dornaika IKERBASQUE Basque Foundation Science University Basque Country Sebastian Spain fadi_dornaika Laboratory Lebanese University Tripoli Lebanon fchakik Résumé article montre dispose connaissance priori problème intégration cette dernière processus appren tissage machine intelligente tâches classification amélio performance cette machine étudions effet intégration connaissance priori convexité processus apprentissage principe Maximum Entropie MaxEnt utilisant exemples virtuels idées proposées problème benchmark connu littéra machines apprentissage problème formes ondes Breiman avons abouti erreur généralisation proche erreur théorique estimé Breiman Introduction désigne connaissances priori informations auxiliaires peuvent lisées aider processus apprentissage informations peuvent priori appelle connaissances priori Srihari Notre travail intéresse extraction connaissances priori application donnée plutôt intéresse manière intégrer processus apprentissage Lauer Bloch améliorer performance généralisation intégrer connais sances priori processus apprentissage partir exemples méthodes possibles représentation connaissances exemples virtuels traduisent cette dernière langage compréhensible algorithme apprentissage intégra connaissances architecture classifieur appelle exemple virtuel exemple proviendra connaissance priori alors exemple proviendrait directement fonction réaliser Après cette connaissance représentée exemples virtuels pourrions mesurer qualité apprentissage regardant performance système ensemble exemples Comme exemple concret intégration connaissances priori processus apprentissage illustrer celle concernant connaissance priori convexité tâche classification formes ondes classifieur principe Maximum Entropie Macaulay Chakik Intégration Connaissances Priori Inférence statistique principe Maximum Entropie Considérons ensemble données formé vecteurs composantes modélisation statistique consiste déterminer probabilité crive mieux densité probabilité laquelle variables tirées connais sance dispose déterminer modèle traduite diverses contraintes riques contraintes définies fonctions nommées observables analogie thermodynamique refléterons exemple relations essentielles entre certaines variables modèle autant complexe demandera autant données nombre observables important fonctions permettent traduire connaissance certaines caractéristiques variables moyen équations mettant leurs espérances méthode recherche concrète celle choix probabilité entropie maximale parle principe maximum entropie principe maximum entropie cédure induire distribution probabilité inconnue partir ensemble partiel connaissances Bessière Macaulay Cette procédure trouvé nombre croissant applications différentes branches science Chaque fonction scalaire vecteur prend valeur discret valeurs moyennes dites théoriques données répresente valeur densité voulons déterminer distribu probabilité vérifie ensemble contraintes contraintes telles valeurs théoriques égales leurs estimateurs déterminés ensemble mesures données dispose autres termes MaxEnt principe modélisation probabilité vérifie contraintes définies partir ensemble observables maximise entropie effet satisfait ensemble contraintes sélectionne celle correspond distribution étalée possible compatible observables contient information celle apportée mesures procédure maximum entropie distribution probabilité choisie celle maximise entropie condition contrainte norma lisation ensemble contraintes soient réalisées Chakik pourrait montrer solution générale densité forme suivante constante normalisation multiplicateurs Lagrange doivent satisfaire contraintes signe identique mateurs empiriques déterminés partir données suppose indépendantes identiquement distribuées valeurs connues équations permettent déterminer permettent déterminer distribution Chakik Problème formes ondes Breiman problème considéré comme problème standard classique comparaison différentes approches méthodes apprentissage classification introduit Breiman étude arbres décision général tester méthode particulier problème consiste discriminer entre trois classes formes Chaque forme simule phénomène chronologique quantitatif mesuré instants régulièrement espacés objet caractérisé point Chaque classe consiste combinaison convexe aléatoire ondes perturbée bruit gaussien ondes notées modales déphasage Figure absence bruit gaussien trois classes représentées trois côtés triangle sommets problème déterministe chaque exemple représenté vecteur dimensions appartient classe correspondant auquel appartient bruit gaussien vient perturber considérations géométriques chaque exemple admissible trois classes problème déterministe Ainsi disposons ensembles apprentissage exemples chacun ensemble exemples Notons principe Entropie Maximale implique estimaton densité probabilité chaque classe Schématique trois ondes Breiman Représentation schématique trois classes absence bruit gaussien Intégration connaissance priori convexité mieux comprendre effet connaissances priori procédons intégrer dernières processus classification MaxEnt façon similaire celle Intégration Connaissances Priori proposée Abumostafa intégration connaissances priori réseau neurones Alors supposons expert fourni connais sance priori phénomène généré exemples procédons intégrer cette dernière créant exemples virtuels soient accord cette connaissance priori architecture classifieur manque place allons présenter uniquement apport utilisation exemples virtuels offre processus appren tissage apprendre cette connaissance manière apprend exemples réels exemple virtuel généré partir combinaison convexe exemples apprentissage classe Comme disposons ensembles appren tissage avons généré exemples virtuels correspondant combinaisons convexes exemples chaque ensemble avons calculé paramètres chaque densité correspondant ensembles augmentés apprentissage derniers apprentissage définie comme étant erreur moyenne commise ensembles apprentissage augmentés Quant évaluation erreur calculée semble chaque ensemble apprentissage ensuite détermine valeur moyenne ainsi variance erreurs calculées avons testé influence ajout exemples virtuels apprentissage origine Ainsi ajout exemples virtuels chaque classe intégrer nouveaux exemples virtuels tableau donne erreur appren tissage pourcentage fonction nombre exemples virtuels ajoutés densité probabilité chaque classe Chaque classe possède seule observation donnée distance euclidienne carré entre vecteur dimensions centre classe trois estimer trois estima teurs empiriques observables calculés exemples classes correspondant trois estimateurs empiriques observables calculées exemples trois classes tableau donne erreur apprentissage pourcentage fonction nombre exemples virtuels ajoutés densité probabilité chaque classe Chaque classe possède observations Cette observation donnée difference carré entre composante composante correspondante centre classe trois ensembles estimer estimer chaque classe Notons servir connaissance priori autre manière Comme information contenue ensemble apprentissage transportée processus apprentissage paramètres alors quantité information apportée ajout exemples virtuels chacun ensembles apprentissage définie comme étant quantité information moyenne calculée paramètres MaxEnt valeurs moyennes essaie construire classifieur moyen paramètres valeurs moyennes valeurs correspondant ensembles apprentissage cadre notre application notre choix observables correspond déterminer distribution probabilité gaussienne variance définie comme valeur moyenne variances distributions probabilité figure montre erreur apprentissage pourcentage obtenu classifieur moyen fonction nombre exemples virtuels ajoutés Chakik exemples virtuels Erreur Apprentissage Erreur Variance exemples virtuels Erreur Apprentissage Erreur Variance Erreur apprentissage pourcentage fonction nombre exemples virtuels ajoutés Chaque classe possède seule observation donnée distance euclidienne carré entre vecteur dimensions centre classe trois estimer trois estimateurs empiriques observables calculés exemples classes correspondant trois estimateurs empiriques calculées exemples classes exemples virtuels Erreur Apprentissage Erreur Variance exemples virtuels Erreur Apprentissage Erreur Variance Erreur apprentissage pourcentage fonction nombre exemples virtuels ajoutés Chaque classe possède observations Cette observation donnée difference carré entre composante posante correspondante centre classe trois ensembles estimer Chaque classe estimateurs empiriques observables calculés exemples classes correspondant estimateurs empiriques calculées exemples classes Intégration Connaissances Priori Erreur apprentissage pourcentage fonction nombre exemples virtuels ajoutés résultats correspondent classifieur obtenu moyennant parametères MaxEnt ensembles aprrentissage augmentés Conclusion avons étudié influence intégration connaissances priori procédure apprentissage maximum entropie supposant exemples prentissage suivent combinaison convexe Cette connaissance représentée ajout exemples virtuels représentant cette connaissance apprentissage avons evalué influence cette intégration prédiction principe maximum entropie Références Abumostafa Hints dimension Neural Computation Breiman Friedman Olshen Stone Classification Regression Trees Chapman Macaulay Maximum Entropy Action Clarendon Press Oxford Chakik Shahin Hasnah approach constructing complex discriminating surfaces based bayesian interference maximum entropy Interna tional Journal Information Sciences Lauer Bloch Incorporating prior knowledge support vector machines classification review Neurocomputing Srihari Incorporating prior knowledge weighted margin support vector machines International Conference Knowledge Discovery Mining Summary paper shows integration prior knowledge maximum entropy principle improve learning process studied impact using convexity assumption through virtual examples proposed schemes evaluated benchmark problem given Brieman waves problem obtained recognition error close theoretical value