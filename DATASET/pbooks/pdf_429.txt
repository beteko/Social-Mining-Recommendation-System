nouvelle mesure évaluation méthodes extraction thématiques Vraisemblance Généralisée Mohamed Dermouche Julien Velcin Sabine Loudcher Leila Khouas Laboratoire Université Lumière Lyon2 Mendès France 69676 Cedex France julien velcin sabine loudcher lyon2 Software Einstein 34000 Montpellier France amisw Résumé méthodes dédiées extraction automatique thématiques issues domaines variés linguistique computationnelle algèbre linéaire statistique méthodes spécifiques peuvent ajouter méthodes adaptées autres domaines notamment apprentissage automatique pervisé résultats produits ensemble méthodes prennent formes hétérogènes partitions documents distributions probabilités matrices clairement problème comparer nière uniforme article proposons nouvelle mesure qualité intitulée Vraisemblance Généralisée permettre évaluation ainsi comparaison différentes méthodes extraction thématiques résultats obtenus corpus documents autour élections présidentielles françaises ainsi corpus Associated Press montrent perti nence mesure proposée Introduction documents textuels tellement abondants information pertinente souvent difficile retrouver objectif offrir meilleure navigation documents exploration contenu recherche information extraction thématiques topic extraction distingue comme tâche fouille textes objectif extraire automatiquement catégories données priori théma tiques sujets partir grands corpus documents extraction thématiques étudiée différentes communautés celle fouille données Anaya Sánchez Traitement Automatique Langues linguistique computationnelle Ferret recherche infor mation Zamir existence différentes méthodes dédiées cette tâche méthodes spécifiques peuvent ajouter méthodes adaptées notamment appren tissage automatique supervisé résultats produits ensemble méthodes prennent formes hétérogènes partitions matrices distributions probabilités clairement problème comparaison résultats article Evaluation méthodes extraction thématiques proposons nouvelle mesure qualité Vraisemblance Généralisée permet évaluer comparer différentes méthodes extraction thématiques mesure propo calculée nouvel espace description documents décrits thématiques appelons espace latent proposons également opérateurs transformer résultats méthodes extraction thématiques espace latent calculer cette mesure section consacrée présentation principales méthodes extraction matiques section présente principales mesures qualité ainsi nouvelle mesure intitulée Vraisemblance Généralisée expérimentations résultats ensuite présen section conclusion perspectives recherche données section Méthodes extraction thématiques plupart méthodes extraction thématiques nécessite corpus documents forme matrice lignes représentent documents colonnes représentent modèle vectoriel Salton Chaque élément matrice contient poids document reflète importance document simple pondérer leurs fréquences apparition documents fréquence existe autres types pondération cependant noter certaines méthodes extraction thématiques notamment celles issues linguistique computationnelle Ferret manipulent documents forme graphe nœuds représentent unités linguistiques phrases documents arêtes représentent relations entre elles exemple relations sémantiques occurrences extraction thématiques ensuite effectuée lisant algorithmes classiques issus théorie graphes comme clustering spectral avons choisi intégrer représentation notre travail moins moment cette présentation proposons regrouper méthodes extraction matiques trois grandes familles méthodes distance méthodes factorisation matrices modèles thématiques probabilistes Méthodes distance méthodes distance fondent calcul distance mesurer similarité entre documents plupart méthodes cette catégorie méthodes classification automatique supervisée vocation initiale méthodes peuvent utilisées extraction thématiques considérant chaque classe définit thématique regroupe ainsi documents relatifs caractéri sation thématiques ensuite faire traitement prenant exemple fréquents chaque classe cherchant discriminants méthodes distance trouve principalement méthodes partition nement méthodes hiérarchiques méthodes paritionnement comme algorithme Means commencent répartir aléatoirement documents certain nombre classes chaque itération documents réaffectés telle sorte chacun classe proche mesure similarité utilisée Dermouche Plusieurs variantes Means existent comme Fuzzy Means permet sification floue documents document affecté seule classe appartient plusieurs classes différents degrés appartenance méthodes partitionnement généralement faible complexité adaptées grands volumes données méthodes hiérarchiques procèdent construction classes mesure agglomération division agglomération chaque classe contient départ document classes proches termes distance ensuite fusionnées récursivement jusqu documents soient classe division documents seule classe divisée récursivement jusqu chaque document classe Porrata méthode rarchique proposée classification documents basant distance prend compte entités temporelles lieux méthodes classification hiérarchiques offrent possibilité contrôler granularité classes avoir ainsi classes aussi fines grandes souhaité revanche méthodes hiérarchiques souffrent problème complexité inadaptées grands volumes documents elles soient partitionnement hiérarchique méthodes tialement créées extraire thématiques Cependant simple traitement permet extraire thématiques centroïdes correspondent vecteurs espace vocabulaire explique présence méthodes cette étude Méthodes factorisation matrices algèbre linéaire factorisation matrices approche permettre traction thématiques principe général partir matrice occurences trouver factorisation matrice produit matrices matrice lignes constituées combinaisons nouvel espace appelé espace sémantique latent défini thématiques chaque thématique étant combinaison matrice projection documents nouvel espace sémantique chaque élément représente degré appartenance document thématique analyse sémantique latente Latent Semantic Analysis permet faire cette factorisa effectuant décomposition valeurs singulières Deerwester Cepen comme cette dernière produire valeurs négatives méthode problème interprétabilité résultats Seung contourner problème facto risation négative matrices negative Matrix Factorization proposée Seung permet trouver factorisation unique matrice négative produit matrices négatives telle sorte objectif minimiser fonction objectif suivante Seung décrivent méthode itérative basée extraction thématiques problème factorisation ramené problème optimisation fonction contraintes négativité problème ensuite résolu utilisant méthode Lagrange donne règles suivantes Evaluation méthodes extraction thématiques Modèles thématiques probabilistes modèles thématiques probabilistes probabilistic topic models famille modèles graphiques objectif découverte thématiques corpus documents principe considérer document comme mélange probabiliste thématiques latentes document composé plusieurs thématiques différentes proportions Parallèlement chaque thématique définie distribution probabilités exemple thématique relative génétique associe probabilités importantes cellule autres modèle thématiques probabiliste autre angle comme processus génération documents partir vocabulaire ensemble notés prenant compte chaque document mélange probabiliste plusieurs thématiques notées supposant distributions connues processus simplifié génération document suivant fixer distribution probabilités thématiques chaque générer Choisir aléatoirement thématique parmi suivant distribution fixée Choisir parmi vocabulaire suivant distribution partir procédure consiste inverser processus génératif utilisant Bayes estimer valeurs paramètres réalisé utilisant techniques apprentissage inférence modèles graphiques probabilistes comme sampling inférence variationnelle modèles thématiques probabilistes proposés littérature partagent globale principe génératif exposé dessus diffèrent principalement manière choisir distributions probabilités Hofmann aucune hypothèse distribution thématiques documents posée chaque document traité chaque thématique caractérisée distribution multinomiale associés utilise Dirichlet permettre choix judicieux paramêtres distributions multinomiales ainsi pallier limites modèles thématiques probabilistes diffèrent également structure effet certains supposent existence autres variables latentes thématiques exemple variables temporelles opinion permettent ainsi extraire connaissances temps thématiques Dermouche trois familles méthodes exposées dessus inspirations différentes moins montré liens théoriques existent entre méthodes méthode équivalente Kernel Means version Means noyau méthode équivalente prenant divergence Kullback Leibler fonction objectif Gaussier Goutte Evaluation méthodes extraction thématiques différentes méthodes extraction thématiques produisent résultats forme hétérogène partitions documents distributions probabilités matrices problème comparaison résultats résoudre problème posons nouvel espace description commun différentes méthodes nouvelle mesure qualité calcule espace permet ainsi comparer approches nature différente manière quantitative Cette section présente mesures qualité existantes nouvelle mesure proposons Mesures existantes méthodes extraction thématiques généralement évaluées manière litative quantitative approche qualitative recours jugement humain qualifier thématiques donner aucun indice quantitatif comparer méthodes entre elles contrario approche quantitative permet mesurer finement qualité modèles basée jugement humain jugement humain utilisé évaluer thématiques selon critères intrusion topic intrusion Chang mesures quantitatives utilisent jugement humain automatiques vraisemblance calcule seulement modèles probabilistes méthodes apprentissage Dempster toutes thodes extraction thématiques problématique évaluation thématiques retrouve classiquement apprentis supervisé mesures recensées Halkidi Celles peuvent réparties catégories mesures externes mesures internes mesures externes évaluent qualité résultats rapport référence définie classes priori documents Comme exemples mesures citer score moyenne harmonique rappel précision entropie mesure désordre ensemble thématiques pureté ratio moyen classe majoritaire chacune thématiques mesures internes appel connaissances extérieures exemple inertie intra classes utilisée comme fonction objectif méthode Means Steinbach mesure similarité Cosinus entre documents thématique mesures peuvent utilisées évaluer thématiques considérant chaque thématique correspond classe elles dédiées cette tâche notre connaissance existe mesure automatique permette évaluer toutes méthodes présentées manière uniforme ainsi pouvoir comparer Evaluation méthodes extraction thématiques Nouvelle mesure Vraisemblance Généralisée mesure proposons intitulée Vraisemblance Généralisée quantitative interne permet évaluer plusieurs méthodes extraction thématiques dernières basées modèles mathématiques différents fondé mesure repose existe analogie entre différentes méthodes effet toutes méthodes permettent manière autre projeter documents espace description formé thématiques décrire thématiques figure Notre consiste proposer mesure calcule espace latent ainsi transformations résultats méthodes espace latent Espace latent documents projetés espace latent caractérisé thématiques matrice thématiques décrites matrice mesure calculée partir matrices matrice projection matrice projection documents espace latent matrice espace latent définit espace matrice caractérisée ensemble vecteurs nécessairement orthonormés décrits espace figure vecteurs positifs matriceW caractérisée ensemble vecteurs correspondant documents décrits espace thématiques autres termes matricesW telles score appartenance document thématique score appartenance thématique définissons trois transformations espace latent trois méthodes issues principales approches présentées section méthode probabilité probabilité méthode matrices directement obtenues factorisation méthode degré appartenance document classe vecteurHk centroïde classe matrices doivent normalisées elles étaient avoir ordre grandeur quelque méthode utilisée éviter ainsi biais éventuel calcul mesure normalisation lignes normalisation lignes Dermouche ensemble thématiques ensemble documents hypothèses définissons score score vraisemblance occurrence document comme score autres termes score obtenu multipliant ligne matrice respond document ligne colonne matrice correspond colonne Ensuite score vraisemblance document défini comme étant ensemble corpus vocabulaire score score nombre occurrences document passant score score mesure basée moyenne géométrique scores individuels documents score chaque score étant produit calculé chaque vocabulaire équation multiplication géométrique forme produit produits normaliser suffit mettre puissance inverse nombre termes multiplication Celui double somme final mesure calculée formule suivante score score calculé formule mesure calculée corpus différent corpus lequel matiques extraites corpus apprentissage suppose modèle prédictif capable affecter nouveaux documents thématiques extraites malheureusement toutes méthodes notamment méthodes prentissage supervisé mieux interpréter résultat mesure avons choisi moment évaluer travaillant corpus apprentissage Expérimentations cette section présentons protocole expérimental corpus prétraitements paramètres méthodes ainsi résultats discussion Evaluation méthodes extraction thématiques Corpus Elections Langue Anglais Français Nombre documents Nombre uniques Présentation corpus Elections Thématiques immobilier économie étrangers sondages commun economie droit sondage crise valeur erreur immobilier payer marge paris argument immigrés institut politique étrangers candidat crédit marché politique journal logement droit sondage immobilier politique étrangers institut construction dollar local marge encadrement municipal erreur loyer marché gauche harris économie spatial situation divorce marketing copain municipal basculer crever défaite immatriculation croissance ajaccio identifier tronquer dollar démontrer attirer Exemple thématiques découvertes trois méthodes corpus Elections nombre thématiques Protocole expérimental tests effectués corpus Elections corpus ments agence presse Associated Press Harman également utilisé Elections corpus documents médias blogs réseaux sociaux traitent élections présidentielles françaises documents lectés durant période plateforme veille AMIEI amisw tableau résume contenus corpus après traitements suivants Suppression outils stopwords exemple Racinisation stemming exemple logement loger deviennent Suppression occurrent seule document tests réalisés choisissant méthode chaque famille dèles thématiques probabilistes méthodes factorisation matrices méthodes distance limiter risque tomber Dermouche optima locaux réalisé moyenne retenue paramètres méthodes fixés comme nombre itérations paramètres méthode fixés comme nombre maximum rations exécuter sommes appuyés outil Mallet McCallum avons utilisé notre propre implémentation avons utilisé langage types expérimentations réalisés suivants comparaison trois méthodes comparées suivant scores obtenues mesure Tests extrêmes extrêmes considérés Crisp chaque document appartient seule thématique Uniforme chaque document appartient toutes thématiques score résultats correspondant extrêmes créés artificiellement affectant score thématique maximise score obtenu Crisp affectant score toutes thématiques Uniforme Résultats discussion résultats exécution trois méthodes représentés tableau comparaison trois méthodes mesure représentée figure résultats extrêmes représentées figure Nombre thématiques Nombre thématiques Variation mesure fonction nombre thématiques corpus gauche Elections droite méthodes présentent comportement similaire variation mesure fonction nombre thématiques figure effet cette nière augmente augmentation nombre thématiques concordance intuition petit nombre thématiques mélanger plusieurs thématiques seule donne ainsi résultats moins bonne qualité revanche grand Evaluation méthodes extraction thématiques Nombre thématiques Crisp Uniforme Nombre thématiques Variation mesure fonction nombre thématiques trêmes corpus gauche Elections droite nombre thématiques permet mieux séparer thématiques permet thématiques petite taille émerger donne ainsi résultat meilleure qualité nombre théma tiques encore grand proche nombre documents résultats convergent modèle thématique extraite chaque document valeur mesure continue augmenter autant résultat forcément meilleure qualité problème similaire problème surapprentissage overfitting connu domaine apprentissage statistique demeure méthode donne meilleurs résultats termes mesure rapport corpus figure qualité résultats donnés méthode remarquablement inférieure termes mesure celle autres méthodes conforme exemples donnés tableau effet thématiques extraites méthode mélangées difficiles interpréter objectif extrêmes analyser comportement mesure extrêmes Crisp Uniforme section Suivant Crisp forme configurations moins bonnes celle produite figure confirme extrêmes donnent résultats ensemble thématiques constitue général compromis entre extrêmes savoir quelques thématiques pertinentes document Conclusion méthodes extraction thématiques étant issues domaines variés produisent résultats forme hétérogène empêche comparaison manière uniforme article avons proposé mesure évaluation Vraisemblance Généralisée permet évaluer cadre commun méthodes extraction thématiques calculer Dermouche mesure résultats méthodes transformés espace latent plonge documents espace latent thématiques mesure qualité permis comparer trois méthodes extraction thématiques corpus différents résultats donné avantage thode suivie résultats donnés méthode apprentissage étaient qualité inférieure termes mesure rapport autres méthodes semble conforme analyse qualitative thématiques extraites cette méthode effet dernières étaient mélangées difficiles interpréter serait intéressant complément travail tester comportement mesure corpus différents corpus apprentissage nécessiterait définition opérations prédiction méthodes extraction thématiques pouvoir affecter nouveaux documents thématiques serait également intéressant envisager analyses poussées vérifier introduit biais comparaison méthodes notre approche transformation résultats espace latent effet transformations employées jouent désavantage certaines méthodes dégradant ainsi artificiellement qualité leurs résultats Références Anaya Sánchez Porrata Berlanga Llavori document cluste algorithm topic discovering labeling Progress Pattern Recognition Image Analysis Applications Jordan Latent dirichlet allocation Journal Machine Learning Research Chang Gerrish Reading leaves humans interpret topic models Proceedings Annual Conference Neural Information Processing Systems Deerwester Dumais Furnas Landauer Harshman Indexing Latent Semantic Analysis Journal American Society Information Science Dempster Laird Rubin Maximum likelihood incomplete algorithm Journal Royal Statistical Society Series Methodological Simon equivalence nonnegative matrix factorization spectral clustering Mining fuzzy relative ISODATA process detecting compact separated clusters Journal Cybernetics Ferret Approches endogène exogène améliorer segmentation thématique documents Traitement Automatique Langues Gaussier Goutte Relation between implications Procee dings annual international SIGIR Conference Research Develop Information Retrieval Evaluation méthodes extraction thématiques Halkidi Batistakis Vazirgiannis clustering validation techniques Journal Intelligent Information Systems Harman Overview first conference Proceedings annual international SIGIR conference Research development information retrieval Hofmann Probabilistic latent semantic indexing Proceedings annual international SIGIR conference Research development information retrieval Seung Learning parts objects negative matrix facto rization Nature Seung Algorithms negative matrix factorization Advances neural information processing systems McCallum MALLET Machine Learning Language Toolkit Jordan Weiss spectral clustering Analysis algorithm Advances neural information processing systems Porrata Berlanga Llavori Shulcloper Building hierarchy events topics newspaper digital libraries Proceedings European conference research Language Environment Statistical Computing Salton vector space model automatic indexing munications Steinbach Karypis Kumar comparison document clustering niques workshop mining Zamir Etzioni Madani intuitive clustering documents Summary topic extraction methods issued various domains linguistics linear algebra statistics addition these specific methods adapt other methods ticular unsupervised machine learning results produced these methods differently formed document partitions matrices probability distributions words difference causes problem trying compare uniformly paper propose measure named Generalized Likelihood allows evaluation comparison ferent topic extraction methods results obtained corpus documents about french presidential election Associated Press corpus relevance proposed measure