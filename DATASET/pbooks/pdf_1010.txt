Champs Markov conditionnels traitement séquences1 Trinh Thierry Artières Université Paris capitaine Scott 75015 Paris France poleia Thierry Artieres Résumé modèles conditionnels modèles Markov entropie maximale champs Markov conditionnels apportent réponses lacunes modèles Markov cachés traditionnellement employés classification segmentation séquences modèles conditionnels essentiellement utilisés jusqu présent tâches extraction information étiquetage morphosyntaxique Cette contribution explore emploi modèles données nature différente signal telles parole écriture ligne proposons architectures modèles adaptées tâches lesquelles avons dérivé algorithmes inférence apprentissage correspondant fournissons résultats expérimentaux tâches classification étiquetage séquences Introduction classification segmentation étiquetage données séquentielles problématiques nombreux domaines comme bioinformatique reconnaissance écriture extraction information problématiques principales domaine consiste effet transformer séquence observée signal écrit exemple séquence étiquettes utilise également terme labels Cette tâche réalisée différents niveaux cherche segmenter signal écrit phrase séquence signal écrit chaque segmenté séquence caractères modèles Markoviens cachés constituent approche utilisée résoudre tâches reposent hypothèses indépendance fortes données soient appris façon discriminante dernier point vient modèles génératifs définissent probabilité conjointe séquence observations séquence étiquettes associée Diverses travail partie financé programme communauté européenne travers réseau Excellence PASCAL 506778 Champs Markov conditionnels traitement séquences méthodes proposées introduire information discriminante systèmes Markovien généralement basés modèles génératifs Jaakkola Bahlmann Moreno travaux reposent grande partie méthodes noyau machines vecteur support modèles apparus récemment visent palier ensemble défauts modèles conditionnels attachent modéliser probabilité conditionnelle citer modèles Markov entropie maximale McCallum champs Markov conditionnels Lafferty modèles essentiellement utilisés jusqu présent traitement documents textuels reconnaissance entités nommées extraction information étiquetage morphosyntaxique caractéristiques employées algorithmes apprentissage conséquent adaptés contextes applicatifs Cette contribution explore emploi modèles conditionnels reconnaissance données signal telles parole écriture ligne Plusieurs adaptations nécessaires abord observations nature différente généralement séquences vecteurs réels Ensuite classes souvent multimodales exemple plusieurs façons écrire Enfin étiquetage données phase apprentissage souvent partiel connaît classe séquence observations connaît séquence états correspondante alors algorithmes proposés littérature requièrent données totalement étiquetée suite commençons introduire modèles conditionnels présentons modèles conditionnels adaptés classes multimodales dérivons algorithmes apprendre modèles données partiellement étiquetées Enfin fournissons résultats expérimentaux tâches classification séquences reconnaissance caractères manuscrits ligne reconnaissance comportements utilisateur basant mouvements comparant modèles conditionnels modèles Markoviens standards Modèles conditionnels données séquentielles modèles présentés utilisés tâches classification segmentation principe suivant étiquetage segmentation séquence observations consiste identifier parmi toutes segmentations possibles meilleure séquence labels étiquettes telle maxarg maxarg maxarg YXPXYPY suite suppose toutes composantes parcourent alphabet exemple ensemble phrases langage naturel ensemble étiquetages morpho syntaxiques phrases ensemble étiquettes morpho syntaxiques possibles commençons décrire modèles données proviennent challenge Inferring Relevance Movements Challenge organisé réseau excellence PASCAL Markov cachés modèles Markov entropie maximale McCallum présentons ensuite champs Markov conditionnels Lafferty ainsi extension Markoviens Sarawagi Cohen Modèles Markov cachés défini structurellement ensemble états espace observation modèles continus alphabet observations dictionnaire modèles discrets également défini paramètres probabilité initial distributions conditionnelles SssssP représentent probabilités transiter probabilité conditionnelles observations représentent probabilités émission observations SssxP traditionnellement utilisés hypothèses données probabilité instant dépend instant précédent observation émise instant dépend instant exploitant hypothèses montrer probabilité jointe séquence états segmentation séquence observations définie yxPyyPyxPyPYXP inconvénients majeurs modèles génératifs définissent distribution probabilité jointe séquence observations labels moyen détourné apprendre modèle classification séquences équation autre hypothèses indépendance jacentes emploi fortes rarement vérifiées Modèles Markov entropie maximale modèles Markov maximum entropie modèles conditionnels permettent éviter partie limites modèles génératifs modèles définissent probabilité conditionnelle supposant séquence étiquettes obéit processus Markovien montre XyyPxyPXYP comparant équation remplacé probabilité transition probabilités conditionnelles forme probabilités émission apparaissent cette formule apprentissage ainsi focalisé différencie transition autre avoir modéliser complètement processus génération données modélise marginale modèles discriminants autre avantage vient cette modélisation réclame hypothèses particulières paramétrer probabilité Champs Markov conditionnels traitement séquences transition caractéristiques exploitant dépendances complexes entre observations séquence Lafferty toutefois évidence comportement indésirable appelé label décrivons succinctement problème vient probabilités transitions partir normalisées induit structure modèle telle successeur observation aucune influence décodage Champs Markov conditionnels champs Markov conditionnels instance particulière champs aléatoires derniers permettent estimer probabilité jointes graphe nœuds représentant variables aléatoires champs aléatoires conditionnels conditionnant valeurs noeuds représentant variables estimer labels valeurs noeuds représentant variables entrée figure illustre différences fondamentales entre modèles représentant forme modèles graphiques figure représentation graphique champ aléatoire conditionnel structure chaîne Cette représentation comparer celle celle Notons derniers modèles représentés graphes orientés permettent exprimer probabilité jointe conditionnelle suivant formules Notons également étant modèles conditionnels requièrent hypothèses particulières explique nœuds décomposés figures Représentation forme modèles graphiques nœuds grisés correspondent variables observées considérons champs aléatoires définis graphe nœuds liens conditionnellement séquence étiquettes obéit propriété Markovienne exprimée graphe nœuds ttVyXyPttyXyP signifie voisins graphe exploitant théorie champs aléatoires Lafferty montrer probabilité conditionnelle séquence étiquettes connaissant observation mettre forme WYXscoreWXYP YXFWeW facteur normalisation vecteur caractéristiques calculées fonction observation séquence états vecteur poids probabilité conditionnelle exprime fonction caractéristiques rassemblées vecteur définir explicitement théorie champs Markov aléatoires caractéristiques définies cliques maximales graphe structure chaîne caractéristiques définies états successifs ensemble fonctions caractéristiques Alors YXtfYXtfYXtfYXtfYXF caractéristiques typiquement utilisées problèmes étiquetage morpho syntaxiques exemple commence majuscule données comme parole écriture ligne définir caractéristiques niveau domaines signal entrée transformé après prétraitement séquence vecteurs caractéristiques réelles utilisera alors caractéristiques inférence réalisée algorithme programmation dynamique topologie graphe chaîne utilise algorithme Viterbi arbre utiliser algorithme Belief Propagation Weiss graphe quelconque utilise Loopy Belief Propagation Pearl Murphy apprentissage dispose apprentissage exemples complètement étiquetés séquence observations séquence étiquettes nœuds correspondant Durant apprentissage cherche paramètres maximisent vraisemblance conditionnelle ensemble apprentissage critère convexe maximisé méthode gradient écrit XZYXFWWXYPWL noter facteur normalisation implique somme nombre exponentiel séquences états possibles cette somme calculée efficacement algorithme programmation dynamique Champs Markov conditionnels Markoviens présentés précédemment permettent déterminer séquence labels probabilité maximale séquence observations donnée correspond problématiques comme étiquetage morpho syntaxique cherche étiquette chaque Cependant souhaiter travailler données séquentielles telles observations prises isolément moins segments plusieurs observations successives intégrer informations segmentales Sarawagi Cohen proposé extension appelée Markoviens Etant donnée observation cherche meilleure segmentation étiquetant TyyyY TxxxX TxxxX Champs Markov conditionnels traitement séquences segments sortie séquence JsssS désigne segment défini position début position étiquette caractéristiques calculées segment plutôt niveau éléments individuels graphe structure chaîne caractéristique segment exemple caractéristique segment calculée partir uniquement SXjfv apprentissage comme maximiser critère vraisemblance conditionnelle encore algorithme apprentissage proposé exploite apprentissage complètement étiquetée classification séquences classification données monomodales comme avons mentionné traditionnellement utilisés étiquetage données séquentielles concevoir système classification séquences utiliser architecture basée structure chaîne chaque classe structure chaîne effet naturelle modéliser séquences classe lesquelles distinguer différentes parties comme début milieu reconnaissance écriture manuscrite ligne exemple utilise modèle chaîne chaque lettre chaîne correspondant caractère premier noeud correspond début tracé second noeud partie intermédiaire utilise alors architecture mélange structures chaîne telle celle illustrée figure branche classe branches peuvent avoir nombres états différents choix priori structure modèle entendu représentation dynamique modèle similaire Figure différence vient toutes transitions entre nœuds autorisées Lorsque apprend modèle données classes apprend paramètres permettent discriminer mieux entre séquences observations différentes classes champ aléatoire classification séquences données monomodales Chaque branche correspond classe étiquetage disponible bases apprentissage classification signaux souvent minime réduit classe signifie segmentation correspondant séquence observations correspond branche particulière celle connaît segmentation précisément algorithmes apprentissage requièrent cette information montrons comment apprendre disposer cette information dispose apprentissage exemples séquence observations classe correspondant Alors probabilité conditionnelle écrire kXSFWe XSYPXYP désigne segmentation désigne ensemble segmentations séquence classe facteur normalisation Cette modélisation ressemble mélange modèles Quattoni limiter problèmes numériques simplifier implémentation choisir approximer quantité précédente XSYPXYP apprentissage estime paramètres maximisant vraisemblance conditionnelle segmentation disponible séquences apprentissage apprise mesure pendant apprentissage noter cause variables cachées critère plusieurs maximums locaux optimisation assure trouver optimum global données multimodales souvent classes multimodales modélisation précédente avérer insuffisante proposons modèles comme mélanges champs aléatoires conditionnels Plutôt avoir structure chaîne chaque classe considère plusieurs chacune pouvant spécialiser pendant apprentissage modalité classe difficulté apprentissage consiste encore manque étiquetage données apprentissage dispose information précédemment séquence apprentissage classe exemple séquence observations connaît allographe branche correspondant segmentation cette branche introduit alors deuxième variable cachée indicateur branche obtient XSBFW XSBYPXYP désigne branche désigne ensemble branches correspondant classe signification précédemment simplifier calculs implémentation choisir approximer quantité précédente approximant somme numérateur maximum comme Champs Markov conditionnels traitement séquences équation apprentissage estime paramètres maximisant vraisemblance conditionnelle segmentation disponible séquences apprentissage apprise mesure pendant apprentissage noter cause variables cachées optimisation assure trouver optimum global Modèle plusieurs modèles classe prendre compte allographes Mises œuvre expérimentales implémentations réalisées exploitant bibliothèque implémentant algorithme gradient méthode quasi Newton mémoire limitée LBFGS Nocedal Déterminer intérêt utilisateur mouvements étude réalisée cette partie réalisée cadre challenge organisé membres réseau excellence PASCAL challenge consiste explorer possibilité inférer intérêt utilisateur différentes lignes texte présentées écran typiquement retournées moteur recherches fonction mouvements Salojärvi données collectées façon suivante requête donnée présente utilisateur titres titre réponse tenant ligne correspondant réponses possibles Parmi titres correct pertinents pertinents observe mouvements utilisateur cherche parmi titres celui satisfait telle expérience requête titres réponses appelée suite assignement trajectoire segmentée fixations saccades détermine quels fixations correspondent Cette séquence ordonnée temporellement fixations transformée séquence vecteurs caractéristiques fixation dispose vingtaine caractéristiques fournies organisateurs challenge Salojärvi dispose également chaque fixation titre auquel appartient position titre longueur titre Puisque segmentation titres connue apprentissage reconnaissance avons calculé caractéristiques segmentales segment fixations correspondant titre vecteurs caractéristiques additionnés segment suite considère assignement représenté séquence vecteurs caractéristiques représente somme vecteurs caractéristiques correspondant visites titre visité cherche étiqueter cette séquence segmentation oùJyyY correspondent étiquettes Pertinent Pertinent Correct noter visites titre effet faire allers retours titres visiter titre plusieurs alors partagent étiquette classe avons utilisé divers modèles premier illustré figure modèle simple trois nœuds représente nœuds correspondant séquence observations chaque classe graphe considérer types caractéristiques caractéristiques locales caractéristiques transition reliant visites consécutives exemple avoir caractéristiques locales permet prendre compte valeurs absolues caractéristiques telles durée fixations titre pertinent utilise également caractéristiques transitions correspondent différence caractéristiques entre visites successives permet exemple prendre compte différence durée entre fixations titre pertinent celles titre pertinent Ainsi notant attributs calculés sommées ensuite segments caractéristiques employées attributs bruts caractéristiques locales différence attributs caractéristiques transition Représentation graphique modèle nœuds modèle états lequel distingue première visite visites ultérieurs titre Lorsqu utilisateur visite seconde troisième titre supposer visite façon informatif avons également utilisé modèles lesquels distingue première visite titre classe deuxième visite titre suffit multiplier nœuds Figure dernier titre visité souvent important souvent titre correct prendre compte automatiquement avons rajouté trois nœuds correspondant dernière visite décodage consiste tester parmi toutes possibilités étiquetage titres parmi celle probabilité maximale avons comparé systèmes conditionnels systèmes Markovien standard avons appris similaires états lesquels probabilités émission modélisées gaussiennes vecteurs Champs Markov conditionnels traitement séquences caractéristiques assignements apprentissage assignements performances calculées suivant procédure proposée Salojärvi évalue système réponses fournit titres effectivement visités tableau montre performances systèmes génératifs modèles conditionnels meilleurs résultats obtenus états alors montré performances supérieures nombre états allant jusqu lorsque distingue première visite dernière visite visites intermédiaires titre Méthode Performance états états états états Performances meilleur système Markovien systèmes étiquetage titres fonction mouvements Reconnaissance écriture manuscrite ligne signal écriture manuscrite ligne signal temporel constitué coordonnées successives stylo capturé tablette digitale stylo électronique UNIPEN Guyon internationale référence domaine reconnaissance écriture avons travaillé partie cette regroupant signaux scripteurs correspondant caractères Notre contient environ 60000 exemples utilisons apprentissage signal écriture étant variable existe nombreux allographes tracer caractère usage modèles mélanges systèmes basés prototypes tracés typiques extrêmement répandu systèmes performants souvent apprentissage modèles nombre allographes ainsi topologie modèles Markoviens modélisant doivent fixés Divers travaux menés apprendre complètement modèles caractères partir données Artières Gallinari basés construction partir tracé utilisent représentation tracés forme séquence codes directionnels système référence système détails peuvent trouvés Marukatat intérêts système référence Markovien réside capacité apprendre topologie nombre branches nombre états modèles partir données savons faire aujourd avons œuvre fixant topologie après celle apprise système Markovien consiste construire modèle multi branches reprenant topologies modèles appris utilisera mêmes caractéristiques celles utilisées tableau résume performances système Markovien systèmes classification caractères minuscules performances améliorent taille modèles Comme surpassent système Markovien toutes expériences Nombre modalités branches caractère Performance systèmes classification caractères minuscules encore systèmes surpassent système Markovien référence intéressant dernier système objet nombreux travaux depuis quelques années notre équipe niveau domaine reste néanmoins travail réaliser mettre œuvre algorithmes apprentissage encore relativement coûteux surtout structure déterminée après structure apprise système Markovien Conclusion avons exploré cette contribution emploi champs aléatoires Markoviens conditionnels classification étiquetage signaux abord sommes focalisés variantes exploitant notion segments caractéristiques segmentales avons développé algorithmes réaliser apprentissage modèles présence information segmentation minimale classe avons fourni résultats expérimentaux montrant modèles surpassent modèles Markoviens traditionnels tâches différentes classification données séquentielles Références Artières Gallinari Stroke level handwriting recognition International Workshop Frontiers Handwriting Recognition Bahlmann Haasdonk Burkhardt Handwriting Recognition using Support Vector Machines kernel approach International Workshop Frontiers Handwriting Recognition Nocedal limited memory method large scale optimization Mathematic Programming Champs Markov conditionnels traitement séquences Rapport stage Master Recherche Université Paris Septembre Artières Gallinari Sélection Modèles Méthodes Noyaux classification données séquentielles Guyon Schomaker Plamondon Liberman Janet UNIPEN project exchange recognizer benchmark International Conference Pattern Recognition Champs Markov conditionnels traitement séquences Jaakkola Diekhans Haussler Using Fisher kernel method detect remote protein homologies International Conference Intelligent Systems Molecular Biology Lafferty McCallum Pereira Conditional random fields Probabilistic models segmenting labeling sequence International Machine Learning Morgan Kaufmann Francisco driven design toplogy handwriting recognition International Journal Pattern Recognition Artificial Intelligence Marukatat approche générique reconnaissance signaux écrits ligne Thèse doctorat Université Paris McCallum Freitag Pereira Maximum entropy Markov models information extraction segmentation Moreno Vasconcelos Generative Model Based Kernel classification Multimedia applications Murphy Weiss Jordan Loopy belief propagation approximate inference empirical study Uncertainty Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann Quattoni Collins Darrel Conditional Random Fields Object Recognition Advances Neural Information Processing Systems Salojärvi Puolamäki Simola Kovanen Kaski Inferring Relevance Movements Feature Extraction Helsinki University Technology Publications Computer Information Science Report Sarawagi Cohen Markov Conditional Random Fields Information Extraction Advances Neural Information Processing Systems Weiss Correctness belief propagation Gaussian graphical models arbitrary topology Neural Computation Summary Maximum Entropy Markov Models Conditional Random Fields designed address limits hidden Markov models traditionally employed classification segmentation sequences These conditional models mainly introduced information retrieval tagging tasks paper investigates these models different nature speech online handwriting propose architectures adapted tasks derive inference training algorithms provide experimental results classification labelling tasks