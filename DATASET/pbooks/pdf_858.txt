Optimisation Primal Trinh Thierry Artières Université Pierre Marie Curie avenue Président Kennedy Paris France Trinh Thierry Artieres Résumé apprentissage optimisation directe primal étudié depuis quelques temps ouvre nouvelles perspectives notamment traitement données structurées proposons nouvel algorithme combine façon originale certain nombre techniques idées comme méthode gradient optimisation fonctions continues partout différentiables heuristique shrinking Introduction Machines Vecteurs Support méthode populaire appren tissage supervisé classification régression forme simple classification classes cette méthode basée classificateur linéaire séparant ensembles points hyperplan originale trouver hyperplan séparant mieux points maximisation marge entre hyperplan séparateur points apprentissage Cette formulation conduit problème optimisation fonction convexe contraintes linéaires Récemment extensions cette technique approche maximisation marge proposées traitement données structurées comme séquences arbres Tsochantaridis méthode originale Vladimir Vapnik résoudre problème optimisation contraintes consiste introduire multiplicateurs Lagrange chaque contrainte optimiser problème équivalent algorithme coûteux temps mémoire exemple espace mémoire nécessaire matrice noyau taille carré nombre exemples caractéristiques complexité rendent difficile emploi machines vecteurs support généralement méthodes maximisation marge certaines situations lorsque traite données structurées lorsque dispose grandes quantités données apprentissage Plusieurs voies dépasser problèmes posés optimisation cadre Certains travaux porté optimisation efficace contrôle nombre contraintes actives Joachims décomposition problème apprentissage Osuna dernier algorithme SVMLight exemple intéresse itération donnée nombre limité variables actives travaux récents porté optimisation directe forme primale usage fonction hinge permet ramener problème optimisation contraintes fonction objectif convexe difficulté dernières approches Optimisation Primal vient fonction hinge dérivable Divers travaux alors proposé utiliser version dérivable partout cette fonction hinge lissage utilisant quadratique méthodes optimisation standard peuvent pliquées exemple Chapelle montré était possible utiliser méthode Newton gradient conjugué autre approche optimiser primal consiste utiliser méthode gradient directement problème optimisation fonction objectif différentiable Zhang avantage cette approche simplicité vitesse convergence dépendante réglage gradient exception algorithme Pegasos Shalev Shwartz algorithme nécessitant réglage hyperparamètre gradient travail plaçons cadre optimisation fonction continue partout dérivable principale minimisation primal attaquée comme problème minimax ensemble fonctions quadratiques convexes définies espaces espace paramètres montrons fonction objectif dérivable frontières entre espaces définition frontières respondent hyperplans associés chaque exemple apprentissage analysant perplans espace paramètres décrivons méthode efficace calculer direction grande pente estimer optimal exploitant résultats posons nouvel algorithme optimisation compare favorablement algorithmes Pegasos batch décrivons abord cadre optimisation lequel plaçons outils allons utiliser Ensuite décrivons notre algorithme détails expérimentalement comparant algorithmes référence Préliminaires formalisons abord problème attaquons consiste optimi fonction objectif partout différentiable rappelons ensuite quelques résultats fonctions décrivons brièvement méthode classique optimisation Formalisation Considérons problème classification données entrée classes Considérons apprentissage exemples intéressons apprentissage sifieur linéaire ensemble paramètres classi fieur apprendre Notons divise espaces frontière hyperplan équation principale apprentissage vaste marge trouver hyperplan séparant mieux points maximisation marge entre hyperplan séparateur points apprentissage Cette formulation conduit problème optimisation fonction convexe contraintes linéaires contraintes hyper paramètre algorithme permet régler importance termes fonction objectif variables négatives représentent pénalités contrainte marge respectée exemple introduisant fonction hinge obtient problème équivalent contraintes Notons cette fonction objectif quadratique morceau convexe différentiable rapport certains points hyperplans hyperplan exemple apprentissage points appartiennent aucun hyperplans fonction objectif localement quadratique Chaque hyperplan divise espace espaces hyperplans divisent ainsi espace hypercubes définis chaque hypercube fonction objectif forme quadratique ensemble indices exemples violent contrainte marge construction ensemble identique hypercube pourquoi marquons dépendance Avant présenter notre algorithme détail commençons quelques résultats théoriques optimisation fonctions différentiables Généralités optimisation fonctions différentiables cette section présentons quelque résultats concernant optimisation fonctions convexes différentiables résultats leurs dérivations peuvent trouvés Bertsekas Demyanov Vasilev introduisons abord définition gradient différentielle fonction convexe fonction convexe vecteur appelé gradient point ensemble gradients appelé différentielle différentielle ensemble compact convexe Optimisation Primal Théorème Demyanov Vasilev condition nécessaire fonction conti partout dérivable éventuellement convexe atteindre nimum fonction convexe condition également suffisante alors direction direction gradient grande pente Proposition ensemble fonctions convexes alors Théorème Danskin ensemble fonctions convexes différentiables alors différentielle gradient signifie enveloppe convexe Méthode gradient façon optimiser problème équation utiliser méthode gradient converge minimum global Bertsekas comparerons notre algorithme méthode référence Shalev Shwartz chaque itération nouveau vecteur paramètres calculé gradient gradient quelconque fonction Descente gradient optimisation primal travail proposons appliquer algorithme descente gradient différence méthode gradient choisissons direction gradient selon grande pente plutôt prendre gradient quelconque différentielle proposons méthode optimale déterminer gradient algorithme résumé pseudo décrit dessous algorithme comme verrons complexité linéaire nombre exemples apprentissage chaque itération effet passe revue exemples apprentissage déterminer direction descente grande pente ensuite calculer gradient optimal cette direction décrivons maintenant étapes calcul direction grande pente gradient calcul gradient Entrées Sorties Initialization faire Calculer direction gradient grande pente alors return Calculer gradient optimal Direction gradient grande pente Rappelons souhaitons minimiser fonction objectif forme primale cherchons premier temps déterminer différentielle point commençons intéresser différentielle fonction élémentaire Cette fonction différentiable points point obtient appliquant théorème ailleurs point fonction dérivable différentielle réduite dérivée différentielle fonction élémentaire prime suivant yixiN yixiN Finalement utilisant proposition section précédente obtient intéressant noter cette formulation proche celle utilisée Eizin Plach vient présence fonctions hinge formalisation apprentisage perceptron écrire comme minimisation fonction linéaire morceaux somme fonctions hinge noter fonction objectif strictement convexe exister séparable finité solutions notre formalisation fonction objectif comporte terme quadratique termes hinge problème strictement convexe ailleurs notre fonction objectif maximiser marge conduire meilleures propriétés généralisation Optimisation Primal recherche direction gradient grande pente comme problème moindres carrés suivant argminβ βiyixi notons posant indépendant paramètres optimisation problème résolu procédures standard Vandenberghe obtient direction gradient grande pente direction donnée Théorème valeurs données Notons terminer alors fonction différentiable direction gradient grande pente simplement gradient optimal espace direction recherche déterminée comme décrit section précédente proposons méthode déterminer gradient façon optimale revient résoudre problème optimisation unidimensionnel suivant intéressons droite comportement cette droite avance droite direction examine croissant alors successivement croiser hyperplans frontières entre hypercubes traverser autres hypercubes passage fonction change fonction quadratique hypercube autre figure fonction cherchons optimiser fonction quadratique morceaux figure points différentiables correspondent intersections entre hyperplan séparateurs Recherche gradient optimal caractériser intersection existe entre droite hyperplan valeur particulière notons déterminée valeur positive signifie hyperplan devant direction valeur négative signifie contraire Imaginons perte généralité droite décrite faisant croître infini traverse successivement hyperplans Figure segment indice hypercube correspondant nieme segment dérivée rapport variable optimum cette dérivée existe nulle ηnopt ηnopt appartient effectivement segment dérivable cette valeur alors optimal ηnopt Cependant dérivable optimum figure signifie optimum correspond frontière entre segments optimal frontière hyperplan séparateur entre hypercubes optimum dehors nieme segment méthode simple Optimisation Primal vérifier nieme point intersection solution calculer alors solution sinon solution final notre algorithme trouver gradient optimal décrit pseudo dessous notons nombre hyperplans envisager priori inconnu nécessairement correspondant exemples lesquels existe intersection Entrées Sorties Initialization Estimer Estimer corespondant intersections hyperplans sélectioner negatives trier ordre croissant indices hyperplans exemples apprentissage correspondents faire ηnopt ηnopt alors return sinon ηnopt alors return ηnopt sinon Shrinking Complexité algorithme itératif présenté section précédente compare avantageusement autres algorithmes optimisation batch proposés récemment termes performance complexité algorithmique proposons variante beaucoup rapide algorithme Rappelons algorithme passe revue exemples apprentissage plutôt hyperplans correspondants déterminer direction descente grande pente ensuite calculer gradient optimal cette direction pratique souvent exemples contribuent estimation direction recherche estimation gradient optimal exemples appelons exemples actifs autres étant passifs espérer casser complexité algorithme réduisant problème estimation chaque itération considérant exemples actifs proposons utiliser méthode shrinking similaire utilisé techniques optimisation sélectionner itération donnée nombre restreint variables actives Joachims minimiser fonction objectif allons considérer chaque itération nombre limité hyperplans actifs cherchant optimiser fonction représente liste exemples actifs représente liste exemples inactifs violant marge itération optimisation réalisée comme décrit Section seule différence venant certains hyperplans considérés termes corres pondants rajoutés cherche solution espace délimité hyperplans actifs procédure sélection exemples hyperplans ristique initialisation exemples actifs itération considère comme actifs hyperplans proches solution courante obtenir optimisa rapide réduit moitié nombre hyperplans actifs chaque itération garantir solution correcte hyperplans sélectionnés restreint espace recherche boule centrée autour solution courante rayon calculée comme distance maximale entre solution courante hyperplans actifs sélectionnés heuristique permet éviter plupart temps traverser hyperplan inactif solution trouvée identique celle trouverait hyperplans actifs cette garantie raison laquelle avons choisi régulièrement réinitialiser réactivant hyperplans lorsque nombre inférieur seuil lorsque rayon tombe Cette procédure permet garantir convergence comme algorithme originel Section Entrées Sorties Initialisation faire Estimer direction recherche optimale considérant fonction objectif hyperplans actifs alors Arrêt Estimer optimal fonction objectif direction hyperplans actifs Contraindre nouvelle solution appartenir alors Complexité complexité chaque itération algorithme shrinking effet opérations coûteuses chaque itération estimation calculs utilisant shrinking sonnement complexité itération algorithme itération plusieurs cycles chaque cycle hyperplans actifs divise Optimisation Primal nombre hyperplans actifs chaque itération jusqu arriver nombre faible complexité cycle Expériences décrivons résultats expérimentaux obtenus classification images chiffres manuscrits MNIST1 contient 60000 exemples apprentissage 10000 exemples images dimension 28x28 avons prétraité données analyse composantes principales réduire dimension données dimensions garde composantes images principaux inertie prétraitement standard données décrit exemple avons comparé notre algorithme HyperPass Hyperplane Passenger rithme Pegasos méthode gradient dernier montré expérimentale efficace rapport méthodes optimisation bases données grand dimension abord comparons vitesse convergence Pegasos batch notre algorithme stratégie shrinking figure montre évolu primal fonction nombre passages données notre algorithme HyperPass converge beaucoup rapidement Pegasos Notons toutefois chaque passage données correspond itération HyperPass complexité tandis complexité itération Pegasos égale courbe Pegasos large valeur primal oscille beaucoup entre itérations successives MNIST Primal objectif avons également étudié problème version HyperPass shrinking observé algorithme converge après quelques cycles itérations algorithme 7ieme cycle correspond complexité comparons solutions HyperPass Shrinking version Pegasos beaucoup version batch complexité algorithmique tableau paramètre Pegasos représente nombre exemples utilisés faire gradient lecun mnist index correspond batch tableau solution Hyper meilleure celles Pegasos termes valeur primal erreur classification Méthodes nombre itérations complexité valeur primal erreur HyperPass Shrinking cycles 5446187 Pegasos 0695504 Pegasos 5464795 Pegasos 5464416 Comparaison HyperPass Shrinking Pegasos complexité temps Enfin comparons solution HyperPass Shrinking celles obtenues rithme Pegasos faisant tourner jusqu arriver valeur primal seuil égale celle obtenue HyperPass différentes valeurs tableau montre Pegasos debut lorsque petit vitesse convergence faible ensuite exemple passages arriver solution tandis passages arriver solution Méthodes nombre itérations complexité valeur primal erreur HyperPass Shrinking cycles 5446187 Pegasos 5545929 5455888 5447137 00001 5446285 Pegasos 5491473 5455721 5447172 00001 5446287 Pegasos 5540219 5455784 5447186 00001 5446285 Comparaison HyperPass Shrinking Pegasos tolérance texte Conclusions avons proposé travail nouvel algorithme apprentissage vaste marge machines algorithme optimise fonction objectif forme primale combinant méthode gradient résultats optimisation fonctions Optimisation Primal partout différentiables nature fonction objectif cherche minimiser exprime comme maximum fonctions quadratiques avons proposé version algorithme version exploitant stratégie shrinking beaucoup rapide aussi performante algorithmes héritent propriétés convergence algorithmes gradient avons comparé algorithmes récents avons démontré expériences supériorité résultats encourageants travaillons moment étendre travail plusieurs directions kernélisation méthode permettant prendre classifieurs linéaires calcul vitesse convergence Références Bertsekas Nedic Ozdaglar Convex analysis optimization Vandenberghe Convex Optimization Cambridge University Press Chapelle Training support vector machine primal Neural Computa Demyanov Vasilev Nondifferentiable optimization Eizinger Plach approach perceptron training Transactions Neural Networks Joachims Making large scale learning practical Advances Kernel thods Support Vector Learning Schölkopf Burges Smola Press Joachims Training linear linear SIGKDD International Conference Knowledge Discovery Mining LeCun Bottou Bengio Haffner Gradient based learning applied document recognition Proceedings Osuna Freund Girosi Training support vector machines application detection Shalev Shwartz Singer Srebro Pegasos Primal estimated gradient solver Press Tsochantaridis Hofmann Joachims Altun Support vector machine learning interdependent structured output spaces Zhang Solving large scale linear prediction problems using stochastic gradient descent algorithms Press Summary Learning through direct optimization primal recent studied problem since opens perspectives instance dealing structured outputs propose algorithm combines original number techniques ideas gradient methods optimization differentiable functions shrink heuristics