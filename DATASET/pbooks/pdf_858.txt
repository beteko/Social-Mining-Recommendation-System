optimis primal trinh thierry artier univers pierr mar cur avenu président kennedy paris franc trinh thierry artier résum apprentissag optimis direct primal étudi quelqu temp ouvr nouvel perspect trait structur proposon nouvel algorithm combin original nombr techniqu idé comm méthod gradient optimis fonction continu partout différenti heurist shrinking introduct machin vecteur support méthod populair appren tissag supervis classif régress form simpl classif class méthod classif linéair sépar ensembl hyperplan original trouv hyperplan sépar mieux maximis marg hyperplan sépar apprentissag formul conduit optimis fonction convex contraint linéair récent extens techniqu approch maximis marg propos trait structur comm séquenc arbre tsochantarid méthod original vladim vapnik résoudr optimis contraint consist introduir multipl lagrang chaqu contraint optimis équivalent algorithm coûteux temp mémoir exempl espac mémoir nécessair matric noyau taill carr nombr exempl caractérist complex rendent difficil emploi machin vecteur support général méthod maximis marg situat lorsqu trait structur lorsqu dispos grand quantit apprentissag plusieur voi dépass pos optimis cadr traval port optimis efficac contrôl nombr contraint activ joachim décomposit apprentissag osun derni algorithm svmlight exempl intéress iter nombr limit variabl activ traval récent port optimis direct form primal usag fonction hing ramen optimis contraint fonction object convex difficult derni approch optimis primal vient fonction hing dériv diver traval alor propos utilis version dériv partout fonction hing lissag utilis quadrat méthod optimis standard pliqu exempl chapel montr possibl utilis méthod newton gradient conjugu autr approch optimis primal consist utilis méthod gradient direct optimis fonction object différenti zhang avantag approch simpliqu vitess convergent dépend réglag gradient algorithm pegasos shalev shwartz algorithm nécessit réglag hyperparametr gradient travail plaçon cadr optimis fonction continu partout dériv principal minimis primal attaqu comm minimax ensembl fonction quadrat convex défin espac espac parametr montron fonction object dériv fronti espac définit fronti respondent hyperplan associ chaqu exempl apprentissag analys perplan espac parametr décrivon méthod efficac calcul direct grand pent estim optimal exploit résultat poson nouvel algorithm optimis compar favor algorithm pegasos batch décrivon cadr optimis plaçon outil allon utilis ensuit décrivon notr algorithm détail expérimental compar algorithm référent préliminair formalison attaquon consist optim fonction object partout différenti rappelon ensuit quelqu résultat fonction décrivon briev méthod classiqu optimis formalis considéron classif entré class considéron apprentissag exempl intéresson apprentissag sifieur linéair ensembl parametr class fieur apprendr noton divis espac fronti hyperplan équat principal apprentissag vast marg trouv hyperplan sépar mieux maximis marg hyperplan sépar apprentissag formul conduit optimis fonction convex contraint linéair contraint hyp parametr algorithm regl import term fonction object variabl négat représentent pénal contraint marg respect exempl introduis fonction hing obtient équivalent contraint noton fonction object quadrat morceau convex différenti rapport hyperplan hyperplan exempl apprentissag appartiennent hyperplan fonction object local quadrat chaqu hyperplan divis espac espac hyperplan divisent ains espac hypercub défin chaqu hypercub fonction object form quadrat ensembl indic exempl violent contraint marg construct ensembl ident hypercub marquon dépend présent notr algorithm détail commençon quelqu résultat théoriqu optimis fonction différenti général optimis fonction différenti présenton quelqu résultat concern optimis fonction convex différenti résultat dériv trouv bertsek demyanov vasilev introduison définit gradient différentiel fonction convex fonction convex vecteur appel gradient ensembl gradient appel différentiel différentiel ensembl compact convex optimis primal théorem demyanov vasilev condit nécessair fonction cont partout dériv éventuel convex atteindr nimum fonction convex condit égal suffis alor direct direct gradient grand pent proposit ensembl fonction convex alor théorem danskin ensembl fonction convex différenti alor différentiel gradient signif envelopp convex méthod gradient optimis équat utilis méthod gradient converg minimum global bertsek compar notr algorithm méthod référent shalev shwartz chaqu iter vecteur parametr calcul gradient gradient quelconqu fonction descent gradient optimis primal travail proposon appliqu algorithm descent gradient méthod gradient chois direct gradient grand pent prendr gradient quelconqu différentiel proposon méthod optimal détermin gradient algorithm résum pseudo decr algorithm comm verron complex linéair nombr exempl apprentissag chaqu iter pass revu exempl apprentissag détermin direct descent grand pent ensuit calcul gradient optimal direct décrivon mainten étap calcul direct grand pent gradient calcul gradient entré sort initializ fair calcul direct gradient grand pent alor return calcul gradient optimal direct gradient grand pent rappelon souhaiton minimis fonction object form primal cherchon premi temp détermin différentiel commençon intéress différentiel fonction élémentair fonction différenti obtient appliqu théorem ailleur fonction dériv différentiel réduit dériv différentiel fonction élémentair prim suiv yixin yixin final utilis proposit précédent obtient intéress formul proch cel utilis eizin plach vient présenc fonction hing formalis apprentisag perceptron écrir comm minimis fonction linéair morceau somm fonction hing fonction object strict convex exist sépar finit solut notr formalis fonction object comport term quadrat term hing strict convex ailleur notr fonction object maximis marg conduir meilleur propriet généralis optimis primal recherch direct gradient grand pent comm moindr carr suiv argminβ βiyix noton pos indépend parametr optimis résolu procédur standard vandenbergh obtient direct gradient grand pent direct théorem noton termin alor fonction différenti direct gradient grand pent simpl gradient optimal espac direct recherch détermin comm decr précédent proposon méthod détermin gradient optimal revient résoudr optimis unidimensionnel suiv intéresson droit comport droit avanc droit direct examin croiss alor success crois hyperplan fronti hypercub travers autr hypercub passag fonction chang fonction quadrat hypercub autr figur fonction cherchon optimis fonction quadrat morceau figur différenti correspondent intersect hyperplan sépar recherch gradient optimal caractéris intersect exist droit hyperplan particuli noton détermin posit signif hyperplan dev direct négat signif contrair imaginon pert général droit décrit croîtr infin travers success hyperplan figur segment indic hypercub correspond niem segment dériv rapport variabl optimum dériv exist ηnopt ηnopt appartient segment dériv alor optimal ηnopt cepend dériv optimum figur signif optimum correspond fronti segment optimal fronti hyperplan sépar hypercub optimum dehor niem segment méthod simpl optimis primal vérifi niem intersect solut calcul alor solut solut final notr algorithm trouv gradient optimal decr pseudo noton nombr hyperplan envisag prior inconnu nécessair correspond exempl lesquel exist intersect entré sort initializ estim estim corespond intersect hyperplan sélection negat tri ordre croiss indic hyperplan exempl apprentissag correspondent fair ηnopt ηnopt alor return ηnopt alor return ηnopt shrinking complex algorithm iter présent précédent compar avantag autr algorithm optimis batch propos récent term perform complex algorithm proposon vari rapid algorithm rappelon algorithm pass revu exempl apprentissag hyperplan correspond détermin direct descent grand pent ensuit calcul gradient optimal direct pratiqu exempl contribuent estim direct recherch estim gradient optimal exempl appelon exempl actif autr passif esper cass complex algorithm réduis estim chaqu iter exempl actif proposon utilis méthod shrinking similair utilis techniqu optimis sélection iter nombr restreint variabl activ joachim minimis fonction object allon chaqu iter nombr limit hyperplan actif cherch optimis fonction représent list exempl actif représent list exempl inact viol marg iter optimis réalis comm decr ven hyperplan term corr pond rajout cherch solut espac délim hyperplan actif procédur sélect exempl hyperplan ristiqu initialis exempl actif iter comm actif hyperplan proch solut cour obten optimis rapid réduit moiti nombr hyperplan actif chaqu iter garant solut correct hyperplan sélection restreint espac recherch boul centr autour solut cour rayon calcul comm distanc maximal solut cour hyperplan actif sélection heurist évit temp travers hyperplan inact solut trouv ident cel trouv hyperplan actif garant raison laquel avon chois réguli réinitialis réactiv hyperplan lorsqu nombr inférieur seuil lorsqu rayon tomb procédur garant convergent comm algorithm originel entré sort initialis fair estim direct recherch optimal fonction object hyperplan actif alor arrêt estim optimal fonction object direct hyperplan actif contraindr nouvel solut apparten alor complex complex chaqu iter algorithm shrinking oper coûteux chaqu iter estim calcul utilis shrinking complex iter algorithm iter plusieur cycl chaqu cycl hyperplan actif divis optimis primal nombr hyperplan actif chaqu iter arriv nombr faibl complex cycl expérient décrivon résultat expérimental obtenus classif imag chiffr manuscrit mnist1 contient 60000 exempl apprentissag 10000 exempl imag dimens 28x28 avon prétrait analys compos principal réduir dimens dimens gard compos imag principal inert prétrait standard decr exempl avon compar notr algorithm hyperpass hyperplan passeng rithm pegasos méthod gradient derni montr expérimental efficac rapport méthod optimis grand dimens comparon vitess convergent pegasos batch notr algorithm strateg shrinking figur montr évolu primal fonction nombr passag notr algorithm hyperpass converg rapid pegasos noton chaqu passag correspond iter hyperpass complex tand complex iter pegasos égal courb pegasos larg primal oscill iter success mnist primal object avon égal étudi version hyperpass shrinking observ algorithm converg quelqu cycl iter algorithm 7iem cycl correspond complex comparon solut hyperpass shrinking version pegasos version batch complex algorithm tableau parametr pegasos représent nombr exempl utilis fair gradient lecun mnist correspond batch tableau solut hyp meilleur cel pegasos term primal erreur classif méthod nombr iter complex primal erreur hyperpass shrinking cycl 5446187 pegasos 0695504 pegasos 5464795 pegasos 5464416 comparaison hyperpass shrinking pegasos complex temp comparon solut hyperpass shrinking cel obtenu rithm pegasos tourn arriv primal seuil égal cel obtenu hyperpass tableau montr pegasos debut lorsqu pet vitess convergent faibl ensuit exempl passag arriv solut tand passag arriv solut méthod nombr iter complex primal erreur hyperpass shrinking cycl 5446187 pegasos 5545929 5455888 5447137 00001 5446285 pegasos 5491473 5455721 5447172 00001 5446287 pegasos 5540219 5455784 5447186 00001 5446285 comparaison hyperpass shrinking pegasos toler conclus avon propos travail nouvel algorithm apprentissag vast marg machin algorithm optimis fonction object form primal combin méthod gradient résultat optimis fonction optimis primal partout différenti natur fonction object cherch minimis exprim comm maximum fonction quadrat avon propos version algorithm version exploit strateg shrinking rapid auss perform algorithm héritent propriet convergent algorithm gradient avon compar algorithm récent avon démontr expérient supérior résultat encourag travaillon moment étendr travail plusieur direct kernélis méthod prendr classifieur linéair calcul vitess convergent référent bertsek nedic ozdaglar convex analys optimiz vandenbergh convex optimiz cambridg university press chapel training support vector machin primal neural comput demyanov vasilev nondifferenti optimiz eizing plach approach perceptron training transact neural network joachim larg scal learning practical advanc kernel thod support vector learning schölkopf burg smol press joachim training linear linear sigkdd international conferent knowledg discovery mining lecun bottou bengio haffn gradient based learning applied docu recognit proceeding osun freund giros training support vector machin appliqu detect shalev shwartz sing srebro pegasos primal estimated gradient solv press tsochantarid hofmann joachim altun support vector machin learning interdependent structured output spac zhang solving larg scal linear predict stochastic gradient descent algorithm press summary learning direct optimiz primal studied sinc perspect instanc dealing structured output propos algorithm combin original numb techniqu ide gradient method optimiz differenti function shrink heuristic