Representative training classification variability empirical distributions Saaid Baraty Simovici University Massachusetts Boston sbaraty Abstract propose novel approach estimation training needed constructing valid models machine learning mining provide representation underlying population without making distributional assumptions technique based computation standard deviation statistics series samples successive statistics relatively close assume samples produced represent adequately underlying distribution population models learned these samples behave almost models learned entire population validate results experiments involving classifiers various levels complexity learning capabilities Introduction Estimating sample allows inference model important learning process determine minimum sample which likely representative underlying population Models learned these samples behave almost models learned entire population increase sample would result insignificant increases quality models determine sample sizes sufficient ensure these samples quately represent underlying population These samples training structing models comparable performance those inferred entire population cheaper build Sections describe detail approach finding sample population respectively experimental presented Section Estimating sample attributes possible states attribute assumed finite commonly referred domain notion domain attributes extended attributes defining attributes Estimating sizes training dataset multi tuples multiplicity member numberMD which equals number occurrences tuple denote unknown joint probability distribution where Define active domain AdomU where specified parameter which refer outlier threshold Definition extracted frequency vector AdomDU extra tuple account those tuples consid outliers andMS otherwise Since tuples sample regard frequency vector arbitrary sample fixed random vector distribution Multinomial where Define statistics sample outlier threshold λwith respect target probability distribution refer statistics because Multinomial distribution random variable converges distribution distribution degree freedom Pearson measure close representing target distribution increase sample strong large numbers becomes smaller estimate sample extracted frequency vectors samples likely closely represent empirical distribution those tuples outliers Therefore specify target distribution empirical distribution define statistics sample outlier threshold respect empirical distribution repeatedly drawn samples fixed Given threshold compute followed process summarized Algorithm where standard deviation among values different iterative estimation sample ulation apply approach estimate sample population without having Since unknown assume AdomU outlier threshold Baraty Simovici Algorithm pseudocode finding sufficient training foreach sample smallest largest samples replacement compute standard deviation sequence output sample sample Definition extracted frequency vector population sample outlier threshold where Definition cases AdomU extra tuple account those tuples considered outliers andMS otherwise AdomU Informally consider sample representative population closely approximates population distribution vector tuples AdomU Similar previous section treat arbitrary population sample random vector Multinomial where However probabilities unknown Hence define random probability vector represent occurrence dimensional probability distribution underlying distribution population represented probability space dimensional probability tribution vectors where sample space standard simplex random variable itself values approximate statistics population sample respect underlying distribution conditional expected value given another sample population conditioned sample approximates shape probability distribution second order distribution large enough unbiasedly represent underlying distribution population sequence pendent samples drawn uniformly random replacement underlying population compute conditional expected value statistics statistics statistics substitute actual statistics respect target distribution compute standard deviation among statistics given large enough probability distributions captured frequencies extracted would similar similar other Therefore variation statistics expected small observe further assume prior Dirichlet shown follows Dirichlet distribution Dirichlet order replacement simple random samples where domain dependent multiple larger observation relative reliable conclusion process evaluated standard deviation among conditional expectations sufficiently small stabilizes certain value choose value threshold Estimating sizes training samples adequate training evaluation Otherwise increase repeat process iterative process expand observation stantial multiple observations update tuples considered according outlier threshold subsequently dimensions probability space Observe expand observation becomes closer approximation AdomU following docode explains process finding sample population explained section Algorithm pseudocode finding sufficient training popula foreach sample smallest largest expand evaluate based independent samples replacement compute standard deviation sequence output sample Experimental results first experiment employed Algorithm estimate sample Marketing which contains records Figure standard deviation drops minimal level around sample likely fairly represent entire which 00039 training sample suitable experiment evaluated approach determining represen tative sample population summarized Algorithm simulated process gathering observations population order expand observation synthetically generating tuples attributes using multinomial distribution randomly selected parameters executed Algorithm generated hundred samples synthetic learn nearest neighbor classifier sample evaluated prediction performance classifiers using fixed which large enough represent unbiasedly underlying distribution domain average standard deviation percentage correctly classified instances shown Figure Similar results obtained Bayesian Networks other experiments naive Bayes classifiers yield quite different results shown Figure improvement average percentage result increasing Baraty Simovici Standard deviation statistics samples respect changes Marketing curve corresponds particular value outlier threshold listed right Average sample smaller previous cases average percentage reaches sample slightly decreases constant level afterwards Finally standard deviation percentage converges slower previous cases These differences naive Bayes classifiers dependent global joint probability distribution classifiers Bayesian networks because naive independence assumption experimental results sense beyond determine because improvement performance insignificant istent evaluated training prohibitively large reduce sample approximation analyzing context specific classifier References Pearson criterion given system deviations probable correlated system variables reasonably supposed Estimating sizes training average standard deviation percentage correctly classified instances naive Bayes classifiers arisen random sampling Philosophical Magazine Laureano Cortez Using Mining Direct Marketing Application CRISP Methodology Proceedings European Simulation Modelling Conference Portugal Schmidtmann Hammer Sariyar Gerhold Evaluation sregisters Schwerpunkt Record Linkage Technical Report IMBEI Résumé proposons nouvelle approche estimation taille ensembles prentissage nécessaires construire modèles valides extraction connais saices visons fournir bonne représentation ensemble données faire hypothèses répartition Notre technique basée calcul écart statistiques série échan tillons Lorsque statistiques successives relativement proches supposons échantillons produits représentent adéquatement vraie distribution jacente popula modèles tirés échantillons comportent presque aussi modèles appris ensemble population validons résultats travaux expérimentaux impliquant variété sificateurs