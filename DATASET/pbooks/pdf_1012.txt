Classification tableau contingence modèle probabiliste Gérard Govaert Mohamed Nadif Heudiasyc Université Technologie Compiègne 20529 60205 Compiègne Cedex France gerard govaert Université Saulcy 57045 Cedex France mohamed nadif Résumé dernières années classification croisée classification blocs recherche simultanée partition lignes partition colonnes tableau données devenue outil utilisé fouille données domaine information présente souvent forme tableaux contingence tableaux occurrence croisant dalités variables qualitatives article étudions problème classification croisée données appuyant mélange probabiliste utilisant approche vraisemblance classifiante proposons algorithme classification croisée maximisation alternée vraisemblance associée mélanges multinomiaux classiques montrons alors certaines contraintes restrictives retrouve critères information mutuelle résultats données simulées données réelles illustrent confirment efficacité intérêt cette approche Introduction classification automatique comme plupart méthodes analyse données considérée comme méthode réduction simplification données données mettent ensembles fréquent classification automatique faisant porter structure recherchée ensembles façon dissymétrique privilégie ensembles contrairement exemple analyse factorielle correspondances obtient simultanément résultats ensembles alors intéressant rechercher simultanément partition ensembles approche suscité récemment beaucoup intérêt divers maines celui biopuces objectif caractériser groupes gènes groupes conditions expérimentales encore celui analyse textuelle objectif caractériser classes documents classes Notons domaine données présentent généralement forme tableau contingence chaque cellule correspond nombre occurrences document Classification tableau contingence modèle probabiliste ailleurs modèles mélange probabilité McLachlan supposent échantillon formé populations caractérisées chacune distribution probabilité modèles intéressants classification permettant donner probabiliste divers critères classiques autre proposer nouveaux algorithmes généralisant exemple algorithme classique means cadre classification croisée ainsi montrer algorithme Crobin Govaert adapté données binaires comme version classifiante algorithme block Govaert Nadif particulièrement simple mélange Bernoulli papier proposons étendre travail classification croisée tableau contingence section définirons modèle mélange croisé adapté données section consacrée présentation algorithme Cemcroki2 jectif maximisation vraisemblance classifiante associée modèle précédent montrerons section liens algorithme critères formation mutuelle section résultats données simulées données réelles confirmeront efficacité algorithme intérêt notre approche considérée comme approche complémentaire analyse correspondances représentation données Notations texte notera tableau contingence construit ensembles ayant respectivement éléments somme éléments tableau marges utilisera aussi tableau fréquences relatives marges profils ligne partition classes ensemble notée classe sinon adoptons mêmes notations partition classes ensemble ailleurs simplifier présentation sommes produits portant seront indicés respectivement lettres indiquer bornes variation seront implicites Ainsi somme portera toutes lignes allant colonnes allant classes ligne allant classes colonne Modèle mélange croisé aborder problème classification croisée aspect modèle mélange avons proposé Govaert Nadif modèle densité écrit forme ϕziwj densités appartiennent famille densités probabilité proportions classes paramètre dépendra situation étudiée représentent respectivement ensembles partitions classes classes Govaert Nadif adapter modèle tables contingence suppose chaque valeur observée table réalisation variable aléatoire suivant Poisson paramètre αiβjδk premiers termes expriment effets ligne colonne dernier correspond effet recherche partition appuyant modèle consiste maximiser vraisem blance classifiante associée notre modèle assurer identifiabilité modèle avons ajouté conditions zikαi problème classification alors trouver partitions paramètre modèle maximisant critère zikwj Algorithme classification croisée maximiser proposons maximiser alternativement cette fonction fixant posant montrer décompose termes premier correspond vraisemblance conditionnelle associée mélange distributions multinomiales appliquées échantillons second terme dépend alors utiliser algorithme classique Celeux Govaert obtenir partition faisant travail analogue recherche partition obtient finalement algorithme Cemcroki2 suivant Choix position initiale Répéter calcul partir jusqu convergence Calcul utilisant algorithme données partir Calcul utilisant algorithme données partir expressions estimations paramètres modèle associés chaque données représente cardinal ensemble Classification tableau contingence modèle probabiliste Liens information mutuelle Après étape maximisation critère écrit information mutuelle associée couple partitions utilisant approximation obtient aussi ainsi observer lorsque proportions fixées maximisation équivalente maximisation information mutuelle approximativement équivalente maximisation critère maximisation critère utilisé exemple algorithme Croki2 Govaert information mutuelle utilisée exemple Dhillon supposent implicitement données issues mélange croisé distributions Poisson proportions égales algorithme proposons considéré comme généralisation algorithmes Expérimentations numériques Données simulées illustrer comportement notre algorithme Cemcroki2 comparer rithme Croki2 avons étudié leurs performances données simulées avons sélectionné types données provenant mélange croisé Poisson classes ligne colonne avons retenu situations proportions égales avons degré mélange taille données chacun types données avons généré échantillons chaque échantillon avons lancé algorithmes Cemcroki2 Croki2 partir situations initiales aléatoires sélectionné meilleure solution résumer comportement algorithmes avons utilisé erreur classification entre partitions simulées partitions obtenues chaque algorithme quelques exemples degré mélange avons reporté figures moyennes erreur obtenus échantillons premières expériences montrent toutes situations particulier tailles échantillon suffisamment grandes algorithme Cemcroki2 donne résultats algorithme Croki2 obtient résultats uniquement proportions égales Govaert Nadif Taille échantillon Degré mélange Taille échantillon Degré mélange Taille échantillon Degré mélange Moyennes erreur Cemcroki2 ligne continue Croki2 ligne tillée Taille échantillon Degré mélange Taille échantillon Degré mélange Taille échantillon Degré mélange Moyennes erreur Cemcroki2 ligne continue Croki2 ligne tillée Données réelles illustrer algorithme Cemcroki2 donnés réelles avons choisi données SMART cornell smart données définies partir résumés issus Medline résumés issus résumés issus CRANFIELD sélectionnant alors intéressants Dhillon définit ainsi données Classic3 avons alors comparé résultats obtenus Dhillon Dhillon algorithmes classification croisée noterons A2001 A2003 obtenus notre algorithme Cemcroki2 table montre matrices confusion obtenues respectivement Cemcroki2 A2001 A2003 apparaît clairement Cemcroki2 fournit meilleurs résultats nombre documents classés contre algorithmes A2001 A2003 Cemcroki2 A2001 A2003 Classification tableau contingence modèle probabiliste Conclusion utilisant modèle mélange croisé distributions Poisson avons proposé algorithme Cemcroki2 montré pouvait comme extension Croki2 permet interpréter algorithme Croki2 déduire exemple utilisation information mutuelle supposent implicitement égalité proportions classes Cette approche permet alors prendre considération nouvelles situations comme celles proportions classes différentes premières expériences données simulées réelles montrent nouvel algorithme apparaît clairement meilleur Croki2 cette situation Références Celeux Govaert classification algorithm clustering chastic versions Computational Statistics Analysis Dhillon clustering documents words using bipartite spectral graph partitio Seventh SIGKDD Conference Francisco California Dhillon Mallela Modha Information theoretic clustering Procee dings Ninth SIGKDD International Conference Knowledge Discovery Mining Govaert Classification croisée Thèse Université Paris France Govaert Nadif Clustering block mixture models Pattern Recognition Govaert Nadif algorithm block mixture model sactions Pattern Analysis Machine Intelligence McLachlan Finite Mixture Models Wiley Summary methods statistical analysis concerned understanding relationships among variables categorical variables these relationships usually studied summarized contingency table giving frequencies observations cross classified variables classify columns simultaneously tingency table Croki2 which employed jointly correspondence analysis paper using Poisson block mixture model proposed Cemcroki2 algorithm which viewed extension Croki2 setting probabilistic interpretation Croki2 constitutes interesting support consider various situations avoids development methods example allows account situations which proportions clusters different applying Cemcroki2 whereas mutual information criteria assume equal proportions implicitly periments algorithm appears clearly better Croki2 situations proportions necessary equal