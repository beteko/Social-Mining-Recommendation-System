shift Clustering scalable distribué Hanane Azzag Mustapha Lebbah Duong Christophe Cérin Laboratoire Informatique Paris Université Paris Paris 93430 Villetaneuse France Email prénom paris13 Résumé présentons papier nouvel algorithme Shift lisant proches voisins montée gradient Nearest Neighbours Shift computationnel intensif dernier temps limité utilisation données complexes partition nement clusters ellipsoïdaux serait bénéfique implémentation scalable algorithme compense augmentation temps exécution fonction taille données raison complexité quadra tique pallier problème avons introduit Locality Sensitive Hashing approximation recherche proches voisins ainsi règle empirique choix combinaison améliorations offre opportunité traitement pertinent problématiques clustering appliquée données massives Introduction objectif recherche supervisée affecter label points labélisés nombre emplacement clusters inconnus sommes concentrés algorithme clustering modal nombre clusters défini terme modes locaux fonction densité probabilité génère données connu algorithmes clustering modal means Comme dernier distribution mélange normale contraint trouver clusters ellipsoidaux inapproprié données complexes shift généralisation means raison capacité calculer clusters topologie aléatoire définis comme bassins attractions modes locaux générés montée gradient Fukunaga Hostetler calculer chemins montée gradient proches voisins appropriés adaptent topologie locale données version actuelle proches voisins shift contient goulots étranglement posés grille recherche multiple choix nombre voisins optimal calcul exact proches voisins proposons nouvel algorithme résout gouffres computationnels échelle normale efficace choix nombre proches voisins évite recherche grille locality sensitive hashing version approximée proches voisins implémentation MapReduce distribuée shift Clustering scalable proches voisins approximés Méthode shift Shift introduit Fukunaga Hostetler génère point dimension séquence points suivent chemin montée densité gradient utilisant relation récurrence échantillon aléatoire obtenu fonction densité commune proches voisins distance proche voisin équation donne Shift raison déplacement successif itérations moyenne proches voisins prochaine itération convergence séquence local version noyau équation établie Comaniciu large classe noyaux fenêtres fixées Cette convergence reste valide quand fenêtre remplacée distance proches voisins décroit augmentation nombre itérations chemin montée gradient modes locaux produit équation forme bases Algorithme notre méthode proches voisins shift entrées échantillons données points candidats souhaitons clusteriser peuvent prérequis paramètres réglage suivants nombre proches voisins seuil lequel convergence itérations considérée comme étant sufisante nombre maximum itérations seuil lequel itérés finaux considérés comme étant membres cluster cardinalité minimale clusters formés sorties labels clusters points candidats trois routines Algorithme lignes correspondent formation chemins montée gradient équation itérés jusqu distance dernière itération infèrieure nombre maximum itérations atteint sorties lignes itérés finaux lignes concernent fusion itérés finaux cluster lorsque distance séparant seuil créant regroupement initial lignes déterminent petits clusters cardinalité supérieure sinon fusionne clusters concernés voisin proche produire ligne assigne labels clusters données originales Choix nombre proches voisins suivant échelle normale paramètre réglage critique shift choix nombre proches voisins travaux pionniers Loftsgaarden Quesenberry Fukunaga Hostetler Algorithm proches voisins shift exacts proches voisins Entrées Sorties Calcul chemin montéé gradient while Création clusters fusions itérés finaux Fusion petits clusters cluster cardinalité minimale while proche cluster cluster cardinalité minimale établissent erreur quadratique optimale sélecteurs locaux globaux estimateurs densité proches voisins sachant auteurs considèrent sélecteurs basés données grille recherche basée données cherche minimiser indices qualité recherche supervisée comme indice Silhouette considéré Notre proposition échelle normale sélecteur hyper volume sphère unitaire dimentionnel dérivation équation donnée Duong assertion sélection paramètres réglages basés gradient densité plutôt densité adéquate shift Chacón Duong complexité contraste grille recherche sélectionner nombre optimal proches voisins sachant nombre recherches valeurs possibles usuellement réglé proportionnel proches voisins approximés Locality Sensitive Hashing tâche calculatoire intensive calcul proches voisins plutôt sélection nombre proches voisins effet chaque point candidat requiert calcul distance usuels ordre grandeur empêche application données importants approche réduction complexité shift Clustering scalable proches voisins approximés prometteuse tient calcul approximés proches voisins plutôt exacts proches voisins Parmi celles existantes locality sensitive hashing introduit Datar Datar approche probabiliste basée projection scalaire aléatoire points multivariés variable aléatoire normale variée variable aléatoire uniforme prise table hashage blocs basés valeurs entières alors construite raison propriétés statistiques distribution normale points proches espace multidimensionnel départ auront tendance tomber mêmes blocs scalaires points distants tomberont blocs différents comme vérifié Slaney Casey importantes valeurs impliqueront moins blocs précision préservation caractéristiques tandis petites valeurs entraineront blocs moins précision avons préféré paramétriser nombre blocs table hashage avons perte généralité projections scalaires ensuite triées ordre statistique valeur hashée index intervalle lequel tombe fonction indication chercher approximés proches voisins réservoir potentiels proches voisins réglé valeur contenant valeur hashage réservoir élargi nécessaire concaténation blocs voisins approximés proches voisins proches voisins contenus réservoir réduit distance seuil proches voisins erreur approximation proches voisins induite recherche plutôt toutes données probabilistiquement contrôllée Slaney Casey Algorithme NNLSH approximation recherche proches voisins fonction hashage fourni équation entrées échantillons données lignes chaque point candidat approximés proches voisins calculés partir réservoir proposition NNLSH intégrée faite réduit complexité nombre blocs paramètre réglage crucial Malgré intérêt Peled existe méthode optimale sélectionner nombre blocs examinerons heuristiques performance prochaine section Implémenter approximatifs proches voisins manière distribuée processus maître processus esclaves réduit complexité notre proposition DNNMS Algorithme entrées sorties mêmes Algorithme itération chemins montée gradient collectés matrice lignes itère jusqu convergence globale jusqu nombre maximal itérations Certains calculs redondants effectués lorsque certains convergé cette forme calcul nécessaire parallélisation effective MapReduce Ghemawat paradigme MapReduce efficace algorithmes séries repensés passant itération chaque candidat itération ensemble candidats simultanément lignes décrivent fusion regroupements reprise Algorithme modification majeure étant donné MapReduce requis Algorithm NNLSH Approximés proches voisins Entrées Sorties Création tables hashage blocs Recherche approximés proches voisins blocs adjacents while adjacent Algorithm DNNMS proches voisins shift distribué approximés proches voisins utilisant Entrées Sorties Calcul chemins montée gradient while Algorithme Identique lignes Algorithme Résultats expérimentaux Influence paramètres parallélisation Nombre noeuds observer figure notre Scala Spark implémentation montée gradient proches voisins scalable temps exécution diminue efficace nombre noeuds esclaves Après investigations lacunes notre première implémentation avons observer étape labélisation était scalable comme montré figure Cependant réutilisant segmenter tâches comme shift Clustering scalable proches voisins approximés Algorithm Labélisation distribuée proches voisins approximés Entrées Sorties Création tables hashages blocs Labelisation données Calcul barycentres barycentre clusters Fusion proches cluster Fusion décrit algorithme avons observé scalabilité solution réprésentée figure troisième étape consistant fusionner petits clusters leurs proches voisins déroule localement noeud maître prend coordonnées barycentres cardinalité cluster associé sortir labélisation finale appliquée parallèle pratique valeurs choisies cette étape immédiate mauvais choix valeurs entrainer génération grand nombre clusters faire exploser temps exécution Nombre esclaves Nearest neighbours gradient ascent Nombre esclaves Première implémentation labélisation Nombre esclaves Nouvelle labélisation Amélioration scalabilité Montée gradient Première labélisation Nouvelle implémentation Influence nombre blocs Notre nouvelle implémentation avantage durant phase montée gradient aussi pendant étape labélisation paramètre celui nombre blocs laisse dernier constant comme présenté Figure constate complexité quadratique montée gradient proches voisins persiste comme étape labélisation Cependant observer Figure Taille donné Montée gradient blocs Taille donné Labélisation blocs Influence taille données nombre constant gradient labélisation Nombre blocs Montée gradient proches voisins esclaves esclaves esclaves Nombre blocs Labélisation Influence nombre blocs données taille gradient labélisation temps exécution diminuer rapidement fonction nombre blocs ralentir atteindre seuil dépendra nombre données entrées observation intéressante concerne augmentation linéaire temps exécution lorsqu nombre élément constant augmentation taille données comme illustré Figure résultat permet conforter notre version Shift algorithme pleinement scalable Application segmentation image résurgence intérêt algorithme shift application segmen tation image Comaniciu image transformée espace colorimétrique lequel chaque cluster correspond régions segmentées image originale dimentionnel couleur Pratt choix commun Sachant image tableau bidimentionnel pixels disons indices lignes colonnes pixel informations spatiales colorimétriques pixel peuvent concaténées vecteur dimensionnel jointure domaines shift Clustering scalable proches voisins approximés Taille données Montée gradient éléments blocs éléments blocs éléments blocs Taille données Labélisation éléments blocs Influence taille données montée gradient labélisation spatio colorimétriques illustrer prenons image entrainement coloré Berkeley Segmentation Dataset Benchmark retrouve pixels originaux image scatter 154401 jointure coordonnées spatio colorimétriques Spatial range Representations couleurs image Image pixels Scatter 154401 transformé espace algorithme segmentation image shift noyau avait introduit avons adapté usage paramètres réglages DNNMS marge maximale portée données exécutons DNNMS blocs temps exécution respectivement minutes amélioration significative comparaison calcul nécessaire ordinateur bureau standard Quant qualité segmentation images exacts proches voisins Cette image segmentée offre considérable réduction complexité image détectant centres fleurs incluant certains détails granularité contours bords pétales certaines ombres berkeley Research Projects vision ainsi différents feuillages arrière preuve raison nature aléatoire projection approximer proches voisins DNNMS clusters moins compacts diffus projections utilisées DNNMS approximés proches voisins blocs certains détails moins visibles absence DNNMS DNNMS centres jaunes fleurs moins clairement délimités pétales considérables épanchements pétales feuillage observons blocs choix fortuit comme aussi amples investigations requises choix optimal général night DNNMS DNNMS DNNMS Segmentation image colorée proches voisins shift proche voisin exact shift série DNNMS approximatifs proches voisins shift distribué blocs temps exécutions respectivement minutes Berkeley Segmentation Dataset Benchmark fournit segmentation image humaine leurs images comparaisons trouvent détections bordures faites utilisateur utilisateur concentre segmentation feuillage arrière contour forme fleurs ignorant détails pétales fleurs utilisateur quant concentre segmentation pétales individuelles premier portons notre attention DNNMS DNNMS DNNMS donne qualité insuffisante détection bords DNNMS capables segmenter seule exécution unique paramètres réglage simultanément feuillage arrière forme pétales premier ainsi segmentation automatique combinant résultat experts humains focalisant différentes zones image shift Clustering scalable proches voisins approximés Utilisateur Utilisateur night DNNMS DNNMS DNNMS Détection bordure image segmentée experts humains utilisateur série exacts proches voisins DNNMS distribué approximatifs proches voisins blocs Conclusion avons introduit plusieurs améliorations algorithme proches voisins shift première heuristique choix valeur optimale nombre proches voisins seconde emploi approximation proches voisins locality sensitive hashing phase montée gradient aussi phase labélisation troisième implémentation écosystème distribué avons démontré améliorations diminuent drastiquement temps exécution maintenant qualité regroupement exacts proches voisins améliorations rendent possible application shift clustering appliqué futur proche Certaines améliorations restent cependant faire paramètres réglages cruciaux nombre proches voisins ainsi nombre blocs locality sensitive hashing approximés proches voisins requis Références Chacón Duong driven density estimation applications nonparametric clustering hunting Electronic Journal Statistics Comaniciu algorithm driven bandwidth selection Transactions Pattern Analysis Machine Intelligence Comaniciu shift robust approach toward feature space analysis Transactions Pattern Analysis Machine Intelligence Zheng Zhang adaptive shift algorithm based Procedia Engineering Datar Immorlica Indyk Mirrokni Locality sensitive hashing scheme based stable distributions Proceedings twentieth annual symposium Computational geometry Ghemawat MapReduce Simplified processing large clusters Communications Duong Azzag Lebbah Nearest neighbour estimators density derivatives application shift clustering Pattern Recognition Letters Fukunaga Hostetler Optimization nearest neighbor density estimates Transactions Information Theory Fukunaga Hostetler estimation gradient density function applications pattern recognition Transactions Information Theory Peled Indyk Motwani Approximate nearest neighbor Towards removing curse dimensionality Theory Computing Loftsgaarden Quesenberry nonparametric estimate multivariate density function Annal Mathematical Statistics Pratt Digital Image Processing Inside Slaney Casey Locality sensitive hashing finding nearest neighbors Signal Processing Magazine Zamar parametric clustering method based local shrinking Computational Statistics Analysis Summary introduce efficient distributed implementation nearest neighbour shift clustering computationally intensive nature restricted application complex where flexible clustering ellipsoidal clusters would beneficial parallel implementation standard serial algorithm brings insufficient performance gains introduce further algorithmic improvements normal scale choice optimal number nearest neighbours locality sensitive hashing approximate nearest neighbour searches Combining these improvements single distributed algorithm DNNMS offers potential efficient method Clustering