Recherche groupes parallèles classification supervisée Lionel Martin Matthieu Exbrayat Teddy Debroutelle Aladine Chetouani Sylvie Treuillet Sébastien Jesset Batiment Léonard Vinci 45067 Orléans cedex prenom orleans orleans Laboratoire PRISME Université Orléans Blois 45067 Orléans cedex prenom orleans orleans prisme Service Archéologique Municipal Orléans Neuve 45000 Orléans sjesset ville orleans Résumé article intéressons situation classification supervisée laquelle souhaitons imposer forme commune clusters cette approche forme commune caractérisée hyperplan groupes translation points supposés distribués autour hyperplans parallèles fonction objectif utilisée naturellement exprimer comme minimisation somme distances chaque point hyperplan Comme means résolution effectuée alternance phases affectation chaque point hyperplan proche phases calcul hyper ajuste mieux ensemble points affectés objectif étant obtenir hyperplans parallèles cette phase calcul menée simul tanément hyperplans méthode régression Introduction clustering tâche classique apprentissage automatique consiste scinder ensemble objets quelques groupes cohérents critères cohérence peuvent varier reposent général distances similarités entre objets observés littérature propose plusieurs classifications méthodes clustering suggérons lecteur consulter exemple Aggarwal Reddy ensemble article intéressons possédons connaissance priori forme groupes particulier groupes supposés distribués autour hyperplans parallèles Divers données notamment reconnaissance formes présentent structuration spatiale objets couches parallèles figure donne exemple distribution spatiale frises parallèles cette figure observe gauche motifs tessons poteries carolingiennes motifs imprimés Clustering groupes parallèles réalisation poterie présentent structure rectiligne visualisation possi blement altérée courbure tesson plusieurs lignes registres motifs peuvent avoir réalisées Notre objectif identifier différentes lignes travaille tessons portion motif exploitable assez restreinte simplifier travail identification prétraitement présenté droite réalisé consiste extraire points intérêt points remarquables image correspondant maximums locaux critères donnés plupart points situés périphérie motifs gravés également périphérie tesson introduit outliers clustering porte points intérêt observer structuration frises parallèles également présence bruit légère courbure registre partiel image points intérêt pertinents registres médian inférieur connectent droite image exemple distribution objets frises parallèles gauche reproduction tesson poterie présentant motifs imprimés structurés lignes registres parallèles visualise variations profondeur surface points intérêt légèrement marqués droite extraction points intérêt image gauche approches permettent introduire connaissance priori distribution données approches mélange fixent modèle probabiliste distribution données généralement multinormales objectif alors rechercher mètres modèle maximisent vraissemblance étant données observations autres approches appuient formes groupes particulières Ainsi techniques cluste point symétriques chercher groupes organisés autour centre centroïde chaque objet groupe puisse observer autre objet groupe approximativement symétrique rapport centre notre situation approches inspirées analyse factorielle typologique Diday Clusterwise Linear Regression Späth semblent mieux adaptées consiste rechercher variétés inertie minimum alors plique contexte régression recherche équations ajustant mieux données objectif répartir données groupes construire modèle chacun groupes méthodes alternent phases calcul modèle phases modification groupes suivant principe means Toutes partent partition initiale effectuent recherche locale Comme montre exemple suivant figure table efficacité méthode Martin initialisation fonction optimisée présenter selon données considéré nombreux optimums locaux données groupes alignés respectivement avons appliqué approche trois données configuration tiale obtenue tirant centres points hasard affectant points centre proche chaque avons répété algorithme calculé score après initialisation après exécution avons également déterminé réussite exécution considérée comme réussie score dépasse réussite score après initialisation score après exécution Score méthode données figure tirer conclusions générales partir exemples observons méthodes sensibles initialisation optimum global difficile atteindre particulier troisième données conforme notre hypothèse distribution lequel exécution algorithme induit dégradation score moyen rapport score après initialisation raisons proposons article méthode inspirée objectif rechercher hyperplans parallèles cette méthode notée Parallel proposons également trois méthodes initialisation visent trouver partition initiale adaptée hypothèse distribution autour hyperplans parallèles article structuré comme section présentons approche générale proposons ensuite diverses techniques initialisation algorithme section expérimentations réalisées présentées commentées section Enfin concluons évoquons possibilités extension notre méthode section Clustering groupes parallèles Approche proposée plaçons contexte ensemble points partitionné groupes suite noterons groupes désigne ensemble points appartenant groupe indice Notre hypothèse travail consiste considérer groupes forment nuages points distribués hyperplans parallèles cette hypothèse implique points distribués autour droites parallèles Notre objectif consiste déterminer équations hyperplans parallèles également déterminer groupe auquel chaque point affecté objectif formulé comme algorithme means minimisation critère désigne affectation objet groupe means carré distance entre centre groupe approche proposée chaque groupe caractérisé équation hyperplan Ainsi groupe associé hyperplan équation désigne produit scalaire entre hyperplans partagent vecteur paramètres leurs équations seuls paramètres spécifiques chaque perplan assure hyperplans soient parallèles résolution alternera phases affectation pendant lesquelles équations hyperplans inchangées phases recalcul équations hyperplans pendant lesquelles affectations inchangées garantissant amélioration critère chacune phases méthode assurée converger optimum local phases calcul détaillées suite cette section consacrerons section suivante étape initialisation objectif obtenir première organi sation groupes assez cohérente Plusieurs méthodes initialisation présentées influence classification obtenue évaluée section expérimentations Étape calcul hyperplans cette phase considérons groupes fixés affec tation chaque point groupe modifiée imposions critère serait identique critère usuel régression linéaire cadre lequel fonction aussi nommée fonction perte existe certain nombre propositions fonctions perte littérature Smola utilisées apprentissage étant moindres carrés insensitive Function introduit machines vecteur support notre cadre puisque supposons étape calcul hyperplans traduit réalisation simultanée régressions Notons régressions indépendantes puisque vecteur paramètres commun toutes équations obtenues Martin réalisation cette étape nécessite étendre méthodes résolution usuelles incluant contrainte parallèléité hyperplans commun suite article concentrons critère moindres carrés méthode régression linéaire moindres carrés Cornillon Matzner Løber variables notée considérée comme variable cible autres riables dites explicatives notons vecteur variables explicatives entrées problèmes couples objectif consiste chercher expres linéaire minimisant somme écarts quadratiques modèle particulièrement adapté contexte valeur variable prédite partir valeurs variables explicatives fonction perte mesure erreur quadratique prédiction notre cadre distinction entre variable cible variable explicative pouvons arbitrairement choisir variable comme variable cible autres seront considérées comme explicatives partir données notons variables autres variables hyperplan correspondant équation écrit forme condition coefficient associé équation nullité coefficient pourrait survenir groupes étaient orientés exactement suivant extrêmement probable pratique avons choisi dernière variable Finalement cherchons équations forme somme quadratique écarts alors écrire forme matricielle matrice définie matrice composée vecteurs ligne vecteur vecteur représente ensemble inconnues problème Comme régression linéaire classique conditions annulation dérivées partielles impliquent solution satisfait réalisation étape calcul hyperplans impliquant minimisation critère nécessitera résolution système équations linéaires Clustering groupes parallèles Étape affectation Critère arrêt cette étape considérons ensemble équations hyperplans objectif étant affecter chaque point hyperplan proche avons mesure naturelle écart entre point hyperplan donné Chaque point affecté groupe indice défini amélioration systématique critère chacune étapes décrites précédemment assure convergence méthode critère étant positif arrêt itérations intervient lorsque solutions consécutives identiques partition change Initialisation section précédente présente étapes calcul mises œuvre proposons plusieurs méthodes initialisation objectif obtenir première partition assez proche solution recherchée commune méthodes initialisation proposées consiste rechercher espace dimension orthogonal hyperplans recherchés espace trouvé partition initiale obtenue exécutant gorithme means espace obtenir espace allons quelque sorte rechercher contraintes cannot Wagstaff arbre recouvrement minimal Joseph Kruskal Étant donné graphe connexe ensemble sommets ensemble arêtes pondérées graphe connecte sommets somme poids arêtes minimale Considérons graphe sommets points ayant arête entre chaque couple points pondérée distance entre points obtenu partir graphe présente structure simple existe arête entre points graphe distance entre points faible relativement poids autres arêtes envisageable points soient groupe distance entre points grande moins envisageable points soient groupe effet arête relie considérée comme entre groupes relativement lointains utilisée consiste éliminer grandes arêtes graphe effet isoler outliers fragmenter graphes petits faisons hypothèse points graphe appartiennent majoritairement groupe Minimisation poids minimaux minDmin principe cette méthode consiste rechercher partition initiale laquelle points reliés arête faible poids appartiennent possible groupe plaçons quelque sorte contraintes couples points notons clustering hiérarchique simple obtenu directement partir Martin choisissons définit nombre arêtes retenues conservons arêtes poids minimum cherchons alors espace projection minimise somme carrés distances entre points espace obtenu forme restreinte analyse composantes principales retenant vecteur propre associé petite valeur propre objectif optimiser somme distances entre couples points adaptons optimiser somme partielle distances correspondant arêtes retenues figure illustre cette initialisation choix valeur discuté section gauche image fragments motifs alignés points intérêt ronds droite arêtes petites épaisses droite orthogonale espace projection orientation présumée motifs gauche arêtes petites droite couples points éloignés reliés arêtes chaque composante connexe graphe gauche droite orthogonale espace projection Minimisation poids maximaux minDmax Lorsque éliminons grandes arêtes graphe obtenu connexe figure gauche possible retenir couples points éloignés chacune composantes connexes ajouter quelque sorte contraintes couples principe calcul espace projection consiste comme Clustering groupes parallèles précédent minimiser somme quadratique distances entre couples points restreinte Écart maximal entre distance euclidienne ISOMAP maxIsomap dernière méthode initialisation proposée fondée distance Isomap induite partir distance Isomap entre points obtenue calculant court chemin entre points exploitons distorsions entre cette distance Isomap distance euclidienne espace origine distance Isomap entre points grande alors distance euclidienne petite imposons points soient groupe contrainte cannot espace projection obtenu maximisant somme quadratique distances euclidiennes couples points retenus nouveau restreinte figure paramètre fixer nombre couples retenus expérimentations avons retenu gauche image fragments motifs alignés points intérêt droite arêtes grande distorsion droite orthogonale espace projection Complexité méthodes initialisation Rappelons désigne nombre objets dimension espace représenta Chaque méthode initialisation nécessite calcul complexité n2log algorithme Kruskal Joseph Kruskal rechercher petites arêtes grandes suffit trier arêtes complexité méthodes minDmin minDmax alors construire matrice diagonaliser maximum produits scalaires espace dimension complexité enfin diagonaliser matrice obtenue complexité méthode initialisation maxIsomap calculer matrice distances Isomap partir graphe ayant sommets arêtes Cette matrice obtenue algorithme Floyd Warshall complexité itérant algorithme Dijkstra chaque noeud algorithme implémenté noeuds complexité globale n2log complexité cette méthode initialisation ordre celle méthodes précédentes Martin Expérimentations présentons résultats expérimentaux données réelles données synthétiques points dimensions données réelles points intérêt obtenus partir images fragments tessons motifs avons manuellement identifié groupes ainsi orientation optimale projection initiale avons développé générateur produit points nombre variable groupes orientation variable nuages écart distance aléatoire entre nuages points groupes moins proches quantité bruit forme paramétrable données synthèse avons également groupes cibles ainsi orientation optimale chaque nombre groupes recherché correspond nombre groupes expérimentations avons choisi initialisation Influence initialisation proposons premier temps mesurer influence méthode initialisa résultats obtenus première expérience porte paramètre spécifie sélection arêtes initialisation minDmin avons généré données bruit pouvons étudier erreur entre orientation obtenue notre méthode orientation optimale angle mesuré degrés faisant entre moyenne données générés déterminer paramétrage satisfaisant Erreur initialisation fonction bruit surprise figure montre bruit dégrade qualité initialisation ailleurs données bruitées nécessaire réduire erreur minimale courbe moyenne obtenue utiliserons cette valeur tests cette méthode initialisation notée minDmin0 autre possibilité choix repose valeur relative petite valeur propre restreinte cette valeur relative petite projection réduit Clustering groupes parallèles tances entre points contrainte appelons minDmin variante minDmin reposant paramètre associé petite valeur propre relative tableau présente erreur moyenne données réelles entre angle obtenu après initialisation angle optimal comparaison colonne présente erreur angle aléatoire noter mauvaise performance maxIsomap données réelles précise certains données mauvaise autres minDmin0 minDmin minDmax maxIsomap Erreur Erreur initialisation angle optimal degrés tableau suivant montre impact initialisation mesure répétitions données synthétiques bruit entre cette comparaison avons ajouté méthodes initialisation simples centres tirés aléatoirement points affectés centres proches projAlea means exécuté après projection suivant vecteur aléatoirement Données synth Données réelles final final projAlea minDmin0 minDmin minDmax maxIsomap Influence initialisation mesure méthode résultats mettent évidence importance étape initialisation score obtenu après initialisation présenté colonne montrent également exécution algorithme proposé dégrader score obtenu après initialisation colonnes final problème essentiellement méthode régression choisie effet méthode moindres carrés minimise distance entre hyperplan points plutôt erreur prédiction variable cible espace dimensions nuage points incliné grande valeur critère Ainsi possible obtenir droite ajuste mieux nuage points moindres carrés optimale terme distances entre points droite corriger cette anomalie avons intégré notre approche étape rotation après calcul hyperplans rotation déterminée hyperplans deviennent alignés horizontal critère moindres carrés correspond somme distances entre points droite résultats obtenus cette version présentés colonne utilisons version rotation tests suivants Martin Comparaison approches Comparons maintenant approches existantes means clustering hiérar chique simple hclust mélange gaussiennes clustering points symétriques dbscan entendu utilisons différentes initialisations proposées Compte présence bruit données synthétiques données réelles calcul mesure reflète partiellement qualité partition produite colonnes bruit avoir précise avons recalculé mesure éliminant points appartenant classe bruit données source colonnes bruit Notons points supprimés uniquement calcul mesure conservés phase classification Données synthétiques Données réelles bruit bruit bruit bruit means hclust dbscan minDmin0 minDmin minDmax maxIsomap minDmin0 minDmin minDmax maxIsomap Comparaison mesure selon méthode utilisée pouvons remarquer quelle méthode initialisation obtient meilleurs résultats alors comme avons évoqué introduction méthode induit amélioration score voire dégradation données synthétiques rapport score obtenu après initialisation présenté tableau Conclusion article avons présenté méthode capable partitionner ensemble points groupes intégrant contrainte forme géométrique partagée groupes Cette contrainte impose points soient distribués autour hyperplans rallèles résultats expérimentaux montrent comportement méthode dbscan avons itéré algorithme obtenir nombre groupes souhaités fixant paramètre taille voisinage faisant varier paramètre distance Clustering groupes parallèles présentée importance étape initialisation amélioration possible cette étape serait pouvoir prévoir quelle méthode utiliser avons effet performances diverses données réelles selon méthode initialisation utilisée données réelles motifs distribués autour lignes moins incur extension naturelle approche présentée consisterait utiliser méthode résolution permettant intégrer noyau exemple basée analyse factorielle autre extension possible consisterait rechercher groupes distribués autour perplans parallèles autour espaces parallèles dimension inférieure Références Aggarwal Reddy Clustering Algorithms Applications Chapman Tuytelaars Speeded robust features Comput Image Underst Cornillon Matzner Løber Régression Théorie applications Statistique probabilités appliquées Springer Diday Introduction analyse factorielle typologique Revue Statistique Appli Joseph Kruskal shortest spanning subtree graph traveling salesman problem Proceedings American Mathematical Society Distinctive image features scale invariant keypoints Comput Vision Smola Burges Drucker Golowich Hemmen Müller Schölkopf Vapnik Regression estimation support vector learning machines Späth algorithm clusterwise linear regression Computing modified version means algorithm distance based cluster symmetry Trans Pattern Intell Wagstaff Cardie Rogers Schroedl Constrained means clustering background knowledge Morgan Kaufmann Summary paper focus unsupervised classification where clusters share shape consider shape consists given hyperplane common clusters given translation Points considered distributed around parallel hyperplanes underlying objective function minimizing distances point hyperplane Similarly means achieved alternating affectation point hyperplane computation hyperplane equation phases Seeking parallel hyperplanes computation phase conducted multaneously hyperplanes