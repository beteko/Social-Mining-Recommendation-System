clustering haute dimension accumulation clusterings locaux ismaël akodjènou jeannin salamatian patrick gallinari avenue président kennedy 75016 paris ismael akodjenou salamatian patrick gallinari résumé clustering tâche fondamentale fouille données dernières années méthodes cluster ensembles objet attention soutenue agréger plusieurs clusterings données obtenir clustering moyen clusterings individuels peuvent résultat différents algorithmes méthodes particulièrement utiles lorsque dimensionalité données permet méthodes classiques basées distance densité fonctionner correctement article proposons méthode obtenir clusterings individuels faible partir projections partielles données évaluons empiriquement notre méthode comparons trois méthodes différents types constatons donne résultats sensiblement supérieurs autres introduction clustering consiste découvrir automatiquement groupes clusters présents données littérature abondante existe sujet revue principales thodes trouvée wunsch plaçons cadre cluster ensembles strehl ghosh cluster ensembles sorte clustering partir plusieurs clusterings données déduit tering moyen strehl ghosh plusieurs alternatives proposées trouver clustering moyen méthodes agglomératives basées graphes indépendamment méthode synthèse choisie clair clustering moyen dépend fortement qualité diversité chaque clustering individuel brodley exemple agréger plusieurs clusterings issus algorithme means initialisations différentes atténuera erreurs particulières chaque clustering individuel cependant mettra contourner limitations fondamentales algorithme clusters forme rique sensibilité dimension situation idéale cluster ensembles celle clusterings individuels variés bonne qualité obtenus faible explorée topchy obtenir clusterings individuels jetant données direction aléatoire faisant clustering simple clustering haute dimension accumulation clusterings locaux projection revient essentiellement trouver modes densité projection intuition suffisamment droites séparations entre clusters seront mises évidence permettront obtenir clustering moyen bonne qualité ticle proposons améliorer cette méthode construisant clusterings individuels autres projections toujours faible meilleures performances reste article structuré façon suivante section explique notre proche décrit formellement projections partielles points recherche modes clustering final section compare performances méthode celles autres méthodes données réels section conclut évoque pistes travaux futurs cluster ensembles projections projections intéressantes clustering question posons suivante comment trouver projections téressantes clustering soient variées obtenues faible trouve littérature nombreux algorithmes clustering utilisent projections linéaires données souvent justifié malédiction dimensionalité quand dimension augmente clusters deviennent épars points cluster moins concentrés distances entre points tendent perdre signification clusters également espaces dimension assez basse rapport dimension espace parsons cette malédiction frein efficacité plusieurs méthodes clustering basées distances densité intérêt projections elles permettent faire apparaître clairement clusters traditionnellement domaine réduction dimensionnelle abord cherché trouver direc tions intéressantes fonction certains critères direction simplement droite passe origine utilisée doute analyse composantes principales consiste déterminer directions données présente grande variance projection pursuit approche problème optimisa résolu trouver directions maximisent intérêt entropie aussi citer projections aléatoires reviennent projeter données ensemble directions choisies uniformément sphère unité démarche répandue consiste abord réduire dimension données projetant appliquer algorithme clustering quelconque cette représentation cependant difficulté paramétrer réduction dimensionnelle obtenir tering conduit certains auteurs intégrer cette réduction directement algorithme clustering exemple certains algorithmes clustering divisif partitionnent récursivement données trouvant direction intéressante coupant données rapport cette direction boley miasnikov projection pursuit approches aggarwal utilisent clustering associer espaces intéressants chaque cluster article topchy introduction droites projection choisies hasard passent origine problème lorsque dimension augmente projection droite aléatoire avoir densité gaussienne unimodale akodjènou jeannin inintéressante clustering amélioration vient immédiatement esprit utiliser exemple plusieurs locales cette alternative coûteuse complexité calcul quadratique dimension direction forte variance ailleurs forcément meilleure séparer clusters problème complexité manque garanties projection pursuit alternatives cherchent directions essentiellement droites passant origine probable cependant direction intéressante clustering endroit donné espace autre notre approche droites inter points proposons idées simples obtenir clusterings individuels bonne première utiliser droites inter points comme droites projection points définissant droite clusters différents chacun espace comme figure projections clusters cette droite seront vraisemblablement séparés revanche points définissant droite espace comme exemple droite pourra séparer clusters appartenant espace clusters droite projection densité résultante deuxième projeter données toutes droites mieux faire ressortir clusters considérons exemple figure clusters espaces différents droite appartient espace seulement exemple vraiment judicieux projeter points dessus généralement seuls points proches droite projection orthogonale intéressants droite donnée illustré figure projette points clusters densité résultante permet séparer clustering haute dimension accumulation clusterings locaux alors projette points densité résultante pointillés unimodale sépare suite données sphère unité notée produit scalaire cardinal ensemble projections partielles droites projection choisissons hasard droites inter points comme droites projection chaque droite définie vecteur directeur origine définie prenant hasard points sorte distance point droite distance orthogonale point droite décomposer vecteur comme étant somme vectorielle étant projection droite distance point droite norme écrit projections partielles éviter effets évoqués section projette réellement point droites proches calcule distances entre chaque point droites sélectionne droites proches triant distances ordre croissant traitement chaque droite associée certain ensemble points projection recherche modes produire clustering individuel partir devons identifier modes densité effectuer étape utilisons estimateur densité noyaux noyau gaussien silverman avons choisi noyau gaussien répandu donne résultats notre simplement disposer fonction lisse simple histogramme recherche modes plusieurs manières choisir bande passante estimateur prenons répandue thumb silverman variance empirique seconde étape consiste identifier modes cette densité devons identifier minimums locaux minimum local point dérivée nulle dérivée seconde positive pratique suffisant parcourir droite identifier zones dérivée devient négative nulle positive akodjènou jeannin faisons parcours linéaire densité discrétisant valeurs dérivée point approchée algorithme décrit pseudo recherche modes entrées sorties modes correspondants estimateur noyaux faire changementsigne alors algorithme recherche modes densité clustering final après étapes précédentes identifié modes chacune droites rassemble cette information façon synthétique matrice xmodes lignes colonnes telle xmodesik modek sinon modek numéro contenant projection ligne xmodes indique quels modes quelles droites projette terminer clustering devons maintenant effectuer synthèse clusterings individuels avons choisi méthode simple effectue clustering agglomé ratif average lignes matrice xmodes mesure distance entre lignes refléter intuition suivante points proches projetés mêmes modes mêmes droites comparant lignes devons ignorer colonnes lignes composantes nulles droites aucun points projetés chacune droites restantes points modes différents contribue éloigner amène naturellement distance jaccard simplement proportion coordonnées clustering haute dimension accumulation clusterings locaux nulles diffèrent djaccard complexité effectuer projections droites complexité calculer distances entre points droites également trier distances droites chacun points complexité totale cherche modes droites complexité respond nombre points discrétisés chaque pratique choisit fonction diamètre obtenir constant exemple finalement procédure agglomération complexité lognu nombre total lignes distinctes matrice xmodes résumer étape projection recherche modes linéaire nombre points dimension étape agglomération n2logn expériences critères évaluation évaluation qualité clustering question délicate article disposons chaque données vraie classe évaluer clustering revient comparer clustering trouvé algorithme vraies classes existe nombreux critères comparaison avons retenu critères classiques complémentaires arrive souvent particulier données réelles clusters trouvés algorithme contiennent points venant classes différentes premier critère classique évaluer clustering pureté calcule façon suivante étiquette chaque cluster classe dominante contient cldomy pureté cluster cldomy pureté clustering simplement moyenne pondérée taille clusters cluster second critère information mutuelle normalisée strehl ghosh information mutuelle entre variables aléatoires alors akodjènou jeannin dénotant entropie discrète critère maximum quand étique tages parfaitement semblables chaque classe cluster correspondant elles divisée clusters alors petite ainsi critère pénalise clustering structure vraie information classe permet juger point étiquetages structure données avons trois données répandus évaluation algorithmes chart séries temporelles contrôle comporte exemples dimension contient sortes séries temporelles différentes second postal service caractères manuscrits mension pixels contient classes caractères manuscrits entre troisième coil20 columbia university image library contient images dimensions pixels données constitué objets chacun photo selon angles différents angles correspondent rotations successives degrés objets table tournante algorithmes paramètres chacun données avons évalué performance quatre méthodes clustering lignes inter points notre méthode méthode topchy mixture gaussiennes précédée gardant variance données algorithme subspace clustering domeniconi means pondère dimensions localiser espace particulier chaque cluster chaque méthode avons effectué clustering paramétrant nombre variable permet mieux rendre compte efficacité algorithme brodley sachant nombre classes données corresponde nombre clusters point géométrique terme chaque expérience algorithme données fixés effectuée graines toires différentes initialisations différentes tables résultats affichent moyenne écarts types performance selon critères pureté information mutuelle normalisée décrits résultats notre méthode obtenus paramètres suivants chart avons droites projeté chaque point droites proches coil20 évalué méthode nombre droites databases synthetic_control synthetic_control cervisia machine_learning_data columbia software softlib clustering haute dimension accumulation clusterings locaux pureté information mutuelle normalisée résultats données chart pureté information mutuelle normalisée résultats données pureté information mutuelle normalisée résultats données coil20 akodjènou jeannin résultats chacun données expériences montrent notre approche donne résultats sensiblement supérieurs autres approches résultats assez stables nombre clusters différents confirme robustesse méthode mauvais résultats montrent quant droites choisies complètement toirement indépendamment données permettent obtenir clusterings indivi duels intéressants avons également testé droites aléatoires contraintes passer origine donné résultats légèrement moins algorithme classique clustering mixture gaussiennes précédée deuxième meilleur après notre méthode algorithme vient ensuite figures illustrent graphiquement clustering obtenu notre méthode chaque données conclusion perspectives avons proposé nouvelle méthode cadre clustering ensembles chaque clustering individuel réalisé partir projection partielle données droite reliant points données hasard avons évalué comparé méthode autres approches expériences montrent notre méthode donne meilleurs sultats perspectives travail futur portent aspects mécanisme sélection droites projection chaque point clustering global point influence droites proches imaginer utiliser points chaque droite pondérant influence chaque point estimateur densité distance droite permettrait affranchir calcul distances entre points droites quant performance clustering global probablement nouvelle représentation données nature agglomérative algorithme synthèse clusterings individuels variante means distance jaccard devrait donner résultats similaires complexité moindre résultats clustering chart clustering haute dimension accumulation clusterings locaux résultats clustering akodjènou jeannin résultats clustering coil20 clustering haute dimension accumulation clusterings locaux références aggarwal finding generalized projected clusters dimensional spaces proceedings sigmod international conference management boley principal direction divisive partitioning mining knowledge covery adaptive dimension reduction using discriminant analysis means clustering proceedings international conference machine domeniconi gunopoulos razgan papadopoulos locally adaptive metrics clustering dimensional mining knowledge discovery stork pattern classification wiley interscience publication brodley random projection dimensional clustering cluster ensemble approach proceedings twentieth international conference machine learning miasnikov haralick hierarchical projection pursuit algorithm proceedings international conference pattern recognition parsons haque subspace clustering dimensional review sigkdd explorations newsletter special interest group knowledge discovery mining wunsch survey clustering algorithms transactions neural networks volume silverman density estimation statistics analysis chapman strehl ghosh cluster ensembles knowledge reuse framework combining multiple partitions journal machine learning research topchy punch combining multiple clusterings third international conference mining summary clustering essential block mining recently called cluster ensembles methodology attracted attention principle aggregate clusterings dataset order obtain average clustering individual clusterings output different clustering algorithms methodology particularly useful dimensionality hinders methods based distance density giving results article propose obtaining individual clusterings partial projections dataset evaluate empirically method compare three methods different types method obtains noticeably superior results