articles assemblage pdfConstruction noyaux apprentissage supervisé partir arbres aléatoires Vincent Pisetta Pierre Emmanuel Jouve Djamel Zighed RITHME Vivier Merle 69003 vpisetta rithme FENICS Vivier Merle 69003 pjouve fenics Laboratoire Pierre Mendès France 69500 abdelkader zighed lyon2 Résumé montrons ensemble arbres décision compo sante aléatoire permet construire noyau efficace destiné apprentissage supervisé étudions théoriquement propriétés noyau trons conditions souvent rencontrées pratique existe séparabilité linéaire entre exemples classes distinctes espace induit celui Parallèlement observons également classique jorité ensemble arbres hyperplan garantie optimalité espace induit noyau Enfin comme montrent expérimentations utilisation conjointe ensemble arbres séparateur vaste marge aboutit résultats extrêmement encourageants Introduction Parmi techniques apprentissage statistique performantes trouvent thodes noyaux représentant célèbre certainement Séparateur Vaste Marge Vapnik emploi motivé initialement premiers résultats riques apprentissage statistique encore largement répandu suite nombreux succès empiriques apportés apprenant Pavlidis Markowska Kaczmar Kubacki Polat Günes particularités utiliser meuse astuce noyau introduire linéarité frontière décision augmenter complexité algorithmique apprentissage Ainsi choix noyau doute paramètre important algorithme différents noyaux pouvant aboutir résultats différents Plusieurs indicateurs proposés littérature évaluer qualité noyau priori autrement avant exécution apprentissage connu probable Kernel Target Alignment Cristianini autres Nguyen polarisation Baram aient également montré sultats intéressants intérêt indicateurs permettent évaluer pertinence Noyaux arbres aléatoires noyau manière relativement rapide complexité nombre exemples recourir calcul modèle Outre évaluation intérêt pratique mesures elles premettent construire manière supervisée noyau adapté données spécifique principal paradigme utilisant principe Multiple Kernel Learning hypothèse combinaison plusieurs noyaux aboutir résultat intéressant seule sélection meilleur Ainsi objectif chercher combinaison linéaire coefficients positifs plusieurs noyaux distincts manière maximiser importe autre indicateur approprié exemple programmation semidéfinie Lanckriet optimisation séquentielle autres méthodes Sonnenburg Rakotomamonjy corporent problème optimisation noyau directement problème optimisation global Toutes stratégies montré excellents résultats capacité prentissage élevée Cependant elles souffrent plusieurs problèmes abord elles résolvent problème choix noyaux initiaux doivent combinés Généralement noyaux gaussiens utilisés aucune justification théorique apportée choix Ensuite elles souffrent toutes complexité algorithmique élevée exemple méthode Lanckriet compexité Enfin elles supposent toutes descripteurs utilisés apprentissage continus alors fréquemment variables catégorielles soient présentes données réelles proposons article utiliser arbres décision aléatoires générer noyaux efficaces apprentissage célèbres Forêts aléatoires Breiman particulier cette méthodologie révèlent constructrices noyaux performantes effet elles motivation similaire principe savoir utilisation ensemble classifieurs plutôt recherche meilleur après avoir brièvement rappelé quelques généralités apprentissage décrivons cadre théorique permettant obtenir noyau apprentissage Après avoir rappelé mécanisme forêts aléatoires verrons comment construire noyaux partir celles section montrerons théoriquement conditions faibles noyau construit ensemble induit espace contenant séparateur néaire entre exemples classes distinctes section montrerons également célèbre décision majorité autre hyperplan espace généré Forêt section Toutes observations suggèrent utilisation espace généré ensemble arbres aléatoires révéler intéressante section montrons expérimentalement fondé cette hypothèse Enfin section concluons Séparateur vaste marge qualité noyau Cadre général considérons article cadre classification supervisée classes Formellement étant donné exemples décrits espace dimensions vecteur correspondant réponses classes étiquettes objectif classification supervisée trouver fonction telle Pisetta fonction fonction perte minimale Généralement cette dernière choisie comme fonction moindres carrés fonction hinge spectre large méthodes dédié résolution problème Parmi celles partie performantes consiste chercher fonction prédéfinie comme signe trouvant solution optimale problème suivant paramètre régularisation obtenue partir mapping classiquement dénomé espace caractéristiques espace Hilbert produit scalaire Schölkopf Smola noter dimension infinie pourquoi obtention solution programme passe généralement forme duale αiαjyiyjK représente noyau défini comme passage forme duale permet réécrire fonction décision devient signe yiαiK Cette écriture utilise uniquement noyau avoir besoin calculer explicitement parle alors astuce noyau principale utilité pouvoir plonger exemples espaces grande dimension calculant uniquement produits scalaires produit espace mémoire rapidité calcul considérable contrepartie modèle toute interprétabilité puisque fonction décision dépend variables initiales Signalons enfin noyau impérativement produit scalaire valide respecter conditions Mercer stipulant symétrique produit scalaire valide seulement matrice définie toujours définie positive Qualité noyau contexte succès dépend complètement noyau utilisé pratique largement répandue consiste faire apprendre partir noyau souvent choisi hasard absence connaissances préalables modifier petit petit paramètres finalement sélectionner celui donnant meilleurs résultats Partant constat Noyaux arbres aléatoires complexité algorithmique varie entre compter construction matrice noyau évaluation nombre important paramé trages rapidement devenir lourd handicap cette problématique besoin pouvoir évaluer noyau amont apprentissage répondre besoin Cristia indicateur qualité noyau appelé Kernel Target Alignment défini échantillon comme yiyjK représente produit scalaire Frobenius matrice habituellement pelée matrice idéale matrice cible Chaque cellule valeur appartiennent classe sinon valeur comprise entre autant élevé noyau considéré comme performant autrement calcule simplement somme similarités induites noyau entre individus classe soustrait somme similarités individus classes différentes dénominateur permet normaliser cette valeur entre caractéristique intéressante relatif faible algorithmique simple évaluation critère aussi utilisé apprendre directement noyau Multiple Kernel Learning doute principe proprié cette optique consiste supposer somme plusieurs noyaux conduire noyau performant sélection meilleur noyau individuellement techniquement désire trouver vecteur noyau alignement maximal matrice idéale contrainte positivité coefficients essentielle assurer définie positive absence informations préalables noyaux initiaux ainsi nombre choisis moins hasard intérêt combinaison linéaire double abord forme objective cette forme permet recours arsenal important techniques optimi sation aspect pratique choix agrégation noyaux combinaison linéaire justifie grâce possible réécriture effet alignement somme matrices noyaux échantillon écrit alors clairement combiner noyaux extension noyaux avantageux situation particulièrement intéressante noyaux alignement individuel élevé indépendants notion indépendance renvoie produit Frobenius entre matrices effet noyaux tiques avons conséquent section suivante montrons paradigme forêts aléatoires Breiman possède fortes connexions celui maximisation consé quent utilisé construire noyaux performants Pisetta Forêts aléatoires espace induit Principe forêts aléatoires algorithme forêts aléatoires apprenants connus formants point méthode révélée comme performante série classifieurs récente étude grande échelle Caruana procédure générique forêts aléatoires consiste répéter façon indépendante processus suivant Echantillonner ensemble apprentissage initial obtenir Induire arbre décision recherchant chaque noeud meilleur tement partir ensemble aléatoire éclatements possibles Développer arbre jusqu obtention feuilles pures possibles prédiction classe nouvel exemple effectue attribuant classe fréquemment votée arbres forêts aléatoires introduisent double randomi zation échantillonnage autre recherche exhaustive meilleur éclatement chaque noeud arbre nombreuses techniques différentes forêts mises point chacune gérant façon différente introduction processus aléatoires procédure Breiman auteur pratique échantillonnage biais bootstrap recherche meilleur éclatement testant nombre restreint variables initiales Geurts utilisent échantillonnage augmentent degré randomization sélectionnant point coupe hasard chacune variables préalablement sélectionnées après auteurs adaptant correctement obtient temps calcul important détérioration performance rapport forêts Breiman particulier forêts aléatoires Bagging Breiman consiste échantillonner selon bootstrap introduire randomization recherche meilleur éclatement utilisation mécanismes aléatoires explique forêt admet existence borne théorique erreur généralisation effet après Breiman représente corrélation moyenne entre prédictions sifieurs force prédiction moyenne autrement fonction moyenne classement renvoyons lecteur intéressé Breiman informations paramètres Cette borne erreur indépendante façon injecte aléatoire construire forêt remarque aisément après cette écriture situation idéale consiste classifieurs soient corrélés aient viduellement faible erreur possible Ainsi divers algorithmes forêts aléatoires spécifiquement point tenter trouver compromis entre pertinence individuelle classifieurs faible corrélation avons section précédente critère suggère combinaison noyaux indépendants performants individuellement stratégie intéressante construire noyau global performant pouvons remarquer emblée connexion Noyaux arbres aléatoires entre cadre décrit forêts aléatoires celui décrit conséquence trouvons moyen construire noyau représentant arbre pourrions alors présenter forêt comme somme noyaux individuellement pertinents corrélés paradigme idéal selon tenter appliquer algorithmes décision espace généré ensemble arbres section suivante montrons comment dériver noyau partir arbre partir ensemble arbres dernier propriétés intéressantes paraît adapté apprentissage Construire noyau partir forêt arbres particularité arbre pouvoir découper espace plusieurs zones dénommées feuilles développement chaque exemple appartient seule feuille exemple alors représenté vecteur taille nombre feuilles arbre composante vecteur appartient feuille sinon vecteur représentant exemple contient seule valeur égale égales cette représentation comme nouvel espace description dimension exemples alors remarquer produit scalaire individus nouvel espace feuille sinon Ainsi noyau feuille sinon noyau espace induit arbre définie positive arbres différents induits également dériver noyau pouvons considérer cette exemple représenté vecteur représente feuille arbre consé quent dimension nombre feuilles arbre matrice produits scalaires ensemble écrit matrice produits scalaires arbre définie positive correspond nombre exemples retrouvés feuille Breiman avait évoqué intérêt matrice presque équivalente analyse similarité matrice question équivalente normalisée nombre arbres ramener chaque valeur entre Notons forte connexion entre construction noyau forêts écriture somme noyaux introduisant aléatoire algorithme forêts objectif augmenter indépendance entre arbres noyaux issus arbres assurant chaque arbre assez pertinent noyau alignement assez élevé article fondateur Breiman développait arbres jusqu obtention feuilles pures signifie noeuds terminaux arbres contenaient individus classe Empiriquement Breiman remarqué cette façon procéder donnait meilleurs résultats hypothèse obtention feuilles pures possible existe exemples classes distinctes ayant toutes variables mêmes valeurs pouvons alors démontrer espace induit ensemble arbres admet existence séparateur linéaire démontrons Pisetta Proposition Arbre parfait séparabilité linéaire arbre développé échantillon jusqu avoir uniquement feuilles pures induit espace lequel existe moins séparateur linéaire entre individus classes distinctes Preuve espace généré arbre dimension nombre feuilles arbre feuilles étant pures exemples alors représentés points chacun représentant ensemble exemples classe espace dimension fonction linéaire espace dimension ayant dimension toujours possible dichotomiser exemples classes distinctes maintenant facile démontrer existence séparabilité Proposition Ensemble arbres séparabilité linéaire ensemble arbres moins développé jusqu avoir uniquement feuilles pures échantillon espace lequel existe moins séparateur linéaire entre individus classes distinctes Preuve après Proposition suffit arbre feuilles pures induire espace lequel séparer linéairement individus classes distinctes Ainsi ajout arbres équivalent ajouter dimensions espace induit arbre feuille conséquent toujours possible séparer linéairement exemples classes distinctes nouvel espace Remarquons aucun arbre développé jusqu obtention feuilles pures néanmoins possible espace généré ensemble arbres contienne séparateur linéaire condition suffisante serait couples possibles exemples classes distinctes tombent feuille moins coule Proposition principe dimension pratique autres situa tions susceptibles aboutir espace pouvant séparer linéairement exemples ayant étiquette différente forêts aléatoires utilisant échantillonnage avant construction arbres entrent cadre décrit Propositions précedentes puisqu partie exemples classée certitude section suivante étudions procédure classement majorité montrons celle hyperplan espace induit ensemble arbres résultat également permettre démontrer existence séparabilité linéaire types forêts existe échantillonage Forêt séparabilité associée Lorsqu nouvelle instance présente algorithme forêts aléatoires effectue prédiction associant cette instance étiquette fréquemment attribuée arbres Cette procédure simple classement communément appelée majorité Plusieurs travaux souligné autres schémas pouvaient apporter amélio rations significatives reclassement Tsymbal Robnik Sikojna plupart autres schéma utilisent système pondération donnant poids Noyaux arbres aléatoires chaque votant chaque arbre proportionnel pertinence Cette dernière généralement évaluée grâce ensemble chaque arbre ensemble désigne plement exemples sélectionnés échantillonnage construc arbre exemple Tsymbal utilise ensemble estimer classement arbre attribuer poids fonction pelons cette dernière technique agrégation pondéré pouvons facilement montrer proposition suivante Proposition Votes ensemble critères majorité pondéré hyperplans espace induit ensemble arbres Preuve Rappelons espace induit ensemble arbres exemple présenté vecteur représente feuille arbre simplicité écriture omettons indice relatif arbre suite preuve suffit observer décision relative majorité écrire signe majorité exemples feuille exemples rifient alors appartiennent classe sinon pondéré consiste simplement choisir ensemble potentiellement différent utiliser hyperplan comme fonction décision garanties intéressantes concernant erreur généralisation Vapnik donne autre vision robus tesse décision forêt arbres forêt ainsi comme construc noyau pouvant induire espace lequel existe séparabilité linéaire entre exemples classes distinctes procédure consiste alors trouver hyperplan espace rappelle entendu principe fonctionne milaire résultat Proposition permet également généraliser existence séparabilité présence échantillonnage Proposition Ensemble arbres séparabilité linéaire ensemble arbres construits chacun utilisant proportion exemples échantillon sélectionnés aléatoirement hypothèse arbres soient construits jusqu avoir uniquement feuilles pures alors probabilité existe séparateur linéaire espace induit ensemble arbres minorée Preuve arbres développés jusqu contenir uniquement feuilles pures alors probabilité exemple choisi hasard échantillon classé arbre minorée probabilité supérieure égale Lorsque arbres construits probabilité exemple classé règle majorité supérieure probabilité strictement correspond probabilité binomiale stric tement supérieure Ainsi probabilité Pisetta exemples soient classés minorée majorité étant hyperplan espace induit ensemble arbres possible séparer linéairement exemples classes distinctes probabilité moins égale Corollaire hypothèses Proposition alors lorsque Preuve après Condorcet lorsque infini Ainsi avons corollaire corollaire précedent montre toujours possible générer arbres indui espace lequel existe séparabilité linéaire entre exemples classes tinctes pourvu échantillonnage amont chaque arbre sélectionne moitié exemples suffisamment grand Notons probabilité borne pessimiste probabilité réelle réalité séparabilité devrait exister relative moins arbres suggérés Proposition mesure arbres classent toujours quelques exemples ensemble correctement objectivement raison penser nécessaire majorité classe correctement individus séparabilité linéaire existe particulier échantillonnage boots explicitement traité remarquera simplement proportion individus distincts tirés approximativement légitimement attendre Propostion vérifie Comme avons critère majorité pondéré sibilités classement partir ensemble arbres peuvent également comme hyperplans espace induit ensemble pondéré offre parfois perfor mances intéressantes majorité accordant poids important certains arbres Devant toutes constatations paraît légitime utiliser espace induit ensemble arbres algorithme consiste alors résoudre programme noyau défini section algorithme comme recherche pondération optimale règles décision issues arbres optimalité objective étant compromis entre maximisation marge classement impor noter noyau utilisé correspondant produit scalaire individus espace généré indicatrices règles poids calculés implicitement attri feuilles règles réprésentées feuilles arbres comme pondéré Finalement utiliser forêts arbres générer noyau spécifiquement dédié pratique intéressante puisque paradigme forêts aléatoires intimement recherche noyau Parallèlement question pondération optimale règles issues forêt résolue Ainsi problématiques symétrie section suivante réalisons plusieurs expérimentations associant noyaux issus forêts Noyaux arbres aléatoires Expérimentations comparons reclassement trois techniques apprentissage données tableau trois techniques testées Multiple Kernel majorité forêt aléatoire construit noyau forêt celle utilisée majorité résultats issus Varma explique bases données aient résultat cette méthode protocole Varma employé évaluer trois algorithmes avons partitionné chaque données parties pourcents individus apprentissage pourcents restants processus répété moyenne reclassement calculée forêt construite utilisant arbres développés chacun échantillon bootstrap jusqu obtention feuilles pures possible meilleur éclatement recher restreignant recherche nombre variables entier proche racine nombre total attributs Enfin concernant quatre valeurs possibles paramètre régularisation utilisées 10000 ensemble apprentissage partitionné parties pourcents apprendre pourcents validation avons alors retenu paramétrage donnant meilleurs résultats échantillon validation données Attributs Noyau Forêt Sonar Liver Parkinsons Ionosphere German Australian Heart Spambase Chess Proportions classement selon trois méthodes apprentissage tableau montre plusieurs choses intéressantes abord majorité forêt aléatoire performance équivalente celle Aucune différence significative Student seuil trouvé entre méthodes performance issue couplage forêt intéressante effet significativement meilleure données communs différence significative marquée Comparativement forêt forêt significativement meilleur Notons différence significative forêt néanmoins performant trois algorithmes excepté forêt meilleure écart significatif Pisetta important souligner apprentissage concernant nombre optimal riables sélectionner évaluer éclatements pourrait améliorer résultats issus forêt forêt Parallèlement montre calibrer forêt tâche simple autre avantage utilisation forêts construction noyaux rapidité calcul laisse ainsi temps recherche paramètre expérimentations différences entre diverses valeurs aient importantes Remarquons algorithme forêts parallèlisable prendre compte attributs catégoriels éventuellement valeurs manquantes cette problématique abordée Conclusion perspectives article montré forêts composante aléatoire étaient particulièrement efficaces construire noyau adapté apprentissage supervisé cadre théorique motivant constuction présente fortes similarités celui apprentissage optimal noyaux avons comment dériver noyau partir ensemble arbres montré conditions exigeantes espace induit ensemble arbres admettait existence séparateur linéaire entre exemples classe distincte utilisation espace produit résultats supérieurs majorité ensemble Parallèlement comme fonction agrégation règles induites forêt aléatoire Plusieurs perspectives découlent travail abord souhaitons analyser riquement empiriquement autres façons construire noyau partir ensemble arbres pouvons exemple prendre compte topologie arbre calculant similarité entre individus biais noeud commun proche souhaitons également étudier possibilité construire noyaux représentant infinité arbres article montre étude théorique méthodes ensemblistes rester premier rappelle était connu savoir souvent judicieux faire coopérer méthodes priori concurrentes plutôt opposer Références Lanckriet Jordan Multiple kernel learning conic duality algorithm Proceedings International Conference Machine Learning Baram Learning kernel polarization Neural Computation Breiman Bagging predictors Machine Learning Breiman Random forests Machine Learning Caruana Karampatziakis Yessenalina empirical evaluation super vised learning dimensions Proceedings International Conference Machine Learning Condorcet Essai application analyse probabilité décisions rendues pluralité Imprimerie Royale Noyaux arbres aléatoires Cristianini Kandola Elisseef Shawe Taylor kernel target alignment Advances Neural Information Processing Systems Geurts Ernst Wehenkel Extremely randomized trees Machine Learning Lanckriet Cristianini Ghaoui Bartlett Jordan Learning kernel matrix definite programming Journal Machine Learning Reasearch Markowska Kaczmar Kubacki Support vector machines handwritten classification Proceedings International Conference Intelligent Systems Design Applications Nguyen efficient kernel matrix evaluation measure Pattern Recogni Pavlidis Wapinski Noble Support vector machine classification Bioinformatics Polat Günes Breast cancer diagnosis using least square support vector chine Digital Signal Processing Rakotomamonjy Grandvalet Simplemkl Journal chine Learning Research Robnik Sikojna Improving random forests Proceedings European Conference Machine Learning Schölkopf Smola Learning Kernels Support Vector Machines larization Optimization Beyond Cambridge Press Sonnenburg Rötsch Schölkopf Schäfer Large scale multiple kernel learning Journal Machine Learning Research Tsymbal Pechenizkiy Cunningham Dynamic integration random forests Proceedings European Conference Machine Learning Vapnik Nature Statistical Learning Theory Springer Verlag Varma generality efficient kernel learning International Conference Proceedings Series Summary ensemble decision trees randomness component allows construction powerful kernels adapted supervised learning study theoretically kernels under conditions possible linear separator space induced these kernels observe classical majority voting hyperplane space induced these demonstrate empirically combining Support Vector Machine ensemble decision trees produces excellent results