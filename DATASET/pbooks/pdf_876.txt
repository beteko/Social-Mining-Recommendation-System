bay14R Structure Inference réseaux bayésiens données nouvelle approche basée Entropie Généralisée conditionnelle Simovici Saaid Baraty Massachusetts Boston Massachusetts 02125 Etats sbaraty Résumé proposons nouvel algorithme extraire structure réseau bayésien partir ensemble données Notre approche basée entropies conditionnelles généralisées famille paramétrique entropies prolonge habituel Shannon entropie conditionnelle résultats indiquent choix approprié général conditionnelle entropie obtient réseaux bayésiens scores supérieurs rapport structures similaires obtenues méthodes classiques inférence Introduction structure réseau croyance bayésien graphique acyclique orienté représente dépendances probabilistes entre ensemble variables aléatoires structure induire ensemble attributs ensemble données problème connu raison énormité espace recherche nombre structures possibles croît façon exponentielle super rapport nombre nœuds Cooper Herskovits algorithme heuristique introduit qualité structure dérivée fonction probabilité postérieure présence ensemble données autre approche calculer structure basée principe cription minimum Longueur introduit Rissanen algorithmes Bacchus Suzuki proviennent principe proposons nouvelle approche induire structures ensembles données basés notion entropie généralisée entropie conditionnelle généralisée correspondant introduit Havrda Charvat axiomatisée Simovici Jaroszewicz comme famille paramètre fonctions définies partitions probabilité distributions flexibilité découle permet générer meilleurs scores résultats publiés avantage important notre approche contrairement Cooper Herskovits repose aucune hypothèse distributive développement formule Generalized Entropy structure Inference ensemble partitions ensemble notée partie trace partition ensemble partition ordre habituel entre partitions ensemble désigné connu PARTIE inférence bayésienne délimitée réseau réseaux borne inférieure partitions notée partition moindre élément réseau partition grande partition notion entropie généralisée entropie introduite Havrda Charvat axiomatisé partitions Simovici Jaroszewicz ensemble partition entropie nombre janvier entropie Shannon obtenu forme fonction PARTIE monotones Ainsi Laissez PARTIE partitions entropie conditionnelle immédiat outre Simovici Jaroszewicz démontré propriété étend propriété similaire entropie Shannon Lorsque doublement monotone rapport premier argument monotone rapport second argument avons ensemble données ensemble attributs domaine attribut projection tuple restriction ensemble ensemble attributs définit partition regroupe tuples projections égales attribut laissez parents Définir avons considéré comme ensemble parent connaître valeur permet prédire valeur forte probabilité proche chaque suffisamment grande toute évidence parent parfaite captures exactement cette mesure qualité hotte Parent effet supposons avons avons Ensuite implique Ainsi réduit revient réduire valeurs autant possible grand triviale référons quantité comme entropie noeud présence ensemble Cependant argminX valeur minimum élevé assurer bonne prévisibilité alternative consiste assurer réduction entropie noeud suite présence groupe comme Depuis avons ensemble parent parfait sorte numéro appelé seuil prédiction considérons parent convient éviter cycles réseau partons séquence attributs cherchons ensemble parents ensemble hypothèse fréquente Simovici Baraty figure Visualisation algorithme Cooper Herskovits Suzuki avons établi nombre maximum parents ensemble contenir plusieurs ensembles convient solution possible choisir ensemble parent convient minimum propriété monotonicity rapport deuxième argument avons Ensuite donnée minimum parmi parents Convient alors taille maximale possible simplifier structure échangeons certaine prévisibilité simplicité adoptant approche heuristique trouve ensemble minimal parents noeud grande réduction possible entropie enfant présence Définir parent convient Lorsque séquence collections vides attributs propriété monotonicité Laissez argminX premier taille ordre lexicographique réduit limitons notre recherche parent séquence ensembles ensembles énumérés ordre croissant taille séquence défini dessus avons ensemble points placé courbe croissante hauteur comme illustré figure initialise parent courant réglé iterate membres ordre croissant taille membres conduit amélioration négligeable prévisibilité produit diminution lorsque ensemble changé supérieure égale décroissance linéaire rapport points extrémité courbe croissante correspondant comme représenté Figure points extrémité courbe diminution linéaire rapport points extrémité courbe lorsque passe correspondent ensembles parent Notez donne penser cessons processus satisfait inégalité dessus avoir ensemble parent amélioration négligeable termes prévisibilité rapport parent actuel inférence bayésienne réseaux Algorithme Entrée BuildBayesNet données seuil prédiction paramètre entropie nombre maximum parents liste attributs élément liste parent élément versa Sortie structure réseau NetworkStructure faire Nombre entier calculent alors briser reste argminx addNode alors Entier faire forall addEdge revenir algorithme augmentation taille ensemble parents pénalisé rendant stricte condition parents grands outre aucun ensembles parents taille satisfont inégalité Résultats expérimentaux avons comparé résultats obtenus structures bayésienne connues littérature utilisant systèmes notation utilisé Bacchus Suzuki méthode notation Cooper Herskovits expériences impliquées ensemble données tumeurs cérébrales Cooper cancer Blake 1998a ALARME Beinlich Blake 1998b résultats expérimentaux présentés tableau dernière ligne chaque table contient scores structures publiées selon Williams Williamson Beinlich partons principe distribution prieurs structures ensemble données uniforme Cooper Herskovits expériences réalisées machine processeur Intel scores structures réseau générées dépend nombreux meilleur scores structures établies partitions scores élevés figure représente quatre structures différentes cerveau données tumeur Structure Simovici Baraty Résultats expérimentaux générés Structures 10000 lignes Score Temps 13631 13474 13680 13693 Structure origine 14410 Structures Création lignes Score Score Temps 13667 Structure originale cancer cerveau Résultats Résultats cancer Generated Structures 20002 lignes Score Score Temps 114931 270298 114981 271590 12801 272665 116914 271469 Structure origine 159306 Structures générés lignes Score Temps Score 127543 13279 Structure originale 261481 Résultats Résultats introduits Cooper Structures générés notre approche Conclusions avons développé approche générer structure réseau bayésien partir données notion entropie généralisée meilleures relations parent enfant entre attributs obtenus valeurs dépendent fortement ensemble données suggère approche préférable utiliser entropie Shannon Références Beinlich Suermondt Chavez Cooper Réglementation générale système surveillance alarme étude techniques inférence probabiliste réseaux croyance Rapport technique Université Stanford laboratoire Knowledge System Blake Newman Hettich 1998a dépôt machine LEARN bases données ensemble données Institut Ljubljana Oncology fourni disponible mlearn MLRepository Blake Newman Hettich 1998b dépôt bases données apprentissage machine ensemble données Fisher offert Michael Marshall disponible mlearn MLRepository Cooper NESTOR diagnostic médical informatisé intègre connaissances décontractée probabiliste thèse Université Stanford Cooper Herskovits Procédé bayésien induction réseaux données probabilistes Rapport technique Université Stanford laboratoire Knowledge System Havrda Charvat méthodes quantification processus classification cepts entropie structurel Kybernetica Inférence Bayesian Networks figure Structures tumeurs cérébrales Bacchus Apprentissage réseaux bayésiens approche fondée principe Computational Intelligence Rissanen Modélisation courte description données Automatica Simovici Jaroszewicz axiomatization entropie partition transac tions Théorie information Simovici Jaroszewicz nouveau critère division métrique arbres décision International Journal Parallel Emergent Systèmes Distribués Suzuki Apprentissage réseaux bayésiens basés principe algorithme efficace utilisant branche technique IEICE Trans Systèmes information Williams Williamson combinaison argumentation filets bayésiens pronostic cancer Journal logique langage information proposons résumé nouvel Algorithme structure verser Extraire réseau bayésien ensemble Données Notre approach entropies basée conditionnelles généralisées famille entropies conditionnelle etend entropique Shannon conditionnelle sultats indiquent choix entropique approprié conditionnelle généralisée obtenons Bayésiens réseaux Ontario structures scores methods inférence classiques