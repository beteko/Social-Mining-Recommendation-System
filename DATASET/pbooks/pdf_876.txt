bay14R dviStructure Inference Bayesian Networks Approach Based Generalized Conditional Entropy Simovici Saaid Baraty Massachusetts Boston Massachusetts 02125 sbaraty Abstract propose novel algorithm extracting structure Bayesian network dataset approach based generalized conditional tropies parametric family entropies extends usual Shannon condi tional entropy results indicate appropriate choice general conditional entropy obtain Bayesian networks superior scores compared similar structures obtained classical inference methods Introduction Bayesian Belief Network structure directed acyclic graph which represents probabilistic dependencies among random variables Inducing structure attributes dataset known problem challenging enormity search space number possible structures grows super exponentially respect number nodes Cooper Herskovits where heuristic algorithm introduced quality structure derived based posterior probability presence dataset alternative approach compute structure based Minimum scription Length principle first introduced Rissanen algorithms Bacchus Suzuki derived principle propose approach inducing structures datasets based notion generalized entropy corresponding generalized conditional entropy introduced Havrda Charvat axiomatized Simovici Jaroszewicz parameter family functions defined partitions probability distribu tions flexibility ensues allows generate better scores published results important advantage approach unlike Cooper Herskovits based distributional assumption developing formula Generalized Entropy Structure Inference partitions denoted trace partition subset partition usual order between partitions denoted known bounded Inference Bayesian Networks lattice infimum partitions denoted partition least element lattice partition largest partition notion generalized entropy entropy introduced Havrda Charvat axiomatized partitions Simovici Jaroszewicz finite partition entropy number Shannon entropy obtained function monotonic where partitions where conditional entropy immediate Simovici Jaroszewicz shown property extends similar property Shannon entropy dually monotonic respect first argument monotonic respect second argument Moreover dataset attributes domain attribute projection tuple restriction attributes defines partition which groups together tuples equal projections attribute parents where Define considered parent knowing value enables predict value probability close every where sufficiently large Clearly perfect parent captures exactly parent quality measure Indeed suppose where which implies minimizing amounts reducing values possible those where large trivial refer quantity entropy presence However argminX value minimum itself insure predictability alternative reduction entropy result presence Since perfect parent number referred prediction threshold regard suitable parent avoid cycles network start sequence attributes parents frequent assumption Simovici Baraty Visualization Algorithm Cooper Herskovits Suzuki addition bound maximum number parents contain subsets suitable possible solution choose suitable parent minimum monotonicity property respect second argument given minimum among suitable parents maximum possible simplify structure trade predictability simplicity adopting heuristic approach which finds minimal parents highest possible reduction entropy child presence Define suitable parent sequence nonempty collections attributes monotonicity property argminX first lexicographical order minimizes limit parent search sequence where listed increasing order sequence defined above points placed increasing curve height shown Figure initialize current parent iterate members increasing order their member leads nontrivial improvement predictability happens decrease parent changed greater equal linear decrease respect points corresponding increasing curve shown Figure points curve linear decrease respect points curve which correspond parent where suggests process satisfy above inequality since there parent where trivial improvement predictability respect current parent Inference Bayesian Networks Algorithm BuildBayesNet input Dataset prediction threshold parameter entropy maximum number parents attributes where element parent element versa output Network Structure NetworkStructure Integer Compute break argminx addNode Integer forall addEdge return algorithm increase parent penalized making condition stricter larger parent parent satisfy inequality Experimental Results compared generated results known Bayesian structures literature using scoring schemes Bacchus Suzuki scoring method Cooper Herskovits Experiments involved Brain Tumor dataset Cooper Breast Cancer Blake 1998a ALARM Beinlich Blake 1998b experimental results presented Table table contains scores published structures according Williams Williamson Beinlich assume distribution priors structures given dataset uniform Cooper Herskovits Experiments performed machine Intel processor scores generated network structures depends cases better scores established structures scores higher scores lower Figure represents different structures Brain Tumor dataset Structure Simovici Baraty Experimental Results Generated Structures 10000 Score Score 13631 13474 13680 13693 Original Structure 14410 Generated Structures Score Score 13667 Original Structure Brain Cancer Results Breast Cancer Results Generated Structures 20002 Score Score 114931 270298 114981 271590 12801 116081 272665 12802 116914 271469 Original Structure 159306 378518 Generated Structures Score Score 127543 13279 Original Structure 261481 Alarm Results Results introduced Cooper Structures generated approach Conclusions developed approach generating Bayesian network structure based notion generalized entropy parent child relationships among attributes obtained values highly dependent suggests approach preferable using Shannon entropy References Beinlich Suermondt Chavez Cooper ALARM monitoring study probabilistic inference techniques belief networks Technical Report Stanford University Knowledge System Laboratory Blake Newman Hettich 1998a repository machine learn databases dataset Ljubljana Oncology Institute provided available mlearn MLRepository Blake Newman Hettich 1998b repository machine learning databases dataset created Fisher donated Michael Marshall available mlearn MLRepository Cooper NESTOR computer based medical diagnosis integrates casual probabilistic knowledge thesis Stanford University Cooper Herskovits Bayesian method induction probabilistic networks Technical Report Stanford University Knowledge System Laboratory Havrda Charvat Quantification methods classification processes cepts structural entropy Kybernetica Inference Bayesian Networks Brain Tumor Structures Bacchus Learning Bayesian belief networks approach based principle Computational Intelligence Rissanen Modeling shortest description Automatica Simovici Jaroszewicz axiomatization partition entropy Transac tions Information Theory Simovici Jaroszewicz metric splitting criterion decision trees International Journal Parallel Emergent Distributed Systems Suzuki Learning Bayesian belief networks based principle efficient algorithm using branch bound technique IEICE Trans Information Systems Williams Williamson Combining argumentation Bayesian breast cancer prognosis Journal Logic Language Information Résumé proposons nouvel algorithme extraire structure réseau Bayésien ensemble données Notre approche basée entropies conditionnelles généralisées famille conditionnelle entropies étend entropie conditionnelle Shannon sultats indiquent choix approprié entropie conditionnelle généralisée obtenons réseaux Bayésiens scores supérieurs structures similaires méthodes classiques inférence