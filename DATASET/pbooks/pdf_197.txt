Plongement métrique calcul similarité sémantique échelle Julien Subercaze Christophe Gravier Frédérique Laforest Université 42023 Saint Etienne France UMR5516 Laboratoire Hubert Curien 42000 Saint Etienne France Université Saint Etienne Monnet 42000 Saint Etienne France Résumé explorons plongement métrique court chemin hypercube Hamming objectif améliorer performances similarité sémantique Wordnet Subercaze montrons plongement isométrique impossible pratique plongements isométriques obtenons améliora performances trois ordres grandeur calcul similarité Leacock Chodorow Introduction concept similarité sémantique encode distance conceptuelle entre unités langage Quand unités discours cette similarité breuses tâches notamment essentielle désambiguisation Basile approches dominent œuvre calcul similarité sémantique approches basées bases connaissance exploitent aussi structure taxonomie Leacock Chodorow contenu Banerjee Pedersen Jiang Conrath approches ignorent informations contextuelles utilisent bases décrites manuellement opposé sémantique statistique encode milarité sémantique basant observations statistiques exemple occur rences corpus Cependant cette approche largement limité quant volume données traiter nouvelles approches plongement sémantique statistique espaces vectoriels compact embedding Collobert Weston apportent réponse efficace problème architectures neuronales permettent traitement larges volumes données temps entrainement ordre plusieurs jours popularité approches neuronales montre enthousiasme certain approches efficaces calcul similarité entre paires article proposons plongement similarité sémantique Leacock Chodorow hypercube Hamming dimensions alignées taille processeurs similarité Leacock Chodorow basée métrique court chemin relation hypernymie Wordnet mesures précises évaluation Miller Charles atteint deuxième performances légèrement dépassées approche Jiang Conrath basée contenu Plongement métrique calcul similarité sémantique échelle techniques plongement hypercube Hamming permette bénéficier temps calcul performants processeurs modernes montrons abord plongement isométrique possible requiert nombre dimensions élevé utilisable pratique Notre résultat principal construction plonge isométrique métrique court chemin arbre maintenant forte corrélation distances originales expériences montrent trois ordres grandeur temps calcul rapport implémentation référence Plongement métrique court chemin introduisons abord notations hypercube dimensions nœuds labellisés tuples binaires nœuds adjacents seule leurs tuples correspondants diffèrent exactement position distance Hamming égale suite article dénote espace métrique Notre problème suivant définir fonction chaque nomie Wordnet associe scalaire raisons faisabilité complexité cette fonction raisonnable petit possible Limites théoriques relation hypernymie Wordnet forme treillis première approche serait réaliser plongement isométrique treillis métrique court chemin Cependant travaux Laurent montré plongement existe requiert dimensions nombre nœuds treillis telles dimensions permettent plonger treillis nœuds hiérarchie Wordnet compte plusieurs dizaines milliers nœuds Cette solution impossible mettre œuvre borne stricte faudrait amélioration drastique obtenir plongement quelconque intérêt pratique Plongement arbre réduire dimension plongement entreprenons plifier structure treillis arbre treillis Wordnet effet dense obtenons arbre coupant liens moins pourcent liens originaux nature coupe objet recherche poussées article suivons heuristique simple ayant plusieurs hypernymes préservons premier ordre Wordnet expériences montrent similarité arbre fortes corrélations similarité calculée treillis Wilkeit montré arbre taille plongé donnons algorithme temps linéaire Wilkeit général servira construction notre solution parcours ordre profondeur assigne tuple suivant concatène zéros tuple parent résultat obtenu plongement nécessitant plusieurs dizaines milliers dimensions notre objectif plusieurs processeurs grand nombre dimensions conséquence cette concaténation signature résulte faible densité opposé approche Stallmann utilise Cette méthode optimale terme dimensions introduit erreur plongement inutilisable tableau montre compacité plongement faibles corrélations Subercaze recherche exhaustive meilleur plongement dimensions cible arbitraire portée puissances calcul actuelles plongement dimension arbre nœuds espace recherche contient possibilités obtient alignement processeur 10100 Plongement isométrique Notre approche compromis entre plongement isométrique approche Stallmann basée codes Notre plongement introduisant erreur pouvons choisir préserver distance arbre autre distance parent enfants distance inter enfants conséquence conception solution basée propriétés arbre plongé hypercube analysons caractéristiques arbre coupe treillis arbre possède facteur branchement écart nœuds facteur branchement inférieur inverse profondeur arbre stable moyenne écart maximum stabilité profondeur décidons favoriser distance parent enfant minimiser dimensions plongement souhaitons allouer moins possédant Notre approche principes suivants 00000 00001 00010 00100 01000 10000 Isometrique ordre additionnel valeur Approches réduction dimensions Héritage branche chaque hérite signature différence plongement isométrique exten induit extension nœuds Cette proche permet garantir taine compacité structure Préservation distance parent enfant allouant moins nombre requis plongement isométrique notre approche introduit naturellement erreur faisons choix allouer noeud dlog2 étendre cette signature garantir unicité signatures Alignement cesseurs principes cédemment énoncés permettent obtenir plongement compact arbres faible profondeur comme celui obtenu partir hypernymie Wordnet Cependant dimen plongement obtenu nécessairement aligné taille souhaitons alors exploiter inutilisés seront toutes façons manipulés processeur proposons trier Plongement métrique calcul similarité sémantique échelle nœuds selon valeur façon déterminer quels nœuds éligibles obtention plusieurs supplémentaires Puisque notre approche favorise conser vation distance souhaitons favoriser allocation nœuds proches racine parents vaste descendance formule suivante donne compromis entre critères maxdepth depth sizebranch améliorer qualité plongement introduisons optimisations supplémentaires optionnelles première choisissons préserver mieux nombreux descendants seconde valeur présenté figure Parmi 2dlog2 signatures possibles seulement utilisées notre approche exemple figure allouons trois signatures favorisons utilisations signatures introduisent erreur minimale différence approche basée codes Expérimentations Dimension plongement Combiné valeur influence optimisations dimen sions corrélation distance calculée arbre Wordnet cette section place expériences évaluer qualité perfor mances notre approche première expérience testons qualité plongement corrélation distance originale ainsi tance calculée arbre coupe deuxième expérience performances exécution notre approche implé mentation référence intègre notamment mécanisme cache réduire temps calcul Notre algorithme Similarity Embedding plémenté disponible publiquement adresse suivante FSELCH implémentons différentes versions algorithme algorithme contenant aucune optimi sations présentées section précédente autres versions permettent utilisation respec optimisations version contenant optimisations Subercaze Qualité plongement Plongement Pearson Spearman Isometric Corrélations entre plonge isométrique toutes tances paires Wordnet values abord mesurons corréla distances obtenus plongement distances obtenues arbre mesurer impact optimisations montre infuence dimensions optimisations valeur coefficient corrélation Pearson version obtient meilleure valeur dimension concerne optimisations valeur toutes mensions Finalement binaison optimisations permet obtenir meilleure corrélation autres versions algorithme mesurons ensuite corrélation mesure treillis originial calculons corrélation millions distances depuis données Wordnet corrélation entre mesure obtenue arbre treillis donne borne théorique pouvons atteindre présente corrélations particulièrement intéressante relativement proches borne théorique Notre approche requiert moins plongement isométrique maintenant forte corrélation mesure similarité Temps exécution Measure Nombre pairs speedup Temps exécution millisecondes calcul similarité tableau présente temps exécution similarité comparons résultats temps exécution librairie librairie efficace utilisant cache mémoire centrale calcul distance nécessitant instruction cesseurs POPCNT offre surprise meilleures perfor mances jusqu trois ordres grandeur faible nombre paires facteur descend grands volumes données notamment mécanisme cache stocke distances intermédiaires court chemin gagne efficacité nombre requêtes wordnet princeton wordnet download standoff google Plongement métrique calcul similarité sémantique échelle Conclusion article montrons nouvelle approche plongement métrique court chemin arbre améliorer substantiellement performances calcul similarité Leacock Chodorow montrons plongement isométrique treillis hiérarchie Wordnet faisable aucun intérêt pratique effectuons coupe treillis obtenir arbre avons proposé heuristique plonger arbre distance court chemin hypercube Hamming dimensions alignées celles processeurs expérimentations montrent amélioration trois ordres grandeur maintenant forte corrélation Références Banerjee Pedersen adapted algorithm sense disambiguation using wordnet Computational linguistics intelligent processing Basile Caputo Semeraro enhanced sense disambiguation algorithm through distributional semantic model COLING Stallmann embedding binary trees hypercubes Journal Parallel Distributed Computing Collobert Weston unified architecture natural language processing neural networks multitask learning international conference Machine learning Laurent Geometry Metrics Springer pages Jiang Conrath Semantic similarity based corpus statistics lexical taxonomy CICLing Leacock Chodorow Combining local context wordnet similarity sense identification WordNet electronic lexical database Miller Charles Contextual correlates semantic similarity Language cognitive processes Subercaze Gravier Laforest metric embedding boosting semantic similarity computations Association Computational Linguistics Wilkeit Isometric embeddings hamming graphs Journal Combinatorial Theory Series Summary paper explore embedding shortest metrics knowledge Wordnet Hamming hypercube order enhance computation performance although isometric embedding untractable possible achieve isometric embeddings report speedup three orders magnitude computing Leacock Chodorow similarities while keeping strong correlations