actes_non_num 351rotes critère évaluation bayésienne construction arbres décision nicolas voisine boullé carine orange avenue pierre marzin 22300 lannion nicolas voisine orange ftgroup boulle orange ftgroup informatique louis broglie 22300 lannion résumé présentons article nouvel algorithme automatique apprentissage arbres décision abordons problème selon approche bayésienne proposant aucun paramètre expression lytique probabilité arbre connaissant données transformons problème construction arbre problème optimisation recherchons espace arbres décision arbre optimum critère bayésien ainsi défini arbre maximum posteriori optimisation effectuée exploitant heuristique élagage expérimentations comparatives trente bases montrent notre méthode obtient performances prédictives proches celles étant beaucoup moins complexes introduction construction arbres décision partir données problème commencé traité construisant premier arbre régression prédire variables numériques morgan sonquist suite leurs travaux toute littérature décrire modèles arbre variables prédire numériques arbres régression variables catégorielles arbres décision pourra référer ouvrage graphe induction zighed rakotomalala amples détails différentes méthodes arbres décision méthodes chaid quinlan début années méthodes restent encore références citer méthodes breiman méthode quinlan années références évaluer performances nouveaux algorithmes premiers algorithmes apprentissage automatique arbre décision basés élagage principe construction consiste partir racine arbre totalité ensemble apprentissage choisir parmi toutes variables plicatives celle donne meilleure partition selon critère segmentation récursive applique algorithme segmentation feuilles processus arrête quand chaque feuille améliorer critère segmentation choix variable coupure points coupure caractérise processus segmentation arbres chaid utilisent théorie information théorie evaluation bayésienne construction arbres décision statistique comme critère évaluer coupure toute difficulté algorithmes élagage consiste savoir arrêter développement mieux suffisamment avoir bonnes performances détaillé éviter apprentissage partir travaux breiman nouveaux algorithmes basés élagage étudiés principe construction arbres élagage alors étapes première étape consiste construire arbre poursuivant processus segmentation possible pertinent seconde étape consiste alors élaguer arbre supprimant branches minimisant critère élagage temps apprentissage performances arbre meilleures méthodes lisent critère élagage estimation erreur classification certaines méthodes utilisent estimateur ensemble apprentissage autres ensemble validation approches nécessitent définir façon heuris tique paramètres choix critères troisième approche nettement moins utilisée consiste utiliser principe minimum description length quinlan rivest arbres décision classe modèles mature laquelle amélioration performances désormais marginale performances arbre dépendent principalement structure arbres arbres petits prudents performances moindres breiman arbres grands apprennent données appren tissage performances effondrent ensemble contre choix variables segmentation segmentation reste problème important permet phase descendante prendre compte variables informatives remises cause phase élagage arrive autant fréquemment nombre variables explicatives important toute problématique construction arbres décision alors savoir quelle branche continuer développement arbre quelles variables utiliser segmentation quand arrêter méthodes référence chaid utilisent plusieurs paramètres apprendre arbre optimum paramètres choix variables coupure variables numériques groupage données catégorielles paramètre élagage arbre aucune méthodes propose critère global homogène prenant compte structure arbre choix variables coupures performances arbre wallace patrick suite travaux rivest quinlan utilisent approche définir critère global arbre prenant compte structure arbre tribution classes feuilles wallace patrick algorithme consiste élaguer arbre jusqu critère optimum méthode implémen quinlan rivest avaient donné intégré méthode néanmoins cette démarche reste incomplète prend compte choix variables segmentation modèle complexité données étudions article problème apprentissage automatique paramètre arbres selon approche bayésienne critère complet objectif transformer problème classification problème recherche opérationnelle meilleur arbre espace famille arbres décision approche montré intérêt sélection variables discrétisation supervisée variables numériques boullé groupage supervisé variables catégo rielles boullé classification supervisée modèle selective naive bayes boullé notre objectif développer arbre décision utilisant approche voisine évaluer comparer performances méthodes alternatives parti culièrement méthodes arbre décision simplecart package garner référence académique article organisé façon suivante section rappelle approche univarié section décrit extension cette approche arbres décision section présente évaluation méthode enfin section conclut article approche cette section rappelle principes approche discrétisation supervisée boullé discrétisation supervisée traite variables explicatives numériques consiste partitionner variable explicative intervalles conservant maximum information relative classes valeurs variable catégorielle expliquer compromis trouvé entre finesse information prédictive permet discrimination efficace classes fiabilité statistique permet généralisation modèle discrétisation approche discrétisation supervisée formulée problème lection modèles approche bayésienne appliquée choisir meilleur modèle discrétisation recherché maximisant probabilité modle donnes sachant données utilisant règle bayes puisque quantité donnees dépend données alors maximiser modele donnees modele terme priori modèles terme vraisemblance données connaissant modèle premier temps famille modèles discrétisation explicitement définie paramètres discrétisation particulière nombre intervalles bornes intervalles effectifs classes intervalle second temps distribution priori proposée cette famille modèles cette distribution priori exploite hiérar paramètres nombre intervalles abord choisi bornes intervalles enfin effectifs classe choix uniforme chaque étage cette hiérarchie distributions classes intervalle supposées indépendantes entre elles soient nombre individus nombre classes nombre intervalles nombre individus intervalle nombre individus classe intervalle contexte classification supervisée nombre individus classes supposés connus modèle discrétisation supervisée entièrement caractérisé paramètres utilisant définition famille modèles discrétisation distribution priori formule bayes permet calculer explicitement probabilités posteriori modèles connaissant données prenant négatif probabilités conduit critère évaluation fourni formule modele donnees modele evaluation bayésienne construction arbres décision trois premiers termes représentent probabilité priori modèle choix nombre intervalles bornes intervalles distribution multinomiale classes chaque intervalle dernier terme représente vraisemblance probabilité observer classes connaissant modèle discrétisation discrétisation optimale recherchée optimisant critère évaluation moyen heuristique gloutonne ascendante décrite boullé issue algorithme optimisation optimisations effectuées voisinage meilleure solution évaluant combinaisons coupures fusions intervalles algorithme exploite décomposabilité critère intervalles permettre après optimisations ramener complexité algorithmique temps arbre décision cette section appliquons approche arbres décision explici famille modèles envisagée présentant critère évaluation global arbres résultant approche bayésienne sélection modèles définition arbre décision consiste prédire variable expliquer catégorielle partir variables explicatives numériques catégorielles problème apprentissage consiste trouver structure arbre meilleures performances gardant petite taille possible toute difficulté consiste trouver compromis entre performance structure arbre permettant bonne généralisation modèle approche arbres décision consiste trouver famille arbres décision celui maximise probabilité modèle connaissant données comme discrétisation section applique approche bayésienne choisir arbre décision maximise probabilité arbre donnees revient maximiser arbre donnees arbre arbre terme priori arbre décision donnees arbre terme vraisemblance données connaissant modèle suite utiliserons notation suivantes modèle arbre décision ensemble variables explicatives ensemble variables explicatives utilisées arbre ensemble nœuds internes arbre ensemble feuilles arbre nombre individus variable segmentation voisine feuille feuille feuille feuille exemple arbre décision nœuds internes représentent règles feuilles représentent distribution classes nombre valeurs variable catégorielle nombre nombre individus nombre individus classe feuille modèle arbre décision représenté structure répartition individus cette structure distribution classes feuilles figure structure modèle arbre représentée ensemble nœuds internes nœuds ayant moins ensemble feuilles nœuds ayant liens entre nœuds répartition individus cette structure définie coupures nœuds internes ainsi effectifs classes feuille ensemble paramètres arbre ainsi défini ensemble variables utilisées modèle nombre riables choix variables prises parmi nature nœuds répartition individus nœuds internes variable segmentation nombre intervalles groupes evaluation bayésienne construction arbres décision distribution exemples intervalles groupes répartition classes feuilles critère évaluation critère évaluation proposons négatif probabilité posteriori arbre connaissant données probabilité données étant constante modèle critère défini arbre arbre donnees arbre choisissons probabilité priori modèle arbre exploitant hiérarchie parmi paramètres modélisation cette hiérarchie objectif définir relations dépendance entre paramètres choix priori inspire extensions hiérarchiques approche bayesienne paramétrage complexe exprime incertitude paramètres niveau conditionnellement incertitude paramètres niveau bayes permet alors formuler arbre selon principe parcimonie proche approche minimum description length selon priori distribution paramètres existe plusieurs voies définir hiérarchie paramètres consisterait finir structure coupures répartition classes feuilles article proposons exploiter hiérarchie implicite arbre définissant modèle niveau racine indépendamment façon récursive continue définir racine jusqu feuilles arbre ainsi définir probabilité arbre décision arbre choisit sélectionner variables selon priori uniforme allant variable informative donne choix sélection variables prenant comme hypothèse choix nombre variables uniforme déduit probabilité priori nombre variables sélectionnées prend comme hypothèse chaque groupe variables sélectionnées équiprobable nombre sélections possibles nombre combinaisons remise variables parmi donne connaissant variables utilisées définir nature chaque arbre décision interne feuille prenant comme hypothèse uniformité états égaux valent voisine considère chaque interne choix variable segmentation indépendant équiprobable variables explicatives sélectionnées connaissant nature variable sélection numérique catégorielle ainsi nombre parties définir probabilité priori interne variable numérique façon analogue discrétisation univariée obtient variable catégorielle façon analogue groupement valeurs univarié définit probabilité interne finir définir probabilité priori feuille probabilité distribution classes feuille considérant distributions équiprobables revient calculer nombre distributions multinomiales individus classes maintenant expliciter probabilité observer données connaissant probabilité données dépend uniquement feuilles arbre connaissant modèle distribution multinomiale défini feuille déduit probabilité obser vation donnees arbre amélioration critère éviter arbre prudent augmenter probabilité priori modèle probabilité nombre coupures interne numérique faible quand nombre individus valeurs variables élevé nombre individus important probabilité choisir nombre parties faible rendre modèle prudent réduire performances prédiction amélioration proposons appuie approche rissanen propose codage optimal entiers naturels positifs bornés donne probabilité rissanen taille optimale entier positif borné alors selon approche écrire probabilité avoir coupures exploitant interne nécessairement partitionné moins éviter modéliser explicitement nature interne feuille evaluation bayésienne construction arbres décision limite alors modéliser nombre chaque feuille entre interne optimisé arbre alors ensembles nœuds internes utilisant variable segmentation numérique catégorielle quatre premiers termes priori correspondent choix structure modèle dernier terme correspond aptitude épouser données terme correspond choix ensemble variables segmentation termes suivants correspondent choix partitions nœuds internes variables numériques catégorielles quatrième terme priori représente choix distribution multinomiale classes chaque feuille dernier terme représente vraisemblance probabilité observer classes feuilles connaissant modèle arbre décision supervisé construction arbre optimum recherche optimum global critère complexité maximale temps nentielle rapport nombre individus impossible utiliser algorithme exhaustif trouver optimum notre article étendons heuristique classique construction arbre basée élagage algorithme consiste rechercher chaque feuille arbre meilleure partition suivant variables explicatives recommencer jusqu feuille partitionner feuille construit partitionnements chaque variable selon approche univariée discrétisation groupage évalue ajout partitions structure arbre global recherche optimum arbre série recherche optimum locaux niveau feuilles algorithme proche utilisés optimiser arbres décision chaid différence réside accroissement feuilles arbre indépendante selon critère global feuille arbre ainsi jamais développer parce jamais meilleur choix noter entre itérations seuls développé réévaluer algorithme garantit arriver optimum global complexité maximal kjn2log voisine algorithm algorithme descendant optimisation arbre entrées racine arbre sorties arbre optimise critère tantque amélioration faire toute feuille arbre faire toute variable faire recherche règle feuille suivant améliore mieux critère alors finsi alors finsi tantque arbre déséquilibré cette complexité réduit kjnlog arbre équilibré algorithme déterministe trouve chaque optimum expérimentation cette section présente résultats expérimentation permettant évaluer notre méthode construction arbres décision supervisés protocole expérimentations menées utilisant données blake décrits table représentant grande diversité domaines nombres indivi variables explicatives numériques catégorielles nombres classes évaluer notre critère qualité arbres avons testé variantes algorithme première consiste avoir aucune contrainte arbre ktree deuxième consiste imposer arbre structure binaire ktree limitant parties chaque interne avons comparé notre approche simplecart plémentations environnement analyse données garner avons comme paramètres définis défaut application avons évalué bonne prédiction nombre nœuds internes ainsi temps calcul critères évaluation évalués moyen validation croisée stratifiée niveaux evaluation bayésienne construction arbres décision résultats résultats évaluation résumés façon synthétique tableau repor chaque méthode moyenne géométrique critères évaluation données regard grande dispersion résultats selon domaines plication préférons étayer notre analyse moyenne géométrique permet comparer ratios entre différentes méthodes moyenne arithmétique affichée figure méthode arbre temps ktree ktree scart moyennes géométriques résultats expérimentaux bases bonne prédiction nombre nœuds temps calcul apprentissage arbre ktree constate globalement bonne prédiction avantage légèrement avantage ktree faibles différences surprenantes arbres décision technologie mature différences performance marginales revanche complexité structure arbres environ quatre moindre ktree moindre simplecart cette propriété interprétation arbre nettement aisée expert déploiement bases rapide niveau temps calcul ktree moyenne rapide simplecart concerne différences entre ktree ktree avantage arbre binaire obtient meilleurs bonne diction restant faiblement complexe constate également critère aussi faible montre performance arbres ktree clairement corrélée valeur critère évaluation analysant résultats détaillés table aperçoit perfor mances ktree moins bonnes domaines variables explicatives corré tictactoe letter bases segmentation images contre marketing adult ktree légèrement meilleur étant moins complexe conclusion critère bayésien présenté article permet évaluer arbre décision compte structure arbre choix variables explicatives coupures ainsi distributions classes chaque feuille critère complet aucun mètre méthode construction arbre décrite article heuristique élagage sélectionnant partitionnant intervalles groupes valeurs chaque variable explicative voisine domaine information données ktree ktree scart ktree ktree scart yeast waveformnoise waveform vehicle tictactoe thyroid sonar sickeuthyroid segmentation satimage pendigits 10992 mushroom letter 20000 led17 10000 ionosphere hypothyroid horsecolic hepatitis heart glass german breast australian adult 48842 résultats expérimentaux bases bonne prédiction nombre nœuds évaluations données démontrent critère permet créer arbres décision équivalents performance beaucoup moins plexe nombre nœuds générés constate aussi algorithme générant arbres binaires meilleur moyenne celui générant arbres aires laisse penser optimisation algorithme amélioration performances gligeable exemple envisageons utiliser heuristique élagage forçant développement arbre élaguant arbre obtenu basant toujours notre critère évaluation globale arbre références blake repository machine learning databases mlearn mlrepository evaluation bayésienne construction arbres décision boullé bayes optimal discretization method continuous attributes machine learning boullé bayes optimal approach partitioning values categorical tributes journal machine learning research boullé enhanced selective naive bayes method optimal discretization guyon nikravesh zadeh feature extraction foundations applications chapter springer breiman friedman olshen stone classification regression trees chapman garner waikato environment knowledge analysis zealand computer science research students conference exploratory technique investigating large quantities categorical applied statistics morgan sonquist problems analysis survey proposal journal american statistical association quinlan rivest inferring decision trees using minimum description length principle comput quinlan induction decision trees machine learning quinlan programs machine learning mateo morgan kaufmann rissanen modeling shortest description automatica wallace patrick coding decision trees machine learning zighed rakotomalala graphes induction france hermes summary paper present automatic training algorithm decision trees exploit parameter bayesian approach propose analytic formula evaluation probability decision given transform training problem optimisation problem space decision models search which maximum posteriori optimisation performed using heuristic extensive experiments databases method obtains predictive performance similar alternative state methods simple trees