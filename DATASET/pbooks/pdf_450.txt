Combinaison classificateurs simples sélection rapide caractéristiques Hassan Chouaib Florence Cloppet Salvatore Antoine Tabbone Nicole Vincent Laboratoire LIPADE Université Paris Descartes Saints Pères 75006 Paris France prenom parisdescartes LORIA Université Nancy Campus scientifique Vandoeuvre Nancy Cedex France tabbone loria Résumé sélection caractéristiques technique permettant caractéristiques pertinentes celles adaptées résolution problème particulier méthodes classiques présentent certains inconvénients exemple elles peuvent complexes elles peuvent faire dépendre caractéristiques sélectionnées classificateur utilisé elles risquent lectionner caractéristiques redondantes limiter inconvé nients proposons article nouvelle méthode rapide sélec caractéristiques basée construction sélection classifica teurs simples associés chacune caractéristiques optimisation algorithme génétique proposée trouver meilleure combinaison classificateurs Différentes méthodes combinaison considérées notre problème Cette méthode appliquée différents ensembles caractéristiques tailles variées construite partir chiffres manuscrits MNIST résultats obtenus montrent robustesse approche ainsi efficacité méthode moyenne nombre caractéristiques sélectionnées diminué conservant reconnaissance Introduction nombreux domaines vision ordinateur reconnaissance formes nombreuses applications résolution problèmes traitement extraites partir données acquises monde structurées forme vecteurs qualité système traitement dépend directement choix consti tution vecteurs nombreux résolution problème devient presque impossible cause dimensionalité importante vecteurs incohérences peuvent apparaitre données conséquent souvent utile parfois néces saire réduire dimension espace représentation taille compatible méthodes résolution cette réduction conduire petite perte informa tions Parfois résolution phénomènes complexes descripteurs grande taille Combinaison classificateurs simples sélection rapide caractéristiques gérée utilisant caractéristiques extraites données initiales elles repré sentent variables pertinentes problème résoudre méthode réduction dimensionnalité souvent définie comme processus prétraitement données permet supprimer informations redondantes bruitées multiplication quantité données redondance bruit informa tions toujours présents méthode réduction dimentionalité sélection caractéristiques méthodes consistent sélectionner caractéristiques perti nentes partir ensemble données méthodes sélection existantes littérature présentent limitations différents niveaux complexité interactions entre caractéristiques dépendance classificateur utilisé évaluation limiter inconvénients présentons nouvelle méthode sélection caractéristiques Cette méthode basée sélection meilleure binaison classificateurs partir classificateurs simples construits chacune caractéristiques cette sélection algorithme génétique suite article organisée comme section motivons notre choix sélection caractéristiques présentant limitations méthodes abordons aussi sujet sélection caractéristiques algorithme génétique section présentons notre méthode sélection menons section étude expérimentale approfondie Enfin concluons approche donnons pectives notre travail section Sélection caractéristiques méthodes sélection caractéristiques généralement définie comme processus recherche permettant trouver ensemble pertinent caractéristiques parmi celles semble départ notion pertinence ensemble caractéristiques dépend jours objectifs critères problème méthode sélection passe généralement quatre étapes premières consistent initialiser point départ partir duquel recherche commencer définir procédure recherche stratégie recherche définie ensembles générés méthode évaluation définie troisième étape étapes trois répètent jusqu critère arrêt critère arrêt représente quatrième étape méthode général stratégies recherche peuvent classées trois catégories exhaustive heuristique aléatoire méthodes utilisées évaluer ensemble caractéristiques algorithmes sélection peuvent classées catégories principales filter wrapper Filter modèle filter anciennement utilisé sélection caractéristiques critère évaluation utilisé pertinence caractéristique selon mesures reposent données apprentissage Cette méthode considérée comme étape prétraitement filtrage évaluation généralement indépendamment classificateur Chouaib principal avantage méthodes filtrage efficacité calculatoire robus tesse apprentissage Malheureusement méthodes tiennent compte interactions entre caractéristiques tendent sélectionner caractéristiques comportant information redondante plutôt complémentaire Guyon Elisseeff méthodes tiennent compte performance méthodes classification utilisées après sélection Kohavi Wrapper principal inconvénient approches filter ignorer influence caractéris tiques sélectionnées performance classificateur utiliser suite Kohavi introduit alors concept wrapper sélection caractéristiques Kohavi méthodes wrapper appelées aussi méthodes enveloppantes évaluent ensemble caractéristiques performance classification utilisant algorithme apprentissage classificateur donné ensembles caractéristiques sélectionnés cette méthode adaptés gorithme classification utilisé forcément valides change classi ficateur complexité algorithme apprentissage appliqué chaque ensemble caractéristiques testé méthodes wrapper couteuses temps calcul problème complexité cette technique impossible utilisation stratégie recherche exhaustive problème complet conséquent méthodes recherche heuristiques aléatoires peuvent utilisées méthodes enveloppantes capables sélectionner ensembles caractéristiques petite taille performants classi ficateur utilisé existe inconvénients majeurs limitent méthodes complexité temps calcul nécessaire sélection dépendance caractéristiques pertinentes sélectionnées rapport classifica utilisé Algorithme génétique sélection caractéristique algorithmes génétiques constituent techniques récentes domaine sélection caractéristiques Honavar Oliveira Kitoogo Baryamureeba Duval application résolution problème nécessite coder solutions potentielles problème chaînes finies constituer chromosomes constituant population formée candidats trouver fonction sélective permettant bonne discrimination entre chromo somes définir opérateurs génétiques fonction fitness sélection caractéristiques définie utili modèle filter wrapper évaluation individus génération fonction fitness risque devenir coûteuse temps calcul augmenta nombre individus particulièrement évaluation wrapper construction classificateur chacun individus indispensable limiter probléme proposons construire classificateur profiter avantages approches filter wrapper qualité associée chacune caractéristiques filter Combinaison classificateurs simples sélection rapide caractéristiques performance classificateur optimisée wrapper combinant classifica teurs simples passer phase apprentissage classificateur chacun ensembles caractéristiques objectif représente principale notre nouvelle méthode sélection crivons section suivante Méthode proposée méthodes filtrage sélection caractéristiques présentent limitations prise compte interactions potentielles entre caractéristiques méthodes enveloppantes wrapper souffrent complexité élevée ainsi pendance classificateurs utilisés évaluation limiter inconvénients proposons nouvelle méthode sélection caractéristiques basée sélection classificateurs simples filtrage rapidité prise compte caractéristiques individuellement retenons cette construisant ensemble classificateurs associé caractéristique vision globale méthode wrapper conservée considérant critère sélection prenant compte ensemble caractéristiques retenues oeuvre détaillerons fonction fitness section tenons ainsi compte interactions entre caractéristiques caractéristiques sociées ensemble final classificateurs sélectionnés notre méthode représentent ensemble final caractéristiques Processus sélection dispose ensemble composé caractéristiques semble apprentissage composé échantillons exemples chaque représente exemple exemple représenté vecteur composantes valeurs caractéristiques dimension nombre total caractéristiques dispose ensemble quettes exemples cette problème classification classe divisons ensemble apprentissage parties apprentissage contient exemples construire ensemble classificateurs validation contient exemples utilisés algorithme génétique figure représente processus notre méthode sélection processus comporte étapes construction classificateurs simples chacun compte caractéristique sélection classificateurs algorithme génétique première étape sélection consiste constituer ensemble classificateurs repré sentent caractéristiques initiales proposées ensuite comme entrées algorithme tique Chaque classificateur modèle simple entraîné seule caractéristique cette étape ensemble construit représente classi ficateur appris caractéristique deuxième étape applique algorithme génétique sélectionner proche Chouaib Schéma général processus sélection proche ensemble classificateurs caractéristiques associées modèles finalement sélectionnés représentent ensemble caractéristiques Construction ensemble classificateurs Cette étape consiste construire ensemble classificateurs chaque élément classificateur entraîné seule caractéristique classificateur perfor possible méthodes usuelles basées seuil classification Donnons exemples classificateur binaire simple proposé Alamdari calcul seuil tique seuil utilisé milieu segment extrémités isobarycentres valeurs caractéristique données chacune classes suite utilisons Classif_Alamdari désigner classificateur autre classificateur decision stump Langley sificateur partie classificateurs faibles connus utilisés algorithme AdaBoost proposons introduire plusieurs seuils classification construire classificateur seuils peuvent seuils associés noeuds arbre décision Breiman utilisant algorithme AdaBoost Freund Schapire construit partir différents classificateurs faibles decision stump partir classificateur decision stump initial définissons nouveau classificateur modifiant poids ensemble apprentissage exemples classés obtenir nouveau classificateur α1hs1 α2hs2 construction classifica alors issue application principe manière itérative avons nombre itérations éviter problème apprentissage suite utilisons désigner classificateur Critère sélection fonction fitness méthodes wrapper fonction fitness construction nouveau classificateur caractéristiques impliquées individu contourner cette approche fallait trouver compromis grandeur pourrait remplacer erreur fournie nouveau classificateur caractéristiques sélectionnées avoir recours nouvel apprentissage classificateurs construits Combinaison classificateurs simples sélection rapide caractéristiques donnent information adaptée problème caractéristiques seules Chaque classificateur sélectionné participer décision notons chromosome ensemble introduisons alors classificateur construit comme combinaison classificateurs Combi combi combinaison classificateurs présents individu Ainsi fonction fitness écrire fitness erreur opérateur prendre multiples formes étudions maintenant combinaison classificateurs avons utilisé méthodes classiques combinaison comme majoritaire majoritaire pondéré moyenne moyenne pondérée médian ainsi Aggregation Weight Functional Operator proposé Dujet Vincent Néanmoins méthode adaptation nécessaire appliquer notre version initiale était entendu ensemble valeurs agréger appartenait intervalle lequel qualité valeurs relativement objectif était monotone notre classification classes classes caractérisées respectivement statut équivalent permettant définir valeur distinguée globale significative cherchons combiner réponses classificateurs réponses comprises entre disposons valeurs optimales valeurs positives négatives propos elles indiquent moins confiance appartenance classe autre avons choisi agréger séparément réponses positives réponses négatives Ainsi avons valeurs distinguées valeurs négatives valeurs positives figure Contexte objectif méthode méthode propose considérer exclusivement réponse classificateur aussi distribution toutes réponses réaliser agrégation poids chaque réponse calcul formule suivante signe signe Chouaib signe signe Expérimentations validation cette section présentons expérimentations avons réalisées illustrer notre méthode sélection commençons décrire bases données avons utilisées ainsi descripteurs associés montrons ensuite apports méthodes combinaison utilisées notre approche comparons résultats obtenus autres méthodes sélection Bases donnée protocole expérimentation expérimentation avons utilisé ensembles données espaces dimension variées construits partir MNIST chiffres manus crits isolés construite Lecun chaque chiffre associées images taille niveaux figure MNIST divisée ensembles ensemble apprentissage exemples ensemble exemples Chaque allons utiliser expérimentations Exemples images extraites MNIST construite calculant descripteur particulier images MNIST Comme cripteurs avons utilisé plusieurs descripteurs formes parmi lesquels descripteur Zernike descripteur Fourier générique Zhang signature Tabbone Wendling luminance pixel descipteur caractérisé variables respectivement résolution radiale résolution angulaire Différentes valeurs testées suite utilisons abréviations designer bases utilisées représente construite partir représente celle construite première ligne tableau résume dimensions espaces représentation associés chacun descripteurs utilisés traitons priori problèmes classes général classes avons choisi construire ensembles ensemble exemples étiquetés permettent utiliser approche contre Chaque ensemble associé classe représentent respectivement bases prentissage bases validation bases construits utili sation méthode contre Chacune bases contient exemples Combinaison classificateurs simples sélection rapide caractéristiques exemples classe exemples représentant toutes autres classes contre contiennent éléments exemples classe exemples représentant toutes autres classes MNIST avons Résultats cette section montrons résultats obtenus bases considérées utilisant classificateurs différents types construction ensemble initial classi ficateurs simples faite classificateurs décrits section premier classificateur considéré classificateur AdaBoost permet trouver plusieurs seuils adaptés selon exemples apprentissage constitue avantage rapport classificateurs basés seuil autre réponse classificateur numérique signe indiquant classe module constituant sorte confiance comprise entre format permet mettre oeuvre différentes méthodes combinaison arbres décision capables trouver plusieurs seuils fournissent étiquettes classes comme réponse finale accompagner grandeur numérique limite utilisation méthodes combinai majoritaire votes pondérés Ensuite comparons résultats obtenus classificateurs obtenus autres classificateurs commençons classificateur Après construction ensemble classificateurs simples après avoir sélectionné meilleur ensemble classificateurs différentes bases avons étude expérimentale influence méthode combinaison qualité ensembles sélectionnés tableau montre nombre moyen caractéristiques moyen classification avant après sélection chaque descripteur classes chacune bases classification évaluer qualité ensembles trouvés notre méthode calculé classificateur entraîné bases apprentissage testé bases remarquons ensembles finaux moyenne ensemble initial relativement proches quelque descripteur conclusion pouvons avons réussi sélectionner ensembles caractéristiques petits taille ensembles initiaux ayant qualité sélection Nombre caractéristiques 784Taux classification sélection Nombre caractéristiques 245Taux classification classification nombre caractéristiques chaque descripteur avant après sélection avons comparé résultats obtenus partir ensemble classificateurs basés mêmes descripteurs ensembles classificateurs autre Chouaib classificateurs simples Decision stump Classif_Alamdari avons testé différentes méthodes combinaison arbres décisions avons majoritaire majoritaire pondéré comme méthodes combinaison tableau montre meilleurs résultats obtenus chacun ensembles classifica Zernike GFD_8 GFD_10 signature Pixels classif_Alamdari decison_stump Arbre décision Comparaison résultats plusieurs bases classificateurs teurs chacune bases résultats montrent sélection partir ensemble classificateurs AdaBoost meilleure partir autres ensembles classificateurs résultats ensemble classificateurs arbres décisions proches AdaBoost possibilité utiliser plusieurs méthodes combinaisons AdaBoost classificateurs avantageux Finalement avons comparé résultats sélection utilisant différentes méthodes combinaison tableau montre résultats comparaison faisant moyenne classes tableau remarquons résultats trois méthodes combinaison Moyenne Moyenne pondérée proches différentes bases utilisées celles performantes méthodes combinaison majoritaire majoritaire pondérée médian Moyenne Moyenne Moyenne pondérée majoritaire pondérée Médian sélection Comparaison différentes méthodes combinaison Comparaison autres méthodes avons comparé notre méthode sélection trois autres méthodes existantes méthodes reposent différentes approches évaluation filter wrapper trois méthodes considérées Relief Rendell Kachouri troisième méthode warpper classique basée recherche aléatoire utilisant algorithme génétique notre méthode mêmes paramètres fonction fitness définie erreur classificateur tableau montre résultats comparaisons entre notre méthode autres méthodes sélection résultats Combinaison classificateurs simples sélection rapide caractéristiques calculés moyenne classes MNIST pouvons remarquer notre Relief Wrapper Notre méthode Zernike GFD_8 GFD_10 signature Pixels Comparaison autre méthodes chaque descripteur méthode nettement meilleure méthodes Relief contre résultats proches obtenus méthode Wrapper toutes bases tableau ailleurs avons calculé temps sélection notre méthode cette dernière méthode wrapper_SVM toutes bases tableau montre résultats pouvons remarquer notre méthode rapide pires meilleur Pixels aussi capable sélectionner ensembles taille petite pires meilleur Notre méthode carac 245Temps Wrapper_SVM Temps carac Comparaison temps relatifs sélection notre méthode méthode Conclusion perspectives article combinaison classificateurs simples algorithmes génétiques utilisés définir nouvelle méthode rapide sélection fonction fitness basée combinaison classificateurs simples chacun étant associé seule carac téristique Plusieurs méthodes combinaison ainsi plusieurs classificateurs testés évalués expérimentations différentes bases données construites partir MNIST montrent notre méthode réussi réduire nombre caractéris tiques conservant classification choix bases associée mêmes données monde permettent comparer comportement méthodes problèmes difficulté comparable ailleurs méthode proposée rapide pires méthode wrapper classique Finalement robustesse approche proposée confirmée futurs travaux seront consacrées étude diver entre classificateurs sélectionner minimiser redondance Ainsi approche multi objectifs utilisé intégrer nouvel objectif autre perspective Chouaib sélectionner caractéristiques utilisant approche hiérarchique appliquée plusieurs niveaux caractéristique descripteur Références Alamdari Variable selection using correlation single variable classifier thods Applications Feature Extraction Volume Studies Fuzziness Computing Springer Berlin Heidelberg Breiman Classification Regression Trees Chapman Dujet Vincent Feature selection classification International journal intelligent system Duval Advances metaheuristics selection classification microarray Briefings Bioinformatics Freund Schapire decision theoretic generalization learning application boosting Proceedings Second European Conference Computational Learning Theory London Springer Verlag Guyon Elisseeff introduction variable feature selection Learn Langley Induction level decision trees Proceedings ninth international workshop Machine learning Kohavi Pfleger Irrelevant features subset selection problem Machine Learning Proceedings Eleventh International Conference Morgan Kaufmann Kachouri Djemal Maaref Adaptive feature selection heterogeneous image databases Djemal Deriche Second International Conference Image Processing Theory Tools Applications Paris France modified zernike moment shape descriptor invariant translation rotation scale similarity based image retrieval ICME00 Rendell feature selection problem Traditional methods algorithm Cambridge Press Press Kitoogo Baryamureeba methodology feature selection named entity recognition International Journal Computing Kohavi Wrappers feature subset selection Artif Intell Lecun Bottou Bengio Haffner Gradient based learning applied document recognition Proceedings Toward integrating feature selection algorithms classification clustering Transations Knowledge Engineering Oliveira Sabourin Bortolozzi Feature selection using multi objective genetic algorithms handwritten digit recognition International Confe rence Pattern Recognition Volume Volume Combinaison classificateurs simples sélection rapide caractéristiques Tabbone Wendling Binary shape normalization using Radon transform International Conference Discrete Geometry Computer Imagery Volume Lecture Notes Computer Science Naples Italy Springer Honavar Feature subset selection using genetic algorithm Intelligent Systems their Applications Zhang Shape based image retrieval using generic fourier descriptors Signal Processing Image Communication Summary Feature selection happens important classification process reduce number features maintain improve formance classifier selection methods described literature present limitations different levels instance complex operated reason dependent classifier evaluation Others overlook interactions between features paper order limit these drawbacks propose selection method based genetic algorithm feature closely associated single feature classifier classifiers considered several degrees freedom mized training dataset Within genetic algorithm individuals classifier subsets evaluated fitness function based combination single feature classifiers Several combination operators compared whole method implemented extensive trials performed MNIST handwritten digits database Results robust approach efficient method average number selected features smaller initial nevertheless recognition decreased