Feedback Study Improvement Random Forest Mahout library context marketing Orange Voisine Lemaire Trinquart Orange avenue Pierre Marzin 22300 Lannion France Predicsis Broglie 22300 Lannion France Abstract realm systems Hadoop emerged popular systems diverse ecosystem grown around kinds functional technical needs niche should place choice ecosystem analytics first because getting value large datasets requires efficient Machine Learning algorithms because large clusters abundant resources appropriate playfields algorithms which often resource intensive computing tasks Unfortunately among myriad source projects there analytics tools ported Hadoop framework Apache Mahout stands among those initiatives project mainly known recommendation application offers warehouse algorithms advertised Reduce investigate twenty algorithms posed within Mahout report focus promising Random Forest implementation Relying extensive tests including specific marketing Orange provide depth feedback practical theoretical perspective suggest several improvements Introduction decreasing storage accumulation large complex datasets which widely opportunities business Orange multinational telecommunications corporation analyze network improve profitability create services sense scale order increase customer satisfaction services Orange analyze Quality Services Quality Experience indicators million mobile customers Those indicators result combination different sources network probe purpose consists detection prediction would allow Orange either improve quality network provide services based Therefore applying techniques these amounts crucial raises numerous issues scalability mining algorithms automation mining process control fitting Feedback Random Forest Mahout library Marketing scalability issue usually first people availability efficient computing environments Hadoop clusters reduce framework often considered solution scalability issue source project named Mahout actually claims provide implementation several learning algorithm repository actually Hadoop cluster taking advantage parallelization project gained increasing attention after companies reported using successful results precise Mahout gathers libraries supervised unsupervised learning success stories about Mahout relate unsupervised learning libraries contrary there little about supervised libraries Still these computing environments originally designed search engine tasks based indexing billions documents efficient families tasks cannot considered universal models parallel computing Among mining tasks deployment phase intensive likely reduce framework contrary modeling phase intensive exploiting efficiently resources Hadoop cluster problem previous study Dream observed Random Forrest method Mahout terms quality models terms scalability since algorithm natively parallel process study improve necessary algorithm Mahout Hadoop cluster first report preliminary study Random Forest Mahout library second suggest several improvements initial library third propose decision algorithm improve performance reduce fitting provide results academic datasets Orange before coming conclusion Preliminary study first study investigated behavior Random Forests Mahout library packaged latest available version section begins reminder algorithm second subsection provides experimental conditions experiments these report third subsection present obtained results which leads discussion about potential issues solutions Random Forest Random Forests supervised classifier introduced Breiman Breiman related decision approach predictive model longer consists single instead gathers multitude trees Random forests combination predictors depends values random vector sampled independently distribution trees forest generalization error forests converges limit number trees forest becomes large generalization error forest classifiers depends strength individual trees forest correlation between Using random selection features split yields error rates compare favorably Adaboost Freund Schapire robust respect noise Internal estimates monitor error strength Voisine Lemaire Trinquart correlation these response increasing number features splitting Internal estimates measure variable importance Random Forests necessary validation Indeed during bagging instances construction classifier learns portion Unused called provide estimate generalization performance classifier Experimental conditions Industrial Context report focusing behavior Mahout dealing which characteristics Orange constraints Heterogeneous Missing values Multiple classes Heavily unbalanced distributions scales millions instances thousands ables types Numerical Categorical Image restrict study supervised learning classification problem churn detection appetency prediction among potential datasets evaluation strong interest those closer marketing problems Parameters influence behavior Mahout installing Mahout library Hadoop platform there several parameters influence performances obtained using Mahout number machines nodes cluster their characteristics configuration distributed filesystem especially regarding blocks created replication factor split criterion algorithm number trees Forests combine trees following paragraphs describe these points indicate section baseline check validity results obtained Cluster tests performed small exploration Hadoop cluster Orange which following properties nodes Intel 20GHz total cores Hadoop version Cloudera Hadoop Mahout packaged within Cloudera Nodes Block primary components Hadoop platform MapReduce distributed system designed store files large volumes large number machines whereas MapReduce framework distributing process large files those components combined machines cluster nodes resulting platform single system providing availability Hadoop availability distributed object oriented platform distributed system addresses issues Hadoop distributed storage system which called Hadoop Distributed System incorporates analysis systems MapReduce Mahout Spark Hadoop allows splitting execution analysis parallel processing Feedback Random Forest Mahout library Marketing balancing parallel processing large decomposed blocks those distributed across nodes cluster Based blocks Reduce functions distributed across cluster perform subsets large allowing better scalability Unlike conventional storage system where blocks matter octets block default default value adjusted cluster administrator changed specific process Typical block sizes 128MB cluster configured default 128MB behavior Random Forests Mahout somewhat different original version Breiman therefore theory First bootstrap samples entire Because divided nodes mappers mapper realizes bootstrap mappers therefore realize training several trees Forest algorithm Random Forests Breiman therefore partly fulfilled mapper performs forest which sense bagging forest mapper combined realize global forest Split criterion Mahout default binary trees information Quinlan default split criterion variable chosen perform split numeric attributes leaves elaborated after categorical ables leaves elaborated after where number modalities chosen variable important Mahout handle missing values first experiment replaced missing values categorical variable substituting missing values entails consider missing value information numeric variable lower value other value Number trees number trees influences value increasing number trees increases chances having discriminating trees improvement especially observable beginning curve plotting versus number trees Forest database small upselling asymptotic value close performances after number trees influence which necessary deploy model experiments presented below paper number trees therefore mapper where number mappers Combining trees After large number trees generated popular class baseline results Orange developed powerful software named Khiops khiops which works large dataset Guyon multi table dataset Boullé software baseline standard version applied database distributed database evaluate results obtained Mahout classifier Averaging Selective Naive Bayes described Boullé Voisine Lemaire Trinquart Datasets types datasets which Adult belonging Machine Learning Repository Bache Lichman Pascal Large Scale Learning Challenge Sonnenburg Large Small marketing provided Orange challenge Orange Guyon Their characteristics given Table criteria evaluate obtained results Under curve Fawcett suggests there randomness algorithm Random Forests therefore performed training remaining paper Train presented Tables average experiments Dataset PCTrain PCTest Mappers Adult Large Upselling 14740 Large Churn 14740 Large Appetency 14740 Small Upselling Small Churn Small Appetency Dataset Number Instances Number numerical variables Number Categorical variabkes Number Classes Percentage target class PCTrain Percentage training PCTest Percentage Results standard library present results obtained Random Forest Mahout using installation default library split criterion information missing value dealt according description given previous section number trees number mappers indicated column Table predicted class popular predicted class forest table shows columns performances obtained ratio Train column element robustness value above indicates overfitting points these results exhibit results small dataset performance largest dataset result seems known machine learning community Debreuve slide results lower results literature these datasets results lower except those obtained Khiops large overfitting exists dataset analyze these results understand where problems comes split criterion information another missing values first information biased there categorical variables which modalities Harris individual trees wider second initial replace missing value Feedback Random Forest Mahout library Marketing appropriate changed obtained better results However Hadoop framework allows decreasing training showed table Dataset Khiops Mahout Robustness Mahout Adult Small Upselling Small Churn Small Appetency Large Upselling Large Churn Large Appetency First result standard Mahout KDDSmall Upselling KDDLarge Upselling Khiops 4h55m 45min Mahout 3m44s Training Khiops Mahout First Ideas circumvent observed problems Faced results obtained previous section apply several patches Mahout order improve performance library Mahout where change Information Ratio Mahout where missing values numerical variables replaced responding median value Breiman Dataset Khiops Mahout Default Mahout Mahout Adult Small Upselling Small Churn Small Appetency Large Upselling Large Churn Large Appetency Result modified Mahout Mahout Mahout Ratio Mahout Mahout information missing values replaced median value results obtained these modified version presented Table Mahout version better results standard version improvement could costly since median value numerical variables required because median value computed complete dataset which split different nodes others modification reported unbalanced database square computation information Flach Section Sensitivity skewed class distributions without improvement Voisine Lemaire Trinquart results Faced these results still wishing robust performances approach which known robust Boullé Adding regularization Random Forest section modifications decision Mahout Random increase classification performance reduce fitting first relies approach developed decision successfully Voisine consists changing splitting method numerical variable introduces grouping method categorical variables second improvement consists changing voting method default Mahout majority voting method estimate class probabilities propose estimate class probabilities averaging probability estimates decision trees Modifications Adding regularization describe splitting grouping methods classes learning classification splitting method discretization criteria based approaches discretization method supervised classification provides probable discretization given Extensive comparative experiments report performance Boullé value grouping categorical variables treated Boullé using similar approach Bayesian approach applied select discretization model which found maximizing probability Model model given Using Bayes since probability constant under varying model equivalent maximizing Model Model decided study elaborate binary decision classification problem classes which present Orange problem Therefore split criterion dedicated split intervals numerical variables grouping groups categorical variables setting allows having algorithm detailed report place consideration number instances denotes number instances interval number instances output value interval context number instances number classes supposed known number interval fixed numerical variables Boullé evaluation criterion number tervals number classes equal number classes evaluation criterion established problem design search algorithm order discretization model minimizes criterion Boullé standard greedy bottom heuristic discretization categorical variables Boullé evaluation criterion numberof groups equal groups where number value catagorical variable Feedback Random Forest Mahout library Marketing Voting Usually predicted class based votes trees popular class change perform averaging probability estimates trees argmaxJ where terminal leave predicted class highest probability confidence probability corresponding averaging probability estimated confidence allow better estimation computation Results Dataset Khiops Mahout Mahout Mahout Mahout Mahout Default Ratio Media voting Adult Small Upselling Small Churn Small Appetency Large Upselling Large Churn Large Appetency Result modified Mahout Mahout Information replaced Ratio Mahout Mahout information missing values replaced median value Mahout prediction based popular class Mahout prediction based averaging probability estimates Dataset Number Nodes Robustness Mahout Default Mahout Mahout Mahout Adult Small Upselling Small Churn Small Appetency Large Upselling Large Churn Large Appetency Robutness Model different number compare which approach split criterion results obtained previous section results Mahout Random Forest interesting results better Table Robustness better Table models smaller Table results interesting worse obtained Khiops Voisine Lemaire Trinquart Future works closer bootstrap sampling distribution study focused understanding Mahout setting impact building process especially focus splitting process thing mention datasets spread across nodes basis bootstrap sampling under scrutiny section Block shuffle dataset dropped divided blocks fixed those blocks spread across nodes cluster Hadoop cluster experiments default block 128MB change learning process Mahout consists Reduce mappers build trees which collected single Reducer mapper proceeds block trees built single mapper based sampling limited block mapper working Hence start principle bootstrap sampling different original version Breiman random sampling replacement whole dataset difference major consequences First initial distribution target dataset impact trees generated different mappers worst possible setting dataset sorted target value mapper might proceeds associated target modality Second depending number columns block contain relatively trees built small sample really depends block effective dataset issue those really large marketing datamarts which easily encompass columns problem block might tackled studying impact block prediction performance different datasets Hopefully where place cursor configure system properly given datamart agile approach environment where consider different datamarts where easier variables datamart would solve issue target distribution robust approach simultaneously tackle issues mentioned above would design takes splitting initial dataset while taking following constraints should produce subsets where degree parallelism should storing output subset specific block subset fully processed exactly mapper should populate subset through process random sampling placement whole initial dataset should offer option control target modalities distributed subsets stratified sampling Feedback Random Forest Mahout library Marketing Algorithm Prepare Dataset Algorithm Input Sample learning stored degree parallelism colTarget index target column dataset Output Sample learning prepared Mahout stored sequence reduce TargetDistribution DatasetSplit TargetDistribution takes input dataset outputs distribution target modalities first reducer DatsetSplit takes input dataset outputs splitted sample learning mappers DatasetSplit access distribution target modalities second reducers Mapper TargetDistribution targetModality extract value column colTarget current write reducer targetModality Reducer TargetDistribution input targetModality counters write targetModality counters Mapper datasetSplit Before iterating input block target distribution input block mod_target extract target modality current split based mod_target distribution write filterRows currentrow reducer Reducer DatasetSplit input reducerindex write output Split could easily designed sequence reduce first would estimate target modalities distribution second would actually perform stratified random sampling subtlety approach resides correctly handling splits respect average number individuals original datasets really large impact number single split consequently sample building large larger block Another strategy might decide columns should carried every split datasets first sampling consists picking family columns final split dataset would allow smaller larger number split sketch reduce functions presented Algorithm Shifting trees across nodes approach sketched previous subsection revolved around prepare appropriate could Mahout without actually changing library terms duplicate Voisine Lemaire Trinquart inital dataset prepared splits process might extra extra storage space Another approach could issue making built broadest sample initial dataset mapper bound block instead having mapper responsible growing complete trees propose transform Mahout process iteration reduce trees growing fraction Consider build forest trees thousand dataset split blocks process would initiated generating files these containing hundred empty trees would randomly affect these files mapper learning process would trees depth degree store updated trees files again randomly associate updated files mappers again iterate until trees fully developed approach would require substantial coding existing Mahout library cially store parse incomplete trees there certainly parameters should develop trees advantage possible build trees wider sample without moving dataset itself Conclusion study exhibits behavior Random Forest implemented Mahout frame several datasets interest Orange first statement observation standard version performances below observe state practical implementation framework Hadoop Mahout respect totally theoretical framework translates performances below expectation propose evolutions being introduction explicit regularization during training trees tests various datasets demonstrate improvement standard library There nevertheless important realize according random forest implemented within framework Mahout reaches performances batch algorithm There known compromise between precision speed volumetry prize seems rather important moment References ApachTM Hadoop project develops source software reliable scalable tributed computing hadoop apache ApachTM Mahout project build scalable machine learning library mahout apache Bache Lichman machine learning repository archive Boullé Bayes optimal approach partitioning values categorical tributes Journal Machine Learning Research Feedback Random Forest Mahout library Marketing Boullé Bayes optimal discretization method continuous attributes Machine Learning Boullé Towards automatic feature construction supervised classication accepted publication Boullé Compression based averaging selective naive Bayes classifiers Journal Machine Learning Research Breiman Missing value replacement training berkeley breiman RandomForests cc_home missing1 Breiman Random forests Learn Debreuve introduction random forests perso toulouse motimo files random forest Dream Evaluer apport plateformes hadoop classification Master thesis Polytech Lille Fawcett introduction analysis Pattern Recogn Flach Machine Learning Science Algorithms Sense Cambridge University Press Freund Schapire Experiments boosting algorithm Inter national Conference Mchine Learning Morgan Kaufmann Guyon Lemaire Boullé Vogel Design analysis scoring large orange customer database SIGKDD Explor Newsl Harris Information versus ratio study split method biases Orange Customer relationship prediction kddlarge kddsmall sigkdd customer relationship prediction Quinlan Induction decision trees Learn Sonnenburg Franc Sebag Pascal large scale learning challenge largescale berlin about Voisine Boullé bayes evaluation criterion decision trees Advances Knowledge Discovery Management Résumé apprentissage automatique apparition écosystème Hadoop créant puissance promise opportunité précédent domaine système Apache Mahout réponse question temps calcul lumétrie consiste entrepôt algorithmes apprentissage automatique portés exécuter Reduce rapport concentre portage utilisation algorithme Random Forest Mahout montre travers notre retour expérience difficultés peuvent rencontrées pratiques théoriques suggère piste amélioration Session Industrielle Feedback Study Improvement Random Forest Mahout library context marketing Orange Cedric Nicolas Voisine Vincent Lemaire Romain Trinquart