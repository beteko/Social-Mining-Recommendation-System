Commentaires Etude amélioration forêt aléatoire bibliothèque Mahout contexte données marketing Orange Voisine Lemaire Trinquart Orange avenue Pierre Marzin 22300 Lannion France Predicsis Broglie 22300 Lannion France Résumé domaine systèmes Hadoop émergé comme systèmes populaires écosystème diversifié grandi autour toutes sortes satisfait besoins fonctionnels techniques créneau aurait place choix écosystème données analyse abord parce obtention valeur grands ensembles données nécessite algorithmes apprentissage machine efficace parce grandes grappes ressources abondantes semblent comme playfields appropriées algorithmes souvent tâches informatiques exigeant beaucoup ressources Malheureusement parmi myriade projets source outils analyse données portés cadre Hadoop Apache Mahout distingue parmi rares initiatives projet principalement connu application recommandation offre également entrepôt algorithmes annoncés lancer carte Réduire avons enquêtons vingt algorithmes posés Mahout présent rapport concentrons prometteurs œuvre forêt aléatoire fondant tests approfondis compris données marketing spécifiques Orange fournissons rétroaction approfondie utilisation outil aussi point théorique pratique proposons plusieurs améliorations Introduction baisse stockage données conduit accumulation grands ensembles données complexes largement considérées comme nouvelles opportunités entreprises Orange société télécommunications multinationale analyser données réseau améliorer rentabilité créer nouveaux services donner échelle accroître satisfaction clients services Orange analyser qualité services qualité expérience indicateurs millions clients mobiles indicateurs résultent combinaison différentes sources données sonde réseau objectif principal consiste détection prévision temps qualité service permettrait Orange améliorer qualité réseau fournir nouveaux services basés conséquent application techniques grandes quantités données données cruciale soulève nombreuses questions telles évolutivité algorithmes exploration données automatisation processus exploration données contrôle surajustement Commentaires Forêt aléatoire données marketing Mahout problème évolutivité généralement premier esprit grandes données disponibilité environnements informatiques efficaces Hadoop clusters carte réduire cadre souvent considéré comme solution question évolutivité projet source nommé Mahout prétend fournir œuvre plusieurs algorithme apprentissage obtiennent seulement données dépôt réalité exécuté cluster Hadoop tirant ainsi profit parallélisation projet attiré attention après certaines entreprises déclaré avoir utilisé résultats précis Mahout fronces bibliothèques encadrés apprentissage supervisé histoires succès Mahout concernent toutes bibliothèques apprentissage supervisées contraire sujet bibliothèques surveillées Pourtant environnements informatiques conçus origine tâches moteur recherche milliards indexation documents efficaces certaines familles tâches peuvent considérés comme modèles universels calcul parallèle Parmi tâches exploration données phase déploiement intensif données susceptible tenir cadre carte reduce contraire modélisation phase exploitation efficace ressources cluster Hadoop problème ouvert étude précédente Dream avons observé aléatoire Forrest meilleure méthode Mahout termes qualité modèles termes évolutivité puisque algorithme nativement processus parallèle notre travail étudier améliorer nécessaire algorithme Mahout utilisation cluster Hadoop première partie rapport présentons étude préliminaire forêt aléatoire bibliothèque Mahout deuxième partie proposons plusieurs améliorations bibliothèque initiale troisième partie proposons nouvel algorithme arbre décision améliorer performances réduire surajustement fournissons résultats ensembles données académiques données Orange avant arriver conclusion Etude préliminaire première partie notre étude avons étudié comportement forêts aléatoires __gVirt_NP_NN_NNPS bibliothèque Mahout comme emballé dernière version disponible Cette section commence rappel algorithme Ensuite deuxième section fournit conditions expérimentales avons utilisées toutes expériences rapports troisième paragraphe présentons résultats obtenus conduit discussion problèmes potentiels solutions Forêt aléatoire forêts aléatoires classificateur supervisé présenté Breiman Breiman approche arbre décision modèle prédictif consiste arbre rassemble multitude arbres forêts aléatoires combinaison facteurs prédictifs arbre telle sorte chaque arbre dépend valeurs ensemble aléatoire vecteur échantillonné façon indépendante distribution arbres forêt erreur généralisation forêts converge limite nombre arbres forêt devient grande erreur généralisation forêt classificateurs arbres dépend force arbres individuels forêt corrélation entre utilisant sélection aléatoire caractéristiques diviser rendement chaque noeud erreur comparent favorablement Adaboost Freund Schapire robustes rapport bruit erreur contrôle interne estimations force Voisine Lemaire Trinquart corrélation utilisés montrer réponse augmentation nombre fonctionnalités utilisées division estimations internes également utilisés mesurer importance variable forêts hasard nécessaire utiliser ensemble données validation effet cours nombreux utilisés construction arbre Chaque classificateur apprend partie données données utilisées appelées fournissent bonne façon estimer performances généralisation classificateur Conditions expérimentales Contexte industriel rapport concentrons comportement Mahout lorsqu traitent données caractéristiques Orange savoir contraintes données Hétérogène valeurs manquantes Plusieurs classes distributions asymétriques nombreuses échelles dizaines millions dizaines dizaines milliers ables nombreux types données numérique catégorielles texte image limitons étude tâche apprentissage supervisé problème classification détection désabonnement prévision appetency parmi ensembles données potentiels évaluation avons intérêt proches problèmes commercialisation paramètres influencent comportement Mahout installation bibliothèque Mahout plate forme Hadoop plusieurs paramètres influencer performances obtenues Mahout nombre machines hochement cluster leurs caractéristiques configuration systèmes fichiers distribués particulier concerne blocs manière créés taille réplication critère répartition utilisé algorithme nombre arbres forêts façon combiner arbres paragraphes suivent décrivent chacun points indiquons également cette section ligne permet vérifier validité résultats obtenus Cluster tests effectués petite exploration Hadoop cluster Orange propriétés suivantes noeuds Intel total cœurs processeur noeud mémoire version Hadoop Cloudera Hadoop Mahout emballé intérieur Cloudera Données nœuds taille principaux composants plate forme Hadoop MapReduce système distribué conçu stocker fichiers volumes données grand nombre machines alors MapReduce cadre distribution Process fichiers volumineux Lorsque éléments combinés ensemble machines nœuds grappe plate forme comme système unique résultant fournissant haute disponibilité charge Hadoop haute disponibilité plateforme orientée objet distribué système distribué adresses questions grandes données Hadoop utilise système stockage distribué appelé Hadoop Distributed System incorpore systèmes analyse comme MapReduce Mahout Spark Hadoop permet données division exécution analyse traitement parallèle Commentaires Forêt aléatoire cornac données marketing équilibrage traitement parallèle grand fichier données décomposé blocs répartis entre nœuds cluster blocs carte réduire fonctions peuvent réparties cluster effectuer ensembles grands ensembles données permet meilleure évolutivité Contrairement système stockage classique blocs question octets taille réglé défaut Cette valeur défaut réglée administrateur cluster changé volée utilisateur mouche spécifique Process tailles blocs typiques 128Mo Notre groupe configuré taille défaut comportement forêts aléatoires Mahout quelque différente version originale Breiman théorie abord échantillons bootstrap effectués ensemble données Étant donné données réparties noeuds données cartographes chaque cartographe réalise propre bootstrap cartographes réalisent formation plusieurs arbres forêt algorithme forêts aléatoires Breiman partie remplie Chaque mappeur effectue partie forêt données Ensuite forêt chaque cartographe combinés réaliser forêt globale critère Split Mahout utilise défaut arbres binaires information Quinlan critère partage défaut utilisé Lorsqu variable choisie effectuer séparation attributs numériques feuilles élaborés après noeud feuilles catégorique ables élaborés après noeud nombre modalités variable choisie important noter Mahout valeurs manquantes Ainsi première partie notre expérience avons remplacé toutes valeurs manquantes variable remplaçant valeurs manquantes implique considère valeur manquante comme information variable numérique objectif mettre valeur inférieure toute autre valeur Nombre arbres nombre influences arbres valeur augmenter nombre arbres augmente chances avoir arbres discrimination Cette amélioration particulièrement observable début courbe tracé rapport nombre arbres forêt cette données petite upselling bonne valeur asymptotique proche Performa après faible nombre arbres aussi grande influence temps nécessaire déployer modèle expériences présentées dessous document fixons nombre arbres chaque mappeur nombre cartographes combinant arbres Après grand nombre arbres généré votent classe populaire résultats Orange développé logiciel puissant nommé Khiops khiops capable travaux ensemble données grand Guyon données multi tables Boullé avons utilisé logiciel comme référence version standard Appliquée données données distribuée évaluer résultats obtenus Mahout classificateur calcul moyenne sélective Naive Bayes décrit Boullé Voisine Lemaire Trinquart datasets avons utilisé types données données Adulte données appartenant référentiel apprentissage automatique Bache Lichman apprentissage grande échelle Pascal Sonnenburg grandes petites commercialisation données fournies orange orange Guyon Leurs caractéristiques données tableau critères utilisés évaluer résultats obtenus Comme indique courbe Fawcett aléatoire algorithme Random forêts avons effectué formation reste article présentés tableaux seront moyenne cours expériences Dataset PCTrain PCTest Mappers Adulte Grand Upselling 14740 Grand Churn 14740 Grande appétence 14740 Petit Upselling Petit Churn Petit appétence Ensemble données utilisé nombre nombre variables numériques nombre Categorical variabkes nombre classes Pourcentage classe cible PCTrain Pourcentage utilisé formation PCTest Pourcentage utilisés Résultats bibliothèque standard présentons résultats avons obtenus forêt aléatoire Mahout utilisation installation défaut bibliothèque critère répartition information valeur manquante traités selon description donnée section précédente nombre arbres nombre mappeurs indiqué dernière colonne tableau classe prédite populaire prédit classe forêt tableau montre colonnes performances obtenues rapport train colonne donner élément robustesse valeur supérieure indique surajustement points résultats présentent résultats médiocres petit ensemble données bonnes performances grand ensemble données résultat semble connu communauté apprentissage machine Debreuve diapositive résultats mauvais inférieur résultats littérature ensembles données résultats faibles obtenus Khiops grand overfitting existe grand nombre ensemble données analysons profondeur résultats comprendre viennent problèmes vient critère répartition information autre façon traiter valeurs manquantes premier biaisé information quand variables catégoriques beaucoup modalités Harris arbres individuels moins profondes larges seconde initiale remplacer valeur manquante edback Forêt aléatoire données appropriées marketing Mahout changé obtenu meilleurs résultats Cependant framework Hadoop permet diminuer beaucoup temps formation comme montré tableau Dataset Khiops Mahout Robustesse Mahout Adult Petit Upselling Petit Churn Petit appétence Grand Upselling Grande Churn Grande appétence Premier résultat standard Mahout KDDSmall Upselling KDDLarge Upselling Khiops 4h55m 45min Mahout temps formation Khiops Mahout Premières idées contourner problèmes observés résultats obtenus section précédente essayons appliquer plusieurs patchs Mahout améliorer performance bibliothèque Mahout change information ratio Mahout remplacés valeur médiane corres pondant Breiman valeurs manquantes variables numériques Dataset Khiops Mahout défaut Mahout Mahout adultes Petit Upselling Petit Churn Petit appétence Grand Upselling Grand Churn Grande appétence Résultat modifié Mahout Mahout Mahout rapport cornac cornac informations valeurs manquantes remplacées valeur médiane résultats obtenus version modifiée présentés tableau Seule version Mahout meilleurs résultats version standard cette amélioration pourrait coûteuse puisque valeur médiane toutes variables numériques nécessaire parce valeur médiane calculée ensemble données complet divisé différents noeuds avons également tester modification autres présentés données déséquilibrée utilisation racine carrée calcul information Flach section Sensibilité distributions classe faussés amélioration Voisine Lemaire Trinquart résultats résultats souhaitent encore robustes performances tournons approche connu robuste Boullé régularisation Ajout forêt aléatoire cette section montrons modifications arbre décision utilisés Mahout Forêt aléatoire augmenter performances classification réduire surajustement première repose approche développée arbre décision succès Voisine consiste changer méthode fractionnement variable numérique introduit méthode regroupement variables catégoriques deuxième amélioration consiste changer scrutin défaut Mahout utilise majorité méthode estimer probabilités classe proposons estimer probabilités classe faisant moyenne estimations probabilité chaque feuille arbres décision Modifications Ajout régularisation décrivons nouvelle division méthodes regroupement classes tâche classification apprentissage méthode utilisée fractionnement critères discrétisation basés approches méthode discrétisation classification supervisée offre discrétisation probable compte données expériences comparatives rapport détaillé haute performance Boullé valeur regroupement variables traitée Boullé utilisant approche similaire approche bayésienne appliquée choisir meilleur modèle discrétisation trouve maximisant probabilité Modèle données modèle compte données Utilisation règle Bayes puisque probabilité constante différents modèle équivalent maximiser Model données Model avons décidé cette étude élaborer arbre décision binaire problème classification classes présents problème Orange conséquent chaque critère répartition consacrée trouver meilleur divisé intervalles variables numériques meilleur groupe groupes variables qualitatives paramètre permet avoir algorithme rapide détaillé rapport raison compte nombre feuille représente nombre instances intervalle nombre occurrences valeur sortie intervalle notre contexte nombre nombre classes censées connues nombre intervalle variables numériques Boullé critère évaluation lorsque nombre tervals entrée nombre classes égaux nombre classes critère évaluation établie problème concevoir algorithme recherche trouver modèle discrétisation minimise critère Boullé heuristique ascendante avide standard utilisé trouver discrétisation variables catégoriques Boullé critère évaluation lorsque groupes NumberOf groupes suivante valeur numérique variable catagorical Commentaires Forêt aléatoire données Mahout marketing général classe prédite basée votes arbres classe populaire changeons cette règle effectuer estimations probabilité moyenne chaque feuille arbres argmaxJ congé arbre classe prédite celui haute probabilité probabilité confiance probabilité moyenne correspondante confiance estimée permettra meilleure estimation calcul Résultats Dataset Khiops Mahout Mahout Mahout Mahout Mahout défaut Ratio Média nouveau adulte Petit Upselling Petit Churn petit appétence Grand Upselling Grand Churn Grande appétence Résultat modifié Mahout Mahout Informations remplacé ratio cornac cornac informations valeurs manquantes remplacées valeur médiane cornac prédiction basée classe populaire cornac prédiction basée estimations probabilité moyenne Dataset Nombre nœuds Robustesse Mahout défaut Mahout Mahout Mahout adultes Petit Upselling Petit Churn Petit appétence Grande Upselling grande baratte grande appétence Taille Robutness modèle différents nombre moyen arbre comparons nouvelle utilisé approche comme critère répartition résultats obtenus section précédente résultats Mahout Random Forest intéressants résultats meilleurs tableau Robustesse mieux tableau modèles petits tableau résultats intéressants elles pires obtenus Khiops Voisine Lemaire travaille Trinquart Future regarder échantillonnage bootstrap données ribution notre étude sommes concentrés jusqu présent compréhension réglage Mahout impact processus construction arbres surtout avons accent processus fractionnement chose avons mention creuser façon ensembles données répartis entre nœuds utilisés comme échantillonnage bootstrap examinée cette section Taille données fichier aléatoire Lorsque ensemble données tombé divisé blocs taille blocs répartis nœuds cluster cluster Hadoop avons utilisé expériences taille défaut changé processus apprentissage Mahout consiste emploi Reduce cartographes construire arbres collectés réducteur Chaque Mapper procède sorte arbres construits mappeur toutes basées échantillonnage limité cartographe travaille conséquent début principe échantillonnage amorçage différente version originale Breiman échantillonnage aléatoire remplacement ensemble données Cette différence conséquences majeures abord distribution initiale cible fichier données avoir impact profond arbres générés différents cartographes paramètre possible lignes ensemble données triées valeur cible pourrait mappeur procède lignes toutes associées modalité cible second fonction nombre colonnes contenir lignes relativement arbres peuvent construits petit échantillon dépend vraiment taille efficace lorsque ensemble données problème datamarts marketing grandes facilement englobent colonnes problème taille blocs abordée étudiant impact taille blocs performance prédiction différents données Espérons pouvons trouver placer curseur puissions configurer notre système correctement datamart donné approche agile environnement devons considérer différents datamarts facile ajouter variables datamart résoudrait problème répartition cible approche robuste aborder simultanément problèmes mentionnés dessus serait concevoir travail prend diviser ensemble données initial prenant contraintes suivantes travail produire ensembles degré parallélisme souhaitons travail prendre stocker chaque ensemble sortie taille spécifique sorte chaque ensemble entièrement traitée exactement mappeur travail remplir chaque ensemble processus échantillonnage aléatoire placement ensemble données initial travail offrir option contrôler façon modalités cibles réparties ensembles échantillonnage stratifié commentaires Forêt aléatoire cornac données marketing algorithme Préparer entrée dataset algorithme apprentissage échantillon stocké degré parallélisme colTarget indice colonne cible ensemble données sortie Echantillon préparé apprentissage Mahout stockés séquence Carte réduire emplois TargetDistribution DatasetSplit travail TargetDistribution prend ensemble données entrée distribution sorties modalités cibles premier emploi réducteur travail DatsetSplit prend ensemble données entrée délivre sortie apprentissage échantillon Splitted cartographes DatasetSplit également accès distribution modalités cibles second emploi Réducteurs Mapper TargetDistribution valeur extrait targetModality colonne colTarget ligne courant écrire réducteur targetModality Réducteur TargetDistribution entrée forme targetModality Liste compteurs écriture targetModality somme compteurs Mapper atasetSplit Avant itération lignes entrée charger distribution cible rangée partir entrée mod_target modalité cible extrait ligne courante dessiner garder ligne cette division basée distribution mod_target écrire filterRows currentRow réducteur réducteur DatasetSplit entrée forme reducerindex Liste rangées lignes écriture sortie Split fichier travail pourrait facilement conçue comme séquence carte reduce emplois premier estimerait modalités cibles distribution second serait effectuer échantillonnage aléatoire stratifié notera subtilité principale réside approche déroulement taille fentes rapport taille moyenne ligne nombre individus ensembles données origine lignes grandes impact nombre lignes pouvons tenir Split conséquent taille échantillon construction arbre Ainsi grandes lignes appellent taille grand autre stratégie pourrait décider toutes colonnes doivent effectuées chaque fraction ensembles données première étape échantillonnage consiste choisir famille toutes colonnes scission définitive ensemble données permettrait petites lignes nombre ainsi lignes chaque fente croquis carte réduire fonctions travail présenté algorithme arbres Shifting entre nœuds approche esquissée paragraphe précédent tournait autour principale préparer données manière appropriée puissions prendre meilleur Mahout changer bibliothèque termes utilisation dupliquer Voisine Lemaire données Trinquart ensemble données inital divisions préparées processus prendre temps additionnel espace stockage supplémentaire autre approche pourrait utilisée question faire arbre construit partir large échantillon ensemble données initial Notre chaque cartographe travail Ainsi avoir chaque cartographe responsable croissance arbres complets proposons transformer processus Mahout itération carte réduction emplois arbres fraction chaque course Pensez voulez construire forêt arbres disons millier ensemble données divisé blocs exemple processus serait lancé générant fichiers contenant chacun centaine Arbres vides Ensuite affecter hasard chacun fichiers chaque cartographe exécuter étape processus apprentissage Cette étape développerait arbres degrés profondeur stocker arbres fichiers nouveau étape suivante consiste associer hasard fichiers nouveau cartographes itérer jusqu arbres complètement développés telle approche nécessiterait substantielle codage bibliothèque Mahout existante particulièrement niveau stocker ensemble Parse arbres incomplètes certainement quelques paramètres régler combien devrions développer arbres chaque étape avantage alors possible construire arbres partir ensemble large échantillon déplacer ensemble données Conclusion Cette étude présente comportement forêt aléatoire œuvre travail cornac plusieurs ensembles données intérêt Orange première déclaration observation version standard performances dessous celle pouvons observer œuvre pratique cadre Hadoop Mahout respecte totalement cadre théorique traduit performances dessous attentes proposons quelques évolutions principal étant introduction régularisation explicite formation arbres tests divers ensembles données montrent amélioration rapport bibliothèque standard néanmoins importation travail fourmi réaliser selon sorte forêt aléatoire œuvre cadre Mahout atteint performances algorithme traitement compromis entre précision connue vitesse volumétrie payer semble assez important moment Références projet ApachTM Hadoop développe logiciels source fiable évolutive Sacrifiés informatique hadoop apache objectif projet ApachTM Mahout construire bibliothèque apprentissage machine évolutive mahout apache Bache Lichman référentiel apprentissage automatique archive Boullé approche optimale Bayes partitionner valeurs hommages catégorique Journal Research Machine Learning Commentaires Forêt aléatoire données marketing Mahout Boullé méthode discrétisation optimale Bayes attributs continus Machine Learning Boullé construction fonction automatique classication supervisé accepté publication Boullé moyenne classificateurs bayésiens naïfs sélectifs compression Journal Research Machine Learning Breiman remplacement Valeur manquante ensemble formation https berkeley Breiman RandomForests cc_home missing1 Breiman forêts aléatoires Apprendre Debreuve introduction forêts aléatoires fichiers perso toulouse motimo forest hasard Dream EVALUER apport Hadoop plateformes classification Mémoire maîtrise Polytech Lille Fawcett introduction analyse Motif reconna Flach Machine Learning science algorithmes données presse Universite Cambridge Freund Schapire expériences nouvel algorithme rappel Conférence nationale Inter Mchine Learning Morgan Kaufmann Guyon Lemaire Boullé Vogel Conception analyse coupe notation rapide grande données clients orange SIGKDD Explor Newsl Harris information fonction rapport étude biais méthode répartition Orange Coupe prévision relation client données kddlarge kddsmall sigkdd customer relationship prediction Quinlan Induction arbres décision Apprendre Sonnenburg Franc Sebag apprentissage Pascal grande échelle largescale berlin about Voisine Boullé Bayes critère évaluation arbres décision progrès découverte gestion connaissances Apprenticeship automatique apparition ecosystem Hadoop creant promesse puissance Opportunité domaine précédent système Apache Mahout question Réponse temps calcul lumétrie Consiste entrepôt algorithmes apprentissage automatique exécuter Portés Reduce rapport CONCENTRE utilisation portage algorithme Forêts aléatoires Mahout montre notre retour Travers difficultés expérience PEUVENT rencontrées Pratiques suggested théoriques piste improvement session Feedbac Industrielle Etude amélioration forêt aléatoire bibliothèque Mahout contexte données marketing Orange Cedric Nicolas Voisine Vincent Lemaire Romain Trinquart