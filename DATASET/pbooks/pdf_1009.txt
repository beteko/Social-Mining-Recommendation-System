Carte organisatrice probabiliste données binaires Rodolphe Priam Mohamed Nadif Université Saulcy 57045 Résumé méthodes factorielles analyse exploratoire statistique définissent directions orthogonales informatives partir ensemble données Elles conduisent exemple expliquer proximités entre individus groupe variables caractéristiques contexte datamining lorsque tableaux données grande taille méthode cartographie thétique avère intéressante Ainsi carte organisatrice méthode partitionnement munie structure graphe voisinage classes souvent planaire travaux récents développés étendre probabiliste Generative Topographic Mapping dèles mélanges classiques données discrètes papier sentons étudions modèle génératif symétrique carte organisatrice données binaires appelons Bernoulli Aspect Topological Model introduisons nouveau lissage accélérons convergence estimation initialisation originale probabilités Introduction visualisation corrélations similarités principales échantillon données objectif méthodes factorielles Lebart méthodes cherchent souvent directions informatives orthogonales nuage données directions concentrent essentiel variance projetée inertie porteuse décomposition perti nente inertie plans projection révèle quels individus similaires quelles variables dépendantes méthodes soient pertinentes grands échan tillons données demandent nouvelles méthodes efficaces analyse contexte cartes Kohonen connues domaine analyse explora toire données généraliser méthodes factorielles telles méthode Analyse Composantes Principales Lebart données continues généralement cartes organisatrices Kohonen méthodes classification contrainte voisinage classes conférant topologique partition finale Generative Topographic Mapping Bishop carte organisatrice probabiliste contraintes moyennes mélange données continues modèle inopérant données catégorielles binaires modèles récents Girolami Kabán Girolami Tipping proposés étendre modèles mélanges classiques données discrètes Hofmann Puzicha contre proposé approche modèle symétrique aspects Carte organisatrice probabiliste données binaires traite classification simultanée lignes colonnes tableau contingence Cette approche bénéfique plusieurs domaines textmining segmentation image papier intéressons données binaires étudions original présentant nouveau lissage carte organisatrice initialisation adaptée accélérer convergence algorithmes estimation paramètres modèle probabilités paramétrées façon adéquate comme induire organisation facteurs latents amène nouvelle méthode visualiser données discrètes vecteurs multidimensionnels composantes papier organisé comme section décrivons notre modèle problème estimation paramètres maximisation vraisemblance section réalisons expériences numériques valider notre modèle Enfin section résume points principaux présente travaux cours modèle modèle proposé repose hypothèse indépendance cellules tableau binaire modélisant chaque probabilité unidimensionnelle observer comme mélange Bernoulli πkiajk propor tions composants telles modèle génératif correspond sélectionner chaque ligne fixée distribution discrète composantes chaque composante sélectionner probabilité attribuer valeur binaire selon Bernoulli paramètre modèle parable classification contrainte récemment proposé vraisemblance écrit alors induire organisation topologique probabilités considérons ordonnées grille bidimensionnelle régulière modélise discrétisé lequel ensemble données disposé grille projetée linéaire espace grande dimension transformation linéaire consti bases fonctionnelles telle matrice forment alors noeuds surface linéaire discrète probabilités Bernoulli paramétrées fonctions fonction sigmoïde paramètre inconnu appartenant vraisemblance devient modèle interprète comme version binaire cartographique probabiliste Hofmann Puzicha paramètres inconnus estimés section suivante Priam Nadif Estimation inférence notre modèle réalise maximisant vraisemblance thode itérative solution analytique exacte existe raison linéarités mélange fonctions sigmoïdes étudions approche algorithme gradient Dempster généralisé McLachlan Cette approche suppose vraisemblance complétée connaissance parti variables latentes ayant support algorithme Expectation Maximisation repose maximisation espérance conditionnelle sachant données paramètres itération précédente Ayant précédent maximi probabilité posteriori générée composant calcul direct donne argmaxπkiQ résoudre argmaxwQ effectuons dérivations élémentaires critère aboutissent gradient hessienne Newton Raphson suivant augmente alors localement vraisemblance convergence obtenons estimateur maximum vraisemblance éviter surapprentissage instabilité numérique ajoutons fonction paramètre régularisation bayésien MacKay Cette correction ajoute gradient diagonale hessienne valeur hyperparamètre choisie manuellement comme plupart temps littérature avons Formulation écrivons algorithme Newton forme matricielle proche Iteratively Reweighted Least Squares McCullagh Nelder régression logistique Carte organisatrice probabiliste données binaires avons matrice taille compte cellules probabilités posteriori matrice diagonale éléments vecteur composante vecteur colonne composante matrice diagonale diagonale enfin matrice identité taille accélérer numériquement algorithme approche Bohning remplace matrice exacte relativement lourde calculer pratique matrice alternative exemple matrice telle négative symétrique permet maximiser vraisemblance Comme convergence lente proposons algorithme variationnel alternatif Estimation variationnelle suivant borne1 obtenons nouveau critère optimiser paramètre variationnel estimer maximisant dérivant nouveau critère obtenons maximisation vecteur colonne ayant composante Finalement trois rithmes quatrième décrit après présentés estimer paramètres modèle Ayant éliminé solution gradient simple inefficace constate algorithme donne meilleure vraisemblance notre comme montre expériences section suivante Simulations cette section abordons abord éléments complémentaires thode proposée initialisation paramètres modèle organisation probabi lités lignes obtenir meilleure carte projective finale possible décrivons alors résultats numériques simulations données binaires réelles raisons concavité Priam Nadif Initialisation modèle tirages aléatoires répétés paramètres initiaux solution minima locaux rencontrent algorithmes basés gradient obtenir meilleure convergence possible procède bonne initialisation Puisque cartes Kohonen néralisations premier cette méthode fournit intéressante première position Elemento centres classes carte Notons coordonnées premier factoriel Jolliffe Benzécri wester celles obtenues suite projection linéaire telle Sammon Alors grille régulière dessinée cette première projection chaque cellule grille correspond facteur modèle affectée classe correspondant cellule laquelle coordonnées jection tombent initialisation initialisons probabilités mélange fonction lissage telle celle voisinage cartes choisi Alors choisi régulariser paramètres correspondent cellules vides Finalement matrice taille cellules valeurs Cette matrice permet évaluer matrice colonnes paramètres initiaux fonctions logistiques solution problème régression associé écrit alors construisons centres effectuant algorithme séquentiel affectations évaluées projection initiale affectation effectue attribuant label noeud proche treillis régulier nuage bidimensionnel projetés discrétiser celui Cette approche également induire ailleurs niveau entropie élevé classification initiale données comparativement simple classification initial facilite ainsi convergence paramètres solution encore meilleure Lissage paramètres lignes intéressant ajouter contrainte topologique paramètres partitionnant lignes accélérer convergence algorithme améliorer carte finale Comme solution Bishop apparaît relativement lourde proposons solution alternative ajoutant simple lissage terme pénalisation approche Priam Brièvement classer vecteurs données lissage spatial composantes modèle manière champ Markov caché Zhang Celeux Ambroise Govaert Carte organisatrice probabiliste données binaires vecteur composantes matrice fonctions voisinage carte organisatrice matrice binaire adjacence treillis correspondant notre carte probabiliste noeud voisin complémentaire associé écrit résout itérant égalité réinjectant membre droite anciennes valeurs courantes jusqu stabilisation leurs valeurs atteinte retrou évidemment estimation contrainte annulant avons éliminé terme additif porte entropies obtenons nouvel rithme appelé TNEM2 général original puisqu applique probabili forcément posteriori alternative estimation paramétrés comme fonction optimiser écrit alors inconnues déterminer estimation effectue comme précédemment montée gradient réalisant boucle indice lignes calculant gradients matrices hessiennes Enfin ramètres initialisent régression nouvelle matrice cellules logarithmes proposons finalement quatrième algorithme mation TNEM2 associe maximisation paramètres colonnes lissage probabilités lignes TNEM2 expliquons suite comment lissage comporte pratique processing carte finale carte finale montre grille centres classes organisées chacune affecte données proche cartes organisatrices classiques utilise distance euclidienne entre vecteur centre vecteur donnée modèle permet alternative probabiliste puisque avons probabilité génération donnée composant mélange chacun vecteurs affecté centre maximum posteriori argmaxk manière variable affectée centre label argmaxk aboutit positions bidimensionnelles lignes colonnes matrice projetée seconde manière projeter chaque donnée position moyenne Bishop précédent positions Expériences expérimentons notre modèle plusieurs échantillons données valider notre approche exemple échantillon compte animaux classes caractéristiques binaires Notre méthode converge carte organisée machine learning databases names Priam Nadif reconnaît classes algorithme projetées segmentation grille figure effectue procédure automatique consistant Vesanto Alhoniemi classification ascendante hiérarchique agrégation diamètre complete linkage associée distance matrice donne meilleurs résultats pratique remarque classe contenant reptiles homogène après riences animaux regroupent évolution vraisemblance quatre algorithmes présentée figure tableau démontrant supériorité algorithme comparativement algorithmes récents alternatifs Cependant surapprentissage amener solution suffisamment lissée préférons proche TNEM2 cause efficacité malgré vraisemblance moins élevée Cette valeur faible explique terme pénalisation rapide meilleure organisation lignes comme vérifiée puisque algorithme arrête trois autres critère arrêt identique vraisemblance relative inférieure seuil Notre initialisation originale régression adéquate premier principal Analyse Correspondances Celle illustre figure démontrant intérêt carte organisatrice alors méthode factorielle linéaire capable montrer classes premier notre carte extrait classes trouve statistique grâce propriété voisinage ensemble données textuelles également projeté Cette échantillon fichier Classic3 Dhillon compte trois classes MEDLINE CRANFIELD avons aléatoirement tirage équiprobable remise documents fichier prenant documents chaque classe avons sélectionné termes fréquence supérieure corpus entier ensemble vocabulaire termes aboutissons éliminant textes vides matrice taille aléatoire environ montrons positions moyennes labels documents correspondants projetés figure matrices sommes mesure visualiser assez distinctement trois classes séparées notre projection linéaire Conclusion discussion avons présenté nouvelle méthode carte organisatrice récapitulée figure données binaires comme trouver domaine traitement image texte nouveaux résultats initialisation méthode projection probabiliste données qualitatives également introduit perspective travaux construction biplots linéaires carte topolo gique travaillons actuellement projection matrices taille importante ainsi ensembles images binaires donnent résultats encourageants Ensuite variantes modèle posent remplaçant hypothèse alternative mélange différent exemple modèle étend également autres types données comme proposé paragraphe suivant mation encore améliorée déterminant notamment meilleur hyperparamètre conclusion récent modèle Block Clustering Govaert Nadif effectue classification simultanée lignes colonnes tableau numérique étendant modèle Carte organisatrice probabiliste données binaires stingray catfish dogfish herring piranha seawasp dolphin haddock porpoise seahorse octopus pitviper sealion starfish seasnake slowworm crayfish platypus lobster scorpion tuatara aardvark cheetah leopard opossum mongoose polecat raccoon tortoise antelope buffalo honeybee housefly elephant ladybird giraffe termite ostrich gorilla penguin squirrel wallaby hamster skimmer pussycat reindeer chicken fruitbat vampire flamingo parakeet pheasant sparrow vulture carte taille échantillon segmentée macro classes représentées visuellement niveaux niveau cellules agrégées Priam Nadif TNEM2 Bohning Variationnel courbes vraisemblance obtenues quatre algorithmes présentés TNEM2 Bohning incomplète variationnel animaux aardvark antelope bearboar buffalo catfish cheetah chicken crayfish dogfish dolphin elephant flamingo fruitbat giraffe gorilla haddock hamster herring honeybee housefly ladybird leopardlion lobster molemongoose octopus opossum ostrich parakeet penguin pheasant piranha pitviper platypus polecat porpoise pussycat raccoon reindeer scorpion seahorse sealion seasnake seawasp skimmerskua slowworm sparrow squirrel starfish stingray termite tortoise tuatara vampire vulture wallaby Initialisation carte animaux Projections moyennes échantillon documents Classic3 Carte organisatrice probabiliste données binaires Initialisation Estimation Évaluation Maximisation Évaluation variationnelle paramètres Évaluation TNEM2 paramètres processing final carte paramètres Tableau segmenté projections moyennes biplot Schéma récapitulatif méthode mélange classique modèle mélange croisé Celui modèle génératif flexible avère alternative efficace prometteuse modèle aspects serait intéressant étendre ajoutant propriété organisation Annexe paramétrisation probabiliste alternative Lorsque matrice données tableau contingence Bernoulli valable hypothèse multinomiale généralement supposée paramétrage alors introduit probabilités contraintes régression classification notamment écrit notre cette paramétrisation implique inversion matrice hessienne pleine procéder optimi sation proposons moyen alternatif efficace principale aboutir nouveaux paramètres contrainte somme unité classique multinomiale écrivant comme jointe variables Bernoulli paramètres inconnus écrire jointe colonne associée mettant unité compo sante intéresse autres supposant Bernoulli semble composantes prises indépendantes vecteur binaire résultant expression donne solution valide maximum vraisemblance multinomiale probabilités assez petites éventuellement ajout composantes artificielles supplémentaires diminuer valeurs optimum global annule dérivée contrainte lagrangien kijxij kijxij retrouvons expression classique estimation paramètres multinomiale probabilités posteriori induisant normalisation Priam Nadif automatique Comme aucune contrainte devient nécessaire paramétrisation sigmoïdes licite organisation valeurs recher chées paramètre aboutissons formulation variationnelle générale multinomiale nouvelle expression vecteur gradient matrice hessienne estimation modèle données catégorielles Références Ambroise Govaert Convergence algorithm spatial clustering Pattern Recogn Benzécri Correspondence Analysis Handbook Dekker Bishop Neural Networks Pattern Recognition Clarendon Press Bishop Svensén Williams Developpements generative graphic mapping Neurocomputing Bohning Construction reliable maximum likelihood algorithms application logistic regression Handbook Statistics Celeux Forbes Peyrard procedures using field approxima tions markov model based image segmentation Pattern Recognition Deerwester Dumais Furnas Landauer Harshman Indexing latent semantic analysis Journal American Society Information Science Dempster Laird Rubin Maximum likelihood incomplete algorithm discussion Journal Royal Statistical Society Series Dhillon Mallela Modha Information theoretic clustering Proceedings Ninth SIGKDD International Conference Knowledge Discovery Mining Elemento Initialisation convergence validation cartes topologiques french Master thesis Rapport INRIA Lechevallier Girolami Document representation based generative multivariate bernoulli latent topics models Cambridge Annual Colloquium Information Retrieval Research Govaert Nadif Clustering block mixture models Pattern Recogni Govaert Nadif algorithm block mixture model sactions Pattern Analysis Machine Intelligence Hofmann Puzicha Statistical models occurrence Technical Report Jolliffe Principal Component Analysis Springer Verlag Kabán Girolami combined latent class trait model analysis visualisation discrete Transactions Pattern Analysis Machine Intelli Carte organisatrice probabiliste données binaires gence Kohonen organizing Springer Lebart Morineau Warwick Multivariate Descriptive Statistical Analysis Wiley MacKay Bayesian interpolation Neural Computation McCullagh Nelder Generalized linear models London Chapman McLachlan Finite Mixture Models Wiley Priam Méthodes carte organisatrice mélange contraintes Application exploration tableaux contingence textuels french thesis Université Rennes Sammon nonlinear mapping structure analysis Transactions Computers Jaakkola Jordan field theory sigmoid belief networks Journal Artificial Intelligence Research Tipping Probabilistic visualisation dimensional binary Advances Neural Information Processing Systems Vesanto Alhoniemi Clustering organizing Trans Neural Networks Zhang field theory procedures markov random fields Transactions Signal Processing Summary mixture models behave cluster large samples continuous categorical Adding vicinity constraint permits project factorial methods nonlinear paper present model called Bernoulli Aspect Topological Mapping generative organizing binary automatic smoothing original initialization