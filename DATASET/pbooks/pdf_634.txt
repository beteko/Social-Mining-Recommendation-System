articles assemblage pdfcomparaisons structurelles grandes bases données apprentissage supervisé guénaël cabanes younès bennani avenue clément 93430 villetaneuse france résumé domaine fouille données mesurer similitudes entre différents ensembles question importante jusqu présent article proposons nouvelle méthode basée apprentissage supervisé différents ensembles parer caractérisés moyen modèle prototypes ensuite différences entre modèles détectées utilisant mesure similarité introduction croissance exponentielle données engendre volumétries bases données importantes toutefois capacité analyser données reste insuffisante nombreux capacité mesurer similitudes entre différents ensembles données devient élément important analyse principaux enjeux étude données obtention description condensée propriétés données gehrke manku aussi possibilité détecter variations changements structure données aggarwal proposons ticle algorithme capable réaliser tâches algorithme apprend abord representation abstraite ensembles comparer évalue similarité cette représentation représentation abstraite calculée apprentissage variante organising kohonen enrichie informations structu relles extraites données proposons méthode estimer partir enrichie fonction densité représentative données mesure dissimilarité entre ensembles alors mesure divergence entre fonctions densité avantage cette méthode comparaison structures intermédiaire dèles décrivent permet comparaisons importe quelle échelle charge mémoire stockage algorithme efficace terme temps exécution mémoire requise adapté comparaison grandes bases données détection changement structure données reste article organisé comme section présente nouvel algorithme section décrit tests validation effectués résultats obtenus enfin conclu donnée section travail soutenu partie projet financé agence nationale recherche comparaisons structurelles grandes bases données nouvel algorithme niveaux modéliser comparer structure données supposons données décrites forme vecteur numérique attributs commencer données modélisées enrichie construire représentation abstraite structure données ensuite fonction densité estimée partir représentation abstraite enrichissement prototypes cette étape certaines informations générales extraites partir données stockées prototypes apprentissage consiste carte dimensions neurones connectés entrées selon connexions pondérées aussi appelées prototypes chaque neurone aussi connecté voisins liens topologiques ensemble apprentissage utilisé organiser cartes selon contraintes topologiques partir espace entrée notre algorithme prototypes enrichis addition nouvelles valeurs numériques extraites structure données mesure densité données voisinage prototype estimateur noyaux gaussiens mesure variabilité données distance moyenne entre prototype données représente ainsi mesure voisinage entre prototypes nombre données ayant prototypes comme meilleurs représentants algorithme enrichissement procède trois étapes algorithme initialisation initialisation paramètres densités locales valeurs voisinages variabilités locales nombre données représentées initialisés tirage aléatoire donnée calcul distance euclidienne entre donnée prototypes recherche meilleurs prototypes match units informations structurelles nombre données variabilité densité voisinage prototypes comme défini kohonen cabanes bennani répéter étapes informations structurelles finales cette étude avons utilisé paramètres défaut somtoolbox vesanto apprentissage particulier choix important résultats valeur optimale difficile calculer coûteuse temps calcul heuristique semble pertinente donne résultats consiste définir comme distance moyenne entre prototype proche voisin cabanes bennani cette étape chaque prototype associé valeur densité variabilité chaque paire prototypes associée valeur voisinage grande partie information structure données stockée valeurs nécessaire garder données mémoire estimation fonction densité objectif cette étape estimer fonction densité associe chaque point espace représentation données densité connaissons valeur cette fonction niveau prototypes déduire approximation fonction supposons cette fonction correctement approximée mélange noyaux gaussiens sphériques fonction gaussienne centrée prototype nombre prototypes fonction densité alors écrire méthode populaire estimer modèle mélange trouver algorithme expectation maximization dempster cependant algorithme travaille espace données avons seulement disposition enrichie proposons heuristique choisir nombre données représentées prototype variabilité distance euclidienne entre ainsi puisque densité chaque prototype connue utiliser méthode descente gradient déterminer poids initialisés valeurs valeurs réduites graduellement jusqu approcher mieux faire optimisons critère suivant ainsi obtenons fonction densité modèle données représentées comparaisons structurelles grandes bases données mesure dissimilarité maintenant possible définir mesure dissimilarité entre ensembles données représentés nombre prototypes modèles fonctions densité calculées dissimilarité entre comparer fonctions densité chaque prototype distributions identiques valeurs doivent proches cette mesure adaptation approximation pondérée monte carlo mesure symétrique kullback leibler hershey olsen utilisant prototypes comme échantillon ensemble données validation description distributions données utilisées façon démontrer performances mesure dissimilarité proposée avons utilisé générateurs données artificielles données réelles générateurs rings spirals génèrent types ensembles données convexes dimensions densité variance différentes noise noise distributions dimensions chacune composée distribution gaussienne accompagnée bruit homogène important finir donnée shuttle vient repository données dimensions 58000 instances données divisées classes données appartiennent classe validité mesure dissimilarité notre mesure dissimilarité performante devrait possible comparer diffé rentes bases données détecter présence distributions similaires dissimilarité données générées selon distribution faible dissi milarité entre données générées selon distributions différentes vérifier cette hypothèse avons appliqué protocole suivant avons généré données distribution spiral spiral chaque contiennent entre 50000 données cabanes bennani chacun données avons appris enrichie façon obtenir ensemble prototypes représentatif données nombre prototype varie entre fonction densité estimée chaque toute fonctions comparées autres selon mesure dissimilarité proposée finir chaque étiquetée fonction distribution étiquettes spiral spiral avons calculé indice compacité séparabilité groupes étiquettes indice généralisé protocole utilisé distributions noise noise données shuttle types distributions extraites shuttle utilisant échantillonnage aléatoire tirage remise données classe shuttle données autres classes shuttle visualisations matrice dissimilarité entre différents données tribution noise cellule sombre similarité élevée comparaisons distribution apparaissent selon quatre carrées diagonale avons comparé résultats certaines mesures dissimilarité généralement lisées comparer ensembles données compare séries prototypes mesures distance moyenne entre toutes paires prototypes petite distance entre ensembles prototypes distance valeurs indice obtenues partir différentes sures montrent mesure dissimilarité proposée efficace mesures basées distance table aussi exemple résultat valable trois types distributions testées données convexes données bruitées données réelles indice diverses mesures dissimilarité différentes distributions distributions comparer spiral noise shuffle comparaisons structurelles grandes bases données conclusion article avons proposé nouvelle méthode modélisation structure données basée apprentissage ainsi mesure dissimilarité entre modèles avantages cette méthode grande rapidité calcul ligne estimations faible quantité information stocker chaque modèle aussi grande précision modélisation obtenue propriétés rendent possible analyse grandes bases données compris grands données nécessitent vitesse économie ressources références aggarwal survey synopsis construction methods streams aggarwal streams models algorithms springer cabanes bennani local density based simultaneous level algorithm topographic clustering ijcnn ester density based clustering evolving stream noise conference mining dempster laird rubin maximum likelihood incomplete algorithm journal royal statistical society series separated clusters optimal fuzzy partitions cybern gehrke srivastava computing correlated aggregates continual streams sigmod conference hershey olsen approximating kullback leibler divergence between gaussian mixture models international conference acoustics speech signal processing volume kohonen organizing berlin springer verlag manku motwani approximate frequency counts streams baggerly scott cross validation multivariate densities journal american statistical association vesanto himberg alhoniemi parhankangas organizing matlab toolbox proceedings matlab conference summary mining problem measuring similarities between different subsets portant issue which little investigated paper novel method proposed based unsupervised learning different subsets dataset characterized means prototypes based model differences between models detected using simila measure based density experiments synthetic datasets illustrate effectiveness efficiency insights provided approach