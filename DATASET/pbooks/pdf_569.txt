Optimisation directe poids modèles prédicteur Bayésien moyenné Romain Guigourès Boullé Orange avenue Pierre Marzin 22307 Lannion Cedex romain guigoures boulle orange ftgroup Résumé classifieur Bayésien outil classification efficace pratique nombreux problèmes réels dépit hypothèse restrictive indépendance variables conditionnellement classe Récemment nouvelles méthodes permettant améliorer performance classifieur sélection variables moyennage dèles article proposons extension sélection variables classifieur Bayésien considérant modèle pondération variables utilisées algorithmes optimisation directe poids expérimentations confirment pertinence notre approche permettant diminution significative nombre variables utilisées perte perfor mance prédictive Introduction classifieur Bayésien outil largement utilisé problèmes classifica supervisée avantage montrer efficace nombreux données réels Cependant hypothèse naïve indépendance variables certains dégrader performances classifieur Ainsi méthodes proposant réaliser sélection variables Elles consistent place heuristiques ajout suppres variables sélectionner meilleur ensemble variables maximisant critère performances classifieur selon approche wrapper Guyon seeff montré Boullé moyenner grand nombre classifieurs Bayésiens naïfs sélectifs réalisés différents ensembles variables revenait considérer modèle pondération variables article propose trouver modèle optimal optimisation directe poids variables introduit critère vraisemblance modèle fonction continue vecteur poids descente gradient ensuite utilisée optimisa critère étant continu dérivable ensemble définition vecteur poids variables méthode régularisation introduite minimiser nombre variables Optimisation directe poids modèles prédicteur Bayesien moyenné sélectionnées dégrader performances classifieur problème optimi sation existe optima locaux multistarts alors réalisés trouver optimum satisfaisant deuxième partie papier introduit notations utilisées article revient principes classifieurs Bayésiens naïfs différentes approches basées pondération variables troisième partie définit critère optimisable descente gradient ainsi pénalisation permettant maximiser performances classifieurs minimum variables expérimentations présentées montrer performances approche Enfin conclusion bilan différents points évoqués article Classifieurs Bayesiens naïfs Notations Soient vecteur variables explicatives variable classe ayant valeurs données contenant instances identifiées façon suivante simplifié meilleure lisibilité modèle décrit vecteur poids effet chaque poids associé variable pondère modèle Notons probabilité priori classe vaille probabilité conditionnelle variable connaissant valeur classe probabilités considérées comme initialement connues grâce traitement discrétisation groupement valeurs Classifieur Bayesien classifieur Bayesien prédit classe chacune tances maximale probabilité conditionnelle hypothèse naïve classifieur bayésien considérer indépendantes variables explicatives conditionnelle classes obtient alors Classifieur Selective Naive Bayes classifieur Bayesien nombreux hypothèse indépendance variables conditionnellement classe certains déteriorer performances classifieur classifieur propose sélectionner ensemble variables maximiser performances réduit ainsi biais apporté hypothèse naïve classifieur Langley formelement revient fixer pondération booléenne chacune probabilités conditionnelles variables connaissant classe Plusieurs méthodes développées exploitant heuristiques proposant faire ajout supression variables optimisant critère précision courbe Guigourès Boullé Approche Maximum Posteriori Cette approche propose déterminer meilleur ensemble variables maximisant vraisemblance pénalisée priori hiérar chique paramètres sélection nombre variables ensembles variables Boullé Approche Bayesian Model Averaging Alors approche permet terminer modèle probable posteriori approche quant propose tenir compte modèles moyenner pondérant probabilité posteriori obtenir modèle performant Hoeting démontré Boullé moyenner grand nombre classifieurs sélectifs revenait élaborer classifieur lequel chaque variable aurait poids compris entre Approche Compression Model Averaging Cette technique proche cédente différence compression moyenner modèles revient lissage logarithmique probabilités posteriori Boullé Cette approche aboutit autre schéma pondération variables performances passent significativement celles approche Optimisation directe poids maximiser vraisemblance modèle optimisant directement poids verra suite comment améliorer critère optimisation réduire maximum nombre variables Descente gradient Reprenons classifieur principe minimiser modèle défini vecteur poids booléens approche compris entre moyennage modèles considère modèle comme fonction variables définies variables correspondant poids fonction objectif optimiser logarithme négatif vraisemblance modèle décrit vecteur poids fonction dérivable ensemble définition logPm wklogp indice marquant itération notera valeur vecteur poids temps itérer jusqu optimiser fonction utilisant algorithme descente gradient vecteur chacune variables temps chacune variables initialisé adaptatif déterminé Optimisation directe poids modèles prédicteur Bayesien moyenné algorithme RPROP Riedmiller sinon contrainte ajoutée consiste réduire espace optimisation chaque itération effet variable prend valeur nulle gradient critère optimiser selon cette variable reste négatif itération suivante alors supprimée modèle permet converger solution optimale rapidement nombre variables réduit condition arrêt algorithme fixée convergence poids algorithme complexité passe linéaire rapport nombre instances variables Méthode régularisation descente gradient efficace optimiser vraisemblance tendance répartition homogène poids ensemble variables modèle finalement sélection variables plutôt inefficace étudiant manière approfondie évolution critère fonction poids compte existe plusieurs optima locaux solutions optimales intéressent celles présentant moins variables introduire probabilité priori distri bution poids favorisant fortes pondérations pondérations nulles Soient distribution priori modèle vraisemblance optimisée précédemment choisit définir comme moyenne normales centrées projetée intervalle fonction erreur Gaussienne variance devant importante déteriorer critère définit alors nouvelle fonction optimiser Multistarts critère précédent conçu manière réduire nombre variables ajout pénalisation optima locaux ainsi garanti selection variables optimale pourquoi propose réaliser multistarts relancer algorithme plusieurs données réinitialisant poids variables manière aléatoire Expérimentations Présentations données conditions expérimentales expérimentations validations croisées données choisies seront différents types viennent Frank Asuncion challenge traitement données effectué approche Boullé Guigourès Boullé Trois algorithmes comparés premier classifieur bayesien classique second classifieur obtenu moyennage modèles Compres enfin méthode optimisation directe poids descente gradient laquelle réalise starts Données Nombre variables Nombre Nombre Numériques Catégorielles leurs classe instances Satimage Segmentation SickEuthyroid Small 50000 Thyroid Vehicle Waveform Caractéristiques données utilisés expérimentations Résultats réalisation descente gradient nécessite fonction entre passes start avant convergence vecteur poids premier start élimine majorité variables suivants permettent affiner sélection Précision pourcentage variables embarquées classifieur Bayésien Compression based averaging Descente gradient Globalement après Figure performances prédictives équivalentes méthode optimisation directe poids descente gradient meilleures simple classifieur Bayesien avantage puisse tirer optimisation directe poids diminution nombre variables variables effet alors approche permettait bonne sélection variables optimisation directe encore efficace diminuant façon importante nombre Conclusion article méthode optimisation directe poids variables sifieur bayesien proposée celle consistant minimiser critère Optimisation directe poids modèles prédicteur Bayesien moyenné semblance modèle variables pondérées réels compris entre distribution poids priori introduite rendre sélection variables efficace optimisation descente gradient possède avantage algorithmiquement coûteuse autre adaptée bases données grande taille faire nombre réduit instances gradient stochastique après Bottou pénalisation bayésienne critère optimiser permet quant sélection nombre réduit variables déteriorer performances approches discutées Boullé résultats expérimentaux présentent modèles parcimonieux aussi performants modèles réalisés sélection variables moyennage modèles améliore interprétabilité modèles autre efficacité deploiement Références Bottou learning large Research articles Stoch Model Boullé bayes optimal discretization method continuous attributes Learn Boullé Compression based averaging selective naive bayes classifiers Journal Machine Learning Research Stork Pattern Classification Edition Wiley Interscience Frank Asuncion machine learning repository Guyon Elisseeff introduction variable feature selection Learn Idiot bayes stupid after International Statistical Review Hoeting Madigan Raftery Volinsky Bayesian model averaging tutorial Statistical Science Langley Induction selective bayesian classifiers Conference uncertainty artificial intelligence Morgan Kaufmann Riedmiller Rprop description implementation details Summary naive Bayes classifier effective datasets which variable assumed independent compared other Approaches improving performance weighting variables developed lately paper describe method directly optimizes weights maximizes classifier performance regularization technique introduced order effective feature selection Experimental results presented discussed efficiency approach could proved