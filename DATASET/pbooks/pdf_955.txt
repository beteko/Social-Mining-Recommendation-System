Microsoft IRP_ROM_EGC_07_FINAL optimale représentation histogramme grands ensembles données Fisher linéaire morceaux approximation Antonio Irpino Elvira Romano Dipartimento studi europei mediterranei Université Naples Setificio Complesso Monumentale Belvedere Leucio 81020 Caserta irpino unina Dipartimento Matematica STATISTICA Universita degli Studi Napoli Federico Cintia Complesso Monte Angelo 80126 Napoli elvrom unina plural Résumé représentation Histogramme grand ensemble données bonne façon résumer visualiser données souvent effectuée optimiser estimation requête article montrons performances propriétés stratégies construction optimale histogrammes descripteur valeur choix préalable nombre sarrasin première basée algorithme Fisher tandis second procédure géométrique interpolation fonction distri bution empirique fonction linéaire morceaux qualité ajustement calculé utilisant métrique Wasserstein entre distributions comparons performances méthode proposée contre certains données celles existent artificiels réels Introduction mécanisme information stockage Aujourd parvient capturer grande quantité données processus intégralité alors seulement résumé stocké histogramme contexte outil produire description récapitulant appropriée répondre rapidement demandes décision suite expression guidage image histogramme représente outil simple graphique intuitive décrire distribution données aplanit données afficher forme générale distribution empirique problème donner fausse impression forme distribution données construction dépend choix nombre longueur intervalles seaux allié appelés lignes réelles lesquelles histogramme pourrait idéalement avoir situation laquelle grands nature ensemble données bimodale petits terrain réduit représentation unimodale question concerne genre largeur prendre compte meilleure représentation graphique jacent comment construit approximation erreur minimale représentation histogramme optimal grands ensembles données communauté données particulier cadre optimisation requêtes recherche histogramme représentation grand ensemble données mieux connu comme problème estimation sélectivité estimations peuvent utilisées sélectionner meilleur parmi beaucoup concurrents existe grandes classes méthodes estimation sélectivité méthodes méthodes échantillonnage Tical statis document deuxième méthodes statistiques paramétriques prise compte quelques méthodes histogramme brièvement passés revue tandis excellente taxonomie histogrammes trouve Poosala nombreux ensembles données attributs continus valeur ensembles données scientifiques statistiques domaines Histogrammes traite implicitement valeur attribut discrètes catégoriques lesquels relativement valeurs distinctes attribut méthodes utilisées estimer rejoindre sélectivités aussi Ioannidis Poosala absence nombreuses valeurs double nombreux ensembles données scientifiques statistiques jointure entraînera efficacement ensemble origine méthodes inefficaces partir point notre approche tente saisir caractéristiques variables statistiques puissions considérer modèle statistique approche étant donné notre objectif rapprocher fonction fonction cumulative polynôme morceaux modèle géométrique méthodes proposées tentent résoudre calcul histogramme présence ensembles données presque continuités selon approches différentes première basée algorithme Fisher partition données ordonnées celle basée meilleure interpolation fonction distribution données sensibilité algorithme remplacement proposée étudiée utilisant plusieurs ensemble données qualité approximation calculée proposant qualité mesure forme métrique Wasserstein entre fonctions distribution application artificiel ensemble données effectuée confirmer notre procédure Touches bienséances histogramme petite revue techniques Isting Examinons définition histogramme Définition histogramme variable réalisé divisant distribution données ensembles appelés seaux rapprocher fréquences valeurs chaque godet façon commune Ioannidis cette définition mentionné comment dessiner classes spécifiques histogramme principaux aspects prendre compte construction principaux aspects considérer construction histogramme Règle partition algorithme construction fréquence approximation approximation Valeur erreur Guarantes Ioannidis première proposition rapproche largeur casiers également espacés propositions essentiellement fondé choix nombre Néanmoins méthodes inconvénient perdre détails partition haute densité données cours dernières années plusieurs types histogramme proposées résoudre problème ligne directrice commune trouver meilleur emplacement points coupe nombre Irpino Romano largeur estimation fonction densité problème attention seulement statistiques communauté données aussi analyse numérique densité approchée classe polynôme piecewise certain degré régimes communs construction histogrammes diffèrent termes contraintes partition première proposition remonte thèse doctorat présente concept couramment utilisé littérature statistique forme simple histogramme laquelle valeurs divisée intervalles longueur égale appelle histogramme largeur fréquences particulières chaque godet approchées hauteur godet Comment jamais histogrammes largeur avaient bonne amélioration rapport répartition uniforme ensemble consommation valeur raison laquelle nouvelles propositions faites disant hauteur histogrammes profondeur Piatetski Shapiro siste particulier diviser ensemble attribut seaux comme approximativement nombre tuples Après propositions attention déplacé étude façon erreurs approximation initiale maintenue données estimation techniques histogrammes optimales Ioannidis proposées minimiser erreur quadratique moyenne problème estimation sélectivité cette technique partition distribution données calculée telle sorte variance valeur paramètre source intérieur chaque godet minimisé Outre contraintes séparation optimale méthodes autres comme dernier point ayant principal regrouper rapidement plusieurs valeurs paramètres source Parmi distinguer MaxDiff Ioannidis place limites godet entre source paramètre adjacent solutions contraintes séparation numérique capturer distribution forme attention Parmi proposé trouver splines linéaires chaque moindre carré blème régression Konig beaucoup attention consacrée nombre paramètres estimer construction efficace règle partition histogrammes peuvent classés fonction leurs propriétés mutuellement orthogonalement Poosala tableau résume temps décrit comment colloqué méthodes existantes cadre notre méthode contexte méthodes utilisent valeurs variable observée fréquence cumulée relative SOURCE PARAMETER paremeter propagation Fréquence cumulation Valeurs Valeurs Somme Somme Optimal comprimé Optimal comprimé Spline optimale Piecewise Fisher fréquence OPTIMALE MaxDiff MaxDiff Carte principales approches construction histogramme algorithmes proposés présent document soulignés représentation histogramme optimal grands ensembles données techiques proposées variable numérique domaine constitué ordonné valeurs liste observation tuples variable tandis ensemble valeurs distinctes assumée ensemble données fonction masse empirique définie décrit fonction répartition empirique iijjcf manière définissons masse intervalle comme domaine divisé seaux supposant distribution uniforme sarrasin calcule densité empirique comme jjjjj bbfbbbb densité affichée histogramme graphique barres lequel proportion liste représentées zones différentes barres algorithme Fisher algorithme Fisher considéré comme algorithme optimale effet fonction réduite minimum somme variance intra godets Optimal rithme étant certain nombre godets diviser domaine valeurs optimise paramètre source selon formule suivante source paramètre poids paramètre source paramètre source domaine valeurs correspond minimisation variance seaux conduit œuvre algorithme dynamique partition raison Fisher données commandées définit quantités suivantes iiivvvivvv borne supérieure nouvelle kième godet choisie fonction formulation dynamique suivante ijjjiijvh jjkvbb notation intervalle calcul termes opérations Irpino Romano interpolation morceaux fonction distribution empirique procédé commence partir histogramme trivial histogramme uniforme proximation chaque limite nouveau godet choisi cette valeur laquelle observe distance maximale entre valeur prédite valeur observée distance pondérée version standard algorithme pondérée nombre observations seaux second valeurs erreur choisi valeur godet peuplée fonction motivation pratique préférable supprimer erreur rapprochent grand ensemble observation partir petit commençons compte histogramme trivial VTriv point artificiel ajouté ensemble données telle sorte identifier meilleur point coupe appartenant seaux résolvons algorithme vantes chaque étape trouver points coupure algorithme standard version pondérée avons iiiiiivv Argmax Argmax fréquence comprend calculée moyen fonction quantile jjijjiijjbbvbcbcvbbjk calcul termes exploitation égale représentation mesure qualité paragraphe suivant présentons façon cohérente calculer carré erreur moyenne histogramme obtenu distribution données selon métrique entre distribution développons mesure prise précision considération somme différences entre carrés prédite valeur observée compte hypothèse nature modèle rapport valeurs observées discrètes Lorsque utilisons fonction continue histogramme savoir mélange uniformes supports chevauchants interpoler fonction continue droite discrète toujours estimation erreur Étant donné vecteur valeurs fonction masse égale meilleur histogramme constitué godets représentée fonction linéaire morceaux pièce linéaire général bornes histogramme meilleure interpolation linéaire morceaux Notre proposition évaluer précision procédure moyen calcul distance entre modèle obtenu mélange uniformes meilleur histogramme représentation histogramme optimal grands ensembles données proposons utiliser distance Wasserstein faire comparaison Gibbs Barrio Verde Irpino considéré comme prolongement naturel distance euclidienne partir données point données distribution propriétés intéressantes decompo sition Etant donné fonctions distribution distance Wassertein peuvent calculés selon formule suivante fonctions quantiles distributions distance Compu lourd lorsque distribution continue Verde Irpino montré faisabilité lorsqu traitent histogrammes annexe montrons distance proposée décomposer comme somme différence carrés moyens différence carré écarts types partie resid comme distance forme entre distributions décompositions résumée comme fgfgfg ShapeLocation Taille DCORR considérons comme erreur maximale autorisée interpolation données histogramme distance Wasserstein entre histogramme trivial histogramme permettant godet celui optimal obtenir qualité relative indice forme rapport entre distance quadratique modèle obtenu valeur optimale histogramme distance quadratique entre modèle trivial valeur optimale appelons cette mesure comme Quare qualité ajustement Ratio Utilisation décomposition distance carré évaluer qualité ajuster compte quantité distance influencée emplacement taille différence forme application ensemble données réelles artificielle testons techniques proposées trois données premier ensemble données artificiel dérive génération aléatoire valeurs dérive mélange trois répartition deuxième ensemble constitué observations dst_bytes variables database1 ensemble données choisi caractéristique exemple distribution peaky discontinue troisième ensemble recueille première observation variable élévation partir couverture forestière database2 ensemble données choisi exemple distribution lisse continue databases kddcup99 kddcup99 databases covertype covertype Irpino Romano valeurs Artificial dataset Godets algorithme mesure Temps 55955 604214 566546 58457 56958 58197 49141 58185 14396 033287 06364 109271 20468 Temps 14913 06039 02176 95111 71032 902464 339665 13697 02490 354292 217371 074347 04719 02008 Temps 07574 99695 FISHER 211255 167489 955339 00252 00097 36562 290044 124707 00633 00386 Temps 240339 23878 315012 52743 11027 512781 048029 004592 00196 00052 091357 027947 00862 00563 00289 Temps 19953 23803 31237 52245 500763 02326 00285 00100 51278 00036 01942 00676 09135 00396 00234 synoptiques performances algorithmes utilisant premier ensemble données MaxDiff optimale Fisher pièce approximation rationnelle fonction distribution pondéré Piece approximation distribution pondérée fréquence godet méthodes comparaison meilleurs résultats montré utilisés MaxDiff optimale algorithme Fisher norme version pondérée algorithme interpolation cumulée morceaux avons mesuré temps fonctionnement utilisant MATLABTM Intel Trino 77Mhz précision calculée utilisant classique fonction erreur nouvelle mesure précision Wasserstein métrique entre distributions principaux résultats recueillies tableaux algorithme MaxDiff meilleures performances termes temps passé estimation histogramme moins précis lorsque nombre valeurs domaine variable grande rithme piecewise toujours meilleur termes temps précision illustrer exacti améliorée projet approche Figure montre principaux résultats artificielle Dataset concerne qualité qualité ajustement seaux meilleur algorithmes MaxDiff Voptimal ensemble données KDD99 biaisé correspondent premiers moments algorithme Fisher mieux définies intervalles perfor autres comparant qualité qualité ajustement entre Fisher algorithmes piecewise dernière semble précis estimation premiers moments nombre seaux augmente supposons algorithmes Fisher étant basés variance RITÈRE permet regrouper données classes sphériques alors méthodes basées morceaux meilleur ajustement linéaire fonction distribution mettant accent implicitement densité uniforme locale données représentation histogramme optimal grands ensembles données Forest valeurs données Godets algorithme mesure Temps 00147 00144 00150 00152 00165 28642 70946 22061 07711 00861 14530 01547 16422 00912 00281 Temps 24917 44977 98323 11676 37233 07816 07039 05140 85188 02625 03025 00849 00801 00689 00481 Temps 11523 80159 00386 46458 41046 FISHER 22606 08548 04502 91894 02175 04506 01500 00875 00603 00368 Temps 24187 32029 53857 19329 47643 44978 08213 03813 01217 00153 02136 00902 00596 00323 00105 Temps 24211 50761 55726 19092 20335 09306 04329 01098 44978 00153 00959 00628 02136 00306 00106 synoptiques performances algorithmes données couverture Forest MaxDiff optimale Fisher Piece approximation rationnelle fonction distribution pondérée Piece approximation rationnelle fonction distribution pondérée fréquence godet meilleurs résultats montré KDD99 dataset Valeurs Godets algorithme mesures Temps 03753 07955 02525 02647 02837 2998E 2516E 2516E 6199E 5893E 24139 24139 10965 78485 08537 Temps 63551 94830 83113 52914 68945 3791E 7991E 7991E 7037E 5893E 52340 52340 28197 62976 08537 Temps 57838 82461 39616 45977 87441 FISHER 1452E 0053E 6098E 8253E 8049E 23301 06158 00154 00068 00027 Temps 19037 24140 34018 56922 34964 0296E 0148E 9798E 5786E 9934E 00965 00370 00098 00046 00015 Temps 24546 33983 56360 19179 16070 4614E 8040E 3669E 8198E 99776 00128 00052 00026 00342 00009 synoptiques performances algorithmes utilisant ensemble données MaxDiff optimale Fisher Piece approximation rationnelle fonction distribution pondéré Piece approximation rationnelle distribution pondérée fréquence godet meilleurs résultats montré Irpino Romano mesures Forêt artificielle 58186 14386 23811 00737 71016 02479 85628 02167 21110 00092 90010 01265 FISHER 51266 00051 42695 00103 51266 00034 42695 00105 Synopse qualité bonté adéquation entre modèle meilleur histogramme Cording décomposition proposée distance carré Wasserstein FIGUE Histogramme représentation ensemble données artificielle illustration approximation fonction répartition empirique différentes méthodes Conclusions perspectives présent document plusieurs algorithmes établis construction histo grammes données montré échec précision lorsque données contenues données quasi continue exemple lorsque valeurs assumées domaine variable Fisher optimale représentation histogramme grands ensembles données rapport tuples stockés techniques proposées semble capable faire problème outre compte qualité ajustement meilleur modèle décomposition Wasserstein métrique permet découvrir qualité approximation histogramme données explique distance termes qualité ajustement premiers instants partie distance raison seulement facteur forme présent document construction histogramme multivariée considéré comme notre prochaine étape naturellement rapport techniques existantes semble souffrir problème malédiction dimensionnalité Deeper besoins perspicacité donner tester techniques proposées cadre données étude leurs propriétés fenêtres mouvement histogrammes mises continues modèles histogramme Références Barrio Matran Rodriguez Rodriguez Cuesta Albertos Tests qualité ajustement fonction distance Wasserstein Annales statistique Fisher regroupement homogénéité maximale American Gibbs choix délimitation mesures probabilité Inter national Statistical Review Ioannidis Équilibre entre optimalité histogramme aspect pratique requête estimation taille résultat ACMSIGMOD Ioannidis Universalité série Histogrammes Actes terre Dublin pages Ioannidis Poosala Équilibre entre optimalité histogramme aspect pratique requête estimation taille résultat SIGMOD Konig ajustement courbe paramétrique mation requête entraînée rétroaction résultat dimension optimisation requêtes bases données relationnelles Thèse doctorat Université Western Reserve Piatetski Shapiro estimation précise nombre tuples satisfaisant condi SIGMOD Poosala Ganti Ioannidis Question approximative Réponse grammes histopathologie données Taureau Poosala Ioannidis Shekita histogrammes améliorés estimation sélectivité prédicats gamme SIGMOD Montréal Canada Irpino Romano Irpino nouvelle distance Wasserstein cluster hiérarchique histogramme données symboliques Données Science classification Batanjeli Ferligoj Žiberna Springer Berlin Baguette choix largeur histogramme données Annexe Preuve décomposition distance Wasserstein ijijijijij ShapeLocation Taille observons fonctions densité ayant premiers moments finis chaque fonction densité associée distribution onctions moyens écarts types effet considérant substitution obtient mêmes substitutions dessus adoptées maintenant supposons centrer distributions utilisant leurs moyens Barrio prouvé développement carré obtenons représentation histogramme optimal grands ensembles données cccccc ijijijiijjiijjijiijjd considérons quantité ccijiijjiijjijcc ijiijj considérer corrélation séries données chaque couple observa tions représenté respectivement quantile première distribution tuile second considérer comme corrélation entre tions quantile représentées courbe points quantile infinies parcelle convient noter différemment gamme classique variation indice corrélation Bravais Pearson équation réécrite forme ijijiijjij Addition soustraction obtient ijijijij ijijij remplacer résultat obtenir ijijijij représentation grand histogramme ensemble bonne Données Manière verser visualiseur résumer Données fréquemment Optimiseur exécutée luation requests Système gestion bases données article trons performances Propriétés construction verser stratégies histogrammes optimale facts REELLES descripteur apriori choix Intervalles Élémentaires Nombre premier basons Fisher Algorithme second Alors basons procédé interpolation géométrique fonction pirique répartition fonction morceaux linéaire qualité calculee ajustement Wasserstein Utilisant distributions Entre métrique comparons methods exécutions contre proposées Quelques Celles ensembles artificialisation Données CIELS Réels