Microsoft IRP_ROM_EGC_07_FINAL Optimal histogram representation large Fisher piecewise linear approximation Antonio Irpino Elvira Romano Dipartimento studi europei mediterranei Seconda Universit√† degli Studi Napoli Setificio Complesso Monumentale Belvedere Leucio 81020 Caserta irpino unina Dipartimento Matematica Statistica Universita degli Studi Napoli Federico Cintia Complesso Monte Angelo 80126 Napoli elvrom unina Summary Histogram representation large summarize visualize frequently performed order optimize query estimation paper performance properties strategies optimal construction histograms valued descriptor prior choice number first based Fisher algorithm while second based geometrical procedure interpolation empirical distri bution function piecewise linear function goodness computed using Wasserstein metric between distributions compare proposed method performances against existing artificial datasets Introduction Today storage information mechanism fails capture large amount process their entirety while summary stored context histogram plays producing suitable summarizing description quickly answering decision support queries Following guide phrase image words histogram represents simple intuitive graphical describe distribution smoothes display general shape empirical distribution problem false impression shape dataset distribution because construction depends choice number length subintervals called buckets lines which histogram based Ideally could situation which large nature dataset bimodal small reduces unimodal representation matter stake concerns width account graphical representation underlying constructed minimal error approximation Optimal histogram representation large datasets database community particular framework query optimization search histogram representation large known selectivity estimation problem Estimates select among competing There classes methods selectivity estimation sampling methods statis tical methods paper second nonparametric statistical methods taken account histogram methods briefly reviewed while excellent taxonomy histograms found Poosala continuous valued attributes scientific statistical state histograms implicitly deals discrete categorical attribute value domains which there relatively distinct values attribute methods estimating selectivities Ioannidis Poosala absence numerous duplicate values scientific statistical effectively result empty causing these methods ineffective Starting point approach tries capture statistical variable characteristics consider statistical model based approach since approximate based cumulative function piecewise polynomial geometrical model proposed methods solve histogram computation presence almost continu datasets according different approaches first based Fisher algorithm partition ordered latter based interpolation distribution function sensitivity alternative proposed algorithm investigated using several dataset quality approximation computed proposing goodness measure based Wasserstein metric between distribution functions application artificial dataset performed order corroborate procedure proprieties histogram little review isting techniques examine definition histogram Definition histogram variable constructed partitioning distribution subsets called buckets approximating frequencies values bucket common fashion Ioannidis definition mentioned specific histogram classes which aspects consider construction There aspects considered histogram construction Partition Construction Algorithm Frequency Approximation Value approximation Error Guarantes Ioannidis earliest proposed approaches widths equally spaced proposals essentially based choosing number Nevertheless these methods disadvantage losing details density partition years several types histogram proposed overcome problem common guideline location points addition number Irpino Romano width estimate density function problem received attention statistics database community numerical analysis where density function approximated class polynomial piecewise fixed degree common schemes constructing histograms differ terms their partition constraints first proposal Thesis introduces common concept statistical literature simplest histogram where value divided ranges equal length called width histogram particular frequencies within bucket approximated height bucket width histograms improvement uniform distribution sumption entire value proposals called height depth histograms Piatetski Shapiro these particular sists dividing attribute buckets approximately number tuples After these proposals attention shifted study which initial approximation errors maintained estimating database through these techniques optimal histograms Ioannidis proposed minimize average square error selectivity estimation problem technique partition distribution computed variance source parameter values within bucket minimized addition optimal partition constraints others methods devel aiming mainly group several source parameter values together bucket Among distinguish Maxdiff Ioannidis which places bucket boundaries between adjacent source parameter addition these solutions partition constraints numerical solution capturing shape distribution received attention Among proposed linear splines least square regression Konig however attention devoted number parameters estimate efficient construction basis partition histograms classified according their mutually orthogonally proprieties Poosala table summarizes describes existent methods collocated frame method takes place context methods values observed variable relative cumulative frequency PARAMETER SOURCE PAREMETER Spread Frequency Cumul Values Values Optimal Compressed Optimal Compressed Spline Based Optimal Piecewise Fisher Frequency OPtimal Maxdiff Maxdiff approaches histogram construction algorithms proposed present paper underlined Optimal histogram representation large datasets proposed techiques numerical variable whose domain consists ordered values observation tuples variable while distinct values assumed dataset empirical function defined describe empirical distribution function define interval domain partitioned buckets assuming uniform distribution calculate empirical density bucket density displayed histogram graph which proportion represented areas various Fisher algorithm Fisher algorithm considered Optimal algorithm Indeed function minimized within buckets variance Optimal rithm being fixed number buckets partitioning domain values optimizes source parameter according following formula Where source parameter weight source parameter source parameter domain values correspond minimiza variance within buckets leads implementation dynamic algorithm partition Fisher ordered define following quantities where upper bound bucket chosen according following dynamic formulation where interval notation bucket computational terms operations worst equal Irpino Romano Piece interpolation empirical distribution function method starts trivial histogram bucket histogram uniform proximation bound bucket chosen value which observed maximum distance between predicted value served value distance unweighed standard version algorithm weighted number observations within buckets second values error chosen value populated bucket according practical motivation better remove error bucket approximate large observation smallest start considering trivial histogram VTriv where artificial point added dataset order identify point belonging buckets solve lowing algorithm order cutpoints standard algorithm weighted version Argmax Argmax where frequency bucket which includes where computed means quantile function computational terms operations worst equal quality measure representation following paragraph present consistent compute error square obtained histogram distribution according metric between distribution develop measure accuracy taking consideration square differ ences between predicted observed value considering hypothesised continu nature model against discrete observed values continuous function histogram mixture uniforms overlapping supports interpolate discrete right continuous function always error estimation Given vector values function equal histogram consists buckets represented piecewise linear where general linear piece bounds histogram sense piecewise linear interpolation proposal evaluate procedure accuracy means distance computation between obtained model uniforms mixture histogram Optimal histogram representation large datasets propose Wasserstein distance comparison Gibbs Barrio Verde Irpino considered natural extension Euclidean distance point distribution interesting decompo sition properties Given distribution functions Wassertein distance computed according following formula where quantile functions distributions distance compu tation heavy distribution continuous Verde Irpino showed feasibility dealing histograms Appendix proposed distance decomposed square difference means square difference standard deviations resid assumed shape distance between distributions decomposi summarized ShapeLocation consider maximum error allowed interpolating histogram Wasserstein distance between trivial histogram histogram allowing single bucket optimal obtain relative goodness index ratio between square distance obtained model optimal histogram square distance between trivial model optimal measure Square Goodness Ratio Using decomposition square distance evaluate quality considering distance influenced location shape difference application artificial dataset proposed techniques three dataset first artificial dataset derives random generation values derives mixture three distribution second consists observations variable dst_bytes database1 dataset chosen characteristic being example peaky discontinuous distribution third collects first observation variable Elevation Forest cover database2 dataset chosen being example smooth continuous distribution databases kddcup99 kddcup99 databases covertype covertype Irpino Romano Artificial Dataset Values Buckets Algorithm Measures 55955 604214 566546 58457 56958 58197 49141 58185 14396 033287 06364 109271 20468 14913 06039 02176 95111 71032 902464 339665 13697 02490 354292 217371 074347 04719 02008 07574 99695 FISHER 211255 167489 955339 00252 00097 36562 290044 124707 00633 00386 240339 23878 315012 52743 11027 512781 048029 004592 00196 00052 091357 027947 00862 00563 00289 19953 23803 31237 52245 500763 51278 02326 00285 00100 00036 09135 01942 00676 00396 00234 Synoptics performances algorithmes using first dataset Maxdiff Optimal Fisher Piece approximation distribution function unweighted Piece approximation distribution weighted bucket frequency results showed comparison methods MaxDiff Optimal Fisher algorithm standard weighted version Piecewise cumulative interpolation algorithm measured operational using MATLABTM Intel trino 77Mhz accuracy computed using classic error function accuracy measure based Wasserstein metric between distributions results collected tables While MaxDiff algorithm performances terms spent histogram estimation accurate number values domain variable large piecewise rithm always terms accuracy illustrate enhanced proposed approaches Figure shows results Artificial Dataset Concerning quality goodness buckets algorithms except MaxDiff Voptimal skewed dataset KDD99 first moments Fisher algorithm piecewise better perform others Comparing quality goodness between Fisher piecewise algorithms accurate estimation first moments number buckets increases suppose Fisher algorithms being based variance criterion allow group spherical classes while piecewise methods based linear distribution function implicitly emphasizing local uniform density Optimal histogram representation large datasets Forest Dataset Values Buckets Algorithm Measures 00147 00144 00150 00152 00165 28642 70946 22061 07711 00861 16422 14530 01547 00912 00281 11676 24917 44977 98323 37233 85188 07816 07039 05140 02625 03025 00849 00801 00689 00481 11523 80159 00386 46458 41046 FISHER 91894 22606 08548 04502 02175 04506 01500 00875 00603 00368 19329 24187 32029 53857 47643 44978 08213 03813 01217 00153 02136 00902 00596 00323 00105 19092 24211 50761 55726 20335 44978 09306 04329 01098 00153 02136 00959 00628 00306 00106 Synoptics performances algorithmes using Forest cover dataset Maxdiff Optimal Fisher Piece approximation distribution function unweighted Piece approximation distribution function weighted bucket frequency results showed KDD99 Dataset Values Buckets Algorithm Measures 03753 07955 02525 02647 02837 2998E 2516E 2516E 6199E 5893E 78485 24139 24139 10965 08537 63551 94830 83113 52914 68945 3791E 7991E 7991E 7037E 5893E 62976 52340 52340 28197 08537 57838 82461 39616 45977 87441 FISHER 1452E 0053E 6098E 8253E 8049E 23301 06158 00154 00068 00027 19037 24140 34018 56922 34964 0296E 0148E 9798E 5786E 9934E 00965 00370 00098 00046 00015 19179 24546 33983 56360 16070 4614E 8040E 3669E 8198E 99776 00342 00128 00052 00026 00009 Synoptics performances algorithmes using dataset Maxdiff Optimal Fisher Piece approximation distribution function unweighted Piece approximation distribution weighted bucket frequency results showed Irpino Romano Artificial Forest Measures 58186 14386 23811 00737 71016 02479 85628 02167 21110 00092 90010 01265 FISHER 51266 00051 42695 00103 51266 00034 42695 00105 Synopses quality goodness between model histogram cording proposed decomposition square Wasserstein distance Histogram representation Artificial Dataset illustration empirical distribution function approximation various methods Conclusions perspectives present paper several established algorithms construction histo grams shown accuracy contained database quasi continuous values assumed domain variable Fisher Optimal histogram representation large datasets respect stored tuples proposed techniques seems capable problem Further considering goodness model decomposition Wasserstein metric allows discover quality approximation histogram explaining distance terms goodness first moments distance shape factor present paper multivariate histogram construction considered naturally comparison existing techniques seems suffer curse dimensionality problem deeper insight needs given order proposed techniques stream framework studying their properties moving windows histograms continuous updates histogram models References Barrio Matran Rodriguez Rodriguez Cuesta Albertos Tests goodness based Wasserstein distance Annals Statistics Fisher grouping maximum homogeneity American Gibbs choosing bounding probability metrics Interna tional Statistical Review Ioannidis Balancing histogram optimality practicality query result estimation ACMSIGMOD Ioannidis Universality Serial Histograms Proceedings Dublin pages Ioannidis Poosala Balancing histogram optimality practicality query result estimation SIGMOD Konig Parametric curve fitting feedback driven query result mation Optimization Queries Relational Databases Thesis Western Reserve University Piatetski Shapiro Accurate estimation number tuples satisfying condi SIGMOD Poosala Ganti Ioannidis Approximate Query Answering using Histo grams Poosala Ioannidis Shekita Improved histograms selectivity estimation range predicates SIGMOD Montreal Canada Irpino Romano Verde Irpino Wasserstein based distance hierarchical cluster histogram symbolic Science Classification Batanjeli Ferligoj Ziberna Springer Berlin based choice histogram width Appendix Proof decomposition Wasserstein distance ShapeLocation observe density functions having first moments finite density function associated distribution functions means standard deviations where Indeed considering substitution obtain where substitutions adopted above assume centre distributions using their means Barrio proven where Developing square obtain Optimal histogram representation large datasets consider following quantity considered correlation series where couple observa tions represented respectively quantile first distribution second sense consider correlation between quantile tions represented curve infinite quantile points worth noting differently classical range variation Bravais Pearson correlation index Equation rewritten Adding subtracting obtain replace result obtaining R√©sum√© repr√©sentation histogramme grand ensemble donn√©es bonne mani√®re r√©sumer visualiser donn√©es fr√©quemment ex√©cut√©e optimiser √©valua requ√™tes syst√®me gestion bases donn√©es article trons performance propri√©t√©s strat√©gies construction optimale histogrammes descripteur valeurs r√©elles choix apriori nombre intervalles √©l√©mentaires premier algorithme Fisher alors second proc√©d√© g√©om√©trique interpolation fonction distribution pirique fonction morceaux lin√©aire qualit√© ajustement calcul√©e utilisant Wasserstein m√©trique entre distributions comparons ex√©cutions m√©thodes propos√©es contre quelques celles existantes ensembles donn√©es artifi ciels r√©els