Documents Settings billot documents rnti2e_latex billot_egc_version finale dviUne approche ensembliste inspirée boosting classification supervisée Romain Billot Henri Maxime Suchier Stephane Lallich Université Laboratoire avenue Pierre Mendès France 69676 Cedex France Laboratoire Ingénierie Circulation Transports LICIT INRETS ENTPE Avenue François Mitterand 69675 Cedex France Laboratoire Mathématiques Appliquées Systèmes Ecole Centrale Paris 92295 Châtenay Malabry France Laboratoire informatique Agrocampus Rennes Saint Brieuc 84215 35042 Rennes Cedex France Contacts billotro gmail hmsuchier gmail stephane lallich lyon2 Résumé classification supervisée nombreuses méthodes ensemblistes peuvent combiner plusieurs hypothèses créer règle cision finale performante Ainsi montré méthodes comme bagging boosting pouvaient révéler intéressantes phase apprentissage généralisation tentant vouloir pirer grands principes méthode comme boosting classification supervisée convient préalablement confronter difficultés connues thématique ensembles regroupeurs correspondance classes agrégation résultats qualité introduire boosting processus itératif article propose méthode ensembliste inspirée boosting partir partitionnement obtenu moyennes floues fuzzy means insister itérativement exemples difficiles former partition finale pertinente Introduction courant séparer domaine apprentissage automatique domaines distincts apprentissage supervisé désigne cadre exemples reliés information relative classe concept méthodes supervisées produisent suite partir exemples apprentissage lesquels classe connue règle décision visant prédire classe nouvelles observations Cette règle décision appellée aussi classifieur hypothèse considérée géométriquement comme hypersurface séparant exemples représentés espace multidimensionnel approche ensembliste inspirée boosting classification supervisée contrario cette notion classe concept absente cadre apprentis supervisé Aucune information priori étant disponible techniques super visées visent détecter structures groupes fondées notions distance similarité entre exemples Lerman précisement cadre place travail recherche constat simple classification supervisée méthodes dites ensemblistes cours dernières années montré performances téressantes particulier boosting question processus itératif repondérer exemples insistant classés méthode apprentissage itération donnée Freund Schapire tentant vouloir inspirer processus comme boosting maine classification supervisée Cette transposition immédiate soulève certain nombre problèmes abord boosting repose justifica tions théoriques solides serait présomptueux prétendre appliquer rigoureusement telle méthode pourquoi contribution travail simplement considérée comme approche ensembliste inspirée grands principes boosting premier temps confronter problèmes classiques ensembles regroupeurs présentés section autre notion exemple difficile intuitive apprentis supervisé exemple difficiles essence exemples classés méthode apprentissage reste domaine supervisé définir logie boosting insister chaque itération exemples difficiles précisement pouvoir détecter évaluer qualité individuelle bonne classification exemple article approche Unsupervised Boosting Approach posée détecte repondère exemples difficiles regrouper partir partition floue construire itérativement matrice association permettra former finalement partition pertinente certains critères qualité ensembles regroupeurs problématique ensembles regroupeurs consiste combiner résultats sieurs algorithmes partitionnement centres mobiles former partition pertinente différentes instances trouvera présentation méthodes pulaires Hornik applications domaines rassemblement réutilisation connaissances nombreuses ensembles regroupeurs peuvent aussi permettre combiner partitionnements obtenus partir ensembles indivi attributs différents formation ensemble regroupeurs heurte certaines difficultés absence information classe instances blème correspondance entre classes construites différents partitionnements autre construction partition finale appuyer méthode consensus efficace tient aussi compte qualité différents partitionnements concerne correspondance classes proposé index cohérence calculer similarité entre classes partitions différentes grand nombre points partagés procédure connue aussi matching score désigner itérativement classes possédant grand score correspondance Strehl Ghosh quant proposé fonction objectif pourrait assurer trouver partitionnement idéal partageant information possible Billot partitionnements appel notion information mutuelle normalisée Dimitriadou question méthode laquelle ensemble partitionnements flous combiné former partition floue données partition finale formée minimise fonction dissimilarité entre partitions initiales Cette approche résiste obstacle correspondance groupes considérant toutes permutations possibles matrices appartenance propose contourner problème correspondance groupes utilisant matrice association individus individus considère fréquence laquelle exemples retrouvent groupe Certaines méthodes ensemblistes supervisées objet transpositions maine supervisé Leisch exemple appliqué bagging contexte super proposer algorithme bagged clustering utiliserons comme procédure notre contribution application boosting classification supervisée abordée Frossyniotis méthode boost clust repondère itérativement exemples difficiles forme partition floue optimale Toutefois algorithme transpose analogie réelles justifications concepts justifiés contexte supervisé significatif auteurs réside utilisation vecteurs appartenance partitions floues détecter exemples sensibles expérimentations proposées succintes permettent malheureusement évaluer façon satisfaisante efficacité méthode différentes variantes algorithme algorithme Unsupervised Boosting Approach constitue nouvelle proche inspirée boosting construire itérativement partition données algorithme déroule quatre phases trois phases répétées chaque itération procédure tandis quatrième dernière phase établit partition finale Chacune itérations procédure boostée commence phase évalua permettre sélection exemples difficiles cours cette évaluation moyennes floues Bezdek effectuées seule caractère utilisé calculer critères qualité partir degrés appartenance exemples repondérés directement cette phase évaluation deuxième phase commence alors appelée phase regroupement stabilisation constitue procédure bagged clustering moyennes floues classiques appliquées échantillons bootstrap obtenus partir nouvelle distribu poids important noter opérer repondération première phase entraîne utilisation données initial pendant algorithme Ainsi première itération procédure bagging effectuée partir nouvelle distribution poids aspect implique toutes partitions obtenues partir distributions poids uniformes problème correspondance entre différents groupes contourné lisation matrice association intervient phase algorithme Ainsi points considérés matrice individus approche ensembliste inspirée boosting classification supervisée individus lorsque points classe tition itération donnée itérations dites boosting partition finale formée majoritaire matrice construite itérativement phase Avant détailler processus algorithme semble indispensable attarder sections suivantes quelques points essentiels méthode critère local qualité exemple approche inspirée boosting mettre accent itérations successives exemples difficiles classer Comment détecter exemples cette notion intuitive apprentissage supervisé exemple difficile essence classé classifieur choisi convient apprentissage supervisé quantifier qualité positionnement chaque exemple grande majorité connaissance priori structure données judicieux comme Frossyniotis vecteur degrés appartenance exemples différentes classes formées algorithme moyennes floues itération vecteur construit algorithme moyennes floues Ainsi représente degré appartenance exemple classe somme termes vecteur degrés appartenance individu quelconque partir vecteur possible calculer entropie degrés appartenance individu précisement entropie Shannon constitue mesure utile connue quantifier notion désordre critère local défini entropie Shannon degrés appartenance exemple critère permet repérer degré indécision rattachement exemple classe particulier exemple partition données quatre classes Sachant somme degrés appartenance exemple suivant forcément difficile classer égalité degrés appartenance entropie Shannon maximale sommes incertitude totale exemple pouvant appartenir croyance importe quelle classe contraire exemple classé appartenance quatre classes apparaît clairement possédera vecteur forme suivante entropie Shannon minimale succinctement exposé traduit exemple devra autant repondéré critère qualité local savoir entropie degrés appartenance Ainsi exemples semblant difficiles classer seront favorisés aucun dégré appartenance classe détache critère ailleurs rappeler notion rejet contexte supervisé Leray critère global qualité Parallèlement qualité locale exemple repondération tenir compte conjoin tement qualité globale partitionnement aucuns verront analogie erreur Billot apprentissage utilisée Freund Schapire boosting construire coefficient entre compte favorisation exponentielle exemples classés Cette qualité globale aussi surtout envisagée souci logique effet pourquoi repondérer exemple apparemment difficile qualité partition laquelle trouve médiocre conserver certaine cohérence approche logique critère global qualité partitionnement cours moyenne critères locaux Ainsi bieme itération critère qualité moyenne courants précisement critère offre possibilité repondérer exemple tenant compte seulement qualité classement intrinsèque partitionnement critère local aussi qualité globale regroupement lequel trouve critère repondération exemples difficiles performances boosting reposent partie construction ensemble pothèses faibles construites partir distributions statistiques lesquelles favorisés exemples difficiles principe transposé notre proposition hypothèse algorithme classique moyennes floues intérêt choix partition floue réside possibilité raisonner terme degré partenance erreur apprentissage remplacée critères qualité local global définis dessus suite mettre poids chaque exemple tenant compte qualité globale partition qualité locale classification point chaque itération poids exemple ajusté façon décrite algorithme conséquent poids exemples entropie degrés apparte nance supérieure moyenne échantillon augmentés tandis autres voient diminuer normalisation argument exponentielle comporte termes premier terme log2K rapproche lorsque qualité globale partition mauvaise log2K borne supérieure coefficient entropie minimiser deuxième terme représente qualité individuelle classification exemple autant élévé exemple difficile Logiquement exemples prendront ponentiellement importance seront points classés partition bonne qualité Utilisation procédure bagged clustering autre approche inspirée méthodes supervisées occurence bagging transposée contexte supervisé Cette méthode appelée bagged clustering proposée Leisch procédure combine méthode regroupe centres mobiles moyennes floues classification hiérarchique nouveauté réside application méthode clustering plusieurs échantillons bootstrap données départ transposition problème initial espace centres formés méthode échantillons bootstrap nombre classes priori donné suffisament grand regroupement centres mobiles moyennes floues appliqué chaque échantillon bootstrap centres finaux regroupés matrice classification hiérarchique centres ensuite effectuée chaque point approche ensembliste inspirée boosting classification supervisée assigné classe contient centre proche figure résume procédure ETAPE partir distribution poids tirage échantillons Bootstrap données ETAPE Application moyennes floues chacun échantillons nombre élévé centres exemple centres Récupération matrice finale centres taille lignes colonnes nombre attributs Cluster Dendrogram ETAPE Classification hiérarchique ascendante nouvelle matrice centres ETAPE partition finale obtenue coupant dendrogramme nombre classes désiré Ensuite parmi classes obtenues chaque exemple affecté classe contient centre était proche parmi centres MEANS MEANS MEANS MEANSFUZZY MEANSFUZZY MEANSFUZZY MEANSFUZZY CFUZZY CFUZZY CFUZZY MEANS MEANSFUZZY MEANS MEANSFUZZY CFUZZY CFUZZY MEANS MEANSFUZZY CFUZZY MEANS MEANSFUZZY CFUZZY CFUZZY MEANS MEANSFUZZY MEANS procédure bagged clustering semblé intéressant introduire processus comme procédure rithme stabilisera résultats partition formée autant dénaturer originale repondérations successives Complexité algorithme intérieur phases complexité algorithmique reste linéaire fonction phase complexité quadratique constante nombre itérations boosting remarquera utilisation classification rarchique augmente complexité celle effectuée matrice centres taille constantes Enfin formation partition finale phase fectue parcours matrice Finalement utilisation matrice association borne façon générale complexité procédure Billot Algorithme Pseudo algorithme Entrées vecteur instances chaque nombre initial groupes nombre itérations Sorties partition données début Initialisation termes matrice association Initialisation vecteur qualités globales Initialisation poids allant faire coefficient normalisation vecteur critères locaux exemples PHASE EVALUATION Application moyennes floues distribution poids Calcul critères qualité Repondération exemples alors Normalisation poids PHASE REGROUPEMENT STABILISATION procédure bagged clustering PHASE MATRICE allant faire allant faire classe alors PHASE FORMATION PARTITION FINALE MAJORITAIRE chaque exemple regrouper exemples classe éventuels exemples seuls formeront singletons Expérimentations Choix critère qualité externe partition finale problème majeur apprentissage supervisé réside absence départager différentes méthodes classification supervisée existe aussi local erreur prédiction global erreur avons arbitrai rement choisi global compare qualité partitions finales différentes mesures validation partitions résumées valider expérimentalement performances approche savoir qualité partition proposée indices fondés mesures compa dispersion intra classe séparation dispersion inter classe semblent adaptés approche ensembliste inspirée boosting classification supervisée algorithmes regroupement classiques cherchent également optimiser critères mêmes notions Ainsi indices comme indice silhouette moyenne ratio intra inter pertinents combinaison trois indices intéressante valeurs seront serrées Rappelons indice silhouette moyenne partition maximiser tandis ratio intra inter minimiser indice qualité suivant silhouette ratiowb combinera simplement trois indices aussi maximiser comparaison effectuée entre méthode centres mobiles MacQueen moyennes floues Bezdek procédure bagged clustering classique Leisch constitue rappelons procédure notre proposition grande majorité meilleure partition domine trois autres trois indices maximisation silhouette moyenne minimisation ratio rapport autres rares incertains valeur finale prise sélectionner méthode formant meilleure partition expérimentation précise Résultats méthode testée environnement statistique disponibles bases données connues Comme rappelait Diday algorithmes regroupement peuvent fournir solutions satisfaisantes forcément optimales Ainsi plusieurs répétitions centres mobiles données peuvent donner résultats différents conséquent avons expérimentations rélancé différents algorithmes sélectionné meilleur résultat chaque méthode serait ailleurs intéressant analyser stabilité partitions gorithmes regroupement comme Bertrand Mufti caractéristiques données nombre individus variables rappellées tableau suivant valeur indice qualité nombre itérations nombre final groupes formé notre méthode rappelons différent également introduits tableau nombre final classes formées était férent initial final calcul indice qualité dépend nombre classes effectue centres mobiles moyennes floues assure cohérence comparaisons partitions formées résultats obtenus assez intéressants total expérimentations thode surpasse centres mobiles moyennes floues aequo considérant comparaisons méthodes manière pendante observé toujours indice qualité notre approche domine centres mobiles autre améliore moyennes floues méthode supérieure bagged clustering égale pouvons constater nombre itérations généralement assez faible contrairement pouvait attendu procédure boosting grand nombre itérations certains entraîner chutes performances répondérations successives exemples déformant structure initiale conclure pouvons résultats montrent processus ensembliste Billot donnees Centres mobiles moyennes floues bagged clustering final Breast cancer individus attributs Baviere individus attributs Diabetis individus attributs Heart individus attributs Thyroid individus attributs German individus attributs Indiens individus attributs Zurich individus attributs individus attributs X8d5k individus attributs X2d2k individus 00052 attributs Tableau résultats approche ensembliste inspirée boosting classification supervisée itératif insiste exemples difficiles classer grandement améliorer parti tionnement critères intra inter classe illustrations graphiques section suivante permettre visualiser performances méthode autres données propices visualisation graphique comportant attributs Illustration graphique figure compare partitions formées approche moyennes floues données Fisher connu statisticiens individus plantes décrits variables existe plans représentation possibles données exemple nombre classes donné priori était figure illustre quant classification trois classes données DNase dimensions classification moyennes classiques classification méthode Visualisation classification moyennes gauche données plans possibles représentation exemples classes bolisés triangles ronds remarquera différents plans mauvaise classification trois individus droite cette erreur corrigée méthode affecte trois exemples bonne classe Conclusion article avons proposé nouvelle approche ensembliste apprentissage supervisé inspire principes boosting chaque itération algorithme repère certains critères exemples difficiles donner importance itéra suivante Cette approche ainsi premier temps partition floue formée méthode moyennes floues pondérées glisser ensuite partition finale Billot Visualisation classification données DNase moyennes classiques gauche droite remarquons sensible milieu gauche figure classes nettement séparées moyennes floues droite incertitude corrigée premiers résultats prometteurs laissent penser telle approche liorer qualité partitions finales formées termes compacité séparation présent serait pertinent comparer performances méthode plusieurs autres approches ensemblistes proposées domaine supervisé Frossyniotis Parmi perspectives travail faudrait trouver alternative matrice association coûteuse proposer nouveaux critères qualité exemple plémentant critères soient essentiellement fondés mesures dispersion intra inter classe serait envisageable affranchir utilisation procé bagged clustering implémenter notre propre procédure stabilité fondée tirages bootstrap modifications échantillon rapport nouvelle pondération conclure pouvons transposition boosting apprentissage supervisé faire grande prudence effet concepts justifiés riquement contexte supervisé appliquent façon directe classification supervisée Notre travail simplement inspiré certains concepts boosting principalement repondération exemples difficiles reste maintenant approfondir cadre théorique méthode proposée Références Bertrand Mufti Loevinger measures quality assessing stability Computational Statistics Analysis available ideas repec csdana v50y2006i4p992 approche ensembliste inspirée boosting classification supervisée Bezdek Pattern Recognition Fuzzy Objective Function Algorithms Norwell Kluwer Academic Publishers Breiman Bagging predictors Maching Learning Diday Optimization hierarchical clustering Pattern Recognition Dimitriadou Weingessel Hornik Voting merging ensemble method clustering ICANN Proceedings International Conference Artificial Neural Networks London Springer Verlag Finding consistent clusters partitions Proceedings Second International Workshop Multiple Classifier Systems London Springer Verlag Freund Schapire decision theoretic generalization learning application boosting Journal computer system sciences Frossyniotis Likas Stafylopatis clustering method based boosting Pattern Recognition Letters Halkidi Batistakis Vazirgiannis clustering validation techniques Journal Intelligent Information Systems Hornik Cluster ensembles Workshop Ensemble Methods Leisch Bagged clustering Working Papers Adaptive Information Systems Modelling Economics Management Science Institut Informationsverarbei Produktionsmanagement Leray Zaragoza Alché Pertinence mesures confiance classification 12ème Congrès Francophone AFRIF Reconnaissance Formes Intelligence Articifielle Paris France Lerman bases classification automatique Gauthier Villars Paris MacQueen methods classification analysis multivariate obser vations Proceedings Fifth Berkeley Symposium Mathemtical Statistics Probability Strehl Ghosh Cluster ensembles knowledge reuse framework combining multiple partitions Journal Machine Learning Research Summary Cluster Ensemble Methods shown their efficiency build better clustering clusterings order efficient consensus partition problems solved correspondence between groups results combination supervised learning ensemble methods produced superior results learning generalization article propose transpose principles boosting clustering Thanks fuzzy clustering algorithm Unsupervised Boosting Approach recognizing sensitive examples reweighting boosting process which leads crisp clustering dataset