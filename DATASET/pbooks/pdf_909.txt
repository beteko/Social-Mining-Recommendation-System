PUBLICATIONS RAPPORTS egc07 dviApprentissage Statistique Topologie Ensemble Données Etiquetées Pierre Gaillard Michaël Aupetit Gérard Govaert Commisariat Energie Atomique 91680 Bruyères Châtel France pierre gaillard michael aupetit Université Technologie Compiègne 60319 60203 Compiègne Cedex France gerard govaert Résumé Découvrir topologie ensemble données étiquetées espace Euclidien aider construire meilleur système décision papier proposons modèle génératif graphe Delaunay plusieurs prototypes représentant données étiquetées extraire graphe topologie classes Introduction extraction topologie discrimination Généralement problèmes apprentissage supervisé impliquent ensemble étiquetées vecteur dimension label classe associée vecteur objectif ultime méthodes apprentissage supervisé construire classifieur prédire classe nouveaux vecteurs minimum erreur Cependant discrimination seulement dernière étape processus apprentissage enrichie travers phase exploration effet plusieurs caractéristiques topologiques classes peuvent utiles parmi lesquelles connexité évaluer complexité problème classification dimension intrinsèque selectionner variables discriminantes moyen capturer structure données modéliser distribution terme variables cachées latentes principaux modèles génératifs traitant apprentissage supervisé variétés Generative Topographic Mapping Bishop Probabilistic Principal Component Analyzers Tipping Bishop mière approche dimension intrinsèque fixée priori permettre visualisation tandis seconde approche dimension intrinsèque capturée connexité perdue dépasser limites autre modèle génératif Graphe Delaunay prototypes représentant données proposé modèle appelé Graphe Génératif Gaussien Aupetit assume aucun priori topologie permet apprendre connexité ensemble données proposons étendre supervisé extraire topologie classes Observant comme généralisation modèles Mélange Gaussien Graphe Génératif Gaussien Supervisé transposés apprentissage supervisé notre approche utilise chemin étendre supervisé section introduit brièvement version supervisée ainsi section introduisons nouvel algorithme permettant représenter topologie ensemble données étiquetées supposées issues variétés génératrices Tibshirani corrompues bruit additif testons données artificielles section avant conclusion section modèles mélange gaussien modèles mélange peuvent comme moyen flexible représenter densité probabilité modèle paramétrique modèle mélange gaussien défini somme pondérée finie composants gaussiens ayant forme suivante nombre composants densité gaussienne moyenne covariance probabilité donnée appartienne composant modèle comme processus génération comportant étapes tirage composant probabilité tirage donnée suivant densité posant Ainsi modèle variétés génératrices supposées ensemble points corrompus bruit gaussien additif contexte apprentissage supervisé Miller suggèrent apprendre allocation classes chaque composant durant apprentissage introduisent mètre additionel représente probabilité conditionnelle assigner composant classe puisque composants communs différentes classes modèle permet réprésenter facilement possible structure commune férentes classes chevauchement classes modèle appelé Generalized Gaussian Mixture prend forme πjβcjg velle contrainte Graphe Génératif Gaussien données supposées avoir générées ensemble points segments constituants variétés génératrices corrompues bruit additif gaussien isotropique moyenne nulle variance inconnue modèle jacent éléments gaussiens appelés points gaussiens segments gaussiens définissent modèle mélange Etant donné ensemble prototypes placés prototypes construit mélange gaussien obtenu somme pondérée sommets convolués bruit gaussien isotropique variance poids babilité donnée issue point gaussien associé segment gaussien associé valeur point point gaussien centré proto variance définie valeur point segment gaussien variance Gaillard 1ajbj Lajbj Graphe Génératif Gaussien Supervisé article données supposées générées ensemble points segments consituant variétés génératrices corrompues bruit additif gaussien tropique moyenne nulle variance inconnue suppose élément gaussien dimension écrit générer données différentes classes probabilités respectives définit ainsi modèle suivant Apprentissage modèle Initialisation Etant donné ensemble prototypes placés riance identique algorihme Miller prototypes construit définit graphe initial Ensuite chaque chaque sommet graphe modèle génératif sorte graphe génère modèle mélange gaussien poids initialisés manière equiprobable paramètre initialisé valeur obtenue alors chaque composant initialisé Enfin initialisons valeur obtenue Apprentissage paramètres fonction objectif choisie comme étant semblance jointe données classes Cette mesure qualité définie maximiser vraisemblance utilisons algorithme algorithme consiste itérations modifiant manière maximiser vraisemblance règles paramètres prenant compte contraintes positivité somme égale Lajbj Qiajbj Lajbj Qiajbj Lajbj Qiajbj Qiajbj Qiajbj Laibi Qiajbj Lajbj Graphe Génératif Gaussien Supervisé Qiajbj Lajbj βcijg probabilité posteriori donnée générée composant Elagage Finalement obtenir topologie supervisée élaguons intial lesquels chance aient généré données associés poids quasi apprentissage stade représentent connexité densité jointe toutes classes Sélection modèle apprentissage statistique sélectionner modèle parcimo nieux parmi collection modèles tâche importante complexité Graphe Génératif Gaussien Supervisé définie nombre sommets Puisque complexité notre modèle intimement nombre prototypes choisissons meilleur critère Schwartz construire initial Ainsi sélectionnons composants maximise nombre paramètres libres modèle Expériences Figure décrit différentes étapes apprentissage montre différences entre Figure présente expérience vérifions capacité apprendre topologie ensemble données étiquetées plusieurs condi tions bruit utilisons données artificielles connaissons logie pouvoir vérifier validité modèles raisons place descriptions commentaires conclusions expériences légende figures toutes expériences utilisons mêmes valeurs paramètres Conclusion Découvrir topologie ensemble données étiquetées fournir importantes informations construire classifieur Suivant principe Generalized Mixture Miller proposons étendre Graphe Génératif Gaussien Aupetit supervisé modéliser variétés génératrices classes utilisons modèle génératif graphe Delaunay comprenant plusieurs prototypes représentant données étiquetées graphe obtenu représente connexité densité jointe toutes classes permettant extraire infor mations topologiques permet exemple extraire dimension intrinsèque variétés ainsi informer chevauchement classes envisageaons poursuivre cette étude construction graphe planaire permettant synthétiser informations pologiques désormais extractibles Références Aupetit Learning topology generative gaussian graph algorithm vances Neural Information Processing Systems Gaillard Principe Graphe Génératif Gaussien Supervisé données issues riétés génératrices quarts cercle forme point probabilités respectives Elles corrompues bruit riance données classes respectivement représentées quart cercle gauche mixte celui droite point forme prototypes placés variance identique connectés optimal obtenu après optimisation vraisemblance rapport élagage associés poids quasi représentant seule classe plusieurs classes courbes densité chaque classe valeurs obtenues meilleur covariance libre critère valeur estimée variance bruit égale légèrement surestimée puisque variétés linéaires approximées ensemble variétés linéaires vraissemblance normalisée échantillon indépendant données chaque modèle NLSGGG NLGGM permet retrouver topologie variétés respectant étiquette classes Contrairement donne aucune information connexité classes informe recouvrement classes paramètre recouvrement caractérisée Graphe Génératif Gaussien Supervisé Robustesse bruit tirons aléatoirement ensembles apprentis différents chaque valeur bruit utilisons processus apprentissage décrit section construire chaque ensemble extrayons caractéristiques topologiques modèle nombre composants connexes degré sommets comparons topologie modèle celle originale exemple vérifions partie modèle représentant semble connecté associé seule classe sommets degré points extrêmes sommet degré valant intersection autres degré valant figures representent ensemble apprentissage parmi différents chaque valeur bruit résultats présentés pourcentage représentent nombre modèles correctement modélisés variétés génératrices gauche droite cercle gauche celui droite point modèle permet retrouver variétés génératrices simples respectant étiquette classes présence bruit Cependant lorsque variance bruit augmente variété souvent modélisée forme présence bruit efficacité modèle diminue demeure relativement robuste Bishop Svensen Williams generative topographic mapping Neural Computation Miller mixture experts classifier learning based labelled unlabelled Neural Information Processing Systems Schwartz Estimating dimension model Annals Statistics Tibshirani Principal curves revisited Statistics Computing Tipping Bishop Mixtures probabilistic principal component analysers Neural Computation Summary Discovering topology labeled Euclidian space design better decision systems propose supervised generative model based Delaunay Graph prototypes representing labeled order extract graph topology classes