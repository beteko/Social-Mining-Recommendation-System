e33co clustering données mixtes modèles mélange aichetou bouchareb boullé fabrice rossi orange prenom orange université paris panthéon sorbonne prenom paris1 résumé classification croisée clustering technique super visée permet extraire structure jacente existante entre lignes colonnes table données forme blocs plusieurs approches étudiées démontré capacité extraire structure table données continues binaires contingence cependant travaux traité clustering tables données mixtes article étendons utilisation clustering modèles blocs latents données mixtes variables continues variables binaires évaluons efficacité cette extension données simulées discutons limites potentielles introduction classification croisée objectif réaliser classification jointe lignes colonnes tableau données proposée hartigan classification croisée extension classification simple clustering permet traire structure jacente données forme groupes lignes groupes colonnes avantage cette technique rapport classification simple réside étude simultanée jointe lignes colonnes permet extraire maximum formations dépendance entre elles utilité clustering réside capacité créer groupes facilement interprétables capacité réduction grande table données matrice significativement petite ayant structure données originales traitement matrice résumée permet étudier prendre décisions données originales réduisant significativement coûts calcul temps mémoire depuis introduction plusieurs méthodes développées effectuer classi fication croisée cheng church dhillon méthodes diffèrent principalement données étudiées continues binaires contingence hypothèses considérées méthode extraction utilisée forme souhaitée résultats classification stricte floue hiérarchie approches connues celle classification modèles blocs latents basée clustering données mixtes modèles mélange modèles mélanges classes lignes classes colonnes définies variables latentes estimer govaert nadif modèles étendent contexte classification croisée mélanges gaussiennes données continues bernoulli données binaires modèles blocs latents ainsi proposés validés données numériques binaires contingence catégorielles govaert nadif toutefois notre connais sance modèles appliqués données mixtes pratique données présentent seulement forme continue binaire extraction informations nécessite traitement données mixtes sachant plupart méthodes analyse données conçues particulier données entrée analyste trouve obligé passer phase traitement données transformant forme souvent binaire utiliser approche appropriée analyser indépendamment effectuer interprétation conjointe résultats traitement données types différents risque faire perdre informations impor tantes traitement indépendant risque rendre difficile interprétation résultats méthodes reposant différents types modélisation modèles mélange utilisés étudier données mixtes contexte clustering mcparland gormley proposent modèle variables latentes suivant gaussienne données numérique binaire ordinales nominales utilisation modèles classification croisée donnés mixtes reste inexistante proposons étendre modèles proposés govaert nadif données mixtes continues binaires adoptant comme auteurs approche estimation maximum vraisemblance reste article organisé comme section commençons définir modèles blocs latents utilisation classification croisée section présentons notre extension données mixtes section présente résultats expérimentaux données simulées section conclusions perspectives clustering modèles blocs latents considérons tableau données ensemble objets ensemble variables caractérisant objets représentés respectivement lignes colonnes matrice objectif obtenir partition lignes groupes partition colonnes groupes notées permettent après avoir réordonné lignes colonnes groupe former blocs homogènes intersections groupes supposons nombre classes lignes classes colonnes connus élément appartient seulement ligne appartient groupe lignes colonne appartient groupe colonnes partitions lignes colonnes représentées matrice binaire appartenance classes lignes matrice binaire appartenance classes colonnes seulement seulement vraisemblance modèle blocs latents écrit alors bouchareb représente ensemble toutes partitions possibles toutes parti tions possibles vérifient conditions dessous ensemble paramètres inconnus modèle hypothèses modèle blocs latents hypothèses principales existe partitions lignes classes partitions lonnes classes telles chaque élément résultat distribution probabilité dépend classe ligne classe colonne partitions peuvent représentées variables latentes estimer appartenances classes lignes classes colonnes indépendantes connaissant appartenances classes données observées indépendantes indépendance conditionnelle couple hypothèses vraisemblance modèle écrit alors πzikk zikwjl vraisemblance donnée πzikk zikwjl sommes produits limites respectivement proportions classe lignes classe colonnes ensemble paramètres vraisemblance celle gaussienne données continues correspond bernoulli données binaires matrice partition classes somme nécessite moins opérations brault lomet impossible calculer directement vraisemblance temps raisonnable empêche appli cation directe algorithme classiquement utilisé modèles mélange govaert nadif utilisent approximation variationnelle algorithme contribution suite considérons table données mixtes ensemble objets décrits variables continues binaires ensemble variables continues ensemble variables binaires objectif obtenir partition lignes groupes partition colonnes continues groupes partition colonnes binaires groupes notées pectivement supposons partition lignes partition colonnes continues partition colonnes binaires indépendantes partitions représentées matrices binaires classification matrices classification floue respectivement conditionnellement clustering données mixtes modèles mélange indépendantes existe moyen distinguer colonnes continues discrètes hypothèses vraisemblance modèle génératif données mixtes écrit wcjclc wdjdld ijcklc zikwcjclc ijdkld zikwdjdld notons hypothèses conduisent simple combinaison situations précé dentes binaire numérique difficulté mathématique nouvelle rapport mixte plutôt conséquences pratiques potentielles induites couplage entre distributions différentes classification lignes caractère commensurable densités variables continues probabilités variables binaires optimisons vraisemblance algorithme itératif inspiré vaert nadif décrit dessous vraisemblance complétée dérivons abord expression vraisemblance complétée demande connaissance valeurs variables latentes wcjclc wdjdld ijcklc zikwcjclc ijdkld zikwdjdld sommes limites respectivement approximation variationnelle modèles blocs latents possible appliquer algorithme direc tement dépendance entre entre autre infaisable calcul distribution jointe intégrer vraisemblance complétée rapport cette distribution comme govaert nadif utilisons approximation variationnelle consiste approcher distributions conditionnelles variables latentes forme factorisable précisément approchons produit distribu tions ajustables paramètres respectivement minore ainsi vraisemblance complétée critère suivant tcjclc tdjdld ijcklc siktcjclc ijdkld siktdjdld tcjclc tcjclc tdjdld tdjdld bouchareb algorithme maximisation borne inférieure jusqu convergence trois étapes rapport fixés revient calculer tcjclc tdjdld rapport fixés revient calculer cjclc djdld tcjlc tdjld rapport revient calculer proportions classes leurs paramètres cjclcxijc cjclc cjclc cjclc djdldxijd djdld notre implémentation algorithme avons choisi boucles rieures boucle extérieure normalisons après calcul prenant valeurs relatives expérimentations difficile évaluer méthodes modèles mélange données réelles distribution jacente inconnue cette section présentons expériences réalisées données simulées première expérience objectif notre implémentation données vérifier apport approche deuxième expérience permettra étudier influence plusieurs paramètres nombre blocs taille matrice niveau bruit première expérience données notre premier données simulé matrice classes lignes classes colonnes continues classes colonnes binaires prises individuellement parties continues binaires permettent distinguer seulement classes lignes clustering données mixtes modèles mélange algorithm latent block étendu require iteration initialiser aléatoirement calculer équation while maxiter unstable criterion while innermaxiter unstable criterion calculer équation calculer équation criterion while while innermaxiter unstable criterion calculer 1jdld équation calculer équation criterion while criterion while ensure conjointement espère trouver quatre classes lignes données étudions influence taille matrice données nombre lignes colonnes continues colonnes binaires considérons tailles matrice représentées lignes colonnes chaque matrices mixtes résultantes éléments niveau confusion entre mélanges considérons trois niveaux faible moyennes gaussiennes écarts types gaussiennes paramètres bernoulli moyen élevé configuration expériences avons remarqué algorithme parfois retrouver structure blocs carrés marginales paramètres égales considérons configurations nommées asymétrie marginales paramètres classes lignes classes colonnes différentes bouchareb symétrie marginales identiques spécification paramètres blocs continus binaires configuration détaillée table noter mélanges gaussiennes colonnes permettent distinguer classes ligne associant autre paramètres bernoulli colonnes permettent distinguer classes lignes associant autre étudiant données mixtes conjointement attend distinguer quatre classes lignes asymétrie symétrie vraie spécification blocs configurations asymétrie symétrie expérimentations effectuées étapes appliquer algorithme coclus tering données continues seules binaires seules valider notre implémentation appliquer ensuite algorithme ensemble données mixtes tailles matrice niveaux confusion types configuration clustering appliqué partie continue partie binaire ensemble variables mixtes total expériences ainsi effectuées connaissant vraies classes données mesurons performance classi fication blocs indice ajusté permet mesurer écart entre partition retrouvée méthodes clustering vraie partition focali capacité notre approche potentiellement mieux identifier classes lignes utilisant toutes variables types mixtes collectons effet lignes trois figure observé étude variables continues ari_cont binaires ari_disc mixtes ari_mix chaque données échantillons données selon paramètres configuration présentons résultats forme diagrammes violon diagrammes violon hintze nelson permettent combiner avantages boîtes moustaches estimateurs densité probabilité différentes valeurs permettant ainsi meilleure visualisation variabilité résultats validation implémentation valider notre implémentation avons appliqué algorithme données conti seules données binaires seules comparant résultats package blockcluster bhatia premier temps avons vérifier notre implémentation obtient moins mêmes performances blockcluster appliquant algorithme données avons néanmoins observé quelques problèmes optimisation critère premier algorithme converge rapidement optimum local souvent optimum correspond seule classe lignes seule classe colonnes point clustering données mixtes modèles mélange amélioré imposant nombre minimal itérations paramètre rithme1 permet améliorer significativement qualité résultats optimisation configuration symétrie marginales paramètres égales problème séparabilité classes devient intrinsèquement difficile algorithme sortir optimum local correspondant classe lignes classe colonnes lequel tombe première itération remédier problème commençons algorithme petits calcul appartenances classes critère stabilise après premières itérations phase initiale itère jusqu stabilisation cette stratégie permet trouver meilleure solution données binaires permet amélioration notable continu configuration symétrie résultats obtenus notre implémentation package blockcluster faible qualité variance importante problèmes optimisation suite article présentons résultats portant cette configuration manque place autre parce faible qualité résultats permis observer variations comportement significatives expériences effectuées apport clustering données mixtes vérifions méthode exploitant données mixtes arrive effectivement tifier quatre clusters lignes comme attendu méthodes exploitant données continues binaires séparément peuvent identifier quatre clusters correctement dépasse jamais revanche remarquons amélioration importante significative quand données continues binaires utilisées conjointement figure confusion faible confusion moyenne confusion élevée première expérience asymétrie lignes continu binaire mixte rappelons échantillons générés chaque configuration données calculés chaque échantillon diagrammes violon permettent statistiques comme moyenne médiane étendue aussi distribution valeurs exemple figure partie continue rouge échantillons taille valeur environ étendue variance nulles étude mixte diagramme violon montre concentration importante autour matrice taille autour matrice taille apparaît clairement quelle configuration quelle taille matrice prise compte données mixtes améliore façon significative bouchareb influence taille matrice quantité données disponibles augmente taille matrice facile convergence algorithmes vraies distributions jacentes cette tendance observée systématiquement figure notamment binaire mixte confusion élevée influence niveau confusion quand niveau bruit augmente devient diffi retrouver bonne partition lignes effet visible particulier figure niveau élevé confusion difficile séparation classes lignes binaire mixte surtout quand matrices petites résumer utilisation conjointe variables continues binaires permet identi structure quatre clusters lignes donné étudié possible variables continues seules binaires seules résultats obtenus clustering données mixtes autant meilleurs niveau bruit faible taille matrices grande deuxième expérience données considérons configurations asymétriques marginales leurs paramètres différentes chaque cluster lignes colonnes utiliser algorithmes clustering performants section avons choisi configuration clusters lignes peuvent identifiés figure utilisant variables continues seules binaires seules mixtes étudier influence nombre blocs clustering mixte notre deuxième données généré fonction paramètres suivants nombre blocs considérons configurations partitions matrice initiale taille matrice données considérons tailles lignes colonnes chaque niveau confusion entre mélanges considérons trois niveaux faible moyennes gaussiennes écart gaussiennes paramètre bernoulli moyen élevé types configuration utilisés présentés table tailles blocs tailles matrice niveaux confusions clustering appliqué partie continue partie binaire ensemble variables mixtes total expériences ainsi effectuées comme première expérience générons échantillons données selon paramètres chaque configuration présentons résultats ligne forme diagrammes violon résultats clustering figure présente lignes clustering variables continues binaires mixtes raisons place montrons configurations clustering données mixtes modèles mélange asymétrie vraie spécification blocs configuration asymétrie blocs illustrent influence augmentation nombre blocs configurations comportant blocs visualisées confirment impact quand nombre blocs augmente faible moyenne elevée faible moyenne elevée deuxième expérience lignes continu binaire mixte comme première expérience utilisation données mixtes améliore nettement performances clustering alors variable continues seules binaires seules permettaient théorie retrouver clusters lignes première constatation partie binaire données sensible taille matrice nombre blocs niveau confusions alors partie continue généralement stable influencée niveau confusion bouchareb influence nombre blocs constatons figure nombre blocs grand difficile séparer classes effet observé particulier variables binaires seules variabilité résultats grande lorsque nombre blocs important cette variabilité moins présente donnés continues encore moins mixte influence taille matrice partie continue séparable quelle taille matrice alors blocs discrets moins séparables petites matrices meilleure séparation blocs mixtes constatée matrices moyennes grandes nombre blocs niveau confusion influence niveau confusion expériences retrouve comportement tendu rapport niveau confusion mélanges niveau confusion élevé difficile classer correctement lignes particulier petites matrices partie gauche figures contre niveau confusion élevé qualité partitions lignes améliore taille matrices résumer utilisation conjointe variables continues binaires permet améliorer qualité clusters lignes obtenus clustering clusters étaient identifiables données continues seules binaires seules résultats obtenus clustering données mixtes autant meilleurs niveau bruit faible nombre blocs faible taille matrices grande conclusion discussion article avons présenté extension modèle blocs latents clustering données mixtes expériences montrent amélioration systématique qualité résultats obtenus utilisant données continue binaires conjointement plutôt séparément ayons montré résultats classification lignes clustering mixte améliore également nettement qualité classification colonnes expérimentations avons remarqué données distributions marginales égales notre algorithme méthodes peinent retrouver structure blocs données restant figés optimaux locaux sortie étapes initialisation problème limitant utilisation pratique approche analyse exploratoire données connait vraie distribution jacente travaux futurs viserons améliorer résolution problème algorithmique étendre approche données catégorielles données binaires étudier solutions régularisation retrouver automatiquement nombre clusters ligne colonnes références bhatia iovleff govaert blockcluster package model based clustering working paper preprint inria 01093554 clustering données mixtes modèles mélange simultaneous clustering objects variables diday analyse données informatique inria brault lomet revue méthodes classification jointe lignes colonnes tableau journal société française statistique cheng church biclustering expression proceedings international conference intelligent systems molecular biology volume press dhillon mallela modha information theoretic clustering proceedings ninth international conference knowledge discovery mining press categorization classification mathematics computer science biology medicine majesty stationery office london govaert nadif latent block model contingency table communications statistics theory methods govaert nadif clustering block mixture models pattern recogni govaert nadif clustering contingency table mixture model european journal operational research govaert nadif block clustering bernoulli mixture models comparison different approaches computational statistics analysis govaert nadif clustering wiley hartigan clustering algorithms wiley hintze nelson violin plots density trace synergism american statistician mcparland gormley model based clustering mixed clustmd advances analysis classification springer dolog zhang clustering analysis weblogs using bipartite spectral projection approach springer berlin heidelberg summary clustering mining technique extract underlying block structure between columns matrix approches studied shown their capacity extract structures continuous binary contingency tables however little perform clustering mixed article extend latent models clustering mixed continuous binary variables evaluate effectiveness extention simulated discuss potential limits