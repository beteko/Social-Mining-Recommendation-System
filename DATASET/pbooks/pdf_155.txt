Clustering apprentissage distance guidé préférences attributs Adnan Moussawi Ahmed Cheriat Arnaud Giacometti Nicolas Labroche Arnaud Soulet Université François Rabelais Tours place Jaures 41000 Blois Prénom tours Groupe Kalidea 92774 Boulogne Billancourt Cedex aelmoussawi acheriat kalidea Résumé dernières années nombreuses méthodes supervisées clustering intégré contraintes entre paires objets étiquettes classe partitionnement final accord besoins lisateur Pourtant certains dimensions études clairement définies semble opportun pouvoir directement exprimer contraintes attributs explorer données telle formulation mettrait éviter écueils classiques malédiction dimensionnalité interprétation clusters article propose prendre compte préférences utilisateur attributs guider apprentissage distance pendant clustering précisément montrons comment ramétrer distance euclidienne matrice diagonale coefficients doivent proche poids fixés utilisateur Cette approche ajuster clustering obtenir compromis entre approches données utilisateur observons ajout préfé rences parfois essentiel atteindre clustering meilleure qualité Introduction méthodes segmentation clustering particulièrement adaptées cherche différents profils clients exemple proposer programmes spécifiques fidélisation Néanmoins usages entreprise Kalidea montrent nombreux problèmes restent posés abord croisement diffé rentes mesures chiffre affaire quantité produits vendus invendus dimensions analyse possibles temporelles géographiques gammes produits attributs didats segmenter données extrêmement nombreux Ainsi avère rapidement illusoire effectuer segmentation ensemble attributs analyse possibles malédiction dimensionnalité difficulté interpréter analyser clusters extraits ailleurs constate souvent clusters produits toujours accord besoins intuitions utilisateurs outils segmentation kalidea Clustering intégration préférences attributs résoudre difficultés différents types approches proposées abord malédiction dimensionnalité premier approche consiste sélectionner pondérer attributs analyse Alelyani Kumar manière augmenter discrimination entre données exemple supprimant attributs jugés redondants pertinents Beaucoup travaux proposent méthodes sélection pondération attributs indépendantes méthodes clustering risque suppri attributs intéressants papier proposons approche pondération attributs directement intégrée méthode clustering utilisée domaine subspace clustering Parsons deuxième proche consiste rechercher différents espaces définis ensemble attri analyse clusters pertinents peuvent extraits inconvénients approche nombre espaces découverts clusters pertinents important rendre finalement interprétation résultats difficiles papier proposons approche utilisateur exprimer départ préférences attributs analyse semblent potentiellement intéressants permet orien recherche espace intéressant segmenter données cadre approche présentée papier combine pendant construction clusters apprentissage poids attributs analyse utilisation préfé rences utilisateurs attributs guider apprentissage poids précisément paramétrons distance euclidienne utilisée méthode clustering trice diagonale coefficients doivent proche poids fixés utilisateur Cette approche ajuste clustering obtenir compromis entre approche guidée données approche guidée utilisateur Notons rapport travaux utilisateurs peuvent exprimer contraintes données Davidson cannot notre approche exprime contraintes attributs contraintes souples peuvent remises cause données elles permettent discriminer Après présentation section présentons formulation notre proposition forme fonction objectif minimiser incluant mesure qualité clustering recherché mesure distance entre poids recherchés attributs analyse poids souhaités utilisateur section proposons nouvel algorithme dérivé Means algorithme Means permettant minimiser fonction objectif précédemment définie Enfin présentons expériences montrant comment ajout préférences utilisateurs attributs analyse conduire obtention clusterings meilleur qualité concluons section quelques perspectives particulier soulignant intérêt notre proposition cadre clustering exploratoire Travaux existants Notre contribution carrefour plusieurs domaines recherche clustering allons mettre perspective Comme indiqué Alelyani Kumar nombreux travaux intéressés sélection pondération attri classification clustering augmenter discrimination entre données supprimant attributs redondants pertinents sélection attributs repose Moussawi exploration nombreux espaces évaluation selon critère Parsons travaux rapprochent pondération attributs features weighting particulier méthodes recherche attributs dites embarquées embedded cherchent attributs pertinents pendant clustering plaçons contexte méthodes filtrage décorrelées méthode apprentissage peuvent supprimer attributs jugés comme pertinents Parmi approches dérivées Means Entropy Weighted Means nalise solutions lesquelles attributs poids LFSBSS minimise recouvrement clusters algorithmes reposent recherche locale chaque cluster attributs pertinents faisant rapprochent domaine cherche espaces subspace clustering Kriegel Zimek Parsons clustering multi Bickel Scheffer Sublemontier cadre subspace clustering Kriegel Zimek Parsons méthodes énumèrent espaces clusters pertinents existe grand nombre clustering différents difficulté réside choix bonne partition comme méthodes pondération attributs précédentes chaque cluster défini propre espace problème partitions multiples existe aussi clustering multi Bickel Scheffer Sublemontier lequel plusieurs partitions possibles estimées ensembles disjoints attributs travaux rapprochent également méthodes supervisées utilisent formations fournies expert forme contraintes étiquettes classe encore entre paires données indiquant objets doivent appartenir contraintes cannot Davidson formellement trois approches principales proposées prendre compte contraintes utilisateurs première consiste modifier directement instructions algorithme prenant explicite compte contraintes durant phase initialisation phase convergence comme algorithme Kmeans Wagstaff deuxième approche consiste sanctionner partitions vérifiant ensemble contraintes ajoutant terme pénalité fonction objectif exemple Antoine Labroche pénalisent solutions partition crédale fonction nombre clusters possibles affectation points Bouchachia Pedrycz introduisent terme pénalité algorithme Means floues prend compte férence entre appartenances observées souhaitées points clusters troisième étend approches précédentes modèles apprennent distance tering fonction contraintes exprimées forme distance Mahalanobis forme distance euclidienne paramètrée matrice poids indiquant importance relative attributs Bilenko travaux proposés article visent produire partition permet optimiser qualité intrinsèque clustering apprentissage distance criminante possible données également respecter contraintes expert exprimées attributs dernier point notre connaissance jamais abordé littérature proposons section suivante formulation notre proposition forme termes pénalité ajoutés fonction objectif permettent apprendre distance forme pondération attributs espace euclidien initial fonction Clustering intégration préférences attributs préférences utilisateurs attributs simplicité différence Bilenko notre solution apprend seule distance ensemble clusters travail restant envisageable futurs travaux Formulation problème proposons modéliser préférences utilisateur matrice préfé rences matrice définie positive diagonale porte dimensions combinaisons dernières Chaque coefficient correspond pondération utilisateur assigne attribut pourrions naïvement utiliser cette distance paramétrer distance euclidienne manière suivante Cependant comme cette représentation données révéler insuffisante séparer clusters préférable ajuster cette distance défor espace Cette déformation réduire distances entre objets cluster maximisant distances entre objets différents clusters Suivant approche revient apprendre distance utilisant matrice symétrique définie positive paramètre également distance euclidienne Maximiser fonction vraisemblance cette généralisation modèle Means revient alors minimiser fonction objectif suivante nombre clusters centre cluster nombre total points stade matrice apprise biaisée matrice préférences contraindre matrice suivre recommandations utilisateur nécessaire troduire pénalité dissimilarité proposons utiliser divergence Kullback Leibler mesure dissimilarité entre distributions notre distributions correspondent coefficients diagonaux matrice apprendre matrice référence conséquent matrices normalisées telles nombre attributs modifions équation utilisant cette divergence niveaux Comme suggéré avant contrebalançons apprentissage distance ajoutant terme mesure divergence entre distance apprise préférences utilisateurs cette distance homogénéisons ensuite partie séparation clusters partie satisfaction préférences adaptant fonction objectif Intuitivement terme équation correspond apprentissage distance empêchant écarte Means traditionnel toutes dimensions poids équivalent terme exactement reformulé comme étant divergence entre matrice apprendre matrice uniforme matrice identité dimension suite comme coefficients manipulés diagonale coefficient dénoté Moussawi Ainsi toute matrice diagonale définie positive trace égale nissons fonction objectif suivante ωNDKL introduisons coefficient pondère importance préférences utilisateurs apprentissage distance classique toutes dimensions poids noter ajout nombre points devant divergence nécessaire cette divergence poids autre matrice tenant compte préférences utilisateur distances intra clusters minimisation correspond minimisation équation fonction objectif Means contraintes Bilenko Etant donné ensemble points nombre clusters matrice préférences notre objectif trouver partition données minimisant fonction objectif apprenant matrice Algorithme Means Notre algorithme schéma optimisation introduit Means Bilenko intuitivement consiste alterner trois phases réaffecter points avoir partition minimisant distance paramétrée recalculer centres chaque cluster ajuster matrice sorte minimiser fonction objectif précisément algorithme Means Metric Attribute Preferential Means présenté algorithme prend entrée ensemble points nombre clusters préférences attributs retourne partition données minimisant fonction objectif apprenant matrice Initialisation algorithme Notre initialisation reprend celle Means Arthur Vassilvitskii ligne premier centre choisi hasard parmi données Ensuite chacun autres centres initiaux aléatoirement distribution proportionnelle somme distances centres choisis Ensuite matrice initialisée sorte donner poids chacun attributs ligne Affecter points chaque cluster affectation points cluster effectue manière Means ligne distance paramétrée chaque itération chaque point affecté centre proche ligne Cette affectation nimisant distance intra cluster également minimiser globalement fonction objectif Recalculer centres chaque cluster points affectés cluster centre chaque cluster calculant barycentre points affectés ligne Contrairement certaines approches comme Means calcul centres insensible ordre affectation points étape précédente Clustering intégration préférences attributs Algorithme Means Input ensemble points nombre clusters matrice préférences Output partition Tirer centres selon initialisation Means Initialiser matrice identité dimension repeat mettre partition mettre matrice Normaliser until partition stable return Apprendre distance Cette dernière étape effectue apprentissage distance recalculée sorte minimiser fonction objectif ligne Chaque matrice calculée prenant dérivée égale posons distance intra cluster mension supposée nulle cette manière obtenons apporter constate poids attribut augmente lorsque distance cette dimension faible numérateur retrouve compromis entre matrice équilibrée matrice préférences pondérées lorsque préférences utilisateur prises compte étape similaire celle Means inverse lorsque chaque poids utilisateur considéré pondération distance inter cluster dimension issue équation opérée ligne nombre points devrait apparaître numérateur supprimé cause étape normalisation effet procédons étape normalisation égale ligne Cette étape essentielle respecter propriétés divergence Kullback Leibler Moussawi Expérimentations Protocole expérimental données illustrer fonctionnement Means avons utilisé trois données données attributs clusters Ionosphere issus tests permettent mettre évidence comportement Means rapport données dimensions réduite également données grande dimension Ionosphere Évaluation résultats évaluer résultats notre algorithme avons étudié évolution facteurs rapport variation valeurs premier facteur concerne poids associés dimensions travaux fectués domaine feature selection Brodley mêmes données permettent connaître forme vérité terrain permet qualifier suite dimensions bonnes mauvaises tests second facteur Indice Ajusté Hubert Arabie valeur proche indiquant clustering produit similaire clustering naturel forte similarité entre partitionnement généré notre démarche clustering proposé données sources Démarche suivie expérimentations objectif montrer impact choix utilisateur résultat clustering effet notre démarche mettre évidence meilleure solution toujours celle obtenue apprentissage automatique celle guidée uniquement utilisateur troisième option utilisateur indique préférences algorithme cherche trouver meilleure solution étant proche possible préférences faire chaque données poids initiaux associés dimen sions selon quatre scénarios suivants poids aléatoires différentes dimensions distribution uniforme poids poids élevés associés bonnes mensions poids élevés accordés mauvaises dimensions Ensuite chacun dessus avons décroître valeur commencer clustering lequel choix utilisateur favorisé aller clustering apprentissage automatique Enfin chaque valeur plusieurs initialisations centres clusters réalisées Means section clustering sélectionné celui permet avoir petite valeur fonction objectif Means archive datasets dimensions éliminées analyse première binaire deuxième invariante égale points Clustering intégration préférences attributs Résultats discussions cette partie présentons résultats obtenus suite quatre scénarios initia lisations poids section utilisant données Ensuite limitons scénario donnant résultat remarquable Ionosphere Données première expérience concerne initialisation poids aléatoires férentes dimensions figure indique évolution poids dimensions rapport valeurs observe préférences utilisateurs respectées lorsque Ensuite réduisant valeur poids attributs changent manière associer poids forts bonnes mensions faibles autres augmente également indique clustering obtenu rapproche clustering naturel noter instabilité poids certaines valeurs initialisation centres clusters aléatoire Distribution uniforme poids Poids élevés bonnes dimensions Poids élevés mauvaises dimensions Variation poids dimensions Indice Ajusté rapport valeurs quatre scénarios différents préférences utilisateur Moussawi Variation poids dimensions gauche Indice Ajusté droite rapport données préférences utilisateurs représentent distribution uniforme poids alors poids chaque dimension stable selon équation section figure montre cette stabilité permis avoir solution unique clustering poids forts cette solution toujours attribués bonnes dimensions Lorsque préférences utilisateurs donnent poids élevés bonnes dimensions exemple poids initiaux dimensions dimen sions figure montre cohérence entre préférences utilisateur poids résultats obtenus Means valeurs élevées associées dimen sions privilégiées départ quasiment stable élevé indique résultat obtenu proche clustering attendu Enfin considère utilisateur donne poids élevés mauvaises dimen sions simuler scénario utilisateur choisit dimensions pertinentes clustering poids initiaux dimensions dimensions Comme montre figure faible départ entre cohérent poids élevés mauvaises dimensions revanche augmente sensiblement quand dimensions nouveaux favorisées apprentissage distance Means quatre résultats décrits dessus illustrent efficacité notre approche données dimensions réduites préférences utilisateur définies toujours respectées proche apprentissage automatique distance tendance associer poids élevés bonnes dimensions stabilité Indice Ajusté certaines valeur montre existe nombre limité clustering alternatifs pouvant proposés utilisateur Données poids initiaux premiers dimensions toutes autres figure poids dimensions tendance décroître lentement allant correspondant constant divergence Clustering intégration préférences attributs Variation poids dimensions gauche Indice Ajusté droite rapport données Ionosphere entre devenue faible divergence distribution uniforme poids force Means équilibrer poids explique grande chute maximisé proche poids dimension qualifié parmi bonnes dimensions devenu élevé Données Ionosphere cette expérimentation poids élevés affectés initialement bonnes dimensions poids faibles autres figure montre valeurs diminuent allant jusqu atteindre équilibre entre différents poids partir valeur correspond quasiment stable décroît rapprochant Cette décroissance apprentissage grand nombre dimensions pouvant contenir bruit initialement résultat généré partir données nombre réduit bonnes dimensions résultats obtenus Ionosphere montrent meilleur clustering obtenu compromis entre apprentissage automatique distance préférences utilisateur résultats montrent intervention utilisateur importante arriver bonne solution nombre important attributs Conclusion avons montré comment prendre compte préférences utilisateurs attri analyse processus clustering faire avons introduit fonction objectif minimiser inclut terme classique visant rechercher clustering meilleure qualité terme supplémentaire mesurant distance entre poids appris avons limité affichage poids premières dimensions raison lisibilité graphe Sachant poids obtenus autres dimensions compris entre toutes valeurs Moussawi attributs poids préférés utilisateur Cette approche permet obtenir clustering compromis entre approche guidée données approche guidée utilisateur partie expérimentale avons montré clustering ainsi peuvent meilleure qualité clustering obtenu préférences utilisateurs particulier lorsque nombre attributs analyse possibles élevé approche suivie ajout terme fonction objectif classique notre approche aisément généraliser matrice poids apprise chacune classes construites Cette généralisation particulièrement importante données grande dimension attributs analyse pertinents nécessairement mêmes clusters ailleurs intégrant notre fonction objectif termes pénalité proposés Bilenko noter notre approche pourrait prendre simultanément compte contraintes objets cannot préférences attributs collaboration entreprise Kalidea étudions actuellement comment intégrer notre approche système clustering exploratoire utilisant opérateurs siques exemple après avoir calculé clustering optimal selon préférences lisateur calculé recherchant paramètre permettant obtenir clustering alternatif significativement différent clustering optimal selon préférences utilisateurs possible recommander utilisateur nouveaux attributs analyse savoir attributs analyse lesquels poids appris significativement différent poids désiré utilisateur Remerciement tenons remercier groupe Kalidea soutien termes données cases ainsi disponibilité équipes remercions également soutien financier cadre thèse CIFRE Références Alelyani Feature selection clustering review Clustering Algorithms Applications Chapman Antoine Labroche Classification évidentielle contraintes étiquettes Conference Arthur Vassilvitskii means advantages careful seeding Proceedings eighteenth annual symposium Discrete algorithms Society Industrial Applied Mathematics Banerjee Mooney supervised clustering seeding Proceeding International Conference Machine Learning Bickel Scheffer Multi clustering Proceedings Inter national Conference Mining Bilenko Mooney Integrating constraints metric learning supervised clustering Proceedings twenty first international conference Machine learning Bouchachia Pedrycz supervised clustering algorithm explo ration Internat Fuzzy Systems Association World Congress Clustering intégration préférences attributs Scheuermann Feature selection clustering filter solution International Conference Mining Davidson survey clustering instance level constraints Transactions Knowledge Discovery Brodley Feature selection unsupervised learning Learn Hubert Arabie Comparing partitions Journal Classification Huang entropy weighting means algorithm subspace clustering dimensional sparse Trans Knowl Kriegel Zimek Subspace clustering ensemble clustering alternative tering multiview clustering learn other Proceedings MultiClust Workshop Discovering Summarizing Using Multiple Clusterings Kumar Feature selection literature review Smart Localized feature selection clustering Pattern Recogni Letters Parsons Haque Subspace clustering dimensional review SIGKDD Explor Newsl Sublemontier Cleuziou Exbrayat Martin Clustering multi approche centralisée Revue Nouvelles Technologies Information numéro spécial Fouille Données Complexes données multiples Wagstaff Cardie Rogers Schroedl Constrained means clustering background knowledge Proceedings International Conference Machine Learning Jordan Russell Distance metric learning plication clustering information Advances neural information processing systems Summary recent years supervised clustering methods integrated constraints tween pairs objects class labels final partition consistent needs However cases where dimensions studies clearly defined seems appropriate directly express constraints attributes explore Further formulation would avoid classic problems curse dimensionality interpretation clusters article proposes account preferences attributes guide learning distance clustering Specifically parameterize Euclidean distance diagonal matrix whose coefficients closest weight approach builds compromise clustering between driven driven solution observe experimentally addition preferences essential achieve better clustering