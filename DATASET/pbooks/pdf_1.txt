level clustering algorithm large Bartcus Marius Boullé Clérot Fabrice Orange prenom orange Abstract clustering mining technique identifying underlying structure between columns matrix homogeneous blocks world applications however current clustering algorithms suited large successfully approach cluster large clustering method optimizes criterion based regularized likelihood However difficulties encountered paper present level clustering algorithm given criterion lowing efficiently large memory experiments simulated world proposed approach dramatically reduces computation without significantly creasing quality clustering solution Introduction clustering Hartigan named block clustering Govaert Nadif clustering Mechelen mining technique identify underlying structure between columns matrix homogeneous blocks Whereas principle standard clustering group similar individuals observations respect features clustering simul taneously group similar individuals respect variables similar variables respect observations extracting correspondence structure between objects features Another advantage clustering standard clustering techniques matrix reduction capacity where large table reduced significantly smaller having structure original matrix Indeed technique finds applications telecommunications Guigourès mining Dhillon graph mining Guigourès Several clustering approaches proposed literature Dhillon Govaert Nadif These methods differ mainly according analyzed categorical numerical underlying hypothesis extraction method expected results Several families approaches proposed perform clustering Govaert Nadif investigated probabilistic models latent variables mixture models Difficulties arise initialization large number parame estimate computational efficiency therefore large manage Indeed level clustering algorithm large methods cluster large proposed literature instance padimitriou developed named DisCo implementing distributed processing clustering using Hadoop reduce implementation DisCo scale efficiently analyze extremely large however needs large tributed infrastructure Another clustering method exploits probabilistic models variables numerical categorical based approach Boullé advantages clustering parameter benefits algorithms quadratic complexity number instances allowing large According advantages previously tioned focus clustering approach Indeed clustering large reaching millions instances thousands values variable quadratic complex However hardly large billions instances variables having millions values example limit reached Delay Record country scale studied granularity antenna level application network dimensioning individual customers marketing application identification grained communities customer experience personalization paper focus extending clustering optimization algorithms these large given clustering criterion Despite numerous numerical categorical mixed variables paper investigate categorical variables paper organized follows First containment reasons Section recalls principles clustering method using criterion Boullé mates joint distribution between categorical variables Section introduces proposed level algorithm large clustering Section gives experimental results evaluates proposed approach simulated Finally Section dedicated discussions concluding remarks clustering categorical variables categorical variables values withN instances example representation given where representation example Bartcus represented contingency table Figure summarized using partition values variable clusters groups cross product partitions forms clustering value parts method differs traditional clustering Govaert Nadif which considers partition observations variables order choose clustering model given model space Bayesian Maximum Posteriori approach explore model space while minimizing Bayesian criterion called criterion implements trade between under fitting fitting defined follows where prior likelihood given clustering model details about criterion optimization algorithm called available Boullé features parameter there setting number clusters groups dimension provides effective locally optimal solution clustering model construction quadratic complexity precisely where number actual value pairs encountered least potentially billions instances variables having millions values These cannot analyzed using unless using machines equipped hundreds still waiting computation Scaled clustering objective extend clustering optimization algorithms large scale given clustering criterion while taking consideration following memory constraints Scaled clustering algorithm store values memory actual pairs values cannot stored memory stored clustering algorithm matrices I2max Finally optimized clustering model memory following memory complexity I2max Suppose observed hardly clustered because follow reasons First number instances large second number values dimension large handled current clustering algorithms handle consider memory constraints propose level clustering algorithm allows produce clustering models faster smallest possible decrease their quality algorithm organized phases first phase consists Split phase given following steps Partitioning obtaining whole future clustering memory constraints clustering builds clustering using level clustering algorithm large second phase consists Aggregation phase following steps Amalgamate consists building global clustering initial large merging clusterings obtained optimization improves model following tracks First merge second values between clusters consequence proposed level algorithm steps process further described precisely Split Phase method adopt divide conquer approach starts split phase Partitioning Example coarse clustering coarse clusters large least violated memory constraint leads first proposed algorithm Partitioning Within clustering terminology Coarse clustering first consists coarse clustering order obtain coarse clusters based coarse clusters based obtain coarse clusters values respectively coarse clusters propose random partitioning algorithm which works follows First shuffle values variablesX Second partition shuffled variable values respectively parts equal Since variable values shuffled initial solution likely blind information patterns using solution continue steps produce informative clustering result order bypass issue optimization similar Boullé optimization consists improving moving values between clusters improving initial clustering solution moving boundaries coarse clusters actually related further easier analyzed accordingly smaller Bartcus where these adapted memory constraints shows example coarse clustering complexity computation grows linearly clustering Example clustering clusters consists running earlier obtained clustering basis clustering obtain number clusters based number clusters based summarize haveG clusters shows example clustering whole produces different sized clusterings different cluster combine obtained clustering results whole complexity Observe contrar partitioning number partitions decreases computation clustering therefore Section dedicated choose optimal number parts Aggregation phase phase aggregate results split phase Amalgamate amalgamate starts aggregation phase level algorithm consists computing clusters entire large combining obtained clusters refer obtained clusters micro clusters level clustering algorithm large Example amalgamate whole micro clusters Succeeding amalgamate obtain micro clusters based micro clusters based illustrates example amalgamate entire optimization Example optimization whole amalgamate recall memory constraint clustering algorithm matrices I2max However amalgamate eventually produce large number micro clusters Before proceeding optimization amalgamate results could necessary reduce propose sampling approach consists randomly grouping micro clusters maximum number possible clusters works follows dimension First shuffle micro clusters group equal clusters Second improve model micro clusters between groups results randomized clustering model further improved optimization Bartcus Boullé proposed optimization types exhaustive merge greedy optimization similar optimization approaches merge clusters values between clusters Merge clusters consists merging clusters until model observed clustering model retained Value moves values between clusters alternatively variable illustrates example optimization entire Choosing optimal number parts encountered problem proposed level clustering algorithm choose optimal partitions partitioning highlight small efficient level clustering algorithm because behavior processes algorithm greater behavior process directly whole According assume least values variable instances global execution level clustering approach where computation Split phase computation Aggregation phase experiments impacted number partitions therefore focus minimize Recall composed computational partitioning which increases partition computational clustering which decreases partition Therefore deduce theoretical proposal choosing number partitions heuristic approach equalizes complexity partitioning complexity clustering stepO assumes partitions proportional their respective number modalities obtain where constant factor adjusted experiments Experiments perform experiments simulated order evaluate posed level clustering algorithm experiments clustering approach generated world compare level clustering algorithm given Indeed clustering anytime fashion until significant changes observed outputs intermediate solutions There results solutions clustering these experiments simple summary level clustering algorithm large evaluate quality clustering model using normalized computed where estimated model model normalized interpreted compression order efficiency proposed algorithm provide computation approach Experiments simulated experiment first generate categorical variables generate following probability distribution where possible categorical values variable respectively number values variable simplicity considered equal parameter controls concentration simulated diagonal matrix mixtures sparsity generating three families These skewed sparse families shows example these three types families First generate uniform respectively skewed families values given Example plots skewed uniform center sparse right values given where random variables drawn independently power shape parameter trolling balance uniform family generated while higher skewed family generated experiments generate skewed better comprehension difference between uniform skewed skewed generates first generated value pairs setting small concentrates diagonal obtaining sparse family effectiveness level clustering algorithm generate types varying number instances values variable Table summarizes generated Dataset 20000 20000 Generated Bartcus First experiments uniform Table shows obtained normalized computation level clustering approach Recall fashion providing intermediate solutions results present first retrieved solutions noted respectively level clustering approach given Observe number values small approach obtains better solution given optimization final solution improved while approach times faster numbers values variable 20000 approach obtains better solution approximately approximately times faster while obtaining models comparable quality Normalized 005354 005311 005381 003270 003127 003282 20324 18113 18113 005534 005525 005537 003792 003718 003793 32015 204137 002533 002447 002538 196974 602534 obtained clustering results uniform Second experiments sparse Table shows obtained results sparse approach obtains clustering quality while computation times these approaches rather small However values variable observe level clustering approach times faster faster while having normalized worse Finally 20000 clustering approach gives slightly better solution being times faster times faster Normalized 08474 08474 08474 01535 01484 01587 34326 21586 21586 08951 08951 08951 01750 01749 01750 29449 142887 01076 01045 01046 193273 405939 obtained clustering results sparse Because space skewed results shown paper However obtained results similar those uniform level clustering algorithm large conclude proposed level clustering approach outperforms compu tation without considerably degrading quality clustering solutions Experiments world perform experiments which enables evaluate level clustering approach complex distribution compared generated conduct experiments several Newsgroups Mitchell Castillo Netflix Bennett Lanning domly chosen users whose characteristics summarized Table clustering variables Newsgroups words WebSpam source target Netflix users films Netflix users films world Newsgroups become popular experiments applications machine learning techniques classification clustering consists collection approximately newsgroup documents comprises observations texts words WebSpam comes detection challenge website consists extract graph links source sites target sites Netflix consists millions observations corresponding ratings users related films order faster results choose investigate approximately randomly chosen users obtain first contains randomly chosen users consisting observations users films second contains randomly chosen users consists observations users films Results Table shows obtained result where evaluate normalized computation First observe results Newsgroups level algorithm times faster normalized about worse first solution results WebSpam shows level clustering algorithm times faster times faster normalized within Bartcus Normalized Newsgroups 12840 597600 WebSpam 43130 84859 716552 Netflix 78399 Netflix 29523 354888 3548888 obtained clustering results world Finally results Netflix performance posed approach Netflix users computation level clustering algorithm three times faster times faster obtaining worse normalized Netflix users level clustering algorithm about times faster about times faster losing clustering quality conclude experiments level clustering approach duces faster solutions without considerably decreasing quality clustering results especially noticed needs hours computation approach instead computation noteworthy level clustering proach memory example Netflix randomly chosen users requires machine least while proposed level clustering approach machine about Conclusions perspectives paper presented level clustering algorithm using criterion allows processing large memory first level Split phase consists partitioning clustering steps while second level Aggregation phase consists amalgamate optimize steps investigated level clustering algorithm highlight performance level clustering algorithm performed experiments simulated world small favorable however larger proposed approach suitable obtain faster solution without considerably decreasing quality clustering solutions Finally future focus level clustering algorithm ameliora example parallelizing experiments larger investigated References Bennett Lanning netflix prize Proceedings Workshop Simultaneous clustering objects variables Diday Analyse données Informatique INRIA level clustering algorithm large Boullé models preparation modeling supervised learning Guyon Cawley Saffari Hands Pattern Recognition Challenges Machine Learning volume Microtome Publishing Castillo Chellapilla Denoyer challenge ternational Workshop Adversarial Information Retrieval AIRWeb Beijing China Dhillon Mallela Modha Information theoretic clustering Ninth SIGKDD International Conference Knowledge Discovery Mining Govaert Nadif Block clustering Bernoulli mixture models Compari different approaches Computational Statistics Analysis Govaert Nadif Clustering Wiley Press Guigourès Boullé Clérot Rossi Country scale exploratory analysis detail records through models Proceedings Springer International Publishing Hartigan Direct Clustering Matrix Journal American Statistical Association clustering disambiguation based occurrence International Conference Computational Linguistics Volume COLING Stroudsburg Linguistics Mechelen Boeck clustering methods struc tured overview Statistical methods medical research Mitchell Machine Learning McGraw Papadimitriou Disco Distributed clustering reduce study towards petabyte scale mining Computer Society Résumé classification croisée clustering technique permet extraire structure jacente existante entre lignes colonnes table données forme blocs Plusieurs applications utilisent cette technique cependant nombreux algorithmes clustering actuels passent échelle approches utilisées succès méthode optimise critère vraisemblance régularisée Cependent tailles importante cette méthode atteint limite article présentons nouvel algorithme clustering niveaux compte critère permet traiter efficacement données grande taille pouvant tenir mémoire expériences montrent approche proposée gagne temps calcul produisant solutions qualité