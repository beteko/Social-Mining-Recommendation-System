algorithme clusters niveaux grandes quantités données Bartcus Marius Boullé Clérot Fabrice Orange prenom orange Résumé classification technique exploration données identifier structure jacente entre lignes colonnes matrice données forme blocs homogènes nombreuses applications monde nombreux algorithmes regroupement cours adaptés grands ensembles données approches grands ensembles données dispersion utilisé succès méthode classification optimise critère probabilité régularisée Cependant rencontre difficultés énormes ensembles données article présentons nouvel algorithme regroupement niveaux étant donné critère mugissement traiter efficacement grands ensembles données correspondent mémoire expériences données mondiales réelles simulées montrent approche proposée réduit considérablement temps calcul froissement manière significative qualité solution regroupement Introduction regroupement Hartigan regroupement blocs également nommé Govaert Nadif regroupement bimode Mechelen technique exploration données identify structure jacente entre lignes colonnes matrice données forme blocs homogènes Considérant principe regroupement norme regrouper individus similaires observations rapport ensemble fonctionnalités tâche regroupement simultané groupe tanément individus similaires rapport variables variables similaires rapport observations ainsi extraction structure correspondance entre objets caractéristiques autre avantage agrégation rapport techniques classiques clustering capacité réduction matrice grande table données réduite petite manière significative ayant structure matrice origine effet cette technique trouve utilisation nombreuses applications comme télécommunications extraction texte Guigourès Dhillon exploitation minière graphique Guigourès Plusieurs approches regroupement proposées littérature Dhillon Govaert Nadif procédés diffèrent principalement fonction données analysées catégoriques numériques hypothèse jacente procédé extraction résultats escomptés Plusieurs familles approches alors proposées effectuer regroupement Govaert Nadif étudié modèles probabilistes utilisation variables latentes modèles mélange difficultés surgissent initialisation grand nombre mètres estimer efficacité calcul grandes données difficiles gérer effet algorithme clusters niveaux ensembles données volumineux quelques méthodes capables grandes quantités données munitions proposées littérature exemple padimitriou développé outil nommé DisCo mettre œuvre traitement regroupement données distribuées utilisant Hadoop œuvre cartographique réduire DisCo évoluer analyser efficacement ensembles données extrêmement importantes cependant besoin grande infrastructure autre méthode regroupement exploite modèles probabilistes plusieurs variables numérique catégorique basée approche Boullé principaux avantages regroupement facile utiliser paramètre gratuit bénéficie algorithmes complexité temps quadratique nombre instances permet traiter grands ensembles données Selon avantages précédemment tionné concentrons approche regroupement effet clustering traiter grands ensembles données atteignant jusqu millions dizaines milliers valeurs variable plexité temps quadratique Cependant difficilement utilisé données importantes jusqu milliards variables ayant millions valeurs exemple cette limite atteinte Enregistrement appel Retard échelle lorsque granularité étudiée antenne niveau application dimensionnement réseau clients individuels application marketing identification grains communautés personnalisation expérience client article concentrons extension algorithmes optimisation regroupement grandes quantités données compte critère clustering Malgré faire nombreuses variables numériques catégoriques mixtes article examinons variables document organisé comme abord raisons retenue section rappelle principes méthode classification utilisant critère Boullé accouple distribution conjointe entre variables Ensuite section présente algorithme proposé niveaux grandes données regroupement article donne résultats expérimentaux évalue approche proposée données réelles simulées Enfin section consacrée discussions remarques finales regroupement variables Soient variables valeurs ensembles ensemble withn instances données exemple cette représentation données donnée lequel Exemple représentation données Bartcus ensemble données représentée cette table contingence figure résumée utilisant partition valeur chaque variable grappes groupes produit croisé partitions taille forme agrégation cellule paire pièces valeur Notez cette méthode diffère classification traditionnelle Govaert Nadif considère partition observations variables choisir meilleur modèle regroupement compte données espace modèle utilisons approche Maximum Posteriori bayésien explorons espace modèle minimisant critère bayésien appelé critère outils compromis entre montage raccord définie comme avant probabilité données indiquées modèle agrégation détails critère coûts algorithme optimisation appelé disponibles Boullé dispose garder esprit paramètre libre savoir besoin régler nombre groupes groupes dimension fournit solution localement optimale efficace construction modèle agrégation complexité temps quadratique précisément nombre paires valeurs réelles rencontrées moins Comment jamais certains ensembles données potentiellement jusqu milliards variables ayant millions valeurs ensembles données peuvent analysées moins utiliser machines équipées centaines attente encore jours calcul Scaled Clustering Notre objectif étendre algorithmes optimisation regroupement données grande échelle étant donné critère regroupement tenant compte contraintes mémoire suivantes regroupement Scaled algorithme stocker toutes valeurs mémoire paires réelles valeurs peuvent stockées mémoire peuvent stockés disque pouvons exécuter notre algorithme clusters matrices taille I2maxi Enfin optimisation cluster modèle tenir mémoire complexité mémoire I2max Supposons données observées difficilement regroupés raison raisons vants premier nombre instances grand deuxièmement nombre valeurs chaque dimension peuvent grand manipulé algorithmes classification actuelle gérer considérons contraintes mémoire proposer algorithme niveau permet produire modèles regroupement rapide petite diminution possible qualité algorithme organisé phases première phase consiste phase Split donnée étapes suivantes étape Cloisonnement obtenir données ensembles ensemble données telles futurs regroupement chacun entre répondent contraintes mémoire étape regroupement construit classification chaque ensembles données utilisant outil algorithme agrégation niveaux ensembles données volumineux deuxième phase consiste phase agrégation étapes suivantes étape Amalgamate consiste construire classification globale ensemble données initial large fusionnant clusterings obtenues partir ensembles données secondaires étape optimisation améliore modèle pistes suivantes clusters fusion première seconde valeurs déplacement entre clusters conséquence notre algorithme niveaux proposé processus quatre étapes décrites précisément Phase Split notre méthode adoptons approche diviser mieux régner commence phase séparation étape partitionnement Exemple classification grossière données secondaires clusters grand ensemble données moins violation contrainte mémoire conduit première étape notre algorithme proposé étape Cloisonnement terminologie regroupement nommons regroupement grossier Cette première étape consiste classification grossière obtenir grappes agrégats grossiers obtient secondaires clusters ensembles valeurs clusters grossières respectivement proposons algorithme partitionnement aléatoire fonctionne comme abord mélanger valeurs variablesX Deuxièmement partitionner valeurs variables respectivement mélangées parties taille égale Étant donné valeurs variables mélangées cette solution initiale susceptible aveugle modèles information Ainsi utilisant telle solution continuer prochaines étapes peuvent produire résultat regroupement informative contourner problème étape optimisation similaire Boullé utilisé Cette étape optimisation consiste améliorer rapport déplaçant valeurs entre grappes améliorant ainsi solution classification initiale déplaçant frontières grossier grappes liées ensembles données outre facile analyser conséquence petit volume données Bartcus intégralité ensembles données outre Chacun ensembles données adaptée contraintes mémoire montre exemple classification grossière données complexité cette étape ainsi temps calcul croît linéairement étape regroupement figure Exemple amende classification chaque donnée ensemble grappes Cette étape consiste faire passer chacun ensembles données précédemment obtenus nommons étape regroupement amende regroupement toutes données ensembles obtient nombre grappes fines nombre grappes fines résumer HAVEG fines groupes chaque données définies montre exemple belle regroupement ensemble données définies Notez cette étape produit différentes tailles fines clusterings différentes clusterings fines chaque ensemble données devons combiner résultats classification ensemble données obtenues complexité cette étape Observez contrar étape partitionnement grand nombre partitions diminue temps calcul étape regroupement section dédié montrer comment choisissons nombre optimal pièces phase agrégation cette phase agrègent résultats phase séparation étape fusion étape fusionnent commence phase agrégation algorithme niveaux constitué grappes calcul ensemble données entier grand combinant fines obtenues ensembles données secondaires cette étape faisons référence grappes obtenues comme micro clusters algorithme agrégation données niveaux grands ensembles Exemple amalgame ensemble données micro clusters Succédant étape amalgame obtient grappes micro groupements micro illustre exemple étape amalgame ensemble données étape optimisation figure Exemple optimisation ensemble données établies concerne étape amalgamé avons besoin rappeler contrainte mémoire notre algorithme cluster fonctionner matrices taille I2maxi Cependant étape consistant amalgame éventuellement produire grand nombre groupements micro Avant procéder étape optimisation certains résultats fusionner pourrait nécessaire réduire telle sorte proposons méthode échantillonnage consiste regrouper façon aléatoire grappes micro nombre maximum groupes possibles fonctionne comme chaque dimension abord mélanger grappes micro regrouper grappes égales second améliorer modèle déplaçons grappes micro entre groupes résulte modèle regroupement aléatoire encore amélioré étape optimisation Bartcus Boullé proposé types optimisation fusion exhaustive optimisation gourmande utilisons approches optimisation similaire fusion clusters déplacer valeurs entre clusters grappes fusion fusion consiste grappes jusqu modèle observée meilleur modèle regroupement alors retenu mouvement Valeur déplace valeurs entre grappes alternativement chaque variable illustre exemple optimisation ensemble données choix nombre optimal pièces principal problème rencontré notre algorithme proposé regroupement niveaux choisir taille optimale partitions étape partitionnement mettons évidence petites données outil efficace algorithme clusters niveaux effet comportement temps ensemble processus notre algorithme grand comportement temporel processus directement ensemble données après supposons chaque ensemble données devons avoir moins valeurs variable temps exécution globale notre approche regroupement niveaux temps calcul phase Split temps calcul phase agrégation expériences montrent impacté nombre partitions concentrons minimiser Rappelons composé temps calcul étape partitionnement augmente taille partition Compu temps tational étape classification diminue taille partition conséquent déduire proposition théorique choisir nombre partitions utilisons approche heuristique égalise complexité temporelle étape partitionnement complexité temps amende classification stepO suppose taille partitions proportionnelles nombre respectif modalités obtenons facteur constant ajusté expériences expériences effectuons expériences données réelles simulées évaluer notre algorithme regroupement niveaux expériences courons approche regroupement ensembles données mondiales réelles comparer notre algorithme regroupement niveaux donné effet pistes regroupement moment jusqu aucun changement significatif observées sorties solutions intermédiaires There avant montrons résultats poing dernières solutions regroupement expériences obtenir bonne simple résumé données algorithme clusters niveaux ensembles données grand ensemble évaluons qualité modèle regroupement partir normalisé calculé modèle estimé modèle normalisé interprété comme compression outre montrer efficacité algorithme proposé fournissons temps calcul chaque approche expériences données simulées cette expérience avons abord générons ensembles données variables générer données utilise distribution probabilité suivante possible valeurs nominales variables respectivement nombre valeurs variable simplicité considérés comme étant égaux paramètre commande concentration données simulées diagonale matrice données varions mélanges données sparsity générant trois familles données forme uniforme familles biaisées rares montre exemple trois familles types données abord générons familles données uniformes respectivement asymétriques valeurs donnés Exemple parcelles asymétrique gauche uniforme centre ensembles données éparses droite valeurs données variables aléatoires tirés indépendamment puissance paramètre forme pêche traîne équilibre données famille données uniforme générée alors grande famille données biais générés expériences fixons générer données biaisées meilleure compréhension différence entre uniforme ensembles données asymétriques données travers génère moins données première générés paires valeurs outre fixation petit concentrés données diagonale obtenant ainsi famille données éparses montrer efficacité notre algorithme clusters niveaux générons types ensembles données faisant varier nombre instances valeurs variable tableau résume ensembles données générés données 20000 20000 Generated ensembles données Bartcus abord courons expériences ensembles données uniformes tableau montre normalisé obtenu temps calcul notre approche regroupement niveaux Rappelons exécute manière quelconque temps fournissant façon intermédiaire lutions résultats présentons première dernières solutions récupérées notées respectivement Notre approche regroupement niveaux donnée Observer lorsque nombre valeurs faible approche obtient meilleure solution celle donnée temps optimisation solution finale améliorée alors notre approche rapide lorsque nombre valeurs variable pouvons approche obtient meilleure solution celle environ moins temps outre environ rapide obtenant modèles qualité comparable normalisé Temps données 005354 005311 005381 003270 003127 003282 18113 18113 005534 005525 005537 003792 003718 003793 32015 204137 002533 002447 002538 196974 602534 résultats classification obtenue ensembles données uniformes second courons expériences ensembles données rares tableau montre résultats obtenus rares données Notez approche obtient qualité regroupement comme celui tandis temps calcul toutes approches assez petites Cependant quand valeurs variable constate notre approche regroupement niveaux rapide ayant normalisée celle Enfin 20000 notre approche regroupement donne solution légèrement mieux rapide rapide normalisé Temps données 08474 08474 08474 01535 01484 01587 34326 21586 21586 08951 08951 08951 01750 01749 01750 29449 142887 01076 01045 01046 193273 405939 résultats classification obtenue ensembles données éparses raison manque espace ensembles données asymétriques résultats présentés document Cependant résultats obtenus similaires celles données uniformes algorithme regroupement niveau ensembles données volumineux conclure approche proposée regroupement niveaux surclasse temps Compu dégrader considérablement qualité solutions regroupement expériences données réelles effectuons expériences données réelles permet évaluer notre approche clustering coopération niveaux données distribution complexe rapport données générées données menons expériences plusieurs séries données réelles Newsgroups Mitchell Netflix Bennett Lanning Castillo utilisateurs domly choisis caractéristiques résumées tableau données variables définies regroupement NVXVY Newsgroups texte webspam source cible Netflix utilisateurs films Netflix utilisateurs films ensembles données monde ensemble données Newsgroups devenu populaire expériences applications texte apprentissage techniques machine telles classification texte regroupement texte compose collection environ documents newsgroup données comprend observations textes ensemble données Webspam vient détection données compose extrait graphique liens sites sources sites cibles ensemble données Netflix compose millions observations correspondant évaluations utilisateurs films avoir résultats rapides avons choisi enquêter environ utilisateurs choisis hasard obtient ainsi ensembles données première contient utilisateurs choisis hasard comprenant observations utilisateurs films seconde contient utilisateurs choisis hasard compose observations utilisateurs films Résultats tableau montre résultat obtenu ensembles données réelles évaluons normalisé temps calcul abord observer résultats groupes discussion notre algorithme niveaux rapide normalisé environ celle première solution Ensuite résultats ensemble données Webspam montre notre algorithme clusters niveaux rapide rapide normalisé celle Bartcus Normalisée Temps données Newsgroups 12840 597600 Webspam 43130 84859 716552 Netflix 78399 Netflix 29523 354888 3548888 résultats classification obtenus données monde Enfin résultats ensembles données Netflix montrent bonne performance notre approche posée pouvons Netflix utilisateurs calcul algorithme clusters niveaux trois rapide rapide obtenant normalisé outre Netflix utilisateurs voyons niveaux algorithme clusters environ rapide environ rapide perdant seulement qualité regroupement conclure expériences données réelles montrent solutions duces rapide approche regroupement niveaux diminuer considérablement qualité résultats regroupement particulièrement remarqué données besoin heures calcul notre approche jours calcul outre noter niveaux classification utilisations proche beaucoup moins mémoire exemple Netflix utilisateurs choisis hasard nécessite machine moins exécuter alors notre approche proposée regroupement niveaux fonctionner machine environ Conclusions perspectives article avons présenté algorithme clusters niveaux utilisant critère permet traiter grands ensembles données correspondent mémoire premier niveau phase Split consiste séparation étapes consistant classification fines tandis second niveau phase agrégation consiste fusionner étapes postérieures optimiser avons étudié chaque étape notre algorithme clusters niveaux mettre évidence performances notre algorithme clusters niveaux avons effectué expériences données réelles simulées notons petits ensembles données outil favorable données importantes approche proposée approprié obtenir solution rapide diminuer considérablement qualité solutions regroupement Enfin notre travail futur allons concentrer notre classification algorithme ameliora niveaux exemple parallélisation outre expériences ensembles données importantes seront étudiées Références Bennett Lanning netflix Actes atelier Coupe regroupement simultané objets variables Diday Analyse Données Informatique INRIA algorithme clusters niveaux ensembles données grand Boullé modèles grille données préparation modélisation apprentissage supervisé Guyon Cawley Saffari Travaux Pratiques Motif Reconnaissance défis matière apprentissage machine volume Microtome Publishing Castillo Chellapilla Denoyer challenge Atelier interna tional recherche information accusatoire Airweb Beijing Chine Dhillon Mallela Modha regroupement informations théorétique Neuvième SIGKDD Conférence internationale découverte connaissances exploration données Govaert Nadif regroupement modèles mélange Bernoulli différentes approches comparai Informatique Statistiques analyse données Govaert Nadif Clustering Wiley Press Guigourès Boullé Clérot Rossi analyse exploratoire échelle enregistrements détaillés appels travers lentille modèles grille données Actes Springer International Publishing Hartigan Direct Clustering matrice données Journal American Statistical Association cluster homonymie partir données cooccurrences Conférence internationale linguistique informatique Volume COLING Stroudsburg Linguistique Malines Boeck méthodes classification modes aperçu struc Tured Méthodes statistiques recherche médicale Mitchell Machine Learning McGraw Companies Papadimitriou Disco cluster réparti carte reduce étude pétaoctet échelle exploitation minière Computer Society classification Croisée classification technique Përmet structure Extraire Entre existante jacente lignes colonnes Table forme Données blocs applications several technique utilisent cependant Nombreux algorithmes Clustering Actuels Passent échelle approaches utilisées succès méthode optimize critere vraisemblance régularisée Cependent verser tailles importantes reached méthode limite article Présentons nouvel algorithme regroupement levels compte Përmet critère EFFICACEMENT Données Traiter grande taille Pouvant tenir mémoire expériences montrent approach proposed temps gagne calcul solutions produisant qualité