Apprentissage supervisé fonctions ordonnancement Truong Massih Amini Laboratoire Informatique Paris Avenue Président Kennedy 75016 Paris France truong amini poleia connex Résumé présentons article algorithme inductif supervisé tâche ordonnancement bipartite algorithmes supervisés posés jusqu maintenant étudiés cadre strict classification Récemment travaux réalisés cadre transductif étendre modèles existants classification cadre ordonnancement originalité notre approche capable inférer ordre utilisée pendant phase apprentissage générique méthode transductive résultats empiriques conte titres résumés journal Communications Association Computer Machinery montrent données étiquetées bénéfiques apprentissage fonctions ordonnancement Introduction développement bibliothèques électroniques devenu nécessaire conce méthodes automatiques recherche données pertinentes rapport requête donnée telles applications ordonner exemples discriminer communauté apprentissage formulé cette problématique travers nouveau digme apprentissage supervisé fonctions ordonnancement prendre correspondance entre ensemble instances ensemble alternatives pable ordonner alternatives rapport instance donnée exemple recherche documentaire instance représente requête alternatives documents concernés cette requête inférer ordre partiel ensemble alternatives façon documents pertinents rapport requête soient mieux ordonnés documents pertinents papier plaçons cadre ordonnancement bipartite lequel instances positives négatives ordonner instances positives dessus instances négatives cadre restreint englobe nombreuses applications recherche information telle résumé automatique Amini recherche Apprentissage supervisé fonctions ordonnancement passages pertinents systèmes questions réponses Usunier cemment objet plusieurs études aussi pratique théorique Agarwal Rudin Freund principal inconvénient apprentissage supervisé fonctions ordonnancement étiquetage instances nécessite intervention expert examiner manuel lement grande quantité données cadre classification communauté apprentissage intéressée depuis années problème apprentissage supervisé consiste prendre compte données étiquetées étiquetées processus apprentissage originalité notre approche proposons algorithme apprentissage supervisé tâche ordonnancement bipartite plupart algorithmes ordon nancement supervisés techniques transductives graphes permettent étiqueter exemples étiquetés préconisons approche ductive problème apprendre fonction ordonnancement partir bases apprentissage étiquetée étiquetée capable ordonner nouveaux exemples utilisés entraîner modèle Notre algorithme adopte proche itérative initialisant abord fonction ordonnancement partir exemples étiquetés apprentissage apprenant structure données étiquetées apprentissage méthode transductive répète ensuite étapes jusqu critères convergence arrêt soient atteints première étape consiste ordonner ensemble exemples étiquetés sortie fonction ordonnancement ensuite calculer dissimilarité entre ordre celui inféré méthode transductive ensemble deuxième étape algorithme apprend nouvelle fonction donnancement partir ensemble données étiquetées ensemble exemples étiquetés trouvé étape précédente montrons efficacité cette approche tâche reviendrons section tâche ordonnancement bipartite supervisé section présenterons notre algorithme ordonnance supervisé section présenterons résultats obtenus CACM1 constituée titres résumés journal Communications Association Computer Machinery Finalement discuterons résultats obtenus section tâche ordonnancement bipartite cadre supervisé formaliser problème ordonnancement bipartite comme considère apprentissage constituée ensemble exemples positifs négatifs exemples caractérisés espace ir_resources test_collections Truong Amini alors apprendre fonction donne scores élevés exemples positifs exemples négatifs Formellement hypothèse exemples positifs négatifs respectivement échan tillonnés suivant distributions inconnues risque fonction score alors mesuré erreur moyenne ordonnancement suivant fonction indicatrice valant prédicat sinon erreur moyenne ordonnancementRD1 probabilité exemple positif échantillonné aléatoirement suivant score faible exemple négatif échantillonné aléatoi rement suivant Cortes Mohri erreur empirique ordonnancement corres pondante apprentissage Freund ordonnancement fonction convexe optimisation triviale apprendre approche couramment utilisée borner fonction convexe optimiser suivant démontré minimiser revient minimiser Bartlett Clémençon allons présenter section suivante algorithme supervisé LinearRank optimisant critère algorithme appliqué succès tâche résumé automatique textes Amini algorithme supervisé LinearRank cherchons apprendre poids combinaison linéaire entrées prenant comme borne supérieure fonction nentielle apprentissage fonction score revient alors trouver poids optimisent critère avantage utiliser exponentiel fonction score linéaire critère calculer complexité linéaire rapport nombre Apprentissage supervisé fonctions ordonnancement exemples effet critère écrire comme autre intérêt fonction exponentiel algorithmes optimisation permettent effectuer minimisation notre avons utilisé algorithme LinearRank Amini adaptation algorithme iterative scaling loppé classification Lebanon Lafferty tâche ordonnancement supervisée Notation formalisme tâche ordonnancement supervisé supposons ensemble apprentissage constitué instances étiquetées étiquette reflète pertinence ainsi ensemble instances étiquetées ainsi apprendre fonction score partir instances étiquetées instance étiquetées taille instances décrites vecteurs dimension Méthode ordonnancement transductive méthodes supervisées proposées ordonnancement bipartite basées hypothèse variétés 2004b Ghahramani Agarwal variété définie comme espace topologique localement eucli exemple toute ligne espace euclidien variété dimension toute surface constitue variété dimension applications variétés nombreuses mathématiques physiques récemment introduites apprentissage principale tâche discrimination méthodes utilisant notion variété supposent exemples trouvent variété dimension inférieure espace départ scores exemples proches variété assez similaires algorithmes cherchent alors exploiter nature intrinsèque données variété améliorer apprentissage fonction décision exemple méthodes supervisées faisant hypothèse variétés utilisent grande quantité exemples étiquetés pouvoir estimer cette structure faire graphe incorporant informa voisinage local construit méthode telle proches voisins noeuds alors constitués exemples étiquetés étiquetés apprentissage poids reflètent similarité entre exemples voisins définition cette similarité dépend algorithmes proposés Après avoir estimé variété plupart méthodes attachent trouver étiquettes exemples étiquetés exploitant directement graphe propageant exemple étiquettes données étiquetées leurs voisins étiquetés 2004a Truong Amini algorithmes peuvent ainsi étiqueter exemples absents phase apprentissage puisqu parti noeuds graphe méthodes dites transductives opposition méthodes inductives capables ordonner autres exemples utilisés apprendre Récemment méthodes transductives graphes adaptées tâche donnancement bipartite exemple 2004b adapté travaux classifica supervisée 2004a ordonnancement transdutif algorithme construit abord graphe valué orienté connectant petit petit points proches jusqu graphe devienne connexe affecte ensuite score chacune instances instances positives autres scores alors propagés travers graphe jusqu convergence scores obtenus permettent induire ordre ensemble instances étiquetées algorithme proposé papier étant partie cette méthode allons décrire détails métrique fonction score donne chaque instance score alors comme vecteur nombre total instances étiquetées apprentissage définit aussi vecteur instance positive autres algorithme proposé 2004b alors résumé ainsi Algorithme Algorithme ordonnancement transductif Entrée Initialisation Calculer distances entre toutes instances apprentissage Ordonner distances ordre décroissant Connecter points suivant ordre obtenu jusqu graphe devienne connexe Calculer matrice affinité définie entre sinon matrice diagonale telle somme éléments ligne Construire vecteur telle instance positive sinon répéter jusqu Convergence Sortie vecteur Apprentissage supervisé fonctions ordonnancement Comme préconisé avons utilisé méthode proches voisins construction graphe avoir connexions entre instances effet utilisée expériences comporte exemples positifs chaque requête Augmenter nombre connexions permet ainsi augmenter influence exemples étiquetés positifs scores exemples étiquetés modèle supervisé inductif méthode supervisée LinearRank technique inductive critère optimisé capable ordonner instances durant phase prentissage méthode transductive quant exploite structure données donner instances basant similarité rapport exemples positifs papier intéressons combiner méthodes profiter chacun leurs avantages Notre approche consiste trouver compromis entre optimiser exponentiel respecter ordre trouvé partir variété ensemble données étiquetées Notre approche consiste premier temps apprendre fonction score algorithme LinearRank minimisant exponentiel ensemble données étique ordre total données étiquetées méthode transductive décrite section précédente partie itérative notre algorithme répète alors étapes jusqu critère convergence nombre maximum itérations atteint algorithme première étape consiste sélectionner instances étiquetées mieux ordonnées sortie fonction Suite calculons dissimilarité entre ordre trouvé fonction celui trouvé méthode transductive cette étape faisons hypothèse ordre ensemble trouvé méthode transductive pertinente celle trouvée fonction méthode transductive exploite effet structure données définissons dissimilarité entre ordres nombre paires préférence différentes fonction retourne index instance ordonnée méthode transductive deuxième étape cherchons trouver nouvelle fonction score minimise exponentiel régularisé λδexp borne supérieure terme régularisation permet pondérer apport données étiquetées apprentissage toutes itérations notre algorithme essayons ainsi trouver fonction ordonnancement nombre couples préférence ordonnées donne ordre instances étiquetées mieux ordonnées proche celui trouvé méthode transductive Truong Amini algorithme général ainsi résumer Algorithme Algorithme ordonnancement inductif supervisé Entrée Initialisation Apprendre fonction score optimisant Apprendre ordre total exemples étiquetés méthode transductive algorithme répéter Sélectionner exemples étiquetés mieux ordonnés Produire fonction index partir méthode transductive Apprendre nouvelle fonction score optimisant exponentiel régularisé argmin jusqu Convergence λδexp Sortie fonction papier avons utilisé fonction exponentielle ainsi mesure dissimilarité nature autres fonctions dissimilarité néanmoins envisageables utilisant autres fonctions convexes bornent fonction indicatrice logit exemple travail similaire nôtre celui Agarwal récemment proposé étendre travaux Belkin Niyogi cadre ordonnancement partite supervisé étude Agarwal concerne cadre ordonnancement transductif auteur propose utiliser technique développée Sindhwani rendre algorithme inductif Notre algorithme inductif construction itérative présente avantage rapide exécution Expériences utilisée montrer façon empirique instances étiquetées peuvent utiles ordonnancement bipartite avons comparé notre algorithme algorithme supervisé nearRank Amini évaluer méthodes sommes basés critères couramment utilisés communauté recherche information Courbe précision moyenne expériences menées rassemble titres résumés provenant journal Communications Association Computer Machinery Apprentissage supervisé fonctions ordonnancement avons premier temps prétraité données chaque requête avons ainsi retiré chaque requête documents pertinents contenant aucun quête partir ensemble documents obtenu ensemble aléatoire sélectionnant moitié documents avons gardé uniquement requêtes contenaient suffisamment documents pertinents apprentissage requêtes alors retenues chaque requête avons évalué méthodes formant bases appren tissage différentes méthode supervisée appliquée toute apprentissage étiquetée étiquetée fixant données étiquetées permet avoir moins instance positive partie étiquetée connaître apport données étiquetées avons entraîné modèle supervisé uniquement ensemble étiqueté résultats obtenus fixant paramètres reportés tableau valeurs correspondent moyennes résultats cision moyenne bases apprentissage considérées tableau donne résultats moyennés rapport requêtes Résultats discussion résultats obtenus précision moyenne montrent modèle supervisé obtient meilleurs performances celui supervisé résultat expliquer notre algorithme optimise critère Cependant critère nombreuses utilisé Recherche Information effet critère facile optimiser précision moyenne plusieurs études montré critères étaient général tement corrélés Caruana Niculescu Mizil Optimiser critère permet ainsi optimiser précision moyenne notre déséquilibre pourrait expliquer partie résultats obtient effet chaque requête existe nombre limité exemples positifs améliorer critère aurions aussi optimiser directement précision moyenne Metzler critère dérivé Rudin permet algorithme concentrer instances ordonnées liste contre résultats obtenus mesure montrent notre méthode supervisée obtient clairement meilleures performances ensemble requêtes notons néanmoins baisse conséquente requête regardant sultats avons remarqué cette requête contient pourrait ainsi biaiser dissimilarité basée variété Néanmoins obtenons gains importants moitié requêtes moyenne approche supervisée permet ainsi environ résultats empiriques obtenus montrent résultats encourageant effet notre méthode cherche améliorer critère utilisant exemples étiquetés Cependant résultats précision moyenne surprenants baisse formances critère observe moyenne grande partie requêtes tempère résultats obtenus Truong Amini Performance colonne précision moyenne colonne rithme LinearRank ligne supervisé ligne chaque requête identifiée entier écart écart Moyenne performances toutes requêtes colonne précision moyenne colonne algorithme LinearRank ligne supervisé ligne chaque résultat indiquons moyennes colonne écart pourcentage colonne écart algorithmes rapport méthode supervisé nearRank Conclusion principale contribution papier méthode ordonnancement bipartite supervisée inductive Notre approche combinaison méthode supervisée méthode transductive graphe générale autres fonctions exponentiel autres méthodes transductives peuvent utilisées résultats obtenus critère montrent amélioration significative rapport méthode supervisée tendant montrer ainsi apport possible exemples étiquetés Cependant résultats obtenus précision moyenne montrent dégradation performances critères étant généralement corrélés résultat assez surprenant suite travaux allons ainsi tester notre algorithme autres bases confirmer perfor mances obtenues papier étant première étude tâche ordonnancement supervisée avons uniquement fourni partie pratique tâche restreinte Apprentissage supervisé fonctions ordonnancement ordonnancement bipartite néanmoins noter techniques inductives appren tissage supervisé exclusivement développées cadre classification Amini Gallinari résultats obtenus papier présage quant utilisation données étiquetées autres cadres apprentissage super direction intéressante explorer serait apprendre conjointement données étiquetées étiquetées tâche extraction information Amini Remerciement travail partiellement financé programme communauté péenne cadre réseau excellence PASCAL 506778 Cette publication concerne uniquement points auteurs Références Agarwal Ranking graph Proceedings International Confe rence Machine Learning Agarwal Learnability bipartite ranking functions Proceedings Annual Conference Learning Theory Amini Usunier Gallinari Automatic summarization based clusters ranking algorithms European Conference Information Retrieval Santiago Compostella Amini Gallinari supervised learning explicit misclassification modeling Proceedings Eighteenth International Joint Conference Artificial Intelligence IJCAI Amini Zaragoza Gallinari Learning sequence extraction tasks Proceedings Recherche Information Assistée Ordinateur Bartlett Prediction learning uniform convergence scale sensitive dimensions Journal Computer System Sciences special issue Belkin Niyogi supervised learning Riemannian manifolds Machine Learning ir_resources test_collections Caruana Niculescu Mizil mining metric space empirical analysis suppervised learning performance criteria Proceedings Tenth International Conference Knowledge Discovery Mining Ghahramani Extensions gaussian processes ranking supervised active learning Proceedings Workshop Learning Clémençon Lugosi Vayatis Ranking scoring using empirical minimization Truong Amini Cortes Mohri optimization error minimization Freund Schapire Singer efficient boosting algorithm combining preferences Learn Zhang Zhang Manifold ranking based image retrie MULTIMEDIA Proceedings annual international conference Multimedia Press Lebanon Lafferty Boosting maximum likelihood exponential models Technical Report School Computer Science Metzler Direct Maximization based Metrics Technical report Rudin Cortes Mohri Schapire Margin Based Ranking Meets Boosting Middle Sindhwani Niyogi Belkin Beyond point cloud transductive supervised learning Usunier Amini Gallinari Combinaison fonctions préférence boosting recherche passages systèmes question réponse Bousquet Weston Schölkopf 2004a Learning local global consistency Volume Cambridge Press Weston Gretton Bousquet Schölkopf 2004b Ranking manifolds Volume Cambridge Press Summary growing availability resources requires conception generic approaches automatically relevant entities respect demand Recently there increasing interest Machine Learning community supervised learning scoring functions learn mapping instances rankings finite alternatives Labeling large amounts require pensive human resources which unfeasible applications shown classification framework learning labeled unlabeled efficient decision learning labeled examples alone paper propose supervised method bipartite learning which unseen instances experiments dataset gathering titles abstracts journal Communications Association Computer Machinery empirical results shown potential approach context Document Retrieval