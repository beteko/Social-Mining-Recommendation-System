Classifieur Bayes pondéré données Christophe Salperwyck Vincent Lemaire Carine Powerspace Turbigo 75002 Paris Orange avenue Pierre Marzin 22300 Lannion Résumé classifieur Bayes classifieur probabiliste application théorème Bayes hypothèse naïve variables explicatives supposées indépendantes conditionnellement variable cible Malgré cette hypothèse forte classifieur avéré efficace nombreuses applications réelles souvent utilisé données classification supervisée classifieur Bayes cessite simplement entrée estimation probabilités conditionnelles variable probabilités priori utilisation données cette estimation fournie résumé super ligne quantiles montre classifieur Bayes amélioré utilisant méthode sélection pondération variables explicatives plupart méthodes peuvent fonctionner ligne elles nécessitent stocker toutes données mémoire chaque exemple conséquent elles peuvent lisées données article présente nouvelle méthode basée modèle graphique calcule poids variables entrée utilisant estimation stochastique méthode incrémentale produit classi fieur Bayes Pondéré données Cette méthode comparée classique classifieur Bayes données utilisées challenge Large Scale Learning Introduction données ligne méthodes extractions connaissances performantes éprouvées depuis plusieurs années existent Différents types classifieurs proposés proches voisins bayésien arbre décision système règles apparition nouvelles applications comme réseaux sociaux publicité ligne données quantité données leurs disponibilités changé données auparavant facilement disponibles pouvant tenir mémoire données ligne venus massives visibles seule données plupart classifieurs prévus fonctionner ligne peuvent généralement appliquer directement données Depuis années extraction connaissances données devenue sujet recherche entière nombreux travaux traitant cette nouvelle problématique proposés Salperwyck Lemaire Parmi solutions problèmes Classifieur Bayes pondéré données apprentissage ligne données algorithmes apprentissage incrémentaux techniques utilisées algorithmes capables mettre modèle partir nouvel exemple Cependant plupart entre étant incrémentaux capables traiter données complexité linéaire article intéresse particulièrement classifieurs utilisés réaliser classification supervisée ligne classifieur Bayes modifions classifieur manière réaliser apprentissage ligne données classifieur nécessite entrée probabilités conditionnelles représente variable explicative classe problème classification complexité prédiction faible adapté Néanmoins cadre apprentissage ligne prouvé sélectionnant variables Koller Sahami Langley pondérant variables Hoeting obtient résultats sensiblement meilleurs Boullé Boullé 2006b montré entre pondération variables moyennage plusieurs classifieurs Bayes apprentissage processus duisent modèles similaires modèles différencient classifieur Bayes ajout pondération chaque variable poids peuvent optimisés directement comme réalisé Guigourès Boullé manière ligne présent article présente nouvelle méthode estimer incrémentalement poids classifieur Bayes Pondéré cadre données Cette méthode utilise modèle graphique proche réseau neurones artificiels article suivant notre modèle graphique ainsi méthode permettant apprendre poids attribuer variables explicatives présentés cours section section décrit comment estimations probabilités conditionnelles classe utilisées entrée modèle graphique estimées section présente étude expérimentale notre classifieur Bayes Pondéré entraîné incrémentalement bases données ayant servies large scale learning challenge Enfin dernière section conclut article Classifieur Bayésien Pondéré incrémental Introduction classifieur Bayes classifieur Bayes Moyenné classifieur Bayésien méthode apprentissage supervisé repose hypothèse simplificatrice forte variables indépendantes conditionnellement classe prédire Cette hypothèse naïve permet modéliser interactions entre férentes variables Cependant nombreux problèmes réels cette limitation impact Langley départ classifieur vient formule Bayes probabilité conditionnelle jointe étant difficilement estimable utilise version naïve appelée suite classi Salperwyck fieur probabilité classe devient indice classe indice variable explicative classe intérêt classe prédite celle maximise probabilité conditionnelle babilités peuvent estimées intervalle discrétisation variables numériques variables catégorielles cette estimation faire directe variable prend valeurs différentes après groupage contraire dénominateur équation normalise résultat tages classifieur contexte données réside faible complexité déploiement complexité dépend nombre variables utilisées montre toutefois classifieur bayésien amélioré manières sélectionnant variables Koller Sahami Langley pondérant variables Hoeting proche moyennage dèles bayésiens Bayesian Model Averaging Hoeting proces sélection pondération pouvant mélangés manière itérative classifieur bayésien moyenné résultant similaire classifieur bayésien ajoute pondération variable chaque variable explicative pondérée poids intervalle approche revient moyennage modèles possède qualités moyennage modèles combiner prédiction ensemble classifieurs façon améliorer performances prédictives principe appliqué succès bagging Breiman exploite ensemble classifieurs appris partie exemples approches classifieur moyenné résultant procède classifieurs mentaires effectuer prédiction opposé approches bagging chaque classifieur élémentaire attribuer poids moyennage modèles bayésiens Bayesian Model Averaging Hoeting pondère classifieurs selon probabilité posteriori approche proposée Bayésien Pondéré incrémental Lorsque place cadre apprentissage ligne poids classifieur Bayes Moyenné peuvent estimés différentes manières moyennage modèles Hoeting moyennage modèles optimisation poids basée critère Minimum Description Length Boullé 2006b optimi sation directe poids descente gradient Guigourès Boullé Toutefois toutes méthodes fonctionnent chargeant toutes données mémoire nécessitent relire plusieurs approche proposée article optimise directement poids classifieur capable fonctionner données Classifieur Bayes pondéré données Modèle graphique optimisation poids classifieur bayésien moyenné première étape consiste créer modèle graphique Figure Whittaker dédié optimisation poids permet réécrire équation forme modèle graphique classifieur bayésien pondéré reçoit poids variable classe présenté équation poids cherchons optimiser breux modèle graphique effet poids seulement associé variable variable conditionnellement classe poids associé variable classe biais classe biais correspond estimation probabilité varier cours temps première couche modèle graphique couche linéaire réalisant somme pondérée chaque classe wiklog seconde couche modèle graphique Softmax Finalement modèle graphique proposé entrées estimation conditionnelles classes donne sortie valeurs telles variables positionnées entrée modèle issues résumés univariés construits seront présentés section suivante section optimisation poids réalisée descente gradient stochastique fonction donnée exemple donné règle modification poids Salperwyck fonction appliquée exemple dérivée paramètres modèle poids calcul cette dérivée détaillé annexe article aboutit désigne valeurs probabilité désirées target sorties obtenues reste ensuite inclure partie couche linéaire notre modèle graphique avoir dérivées partielles modification poids complexité calculatoire faible méthode descente gradient ligne utilisée article celle utilisée tuellement réaliser rétropropagation principaux paramètres Lecun prendre compte fonction nombre itérations apprentissage cadre classification supervisée meilleur choix fonction sorties modèle graphique apprendre prennent uniquement valeurs vraisemblance Bishop optimise nombre rations notre modèle après chaque exemple seulement Etant donné apprentissage réalisé données choisissons effectuer itération exemple utiliser early stopping Prechelt ensembles validation Amari Finalement paramètre ajuster apprentissage valeur faible aboutit convergence longue atteindre minimum fonction alors grand permet atteindre minimum apprentissage ligne possible régler valeur méthode validation croisée apprentissage passe envisageable expérimentations article choisi Cependant présence dérive concept suspectée intéressant avoir apprentissage adaptatif Kuncheva Plumpton Estimation densités conditionnelles Cette section présente comment estimées probabilités conditionnelles doivent placées entrée modèle graphique présenté cours section précé dente méthodes estimation présentées dessous brièvement étant contri bution majeure article expérimentations trois estimations utilisées Figure calculer chaque variable numérique explicative chaque classe méthode discrétisation niveaux basée statistiques ordre décrite Salperwyck Lemaire méthode discrétisation niveaux version modifiée méthode Pinto approximation gaussienne approche variables catégorielles détaillée Classifieur Bayes pondéré données article mettant premier niveau approche count sketch Cormode Muthukrishnan deuxième niveau méthode groupage lecteur intéressé pourra trouver davantage détails techniques estimation densités conditionnelles chapitre Salperwyck Méthode niveaux estimation notre méthode niveaux basée premier niveau produit quantiles chaque variable explicative données Chaque quantile tuple contient chaque variable explicative valeur observée correspond nombre valeurs entre classes problème classification supervisé nombre éléments appartenant classe second niveau algorithme ligne appliqué quantiles article nombre tuples utilisé correspondant estimation centiles réglage valeur nombre tuples discuté Salperwyck GkClass décrits dessous méthodes permettant obtenir quantiles Pinto proposé méthode discrétisation niveaux variable numérique premier niveau mélange entre méthodes Equal Width Equal Frequency détaillé Pinto premier niveau tualisé manière incrémentale nécessite avoir intervalles second niveau second niveau utilise information contenue premier niveau construire deuxième discrétisation nombreuses méthodes peuvent utilisées second Equal Width Equal Frequency Entropy Kmoyenne avantage avoir premier niveau rapide purement incrémental apportons modifica avoir mémoire constante augmentation consommation mémoire méthode création nouveaux intervalles effet intervalle devient peuplé alors divisé intervalles contenant chacun moitié individus Notre modification consiste suite division intervalle fusionner intervalles Salperwyck consécutifs somme comptes faible Ainsi nombre intervalles stockés reste toujours Aucune comparaison réalisée article entre notre intérêt porte utilisation mémoire constante méthodes GKClass algorithme proposé Greenwald Khanna algorithme destiné calculer quantiles utilisant mémoire nombre éléments observés erreur souhaitée Cette méthode requiert connaitre préalable taille insensible ordre arrivée exemples avantages cette méthode selon besoin définir erreur maximale souhaitée mémoire maximale utiliser première erreur maximale fixée résumé consomme autant mémoire nécessaire erreur maximale dépassée deuxième quantité mémoire maximale utilise mieux minimiser erreur avons adapté algorithme stocke directement comptes classe second niveau utilise méthode discrétisation supervisée Boullé 2006a Approximation Gaussienne Cette méthode suppose distribution données rapproche normale chercher approximer suffit conserver trois valeurs classe moyenne écart variance nombre éléments définissent cette gaussienne maintien trois valeurs faire manière incrémentale cette méthode parfaitement adaptée utilisation ligne comporte niveau utilisée comme référence cadre expérimentations bases données Large Scale Learning générées générateurs gaussiens indicateur évaluation retenu précision accuracy respectivement nombre positifs négatifs positifs négatifs Expérimentations Protocole expérimentations classifieur réalisées bases challenge Large Scale Learning proposé réseau excellence PASCAL Toutes bases contiennent exemples étiquetés considéré comme suffisant évaluer gorithme ligne utilisons bases alpha delta gamma possèdent variables numériques bases epsilon contiennent séparés ensemble apprentissage premiers exemples comme ensemble autres comme ensemble apprentissage largescale berlin about csail papers topic large_scale_learning Classifieur Bayes pondéré données Résultats première partie expérimentations classifieur Bayes dération utilisé utilise estimations densités conditionnelles classes issues méthodes décrites section précédente résultats présentés tableau montrent estimation probabilités conditionnelles précise trois méthodes classifieur Bayes obtient résultats chacune entre elles données challenge aient générées générateur gaussien autres méthodes GKClass obtiennent résultats similaires méthode basée approximation Gaussienne Entre méthodes GKClass résumé GKClass apporte garanties précision mémoire utilisée résultats comparable hypothèse nature distribution données choisi suite expérimentations Alpha Delta examples GKClass Gamma Epsilon examples GKClass Précision classifieur Bayes pondération utilisant GKClass approximation gaussienne calculer probabilités conditionnelles Grâce résultats présentés tableau savons estimation densités conditionnelles précise conséquent pouvons présent évaluer comportement notre classifieur Bayes Pondéré résultats comparent quatre classifieurs classifieur Bayes entraîné ligne utilisant discrétisation Boullé 2006a toutes données chargées mémoire classifieur Bayes Moyenné entraîné ligne utilisant discréti sation Boullé 2006a algorithme décrit Boullé 2006b calculer poids variables explicatives cette méthode meilleures Guyon classifieur Bayes entraîné ligne utilisant méthode discrétisa niveaux utilise GKClass niveau discrétisation niveau notre classifieur Bayes Pondéré entraîné ligne poids estimés notre méthode basée modèle graphique méthode crétisation utilise GKClass comme niveau discrétisation comme niveau Salperwyck table montre résultats obtenus notre ligne encourageants meilleur ligne données Gamma Delta performance proche ligne doute meilleurs classifieurs bayésien obtenu données version ligne nécessite avoir toutes données mémoire plusieurs possible cadre apprentissage données Notre approche construire classifieur utilise mémoire grâce notre résumé niveaux estimer probabilités conditionnelles entièrement incrémentale grâce modèle graphique descente gradient estimer poids premiers résultats encourageants semble indiquer pourrait avoir mêmes résultats version ligne notre classifieur ligne exemples étaient disponibles travaux futurs permettront confirmer infirmer cette hypothèse Alpha Delta examples 40000 100000 380000 40000 100000 380000 40000 100000 380000 ligne ligne ligne ligne Gamma Epsilon examples 40000 100000 380000 40000 100000 380000 40000 100000 380000 ligne ligne ligne ligne Précision différents classifieurs Bayes étudiés Conclusion résultats notre version ligne classifieur Bayes prometteurs performances meilleures celles version ligne pondérées proche version pondérée ligne Cependant résultats pourraient encore améliorés prochains travaux Notre première piste amélioration serait utiliser résumés GKClass comme batch Cotter réaliser plusieurs itérations accélérer descente gradient Notre seconde proposition serait avoir adaptatif cente gradient rapide début apprentissage suite prendre compte erreur comme Kuncheva Plumpton apprentissage pourrait aussi contrôlé méthode détection change concept augmenter détection apprendre rapidement faudrait mettre résumés suite détection avoir estimations correspondant nouveau concept nombreuses méthodes détection existent rester cohérent notre approche grille Salperwyck pourrait utilisée détecter changements distribution Classifieur Bayes pondéré données Références Amari Murata Muller Finke Asymptotic statisti theory overtraining cross validation transactions neural networks publication Neural Networks Council Bishop Neural Networks Pattern Recognition Oxford University Press Boullé 2006a Bayes optimal discretization method continuous attributes Machine Learning Boullé 2006b Regularization Averaging Selective Naive Bayes classifier International Joint Conference Neural Network Proceedings Breiman Bagging predictors Machine learning Cormode Muthukrishnan improved stream summary count sketch applications Journal Algorithms Cotter Shamir Srebro Sridharan Better batch algorithms accelerated gradient methods Knowledge Discovery Streams Chapman Press Pinto Discretization streams applications histograms mining Proceedings symposium Applied computing Greenwald Khanna Space efficient online computation quantile summa SIGMOD Record Guigourès Boullé Optimisation directe poids modèles dicteur Bayésien moyenné Extraction gestion connaissances Guyon Lemaire Boullé Vogel Analysis Scoring Large Orange Customer Database Workshop Conference Proceedings Idiot Bayes Stupid After International Statistical Review Hoeting Madigan Raftery Bayesian model averaging tutorial Statistical science Koller Sahami Toward Optimal Feature Selection International Conference Machine Learning Kuncheva Plumpton Adaptive Learning Online Linear criminant Classifiers Proceedings Joint International Workshop Structural Syntactic Statistical Pattern Recognition Springer Verlag Langley Thompson analysis Bayesian classifiers Procee dings National Conference Artificial Intelligence Number Langley Induction Selective Bayesian Classifiers Poole Proceedings Tenth Conference Uncertainty Artificial Intelligence Salperwyck Morgan Kaufmann Lecun Bottou Müller Efficient BackProp Müller Lecture Notes Computer Science Volume Lecture Notes Computer Science Springer Verlag Prechelt Early Stopping Neural Networks Tricks Trade volume chapter Springer Verlag Salperwyck Apprentissage incrémental ligne données thesis University Lille Salperwyck Boullé Lemaire Grille bivariée détection change étiqueté Salperwyck Lemaire Classification incrémentale supervisée panel intro ductif Revue Nouvelles Technologies Information Numéro spécial apprentis fouille données Salperwyck Lemaire layers incremental discretization based order statistics Statistical Models Analysis Whittaker Graphical Models Applied Multivariate Statistics Wiley Annexe Calcul dérivée fonction Cette annexe explicite calcul dérivée fonction classifieur bayésien moyenné optimisation poids descente gradient modèle graphique permet avoir directement sortie valeur étant maximiser vraisemblance suffit alors minimiser vraisemblance décompose abord partie softmax considérant chaque sortie fonction softmax avant phase normalisation comme étant succession étapes phase activation suivi fonction recevant valeur activation fonction activation comme étant sortie partie softmax notre modèle graphique dérivée fonction activation fonction étant vraisemblance considérer désire apprendre valeur désire apprendre valeur suite désire obtenir valeur remplaçant Classifieur Bayes pondéré données obtient finalement désire obtenir valeur effet erreur uniquement transmis normalisation issue fonction softmax dérivée fonction erreur unité sortie laquelle sortie désirée nulle obtient calculs similaires conclut alors désigne valeurs probabilité désirées target probabilité estimée modèle graphique reste ensuite inclure partie couche linéaire notre modèle graphique avoir dérivées partielles Summary naive Bayes classifier simple probabilistic classifier based applying Bayes naive independence assumption explanatory variables assumed independent target variable Despite strong assumption classifier proved effective applications often stream supervised classification naive Bayes classifier simply relies estimation variate conditional probabilities estimation provided stream using supervised quantiles summary literature shows naive Bayes classifier improved using variable selection method weighting explanatory variables these methods related learning store memory require reading example Therefore cannot stream paper presents method based graphical model which computes weights input variables using stochastic estimation method incremental produces Weighted Naive Bayes classifier stream method compared classical naive Bayes classifier Large Scale Learning challenge datasets