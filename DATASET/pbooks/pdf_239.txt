Classification évidentielle contraintes étiquettes Violaine Antoine Nicolas Labroche Université Blaise Pascal LIMOS 10125 63000 Clermont Ferrand violaine antoine bpclermont Université François Rabelais Tours Campus Blois 41000 Blois nicolas labroche tours Résumé papier propose version améliorée algorithme classi fication automatique évidentielle supervisée Celui bénéficie introduction données étiquetées améliorer pertinence résul utilise théorie fonctions croyance produire parti crédale généralise notamment concepts partitions dures floues pendant expressivité complexité exponentielle nombre classes impose retour utilisation schémas ficaces optimiser fonction objectif proposons article heuristique relâche contrainte classique positivité masses croyances méthodes évidentielles montrons ensemble données notre méthode optimisation permet accélérer sensi blement algorithme schéma optimisation classique améliorant également qualité fonction objectif Introduction papier propose nouveau mécanisme optimisation algorithme classifica automatique évidentielle supervisée Antoine premier reposer contraintes exprimées forme données étiquetées algorithmes classification évidentielle Masson Denœux reposent cadre théorique fonctions croyance permettent représenter types affectations partielles grâce concept partition crédale étend notion partition stricte floue possibi liste méthodes évidentielles étendues cadre supervisé Antoine pouvoir tirer partie contraintes Cannot spécifient données doivent appartenir classe transfor mation informations disponibles priori contraintes néanmoins induire perte connaissance algorithme proposé récemment tirer partie données partiellement étiquetées Antoine Cependant algorithme repose optimisation stricte respecte ensemble contraintes notamment positivité masses croyances associées affectation point classe Cette contrainte théorique entraîne formation problème complexe proposons modifier mécanisme optimisation relâchant contrainte positivité instar Classification évidentielle contraintes Bouchachia Pedrycz assurant posteriori optimisation masses croyances positives résultats expérimentaux montrent notre heuristique dégrade performances algorithme permet gagner manière significative complexité tests papier organisé comme section présente concepts fondamentaux théorie fonctions croyance principales méthodes classification automatique contraintes algorithme supervisé Antoine ensuite décrit section nouveau schéma optimisation proposé Enfin résultats algorithme présentés section papier conclut intérêt nouvelle méthode optimisation Travaux existants fonctions croyance intérêt principal algorithme classification évidentielle pouvoir représenter doute concernant affectation point cluster faire méthodes reposent théorie évidence Dempster Shafer également appelée théorie fonctions croyance Shafer Smets Kennes variable prenant valeurs ensemble appelé cadre discernement connaissance partielle concernant valeur représentée fonction masses plication ensemble parties intervalle vérifiant ensembles appelés éléments focaux quantité interprète comme quantité croyance allouée faute infor mation complémentaire allouée aucun autre ensemble ignorance totale correspond alors certitude totale rapporte allocation complète masse croyance unique singleton ensembles focaux singletons alors fonction masses croyances équivalente distribution probabilités quantité interprétée comme croyance valeur réelle appartient Quand fonction croyance normalisée connaissance exprimée fonction croyance aussi représentée fonction plausibilité définie comme quantité interprétée comme degré maximal croyance potentiel lement affecté hypothèse selon laquelle vraie valeur appartient Quand décision prise concernant valeur intéressant transformer fonction masses probabilité pignistique Smets Kennes Antoine Labroche dénote cardinalité Quand existe étape normalisa précéder transformation pignistique normalisation Dempster consiste diviser toutes masses méthode classique normalisation Algorithme moyennes évidentielles version évidentielle moyennes algorithme classification automa tique construit partition crédale partir données formalisme connais sance partielle concernant appartenance objet représentée fonction croyance ensemble classes possibles Ainsi degré croyance fecté singletons comme approches floues possibilistes également ensembles ensemble individus classer ensemble classes chaque objet fonction croyance calculée plaçant grande petite quantité croyance ensemble proche éloigné terme distance distance métrique définie entre objet représentation ensemble Similairement gorithme moyennes floues chaque classe représentée prototype chaque ensemble centre calculé comme barycentre centres associés classes composant skjvk sinon distance définie comme distance euclidienne Masson Denœux récemment variante proposée prendre compte distance Mahalanobis Antoine Similairement travaux Gustafson Kessel cette distance permet détecter clusters ayant différentes formes géométriques grâce matrice covariance floue associée chaque cluster optimisée Ensuite similairement prototypes chaque ensemble singleton matrice calculée moyennant matrices incluses distance entre objet centre alors algorithme minimise fonction objectif suivante fonction matrices précédentes αmβijd2ij δ2mβi Comme correspond croyance point aberrant traité séparément reste autres ensembles paramètre Classification évidentielle contraintes indique distance ensemble objets ensemble intéressant remar niveau pénalité ensembles grande cardinalité introduite pondération exposant permet contrôler degré cette pénalisation comme moyennes floues partition construite selon processus itératif optimise alternativement matrices complexité algorithme évidentiel linéaire nombre données exponentiel nombre classes conséquence crucial méthodes minimiser calculs réalisés phases optimisation comme proposé article Algorithmes supervisés plupart méthodes classification automatique améliorées prendre compte connaissance experte forme contraintes entre paires données Cannot indiquent points doivent appartenir cluster forme données étiquetées Wagstaff Citons exemple algorithmes moyennes Wagstaff hiérarchiques Davidson basés densité Lelis Sander méthodes spectrales Davidson ainsi algorithmes dédiés données autres travaux intéressés intégration contraintes algorithme moyennes floues Grira Pedrycz Bensaid Pedrycz Waletzky 1997a palier limitations algorithmes flous présence bruit points aberrants méthodes possibilistes Krishnapuram Keller récemment évidentielles proposées Masson Denœux dernières également étendues supervisé bénéficier avantages modèles basés fonctions croyance prise compte connaissance experte travaux proposés reposent contraintes Antoine récemment données partiellement étiquetées algorithme Antoine point formel approches proposées littérature prendre compte contraintes étiquettes pendant processus classification automatique premier possible modifier processus algorithmes classification durant phase initialisation pendant phase convergence dernier imposer respect strict contraintes comme Kmeans Wagstaff modifier fonction objectif pénaliser solutions respectent complètement contraintes Pedrycz Waletzky 1997b exemple Bouchachia Pedrycz auteurs décrivent amélioré fonction objectif introduit terme pénalité considère appartenance actuelle points classes notée également appartenance telle devrait partir contraintes expert notée comme montre équation uβikd βd2ik Antoine Labroche Exemple partition crédale Plausibilités calculées partir partition crédale dénotent respectivement matrice appartenance coordonnées centres clusters paramètre régulation permet équilibrer importance respect contraintes fonction objectif second autres méthodes proposent adapter métrique fonction traintes étiquettes fournies expert comme algorithme means Bilenko exemple Bouchachia Pedrycz auteurs proposent thode adapter distance Gustafson Kessel Gustafson Kessel posons papier considérer modèle contraintes flexibles modification fonction objectif pénalise solutions respectant données étiquetées Algorithme Formalisation problème principale algorithme proposé Antoine ajouter terme pénalité fonction objectif prendre compte ensemble objets étiquetés démarche suivie Bouchachia Pedrycz portée algorithmes évidentiels expression objet étiqueté traduire forme fonction quantifiant croyance appartenance objet classe Consi dérons premier temps partition crédale connue définie tableau représente connaissance partielle appartenance quatre objets classes alors possible calculer plausibilité chaque objet chaque classe comme illustré tableau remarque alors plausibilité nulle permet déduire certitude élément appartient classe Ainsi observation déduire objet atypique objet affecté certitude classe partie classe revanche objets plausibilité classe élevée chances appartenir cette classe Ainsi apparaît objet appartient classe certitude objet appartient Classification évidentielle contraintes Supposons maintenant dispose partition crédale existe contraintes formes étiquettes exemple objet inclus classe alors possible imposer contrainte effet exiger croyance élevée fonctions masse ayant ensemble comprenant toutes fonctions masses degré croyance moins appartienne valeurs faibles toutes fonctions masses ayant ensemble cluent contrainte entre objet classe respectée nombreuses solutions allant certitude totale appartienne jusqu incertitude complète affectation entre plusieurs autres classes contrainte flexible permet nécessaire garder doute quant affectation objet classe conséquent limite influence négative contrainte bruitée Lorsqu expert contraintes étiquettes avoir doute entre plusieurs classes unique objet exemple objet appartient classes ensemble Cette information modélise alors forme contrainte plausibilité revient favoriser fonctions masses ayant moins classe Cette contrainte généralise précédente permet établir terme pénalité ajouter fonction objectif sinon nouvelle fonction objectif alors suivante JSECM contraintes paramètre correspond nombre contraintes existantes coefficients ajoutés normaliser chaque terme Ainsi paramètre utilisé contrôler importance donnée contraintes rapport modèle géométrique allocation croyance ensembles fortes cardinalités pénalisée uniquement terme biais coefficient système permet adapter aisément contraintes structure inhérente données Optimisation optimisation nouveau critère consiste manière algorithme minimiser alternativement matrices terme pénalité pendant similaire formule présen Masson Denœux partition crédale contraire présente fixant alors minimisation fonction objectif rapport devient problème quadratique contraintes linéaires problème résolu méthode classique optimisation néanmoins nombreux auteurs trouvant Antoine Labroche contexte similaire proposent optimiser directement fonction objectif prendre compte contraintes positivité partition réduire temps convergence algorithme résoudre problème minimisation contraint multiplicateurs Lagrange introduits Lagrangien défini αm2ijd2ij δ2m2i simplifier écriture équations dérivées partielles Lagrangien αmijd2ij Annuler dérivées partielles permet obtenir équations suivantes αd2ij αd2ij utilisant possible écrire αd2ij αd2ij Cette équation finalement utilisée obtenir fonctions masse αd2ij αd2ij αd2ij αd2ij Classification évidentielle contraintes objets attributs classes Métrique Mahalanobis Euclidienne LettersIJL Mahalanobis Ionosphere Mahalanobis Description données Comme numérateur première partie équation permet obtention valeurs négatives fonction masse négative valeurs négatives accentuées importance donnée contraintes utilisateur reste normal utilisation contraintes partie valeurs seront proches fonction réajustement envisageable optimisation dégradée sinon nouvelle fonction masse objet alors comprise entre respecte contrainte Expérimentations expériences menées plusieurs données consistent comparer résultats obtenus lorsque fonctions masse utilise optimisation classic optimisation proposée Données méthode évaluation données Plusieurs données issue Machine Learning Repository employés tableau indique leurs caractéristiques ainsi métrique utilisée expériences noter LettersIJL correspond données Letters modifié comme Bilenko Méthode évaluation partition réelle données utilisés initialement connue comparée partition calculée utilisant probabilité pignistique évaluer qualité indice Ajusté Antoine Labroche classic LettersIJL Ionosphere Temps moyens secondes trouvés classic contraintes temps moyen algorithme également renseigné représente indice normalisé rapport distribution hypergéo métrique utilisé Indice Indice espéréindice maximum indice espéré autre manière intéressante thétiser information contenue partition crédale affecter chaque objet ensemble forte masse Ainsi possible obtenir partition nommée partition crédale groupes Paramétrages chaque expérience faiblement pénaliser ensemble fortes cardinalités manière éviter allouer croyance ensemble effet données utilisés contiennent objets atypiques coefficient permettant ajuster importance attribuée contraintes rapport structure globale classes testé trois valeurs Enfin contraintes choisies aléatoirement varient entre taille données Protocole expérimental expérience consiste certain pourcentage contraintes exécuter algorithme contraintes différents éviter optima locaux chaque exécution teste initialisations aléatoires centres gravité récupère résultats obtenus initialisation ayant fonction objectif minimale Résultats données réelles figure montre évolution indice moyen obtenu classic rapport pourcentage contraintes résultats similaires trouvés Ionosphere LettersIJL coefficient ainsi possible remarquer ajout progressif contraintes améliore indice algorithme présente meilleurs résultats classic expériences avons également constaté valeurs fonctions objectif petites celles classic résultats similaires trouvés nouvelle optimisation permet obtenir meilleur minimum grâce relaxation contraintes implique meilleurs résultats classification également noter résultats obtenus prouvent fonction réajustement dégrade solutions mêmes expériences temps observé comparer vitesse cution algorithmes tableau présente résultats obtenus ainsi algorithme rapide algorithme classic Classification évidentielle contraintes Pourcentage contraintes classic Pourcentage contraintes classic moyens intervalle confiance image originale partition crédale obtenue contraintes partition crédale obtenue zones blanches noires partitions représentent zones grises Application segmentation image intérêt maintenant illustré exemple segmentation image avion Figure isoler avion reste image première expérience avons utilisé distance Mahalanobis avons considéré existe objet atypique avons forte valeur avons choisi réduire incertitude trouvée partition finale fixant valeur élevée Ainsi trouve partition crédale représentée figure pouvons remarquer permet isoler correctement avion seconde expérience introduisons contraintes partition comme illustré Figure Chaque pixel première respec tivement seconde affectée respectivement algorithme alors exécuté mêmes paramètres partition crédale résultante présentée pouvons constater contraintes permis lever indétermination plupart pixels alloués Conclusion avons présenté article nouvelle méthode optimisation rithme classification automatique intitulé dernier variante algorithme évidentiel prenant compte contraintes étiquettes repose minimisation Antoine Labroche fonction objectif contraintes linéaires linéaires impose utilisa méthodes optimisation avancées utilisation fonctions masses liées méthodes évidentielles complexité algorithme linéaire rapport nombre objets exponentielle rapport nombre classes proposons contraintes positivité fonctions masses contraintes linéaires réduire optimisation partition méthode multiplicateurs Lagrange respect contraintes positivité ensuite vérifié méthode réajustement avons montré ensemble données cette nouvelle nique permet seulement augmenter rapidité permet également améliorer performances trouvant meilleurs minima travaux futurs porteront étude nouveaux formalismes rapides permettant conserver majeure partie expressivité méthodes évidentielles complexité largement réduite mettre traiter données nombre classes important Références Antoine Labroche Evidential based supervised cluste Joint International Conference Computing Intelli Systems International Symposium Advanced Intelligent Systems Antoine Quost Masson Denœux Constrained evidential means algorithm Computational Statistics Analysis Antoine Quost Masson Denœux Evidential clustering instance level constraints proximity Computing Banerjee Mooney supervised clustering seeding Proceeding International Conference Machine Learning Bensaid Bezdek Clarke Partially supervised clustering image segmentation Pattern Recognition Bilenko Mooney Integrating constraints metric learning supervised clustering Conference Machine Learning Bouchachia Pedrycz supervised clustering algorithm explo ration Internat Fuzzy Systems Association World Congress Bouchachia Pedrycz Enhancement fuzzy clustering mechanisms partial supervision Fuzzy Systems Davidson Agglomerative hierarchical clustering constraints retical empirical results Proceeding European Conference Machine Learning Principles Practice Knowledge Discovery Databases Grira Crucianu Boujemaa Fuzzy clustering pairwise constraints knowledge driven image categorization Vision Image Processing Gustafson Kessel Fuzzy clustering fuzzy covariance matrix Decision Control Krishnapuram Keller possibilistic approach clustering Transactions Fuzzy Systems Classification évidentielle contraintes Lelis Sander supervised density based clustering Proceedings International Conference Mining Washington Masson Denœux evidential version fuzzy means algorithm Pattern Recognition Masson Denœux Relational evidential means algorithm Pattern Recognition Letters Pedrycz Algorithm fuzzy clustering partial supervision Pattern Recogni Letters Pedrycz Waletzky 1997a Fuzzy clustering partial supervision Transac tions systems Cybernetics Pedrycz Waletzky 1997b Fuzzy clustering partial supervision Transac tions systems Cybernetics Menasalvas Spiliopoulou denstream Using domain knowledge stream Heidelberg Spiliopoulou Menasalvas Density based supervised clustering Mining Knowledge Discovery Clustering relational containing noise outliers Fuzzy Systems Proceedings Volume Shafer Mathematical Theory Evidence Princeton Press Princeton Smets Kennes transferable belief model Artificial Intelligence Wagstaff Cardie Rogers Schroedl Constrained means clustering background knowledge Proceedings International Conference Machine Learning Davidson Flexible constrained spectral clustering Proceeding Conference Knowledge Discovery Mining extension Karmarkar projective algorithm convex quadratic programming Mathematical Programming Summary paper proposes improved optimization evidential clustering rithm benefits introduction labelled objects guide output partition towards desired solution takes advantage belief functions theory generate credal partition generalizes concept crisp fuzzy partition counterpart expressivity complexity which grows exponentially number clusters efficients methods should order optimize objectif function propose article heuristic releases classic constraint positivity related functions coming evidential methods several datasets efficiency optimization method accuracy speed Classification Clustering Similarité Clustering évidentiel supervisé étiquettes Violaine Antoine Nicolas Labroche