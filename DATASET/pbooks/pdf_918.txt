Consignes articles Classification grands ensembles données nouvel algorithme Thanh François Poulet Equipe InSitu INRIA Futurs Université Paris 91405 Orsay Cedex Thanh dtnghi ESIEA Ouest Docteurs Calmette Guérin 53000 Laval francois poulet esiea ouest Résumé nouvel algorithme boosting Least Squares Support Vector Machine présentons classification grands ensembles données machines standard méthodes noyaux permettent obtenir résultats concerne précision tâche apprentissage grands ensembles données demande grande capacité mémoire temps relativement présentons extension algorithme proposé Suykens Vandewalle boosting cette avons ajouté terme régularisation Tikhonov utilisé formule Sherman Morrison Woodbury traiter ensembles données ayant grand nombre dimensions avons ensuite étendu application boosting traiter données ayant simultanément grand nombre individus dimensions performances algorithme évaluées ensembles données Twonorm Ringnorm Reuters 21578 machine standard Introduction volume données stocké double actuellement Lyman besoin extraction connaissances grandes bases données important Fayyad fouille données Fayyad confrontée challenge traiter grands ensembles données identifier connaissances nouvelles valides potentiellement utilisables compréhensibles utilise différents algorithmes classification régression clustering associations intéressons particulièrement algorithmes Séparateurs Vaste Marge Support Vector Machine proposé Vapnik montrent particulièrement efficaces classification régression détection nouveauté trouver nombreuses applications comme reconnaissance visages catégorisation textes bioinformatique Guyon approche systématique motivée théorie apprentissage statistique connus parmi classe algorithmes utilisant méthodes noyau Cristianini Boosting méthodes noyaux permettent construire modèles précis deviennent outils fouille données populaires malgré qualités peuvent traiter facilement données volumineuses solutions obtenues résolution programme quadratique calcul approche moins complexité égale carré nombre individus ensemble apprentissage quantité mémoire nécessaire impossible utiliser grands ensembles données besoin permettre passage échelle algorithmes traiter grands ensembles données machines standard heuristique possible améliorer apprentissage décomposer programme quadratique original série petits problèmes Boser Chang Osuna Platt méthodes apprentissage incrémental Cauwenberghs Poulet Poulet Mangasarian Poulet permettent traiter grands ensembles données solutions partielles augmentant ensemble apprentissage avoir charger ensemble données total mémoire algorithmes parallèles distribués Poulet Poulet utilisent machines connectées internet améliorer temps exécution apprentissage grands ensembles données algorithmes apprentissage actif Poulet Koller permettent choisir ensemble individus ensemble actif construction modèle présentons nouvel algorithme boosting classification grands ensembles données machines standard algorithme proposé Suykens Vandewalle effectue changement contrainte inégalité égalité résolution problème optimisation permettant obtenir solution résolution système équations linéaires programme quadratique nouvel algorithme beaucoup rapide temps exécution avons étendu algorithme construire nouvel algorithme incrémental parallèle distribué permettant traiter ensembles données ayant grands nombres individus avons ajouté terme régularisation Tikhonov Tikhonov utilisé formule Sherman Morrison Woodbury Golub permettre traiter ensembles données ayant grand nombre dimensions Enfin avons appliqué technique boosting obtenir algorithme permettant classification ensembles données ayant simultanément grand nombre individus dimensions performances algorithme évaluées ensembles données Blake Twonorm Ringnorm Delve Reuters 21578 Lewis Musicant résultats comparés obtenus LibSVM Chang paragraphe présente brièvement algorithme paragraphe décrit algorithme incrémental paragraphe présentons algorithme boosting résultats tests numériques paragraphe avant conclusion travaux futurs Quelques notations utilisées article vecteurs représentés matrices colonne produit scalaire vecteurs norme vecteur matrice taille contient ensemble individus dimension classe stockée matrice diagonale taille vecteur colonne coefficients scalaire hyperplan variable ressort constante positive représente matrice identité Poulet marge optimal marge optimal Séparation linéaire données classes algorithme Considérons tâche classification binaire linéaire comme représentée figure points espace representés matrice étiquettes classe stockées matrice diagonale algorithme cherche meilleur hyperplan séparation données meilleur éloigné possible classes revient maximiser marge distance entre plans supports classes support classe sépare individus classe autres écrire forme suivante marge entre plans support représente norme vecteur linéairement séparable contraintes doivent relaxées permettre point mauvais support classe variable ressort alors ajoutée partie gauche équation Ensuite point mauvais support considéré comme erreur valeur positive Ensuite algorithme simultanément maximiser marge minimiser erreurs formulation standard algorithme noyau linéaire alors programme quadratique représente variable ressort constante positive régler erreurs taille marge hyperplan obtenu solution programme quadratique Ensuite classification nouvel individu position rapport hyperplan obtenu classe signe algorithmes peuvent utiliser autres types fonctions classification comme exemple fonction polynomiale degré fonction Radial Basis Function sigmoïde passage linéaire linéaire utilisation fonction noyau place produit scalaire Boosting équation détails méthodes noyaux peuvent trouvés Cristianini Shawe Taylor solution obtenue résolution programme quadratique temps exécution moins proportionnel carré nombre individus place mémoire nécessaire incapables traiter ensembles données volumineux algorithme proposé Suykens Vandewalle utilise égalité inégalité problème optimisation fonction suivante minimisation erreur contrainte substituant fonction objectif programme quadratique obtenons alors résoudre problème optimisation calculons dérivées partielles donne système linéaire inconnues suivant équations peuvent réécrites forme suivante DeEEEI juxtaposition matrice colonne matrice diagonale identité dernier élément formulation nécessite résolution système linéaire inconnues programme quadratique nombre dimensions ensemble données inférieur algorithme tableau capable traiter grand nombre individus temps restreint machine standard tests numériques montré résultats satisfaisants comparaison algorithmes comme libSVM montrant beaucoup rapide exemple classification million points dimension effectuée seconde Pentium 512Mo traiter classification linéaire remplacer matrice entrée algorithme matrice noyau linéaire exemple fonction polynomiale degré points fonction points algorithme utilisant matrice noyau nécessitera aussi temps calcul place mémoire importante Entrée ensemble données paramètre contrôle marge erreurs Apprentissage créer matrice résoudre système linéaire obtenir Classification nouvel individu signe Algorithme linéaire Poulet incrémental algorithme efficace rapide classification grands ensembles données nécessite charger ensemble données mémoire grands ensembles données exemple milliard points dimension espace mémoire nécessaire plupart algorithmes classification actuels confrontés problème allons intéresser figure traitement grands ensembles données algorithmes classification incrémentaux Poulet Poulet méthode efficace traiter grands ensembles données nécessitent chargement totalité données mémoire petit données considéré instant donné modèle construit modifications successives incrémental ligne Supposons avons traiter ensemble données ayant grand nombre points nombre restreint dimensions pouvons décomposer ensemble données blocs lignes version incrémentale ligne calculer solution équation manière incrémentale Considérons exemple simple ensemble données décomposé blocs lignes illustrons maintenant comment effectue calcul solution système équation linéaire 222111 TEeDTEDeTE ETETE TEETEETE ETiEIc partir équations déduire équation algorithme incrémental ligne ensemble données décomposé blocs lignes algorithme incrémental ligne tableau classifier Boosting données volumineuses machine standard précision algorithme exactement celle algorithme original nombre dimensions ensemble données inférieur alors algorithme capable classifier ensembles données plusieurs milliards individus machine standard Entre étapes successives algorithme nécessaire conserver mémoire matrice taille vecteurs taille étant nombre individus nombre dimensions tests numériques montré algorithme capable classifier ensemble milliard individus dimension minutes Pentium 512Mo Entrée blocs données paramètre contrôler marge erreurs Apprentissage initialiser faire charger mémoire calculer EiTEi calculer EiTDiei finpour résoudre système linéaire obtenir Classification nouvel exemple signe Algorithme incrémental ligne linéaire incrémental colonne Certaines applications comme bioinformatique fouille textes nécessitent traiter données ayant nombre important dimensions nombre individus réduit matrice taille importante résolution système inconnues nécessite temps calcul élevé adapter algorithme données avons appliqué formule Sherman Morrison Woodbury système équations faisant avions matrice singulière inverser avons ajouté terme régularisation Tikhonov méthode couramment utilisée résoudre genre problème terme Tikhonov ajouté obtenons alors système équations DeEEEII système réécrit forme suivante DeEEEH 1n321 représente matrice diagonale terme autres termes valent Ensuite appliquons formule Sherman Morrison Woodbury partie droite système Poulet DeEEEH 1n321 DeEEHEEHIEHH 11111 DeEEHEEHIEHEH 11111 DeEEHEEHIIEH obtenons nouveau système équations DeEEHEEHIIEH 1111n321 nouveau système nécessite inversion matrice taille matrice taille Cette nouvelle formulation permet traiter facilement ensembles données ayant nombre dimensions important avons ensuite construit version incrémentale colonne algorithme manière analogue version incrémentale ligne ensemble données découpé blocs colonnes calcul terme effectue manière incrémentale Entre étapes calcul conservons mémoire matrice taille Boosting pouvoir traiter ensembles données ayant simultanément grand nombre individus colonnes moins problèmes résoudre temps apprentissage devient rapidement déraisonnable quantité mémoire nécessaire dépasse capacités mémoire machines courantes algorithmes incrémentaux puissent efficacement charger mémoire petits blocs données successifs nécessitent inversion matrice taille quantité mémoire nécessaire calcul deviennent importants pouvoir traiter ensembles données volumineux avons appliqué approche boosting manière analogue Poulet Cette solution présente avantages résoudre problème passage échelle conserver précision algorithme original détails boosting peuvent trouvés Freund Schapire boosting décrivons brièvement mécanisme boosting années Freund collègues introduit boosting améliorer précision algorithmes apprentissage méthode boosting consiste utiliser algorithme apprentissage basique concentrant chaque étape erreurs commises étape précédente faire nécessaire tenir distribution poids ensemble individus apprentissage Initialement poids identiques chaque étape boosting poids individus classifiés augmenté obliger algorithme prendre compte manière significative considérons algorithme comme algorithme apprentissage basique chaque étape boosting échantillonnons ensemble individus tenant compte distribution poids remarquer effectue apprentissage ensemble individus taille moindre ensemble original taille échantillon inversement proportionnelle nombres étapes boosting algorithmes incrémentaux ligne colonne peuvent ainsi adaptés traitement grands ensembles données Boosting nombre individus dimensions résultats précision besoin mémoire Quelques résultats avons développé programme Linux utilisant librairie Lapack Dongarra bénéficier bonnes performances calcul matriciel programme classifier grands ensembles données efficacement allons présenter évaluation prenant compte critères suivants précision temps apprentissage place mémoire requise avons sélectionné ensembles données artificiels générés Twonorm Ringnorm ensembles données caractéristiques ensembles décrites tableau attributs catégoriques ensembles Adult Mushroom convertis binaire avons utilisé nouvel algorithme boosting Boost LibSVM algorithmes efficaces effectuer classification Pentium 512Mo Classes Individus Dimensions Protocole Twonorm Ringnorm Ionosphere Mushroom Adult 48842 32561 16281 Reuters 21578 10789 29406 Forest cover 581012 55000 20000 50000 Description ensembles données premiers petits ensembles données utilisés comparer précision temps apprentissage tableau Boost obtient meilleures précisions meilleur temps apprentissage moitié remarquer temps apprentissage libSVM croit manière importante lorsque taille fichiers augmente exemple ensemble données Adult Boost rapide libSVM Reuters 21578 ensemble données réputé catégorisation textes avons utilisé McCallum prétraitement données Chaque document comme vecteur avons obtenu 29406 dimensions sélection dimensions avons effectué classification classes nombreuses ensemble données ayant classes avons utilisé approche against résultats présentés tableau moyenne précision rappel breakeven point catégories Boost obtenu meilleure précision catégories temps exécution Boost Poulet celui LibSVM nombre itérations important arriver précision Temps Précision Boost LibSVM Boost LibSVM Twonorm Ringnorm Ionosphere Mushroom Adult Performances terme temps exécution précision Précision Temps Boost LibSVM Boost LibSVM Money Grain Crude Trade Interest Wheat Résultats catégories ensemble données Reuters 21578 grands ensembles données utilisés évaluer temps exécution quantité mémoire nécessaire algorithmes LibSVM nécessite charger totalité ensemble données mémoire avons étendu capacité ensemble données Forest Cover avons effectué classification classes nombreuses Spruce 211840 individus Lorgepole 283301 individus dimension avons généré ensemble données 55000 individus dimension 20000 classes programme ensembles données LibSVM donner résultat Forest Cover programme tourné pendant jours second chargé mémoire Boost utilisé 512Mo mémoire relire données chaque étape boosting temps ainsi passé charger données mémoire résultats présentés tableau montrent Boost capable effectuer classification ensembles données ayant simultanément grand nombre individus dimensions machine standard temps raisonnable Boosting Précision Temps exécution Forest covertype Performances algorithmes grands ensembles données Conclusion perspectives avons présenté nouvel algorithme boosting capable effectuer classification grands ensembles données machines standard principale étendre algorithme récent Suykens Wandewalle construire version incrémentale boosting précision nouveaux algorithmes exactement celle algorithme original complexité version incrémentale lignes linéaire nombre individus nombre dimensions suffisamment restreint inférieur 10000 permet classifier plusieurs milliards données simple Quelques applications comme bioinformatique fouille texte utilisent données nombre dimensions important nombre individus faible avons ajouté terme régularisation Tikhonov utilisé formule Sherman Morrison Woodbury construire version incrémentale colonne algorithme traiter ensembles données ayant grand nombre dimensions avons étendu algorithmes utilisant technique boosting classification ensembles données ayant simultanément grand nombre dimensions individus résultats tests numériques montrent nouvel algorithme boosting rapide bonne précision permet passage échelle obtient précision comparaison libSVM algorithmes efficaces petits ensembles données présente précision bonne rapidité exécution ensembles données grandes tailles nombre dimensions individus montré possibilités précision première extension travaux consister étendre algorithme faire version parallèle distribuée ensemble machines Cette extension permettra améliorer temps tâche apprentissage seconde proposer nouvelle approche classification linéaire Remerciements tenons remercier vivement Jason Rennie préparation ensemble données Reuters 21578 Références Blake Repository Machine Learning Databases mlearn MLRepository Boser Guyon Vapnik Training Algorithm Optimal Margin Classifiers Annual Workshop Computational Learning Theory Pittsburgh Pennsylvania Poulet Cauwenberghs Poggio Incremental Decremental Support Vector Machine Learning Advances Neural Information Processing Systems Press Chang LIBSVM Library Support Vector Machines cjlin libsvm Cristianini Shawe Taylor Introduction Support Vector Machines Other Kernel based Learning Methods Cambridge University Press Delve evaluating learning valid experiments toronto delve Poulet Classifying billion distributed algorithm International Conference Computer Science Research Innovation Vision Future Vietnam Poulet Incremental Visualization Tools medical Mining Workshop Mining Mining Bioinformatics Cavtat Dubrovnik Croatia Poulet Towards Dimensional Mining Boosting Visualization Tools ICEIS Entreprise Information Systems Porto Portugal Poulet Mining Large Datasets Visualization ICEIS Entreprise Information Systems Miami Dongarra Walker LAPACK design overview object oriented extensions performance linear algebra Supercomputing Press Fayyad Piatetsky Shapiro Smyth Mining Knowledge Discovery Databases Magazine Fayyad Piatetsky Shapiro Uthurusamy Summary Panel Mining Years SIGKDD Explorations Freund Schapire Short Introduction Boosting Journal Japanese Society Artificial Intelligence Mangasarian Incremental Support Vector Machine Classification Mining Arlington Virginia Golub Matrix Computations Hopkins University Press Balti Maryland Guyon Applications clopinet isabelle Projects applist Lewis Reuters 21578 Classification Collection david dlewis resources testcollections reuters21578 Boosting Lyman Varian Swearingen Charles Jordan information berkeley research projects McCallum Toolkit Statistical Language Modeling Retrieval Classification Clustering mccallum Musicant Normally Distributed Clustered musicant Osuna Freund Girosi Improved Training Algorithm Support Vector Machines Neural Networks Signal Processing Principe Morgan Wilson Platt Training Support Vector Machines Using Sequential Minimal Optimization Advances Kernel Methods Support Vector Learning Schoelkopf Burges Smola Poulet Mining Large Datasets Support Vector Machine Algorithms Enterprise Information Systems Filipe Hammoudi Piattini Kluwer Academic Publishers Suykens Vandewalle Least Squares Support Vector Machines Classifiers Neural Processing Letters Incremental Learning Support Vector Machines SIGKDD Diego Tikhonov stability inverse problems Koller Support Vector Machine Active Learning Applications Classification Machine Learning Stanford Vapnik Nature Statistical Learning Theory Springer Verlag Summary Boosting Least Squares Support Vector Machine algorithm proposed paper classifying large datasets standard personal computers kernel related methods shown build accurate models learning usually needs quadratic program learning large datasets requires large memory capacity extend recent proposed Suykens Vandewalle building boosting algorithm added Tikhonov regularization Sherman Morrison Woodbury formula adapt process datasets large number dimensions extended applying boosting mining massive datasets simultaneously large number datapoints dimensions evaluated performance Twonorm Ringnorm Reuters 21578 datasets Pentium