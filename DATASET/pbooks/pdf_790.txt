actes_non_num 351rotes nouvel algorithme forêts aléatoires arbres obliques particulièrement adapté classification données grandes dimensions Thanh Stéphane Lallich Nguyen Khang Philippe Lenca Institut TELECOM TELECOM Bretagne LabSTICC Brest France philippe lenca telecom bretagne Université Laboratoire France stephane lallich lyon2 IRISA Rennes France pnguyenk irisa Université Européenne Bretagne France Résumé algorithme forêts aléatoires proposé Breiman permet tenir résultats fouille données comparativement nombreuses approches Cependant utilisant attribut parmi ensemble attributs aléatoirement séparer individus chaque niveau arbre algorithme information particulièrement pénalisant ensembles données grandes dimensions exister breuses dépendances entre attributs présentons nouvel algorithme forêts aléatoires arbres obliques obtenus séparateurs vaste marge comparaison performances notre algorithme celles algorithme forêts aléatoires arbres décision algorithme montre avantage significatif notre proposition Introduction performances classifieur dépendent différents facteurs Parmi derniers paramètres nécessaires initialisation exemple nombre classes clusters algorithme means données utilisées construire classifieur exemple construction ensembles apprentissage valida atténuer influence différents choix possibles compenser limites différents classifieurs pouvant utilisés combinaison classifieurs encore thode ensembliste retenu attention chercheurs apprentissage automatique depuis longtemps méthodes ensemblistes cherchent notamment réduire variance erreur riabilité résultats fonction échantillon apprentissage biais erreur cision dépendante échantillon apprentissage algorithmes apprentissage Forêts aléatoires arbres obliques exemple Buntine Wolpert Parmi différentes techniques proposées citera Boosting Freund Schapire permet réduire façon simultanée biais variance Bagging Breiman permet réduire façon notable variance dégrader biais Parmi méthodes issues Bagging forêts aléatoires Breiman certai nement méthodes efficaces apprentissage arbres décision méthode forêts aléatoires consiste créer famille arbres élagués chaque séparation individus réalisée partir ensemble attributs prédic choisi aléatoirement ensemble attributs partir échantillon bootstrap échan tillon construit tirage aléatoire remise partir ensemble apprentissage performances forêt arbres dépendent qualité individus composant indépendance forêts aléatoires fondées arbres élagués duire erreur biais outre processus aléatoire permet assurer faible corrélation entre arbres Elles fournissent bonnes performances comparativement algorithmes référence apprentissage supervisé AdaBoost Freund Schapire Vapnik technique apprentissage rapide robuste bruit apprend Breiman contrairement AdaBoost Dietterich Cependant construction arbres attribut utilisé chaque séparer individus information éventuelles dépendances entre attributs utilisée pénalisant données grandes dimensions niquement exister nombreuses dépendances entre variables notamment lorsque nombre variables grand regard nombre individus tenir compte cette dépendance proposons utiliser algorithme proximal Mangasarian comme heuristique séparation individus construction chaque arbre obtenons ainsi arbres décision obliques chaque multi varié utilise information dépendance entre attributs compare performances termes précision rappel mesure global précision forêts aléatoires arbres obliques celles forêts aléatoires arbres construits partir algorithme Quinlan celles LibSVM Chang tests réalisés bases données classiques Newman projet Statlog Michie grandes dimensions Jinyan Huiqing montrent intérêt notre proposition obtenons gnificatif grandes dimensions meilleurs résultats quoique significatifs bases classiques article organisé façon suivante section introduisons brièvement forêts aléatoires ainsi forêts aléatoires arbres décision obliques classifi cation section présente expérimentations menées discute résultats Enfin concluons section Forêts aléatoires arbres obliques proposé méthode ensembliste sélectionne hasard ensemble attributs construction arbres Breiman proposé Bagging arbres décision construit ensemble arbres partir différents échantillons bootstrap remise depuis ensemble apprentissage prédiction agrégation arbres obtenus majoritaire classification moyenne régres suite Geman proposé tirer aléatoirement ensemble attributs parmi lesquels algorithme recherche meilleure coupe construction arbres approches étendues reformulées Breiman créer rithme forêts aléatoires Forêts aléatoires algorithme forêts aléatoires Breiman ensemble arbres décision réduire erreur biais assurer faible corrélation entre arbres arbres construits élagage assurer faible biais arbre forêt construit partir échantillon bootstrap remise depuis ensemble apprentissage recherche meilleure coupe fondée ensemble attributs aléatoirement dernières propositions objectif maintenir niveau corrélation entre arbres assurer diversité Considèrons tâche classification individus attributs arbre décision forêt arbres façon suivante tirage remise depuis ensemble apprentissage échantillon bootstrap utilisé construction arbre recherche meilleure coupe chaque décision partir ensemble aléatoire attributs construction arbre profond possible élagage prédire étiquette nouvel individu utilise majoritaire arbres forêt classification moyenne prédictions arbres régression Breiman aussi proposé utiliser individus dehors échantillon bootstrap environ estimer attributs importants erreur forêt arbre courant construit algorithme forêts aléatoires donne résultats ailleurs rapide robuste données bruitées Breiman étendu forêts aléatoires apprentissage supervisé Récemment Robnik Sikonja proposé améliorations possibles portant notam contrôle corrélation entre arbres forêt également proposé liser pondéré Geurts proposé arbres extrêmement aléatoires Extra Trees Chaque arbre construit partir ensemble apprentissage échantillon bootstrap coupe effectuée hasard attribut choisi aléatoirement ensemble attri tenir compte classes Ainsi Extra Trees rapides apprentissage réduisent significativement corrélation entre arbres forêt Geurts Extra Trees proches algorithme proposé Cutler Guohua Arbres obliques construction arbre algorithmes forêts aléatoires utilise tribut parmi ensemble attributs aléatoirement séparer individus chaque niveau arbre exemple Breiman Quinlan Geman Cutler Guohua Robnik Sikonja Geurts Forêts aléatoires arbres obliques Coupe selon attribut gauche selon attributs droite dépendance éventuelle entre attributs ainsi prise considération entraînant perte information exemple figure montre existe aucune coupe perpendicu laire permettant séparer totalement individus seule coupe obliqueH1 combinaison attributs classifie parfaitement individus classes remédier problème proposé construire arbres multivariés fectuent coupes obliques séparer individus complexité construction arbre oblique difficile Heath Breiman proposé méthode construction arbres obliques combine linéairement attributs utilisant coeffi cients générés aléatoirement intervalle Cependant cette approche traiter petit nombre attributs cause explosion combinatoire Murthy proposé système induction arbres obliques recherche locale méthode heuristique trouver bonne coupe oblique hyperplan étendu algorithme modifiant profondément recherche coupe oblique proposé utiliser hyperplan optimal robustes algorithme Cette approche améliore performance algorithme Cepen utilisation standard ramène résoudre programme quadratique complexité moins carré nombre individus Forêts aléatoires arbres obliques Notre algorithme forêts aléatoires construit collection arbres obliques manière celle proposée Breiman différence arbre aléatoire oblique forêt utilise hyperplan effectuer coupes obliques proposons utiliser algorithme proximal gasarian faire coupes partir ensemble aléatoire attributs parce algorithme rapide apprentissage comparaison standards exemple Poulet Considérons tâche classification binaire linéaire figure avecm individus dimensions attributs représentés matrice leurs classes Séparation linéaire individus classes représentées matrice diagonale appartient classe classe recherche meilleur hyperplan vecteur scalaire algorithme ramène simultanément maximiser marge classes distance entre plans supports minimiser erreurs individus mauvais support algorithme revient résoudre programme quadratique vecteur colonne composé constante positive utilisée contrôler marge erreurs vecteur variables positives relâchement contraintes classification nouvel individu déterminée predict signe résolution programme quadratique coûteuse temps mémoire moins carré nombre individus pouvoir améliorer performances standard Mangasarian proposé algorithme proximal algorithme modifie formule standard maximisant marge minimisant erreurs contrainte substituant fonction objectif obtenons problème misation Forêts aléatoires arbres obliques configuration fonction objectif problème minimale dérivées partielles soient nulles donne système linéaire suivant 1ETDe matrice identité PSVMs demandent résolution système linéaire inconnues programme quadratique complexité linéaire fonction nombre individus capable traiter grand nombre individus temps restreint tests empiriques montré PSVMs donnent précision comparaison obtenus standard LibSVM Chang surtout beaucoup rapides classiques proposons utiliser PSVMs effectuer coupes obliques construction arbres recherche coupe oblique fonde ensemble attributs situation PSVMs efficaces temps exécution utilisons méthode Veropoulos régler problème déséqui libre entre positifs négatifs construction arbres Notre rithme forêts aléatoires construit ensemble arbres obliques élagage liorer performance différents arbres forêt maintenant faible corrélation entre Evaluation évaluer performances notre algorithme comparons résultats obtenus celui classification forêts aléatoires avons loppé notre algorithme forêts aléatoires arbres obliques rithme Quinlan modifié obtenir algorithme forêts aléatoires arbres Enfin avons utilisé algorithme standard LibSVM Chang comparaisons faites partir ensembles données grandes dimensions décrits tableau ensembles données standard décrits tableau ensembles données grande dimension proviennent médical Jinyan Huiqing ensembles données standard proviennent sites Asuncion Newman Statlog Michie tableaux colonnes indiquent ensemble données nombre individus nombre attributs Lorsqu ensemble données comportait classes sommes ramenés regroupement classes données classes colonne indique comment avons opéré regroupement exemple ensemble Subtypes Acute Lymphoblastic Hyperdip classe Hyperdip considérée comme classe positive regroupement autres classes constituant classe négative protocole présenté colonne certains ensemble divisé ensemble apprentissage ensemble Sinon avons procédé validation croisée leave utilisé lorsque ensemble comporte moins individus Autrement avons utilisé validation croisée segments échantillonnage chaque algorithme Description ensembles données grandes dimensions Ensemble Individus Attributs Classes Protocole Colon Tumor tumor normal Leukemia Leukemia 12582 Breast Cancer 24481 relapse relapse Breast Cancer cancer normal Prostate Cancer 12600 cancer normal Cancer 12533 cancer normal Central Nervous System positive negative Translation Initiation 13375 positive negative Ovarian Cancer 15154 cancer normal Diffuse Large Lymphoma germinal activated Subtypes Acute Lymphoblastic Hyperdip 12558 Hyperdip Subtypes Acute Lymphoblastic 12558 Subtypes Acute Lymphoblastic 12558 Subtypes Acute Lymphoblastic Others 12558 Others diagnostic groups Description ensembles données standard Ensemble Individus Attributs Classes Protocole Breast Cancer Wisconsin Segment Spambase Opticdigits Satimage Pendigits 10992 Letters 20000 Shuttle 58000 Résultats ensembles données grandes dimensions avons comparé performances celles LibSVM ensembles données présentant grandes dimensions tableau résultats présentés tableau Globalement tableau recours améliore significa tivement global précision points rapport value points rapport LibSVM value comparaison ensemble ensemble signe montre ensembles emporte systéma tiquement victoires égalités défaite value battue LibSVM victoires égalités défaite value avoir appréciation performance LibSVM précision globale avons aussi comparé précision rappel mesure Rijsbergen Forêts aléatoires arbres obliques concerne comparaison apporté porte principalement rappel tableau amélioré points moyenne value comparaison ensemble ensemble montre battu victoires égalités défaite value précision tableau points raison excellentes performances ensembles significatif résultat ensemble ensemble victoires égalités défaite value recoupe observations Globalement tableau mesure améliorée moyenne points significatif value comparaison ensemble ensemble donne toujours avantage égalité comparaison LibSVM donne résultat inverse supériorité LibSVM essentiellement amélioration précision effet améliore moyenne points précision LibSVM tableau significatif value ensembles battu seule LibSVM victoires égalités défaite value améliore moyenne points rappel tableau significatif Cependant remarque victoires particulier ensembles larges défaites Globalement mesure améliorée moyenne points value significatif comparaison ensemble ensemble recoupe cette conclusion victoires égalités défaite value Résultats classification ensembles données grandes dimensions Ensemble Précision Rappel Mesure Précision globale LibSVM LibSVM LibSVM LibSVM Comparaison globaux précision global précision LibSVM LibSVM moyenne écartype ratio Student value résultat victoire égalité défaite value résultat Comparaison précision Précision LibSVM LibSVM moyenne écartype ratio Student value résultat victoire égalité défaite value résultat Comparaison rappel Rappel LibSVM LibSVM moyenne écartype ratio Student value résultat victoire égalité défaite value résultat Comparaison mesure Mesure LibSVM LibSVM moyenne écartype ratio Student value résultat victoire égalité défaite value résultat Résultats ensembles données standard Comparaison globaux précision ensembles données standard global précision moyenne écartype ratio Student value résultat significatif victoire égalité défaite value résultat limite signification intéressant compléter comparaison précède comparaison ensembles standard avons travaillé ensembles tableau chacun nombre variables compris entre ratio nombre dimensions nombre indidivus dépasse conclure moins aussi Forêts aléatoires arbres obliques performant ensembles standard effet moyen points significatif différences entre faibles exception ensemble emporte points remarquera cependant emporte limite signification conclusion résultats expérimentaux confirment fondé notre approche moins ensembles standard surclasse façon significative ensembles grandes dimensions cette recherche Temps exécution Avant comparer temps exécution différentes forêts aléatoires rappelons plexité théorique algorithmes ensemble données ayant individus tributs construction forêt aléatoire arbres décision attributs tirés aléatoirement complexité attribut nominal attribut numérique concerne notre forêt aléatoire arbres obliques complexité théorique profondeur moyenne arbres pratique taille arbres obliques compacte celle arbres arbre oblique général moins profond ensemble aléatoire attributs grand ordre raisons temps exécution forêts aléatoires arbres obliques court forêts arbres avons utilisé Linux Mandriva faire expérimentations temps exécution présenté tableau Temps exécution Ensemble Temps exécution Ensemble Temps exécution Moyenne donnons également résultats ensemble données Forest cover grande taille grandes classes Spruce 211840 individus Lorgepole 83301 individus attributs Asuncion Newman avons utilisé 495141 individus ensemble apprentissage 45141 individus construit forêts aléatoires arbres apprentissage algorithme forêts arbres obliques secondes atteint classement algorithme forêts classement 17484 secondes apprentissage Notre algorithme améliore précision rapide algorithme forêts arbres Conclusion perspectives construction arbres décision algorithme forêts aléatoires proposé Breiman utilise chaque attribut partir ensemble attributs aléatoirement particulièrement pénalisant ensembles données grandes dimensions exister nombreuses dépendances entre attributs notamment lorsque nombre variables grand relativement nombre individus remédier problème avons proposé nouvel algorithme arbres aléatoires obliques obtenus séparateurs vaste marge proximal comparaison performances notre algorithme celles algorithme forêts aléatoires arbres décision celles algorithme montre avantage significatif notre proposition envisageons étendre travail classification multi classes régression détection individus atypiques Références Geman Shape quantization recognition randomized trees Machine Learning Asuncion Newman machine learning repository Breiman Bagging predictors Machine Learning Breiman Random forests Machine Learning Breiman Friedman Olshen Stone Classification Regression Trees Wadsworth International Buntine Learning classification trees Statistics Computing Chang LIBSVM library support vector machines Technical report Department Computer Science Information Engineering National Taiwan University cjlin libsvm Cutler Guohua Perfect random ensembles Computing Science Statistics Dietterich experimental comparison three methods constructing sembles decision trees Bagging boosting randomization Machine Learning Poulet Classifying billion distributed algorithm International Conference Computer Science Research Innovation Vision Future Freund Schapire decision theoretic generalization learning application boosting Computational Learning Theory Proceedings Second European Conference Forêts aléatoires arbres obliques Mangasarian Proximal support vector classifiers Proceedings Knowledge Discovery Mining Geurts Ernst Wehenkel Extremely randomized trees Machine Heath Geometric Framework Machine Learning thesis Johns Hopkins University Baltimore Maryland Random decision forest Proceedings Third International Confe rence Document Analysis Recognition Jinyan Huiqing ridge medical repository Technical report GEDatasets Michie Spiegelhalter Taylor Machine Learning Neural Statistical Classification Ellis Horwood Murthy Kasif Salzberg Beigel Randomized induction oblique decision trees Proceedings Eleventh National Conference Artificial Intelligence Quinlan Programs Machine Learning Mateo Morgan Kaufmann Robnik Sikonja Improving random forests Proceedings Fifth European Conference Machine Learning Rijsbergen Information Retrieval Butterworth Vapnik Nature Statistical Learning Theory Springer Verlag Veropoulos Campbell Cristianini Controlling sensitivity support vector machines Proceedings International Joint Conference Artificial Intelli gence Wolpert Stacked generalization Neural Networks Bennett Cristianini Shawe Taylor Large margin trees induc transduction Proceedings Sixth International Conference Machine Learning Summary random forests method robust noise overfit However random forests performance dealing dimensional dependency investigate approach supervised classification number attributes propose random oblique decision trees method consists randomly choose subset predictive attributes split function compare effectiveness classical measures precision recall measure accuracy random forests random oblique decision random forests results obtained promising particular proposal significant better performance dimensional better significant results lower dimensional