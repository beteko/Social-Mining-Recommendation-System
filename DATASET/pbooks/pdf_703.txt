articles assemblage pdfUn modèle extraction masses croyance partir probabilités posteriori amélioration performances classification supervisée Amouh Monique Noirhomme Fraiture Benoît Université Namur Faculté informatique grandgagnage Namur Belgique fundp fundp Université catholique Louvain Laboratoire Télécommunications Télédétection Place Stevin Louvain neuve Belgique benoit uclouvain Résumé objectif article montrer utilisation règle décision maximum masse croyance place celle maximum probabilité posteriori permettre réduire erreur classifi cation supervisée proposons technique efficace extraire partir vecteur probabilités posteriori vecteur masses croyance lequel baser décision maximum masse croyance application notre méthode domaine classification automatique stades sommeil montre amélioration performances pouvant atteindre réduction erreur classification Introduction classification supervisée information sortie classifieur présente générale forme vecteur probabilités posteriori chacune composantes rapporte classes connues règle décision maximum probabilité teriori semble naturelle atteindre meilleures performances application telle règle décision suppose entièrement confiance probabilités posteriori produites classifieur plupart raisonnablement avoir confiance totale classifieur Notre processus décision devrait dicieusement tenir compte confiance croyance mettons résultats produits classifieur Cette croyance abord mesurée sorte puisse avoir valeur numérique utilisable calculs théorie Dempster Shafer fournit cadre puissant mesure croyance proposant concepts permettant modéliser information imparfaite probabilités posteriori masses croyance imperfection information englobe imprécision incertitude incomplétude travers concept fonction masses croyance théorie Dempster Shafer permet modéliser trois dimensions information imparfaite reflétant ainsi notre confiance source information fonction masses croyance fonction masses court fonction tabulaire partage quantité unitaire entre ensembles ensemble classes connues contrairement fonction babilités partage quantité unitaire entre classes ensembles singletons valeur attribuée ensemble classe correspond masse croyance ensemble principale difficulté cette théorie réside manière distribuer masses croyance méthodes disponibles littérature dévoloppées contexte fusion classifieurs suppose application nécessite avoir moins classifieurs article proposons méthode calcul valeurs masse croyance répond préoccupations suivantes prendre compte toute information disponible priori succeptible influencer confi pourrait avoir classifieur pouvoir appliquer méthode améliorer performances importe classi fieur unique donné contexte fusion classifieurs Notre approche schématisée figure classifieur donné reçoit entrée vecteur caractéristiques produit sortie vecteur probabilités vecteur probabilités ensuite transformé notre méthode vecteur masses croyance lequel baser décision assignation objet classes connues éventuellement décision rejet Schéma transformation vecteur probabilités posteriori vecteur masses croyance amélioration performances classification supervisée Outre fonction masses théorie Dempster Shafer propose également certain nombre règles décision Seule règle maximum masse utilisée approche développons Rappelons règle maximum probabilité posteriori rejet maximum unique maximum atteint certain seuil impossibilité quantifier confiance rejet seulement notre approche permet réduire erreur offre également opportunité mesurer notre confiance rejet Cette confiance correspond valeur croyance attribuée ensemble complet classes Après avoir rappelé section définition notion fonction masses discutons section quelques méthodes proposées littérature calcul valeurs masse croyance section détaillons notre technique testons Amouh section classifieurs différents domaine classification stades sommeil discutons résultats section concluons section Fonction masses croyance notion fonction masses notions introduites théorie dence Dempster Shafer généralisant théorie probabilités variables crètes cette section donnons définition formelle cette fonction avant intro duire manière utilisons améliorer performances classifieur donné lecteur invité référer travaux originaux Shafer Shafer cours détaillé théorie évidence ensemble classes ensemble ensembles définit fonction masses manière suivante mesure confiance disposé accorder exactement ensembleA étant donné information disponible Autrement correspond croyance soutient hypothèse compositeA soutenir aucun ensemble strict Contrairement théorie probabilités quantité unitaire divisée entre plusieurs hypothèses atomiques théorie évidence toute hypothèse composite reflète certaine ignorance représente croyance subdivisée ensembles conduit inégalité suivante complément Cette inégalité contraste postulat additivité théorie probabilités alors appelé élément focal Comme indiqué introduction composantes vecteur sortie notre méthode figure considérées comme étant valeurs fonction masses Cette fonction masses éléments focaux étant nombre total classes application classification supervisée convention représente composante vecteur éléments focaux Chaque valeur masse croyance accordée classe étant donné vecteur probabiltés produit classificateur règle décision maximum masse croyance énonce affecter objet composante telle exemple lorsque composantes différentes atteignent valeur maximale décision alors rejet affectation objet correspond également rejet puisque représente masse croyance associée ensemble complet Avant détailler notre approche allons discuter quelques méthodes calcul fonction masses rencontre littérature probabilités posteriori masses croyance Méthodes calcul fonction masses stratégie développons article calcul valeurs masse croyance appliquer contextes différents contexte classifieur unique souhaite améliorer performances classifieur quelconque donné sortie vecteur probabilités posteriori contexte fusion classifieurs dispose classifieurs cherche exploiter simultanément réponses différentes originalité notre approche réside toutes méthodes existantes calcul valeurs masse croyance applications classification conçues fusion plusieurs classifieurs Elles appliquent telles quelles contraire notre méthode contexte classifieur unique voudrait améliorer performances méthode chacun classifieurs fusionner reconnaissance erreur rejet respectivement assimilés masses croyance fonction masses définie comme décision maximum probabilité posteriori rejet alors décision maximum probabilité posteriori choix classe alors distribue quantité unitaire trois élements focaux manière suivante Cette méthode calcul valeurs masse croyance pourrait appliquer contexte classifieur unique guère théoriquement satisfaisante reconnaissance erreur rejet peuvent difficulté considérés comme babilités peuvent assimilés masses croyance certaines condi tions puisque croyance réponses classifieur nécessairement affectée confiance classifieur masses croyance probabilités coïncident lorsque confiant quant réponses classifieur méthode Rogova Rogova inclut étape apprentissage fusion classifieurs apprentissage consiste calculer chaque paire vecteur référence comme étant moyenne vecteurs sortie lorsque appliqué données apprentissage appartenant classe considère ensuite fonction proximité varie entre atteint valeur lorsque arguments égaux donnée inconnue quelconque ensemble valeurs prises permet générer ensemble fonctions masses combinaison opérateurs proposés théorie évidence résulte autre fonction masses décision classification alors basée technique proposée Denoeux Denoeux calcul valeurs masse croyance inscrit contexte contexte puisqu plusieurs classifieur existant technique apprentissage permet construire classifieur utilisant notions celle fonction masses introduites théorie évidence Denoeux propose extraire partir ensemble apprentis vecteurs prototypes prototype étant vecteur situé espace caractéristiques connaît probabilité appartenir classe donnée inconnue chaque prototype génère fonction masses éléments focaux αjujn Amouh vecteur caractéristiques extrait donnée inconnue mètres associés prototype paramètre varie entre combinaison fonctions résulte autre fonction masses laquelle décision classification méthode Deriche similaire celle Rogova principale différence entre méthodes réside manière déterminer vecteurs référence propose contruire apprentissage alors Rogova calculait moyennage instar méthode Rogova méthode définit approche fusion classifieurs nécessite moins classificateurs appliquer contexte fusion classifieurs sorties nominales méthode Appriou Appriou propose extraire abord forme fonctions probabilités condi tionnelles connaissance sujet chacun classifieurs facteur confiance associé chaque fonction probabilités conditionnelles reflète notre confiance estimation parfaitement représentative population données concernant paire alors suite lorsque donnée inconnue classifieur produit label fonctions masses ayant chacune trois éléments focaux calculées facteur normalisation Notons Appriou également proposé modèle fonction masses ayant seulement éléments focaux combinaison grâce règle somme orthognonale Dempster toutes fonctions masses ainsi calculées résulte autre fonction décision classification alors basée Détail approche proposée procédure étapes suite article désignerons hypothèses éléments focaux considérant chaque hypothèse représentée vecteur référence composantes vecteur sortie notre méthode figure obtenu calcul étapes étant donné objet inconnu Etape chaque hypothèse dissimilarité entre vecteur référence vecteur correspondant mesurée grâce fonction distance exemple distance euclidienne fonction monotone décroissante telle exponentielle décroissante permet surer proximité entre Etape normalisation valeurs proximité obtenues étape conduit interprété comme fonction masses croyance rapportant information véhiculée probabilités posteriori masses croyance Notons composants satisfont expression Implémentation connexionniste Notre approche exposée dessus présente quelques similitudes réseaux tions radiales Ghosh réseaux neurones composés couche entrée couche cachée couche sortie Etant donné vecteur entrée réponse unité cachée définie comme étant fonction décroissante distance entre vecteur entrée vecteur relatif unité considérée réponse unité sortie définie comme étant somme pondérée réponses unités cachées instar réseaux fonctions radiales technique proposons également représentée formalisme connexionniste seule couche cachée figure couche cachée couche numéro couche sortie couche numéro correspondent respectivement étapes procédure décrite dessus unités chacune couches Schéma connexionniste notre méthode unités couche entrée unités couche cachée unités couche sortie représente composante vecteur entrée produit classifieur voulons améliorer performances vecteur satisfait contrainte suivante puisqu vecteur probabilités couches numéro unité associée hypothèse unité associée hypothèse Amouh valeur activation unité couche fonction activation utilisée toutes unités cachées fonction activation utilisée unités sortie fonction identité réponse unité couche notée couche numéro paramètre correspond composante vecteur référence hypothèse valeur activation mesure dissimilarité entre fournie carré distance euclidienne entre vecteurs réponse activation mesure proximité entre vecteurs Cette proximité supposée directement proportionnelle croyance supporte thèse maximale lorsque décroît lorsque éloigne comportement engendré fonction exponentielle décroissante dissimilarité entre vecteurs couche numéro permet normaliser valeurs proximité valeurs malisées correspondent composantes vecteur sortie Apprentissage vecteurs référence critère erreur nombre total données ensemble apprentissage vecteur masses croyance produit notre méthode donnée apprentissage vecteur masses désiré composantes étant inconnues proposons raisonnement suivant calcul considérons sorties désirées comme étant valeurs masse croyance devrait obtenir était monde parfait monde classifieur serait parfait ferait aucune erreur ferait entièrement confiance classifieur puisque objet appartenant classe classifieur produirait vecteur ayant forme composantes vecteurs référence auraient forme probabilités posteriori masses croyance déterminer valeurs appropriées observe hypothèse monde parfait vecteur référence rejet toutes composantes valent égale distance euclidienne autre vecteur référence notant carré cette distance remplaçant valeurs indiquées obtient paramètre ainsi calculé retrouvera expressions composantes vecteurs pourra déduire valeurs appropriées phase apprentissage hypothèse monde parfait pendant paramètres gardent leurs formes indiquées lorsque propage vecteur réseau calcule valeurs activation sortie grâce équations vecteur sortie obtenu correspond vecteur masses désiré Ainsi donnée apprentissage appartenant classe obtient propageant fourni classifieur donnée représente composante vecteur masses désiré alors déduire valeurs appropriées considé étant fonction masses sensée représenter situation parfaite valeurs doivent satisfaire inéquations suivantes résolvant inéquations trouve observant déduit choix approprié puisque nécessairement positif pourra remplacer trouver valeurs optimales minimisant critère erreur descente gradient stochastique partir équation amène évaluer dérivée suivante Amouh fonctions définies comme Généralisation vecteurs référence appris calculer vecteur masses chaque vecteur entrée inconnu prendre décision classification règle maximum masse croyance Tests approche proposée Cotation stades sommeil laboratoires analyse sommeil humain cotation stades sommeil activité classification pages successives enregistrement polysomnogra phique enregistrement polysomnographique constitué signaux électriques électro encéphalogrammes électro occulogrammes électro myogrammes enregistrés moins heures capteurs positionnés corps patient procéder cotation stades gistrement polysomnographique logiquement segmenté plusieurs pages successives secondes figure illustre premiers signaux sections signaux troisième quatrième sections signaux dernier section signal cotation stades sommeil consiste identifier chaque stade correspondant parmi stades prédéfinis éveil sommeil paradoxal stade stade stade exposé application notre approche problème cotation stades sommeil nécessite expliquer détail chacun stades suffit noter stades correspondent classes pages successives correspondent objets classer tester notre approche plusieurs différents classifieurs loppés indépendamment notre technique amélioration performance Chacun classifieurs produit sortie vecteur probabilités posteriori deviendra entrée notre méthode Ensembles données apprentissage avons disposé ensemble données apprentissage contenant enregistre ments polysomnographiques fournissant total 48579 pages classées expert humain domaine enregistrements partagés ensembles probabilités posteriori masses croyance secondes ensemble servait développement classifieurs indépendamment notre méthode ensemble servait apprentissage notre méthode ensemble servait ensembles forment ainsi partition vérifier efficacité notre technique différents scénarios avons aléatoirement généré trois partitions chacune constituée ensembles Résultats discussion problème cotation stades sommeil avons également établi choix approprié implémentons algorithme suivant générer graphiques comparaison performances chaque partition fixer utiliser ensemble entraîner notre algorithme amélioration performance chaque classifieur préalablement entraîné ensemble chaque appartenant ensemble calculer vecteur prendre décision règle maximum probabilités posteriori comparer classe obtenue vraie classe partir calculer vecteur prendre décision règle maximum masse croyance comparer classe obtenue vraie classe dessiner graphe ayant abscisse patients ordonnées perfor mances classifieur avant après application notre méthode Amouh total graphes comparaison partitions classifieurs ainsi générés graphes similaires celui figure apparaît clairement concerne problème classification stades sommeil application notre technique permet systématiquement améliorer performances réduisant erreur Cette amélioration performances étonnamment élevée exemple patient Performance Performance technique Graphe comparaison performances avant après application notre thode représenté valeur abscisse reconnaissance pages passé moins avant application notre méthode après application notre méthode mauvais résultat avant application notre méthode nécessairement résultat final Conclusion perspectives classification supervisée utilisation règle décision maximum masse croyance place celle maximum probabilité posteriori permet réduire erreur classification article technique permettant transformer vecteur probabilités posteriori vecteur masses croyance cadre théorie évidence Dempster Shafer proposée testée domaine tation stades sommeil avons observer évolution reconnaissance pouvant aller réduction erreur noter notre méthode usage aucune caractéristique propre domaine application totalement indépendante classifieurs utilisés prochain article testerons efficacité autres problèmes classification supervisée probabilités posteriori masses croyance Références Deriche Technique Combining Multiple Classifiers Using Dempster Shafer Theory Evidence Journal Artificial Intelligence Research Appriou Discrimination Multisignal Théorie Evidence Lavoisier Décision Reconnaissance Formes Signal Lavoisier 75008 Paris Denoeux Neural Network Classifier Based Dempster Shafer Theory Transaction Systems Cybernetics Ghosh Overview Radial Basis Function Network Howlett Basis Function Network Physica Verlag Rogova Combining Results Several Neural Network Classifiers Neural Networks Shafer Mathematical Theory Evidence Princeton Princeton University Press Krzyzack Methods Combining Multiple Classifiers Their Applications Handwriting Recognition Trans Cybern Summary performance given measurement level classifier enhanced maximum posterior probability decision replaced maximum belief decision framework Dempster Shafer theory evidence shift decision raises method extracting class belief values output posterior probabilities paper propose effective method calculating class belief values which class assignment decision order improve accuracy reliability given measurement level classifier method allow reduction misclassification error applied given measurement level classifiers automatic sleep stages scoring application domain