Statistical Subspace Clustering Laurent Candillier1 Isabelle Tellier1 Fabien Torre1 Olivier Bousquet2 GRAppA Universite Charles Gaulle Lille candillier grappa lille3 grappa lille3 Pertinence neurs 75002 Paris olivier bousquet pertinence pertinence article place cadre subspace clustering proble matique double identifier simultane clusters espace cifique lequel chacun caracte riser chaque nombre minimal dimensions permettant ainsi sentation sultats compre hensible expert domaine application thodes propose jusqu cette restreindre cadre rique objectif article poser algorithme subspace clustering capable traiter donne crites attributs continus attributs goriels sentons thode algorithme classique simplifie donne suivi technique ginale lection attributs garder dimensions perti nentes chaque cluster rimentations sente ensuite bases donne aussi artificielles elles montrent notre algorithme sente sultats robustes termes qualite classification compre hensibilite clusters obtenus Introduction quantite informations cessent augmenter bases donne monde entier extraction automatique connaissances partir bases techniques visualisation sultats devenues indispensables raison fouille donne cadre apprentissage supervise clustering depuis longtemps utilise identifier groupes clusters ments similaires survey Berkhin proble matique supple mentaire appara bases donne grande dimensionnalite groupes peuvent caracte uniquement certains ensembles dimensions dimensions pertinentes peuvent diffe rentes groupe autre proble techniques classiques clustering fonctionnent fonde distance entre objets finie globalement espace description elles peuvent appre hender notion similarite varie groupe autre nouvelle proble matique merge cemment celle subspace clustering enjeu cibler groupes objets chacun espace cifique Statistical Subspace Clustering lequel objectif accompagne second celui fournir description compre hensible groupes identifie thodes propose cette focalise premier objectif glige second partie rimentale travaux porte exclusivement donne riques objectif article proposer algorithme subspace clustering pable traiter donne crites attributs continus attri goriels demandant utilisateur moins parame possible fournissant sortie repre sentation simple clusters identifie basons algorithme adapte clustering proposons version simplifie ajoutant hypothe donne selon distributions pendantes chaque dimension permet river compre hensible forme puisque chaque dimension caracte pendemment autres suite article organise comme section sentons notre thode subspace clustering rimentations sente ensuite section bases donne aussi artifi cielles elles sentent sultats notre algorithme terminons section quelques conclusions perspectives ouvertes travail Algorithme Parsons auteurs tudie compare thodes existantes subspace clustering Toutes capables retrouver efficacement clusters espace cifique elles cessitent souvent parame difficiles utilisateur influant leurs performances seuil densite nombre moyen dimensions caracte ristiques clusters distance minimale entre clusters tests effectue bases donne exclusivement riques Enfin aucune proposition aboutie sentation simple sultats effectue point pourtant crucial dimensionnalite clusters duite espaces propres celle encore expert domaine application puisse appre hender sultat souvent possible ignorer certaines dimensions conservant partitionnement objets probabiliste fournir sortie notre algorithme subspace clustering description simple clusters trouve choisissons repre senter forme percubes espaces espace original repre sentation reconnue comme facilement interpre table cette contrainte algorithme classique proposons ajouter hypothe donne selon distribu tions pendantes chaque dimension Cette hypothe comme effet affaiblir classique prenant galement compte corre lations possibles entre mensions ainsi lisation adapte sentation forme diffe rence proble matique lection attributs feature selection espace cible local chaque cluster global Parsons Candillier clusters trouve chaque dimension caracte pendemment autres algorithme alors rapide algorithme classique nouveau cessite moins parame classique nombre dimensions rations matricielles notre supposons donne selon distributions gaussiennes dimensions continues selon distributions tinomiales dimensions discre compose parame suivants chaque cluster poids chaque dimension continue moyenne variance chaque dimension discre quences chaque modalite Freqskd suppose donne nombre clusters recherche Algorithme utilisons algorithme classique notre parame cache correspondent probabilite appartenance chaque objet chaque cluster notre dimensions suppose pendantes probabilite appartenance objet cluster correspond produit probabilite chaque dimension 2πσkd continue Freqskd discre viter probabilite nulle dimension annule probabilite globale utilisons constante positive faible constitue borne minimale probabilite algorithme connu converger lentement certains proposons ajouter heuristique suivante lorsque attributions clusters objets changent crite ressemble alors fortement crite means chaque ration galement valuer tributions clusters objets comme Cluster ArgMaxkP nalement algorithme relance certain nombre solutions initiales atoires partition maximisant fonction conserve sentation sultat sultat compre hensible possible souhaitons seconde chaque cluster correspondant repre sentation simplifie forme chacune crite moins dimensions possible premier temps chaque cluster repre sente intervalle minimum contenant semble valeurs objets inclus cluster dimensions continues modalite probable dimensions discre Ensuite support calcule ensemble objets compris poids attribue chacune dimensions cluster fonction dispersion relative objets dimension dimensions continues rapport entre variance locale variance globale rapport correspond nombre objets dimensions discre quence relative modalite probable Modalitesd correspond ensemble modalite possibles dimension Frequencesd ensemble quences chacune modalite ensemble Statistical Subspace Clustering continue Freqskd Frequencesd Frequencesd discre ArgMax Modalitesd Freqskd lection dimensions pertinentes effectue comme toutes dimensions sente ordre croissant poids supprimer dimension suppression modifie support Enfin visualiser graphiquement sultats obtenus proposons calculer poids associe chaque couple dimensions sentes description clusters poids important projete dimensions cifiques rimentations Tests donne artificielles comparer thodes existantes subspace clustering posons mener riences bases uniquement riques Parmi centes Domeniconi thode efficace comme cessite parame utilisateur nombre clusters recherche proposons comparer algorithme utilisons bases artificielles valuer erreurs notre algorithme classification chaque partition associe purete moyenne clusters produits purete correspond pourcentage maximum objets cluster appartiennent concept initial points ancrage atoirement espace description dimensions utilise comme centro clusters chacun clusters associe partie objets ensemble dimensions constituant dimensions caracte ristiques coordonne objets appartenant cluster selon normale centre toute dimension caracte ristique elles selon uniforme espace description dimensions caracte ristiques riences faisant varier parame ration bases artificielles avant robustesse notre thode particulier efficace faire bruit existant donne purete moyenne clusters bruit contre Concernant temps cution thode heuristique avons propose permet obtenir sultats qualite similaire temps calcul proches means connu rapidite Concernant parame algorithme nombre clusters recherche celui rieur nombre clusters alors quelques concepts fusionne sultat loigne comple tement solution rieur nombre alors plusieurs concepts recouvrent Enfin notons sultats notre algorithme aussi robustes donne selon uniformes intervalles finition leurs dimensions caracte ristiques gaussiennes Candillier Tests donne elles riences galement bases donne elles Parmi elles Automobile issue bases donne Blake contient description rique gorielle ensemble voitures cette visualisations graphiques correspondant couples dimensions poids maximum fournies figure algorithme ainsi avant voitures augmente fortement lorsque longueur passe figure voitures ayant traction arrie poids rieur tractions avant roues motrices figure majorite voitures traction arrie correspondance entre figures concernant cluster tails rimentations Candillier 11104 14097 17090 20083 23076 26069 29062 32055 35048 length Objets Projections longueur drive wheels Objets Projections traction poids sultats Automobile Conclusions perspectives avons sente article nouvelle thode subspace clustering algorithme ajoutant hypothe donne distributions pendantes chaque dimension Cette tudie Pelleg Moore existe plusieurs diffe rences entre notre thode premie appara lisation supposer distribution gaussienne dimension continue auteurs supposent uniforme rieur intervalle donne utilisent queue distribution bords intervalle pendant parame volue cours algorithme Cette diffe rence retrouve ensuite thode finale clustering particulier thode capable mettre incre mentale alors adapter sentation nouveaux exemples avons effec tivement proble matique gorielle voque titre perspectives article avons propose thode originale lection attributs mettant fournir sortie sultat compre hensible visuel clusters identifie avons galement heuristique originale algorithme poursuivre recherche semble ressant inspirer article Statistical Subspace Clustering Bradley traite ration algorithme autre piste possible viter conside toutes dimensions cours algorithme lectionnant dimensions poids maximum Notons enfin notre thode cessite donne parame utilisateur nombre clusters recherche lioration possible consisterait identifier automatiquement parame classique utiliser crite notre autre piste originale serait utiliser lorsque rieur nombre clusters recherche alors associe clusters chevauchent rences Berkhin Survey clustering mining techniques Technical report Accrue Software California Blake Repository machine learning databases mlearn MLRepository Bradley Fayyad Reina Scaling Expectation Maximization Clustering Large Databases Microsoft Research Report Candillier Tellier Torre Bousquet Statistical Subspace Clustering Rapport technique GRAppA grappa lille3 candillier publis Domeniconi Papadopoulos Gunopolos Subspace clustering dimensional Mining Parsons Haque Evaluating subspace clustering algorithms Workshop Clustering Dimensional Applications Mining Pelleg Moore Mixtures rectangles Interpretable clustering Brodley Danyluk editors Spetsakis Clustering Unobserved using Mixture Gaussians Technical Report University Summary paper focus subspace clustering goals simultaneously identify clusters subspaces which defined describe cluster dimensions possible results easily interpretable human default existing methods consider numerical databases paper propose subspace clustering algorithm tackle databases contain continuous discrete attributes present method based classical algorithm applied simpli model followed original technique feature selection keeps dimensions relevant cluster Experiments conducted artifical databases rithm gives robust results terms classification interpretability output