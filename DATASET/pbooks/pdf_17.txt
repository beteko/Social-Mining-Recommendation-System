contextualisation singularités temps extraction connaissances données badre belabbess jeremy musab bairat olivier innovation 95870 bezons france prénom 77454 france prénom paris résumé émergence traitement temps oblige treprises considérer détection anomalies comme élément activité garantir haute précision processus détection métadonnées fournissant contexte spatio temporel mesures teurs nécessaires article présentons système générique capturer analyser qualifier stocker informations contextuelles domaine application donné approche proposée basée thodes sémantiques exploitent ontologies évaluer pertinence information contextuelle après description composants principaux architecture performance pertinence système démontrées évaluation ensembles données monde introduction capteurs internet objects génèrent continu grandes quantités données accumulées traitées plates formes spécialisées analyse données biais processus avancés basés apprentissage automatique calcul numérique approches sémantiques basées représentation connaissances inférence parmi problématiques phares identification singularités conduisant détection anomalies domaine recherche actualité effet sujet touche domaines aussi variés médecine identification tumeurs malignes gerie finance découverte fraudes transactions financières technologies information détection piratage réseaux informatiques cadre projet waves sommes intéressés détection anomalies grands réseaux potable gérés leader national expert domaine détection automatique telles anomalies question importante environnemental économique notera volume pertes potable enregis monde dépasse milliards milliards euros reste difficilement identifiable raison nature souterraine réseau théoriquement waves contextualisation singularités temps fuites peuvent détectées fonction pression mesures écoulement extraites capteurs installés points stratégiques réseau article intéressons réseau national français constitué environ canaux équipés capteurs distribuant potable millions clients selon experts possible garantir grande précision processus détection contextualisation mesures effectuée lorsqu singularité apparaît exemple signaux anormaux haute pression importants pourraient indiquer fuite cependant nombreux événements particuliers compétitions tives rencontres culturelles catastrophes naturelles singularités pourraient sément expliquer rendant réactions exploitant réseau efficaces conditions météorologiques telles canicule arrosage important incendie criminelle impliquent utilisation quantités importantes véritables anomalies conséquent approche efficace détection anomalies faire économie contextualisation précise intégrant dimension spatiale dimension temporelle dimension sémantique conçu système générique scouter simplifier toutes tâches proposant implémentation efficace cilitant considérablement configuration composants architecture scouter développé système complet traiter nements statiques dynamiques ainsi analyser puissant ensemble fonctions traitement langage naturel méthodes sémantiques avancées tièrement configurable objectif principal scouter extraire données efficacement partir différentes sources traiter rapidement quantifier potentiel chaque événement expliquer anomalies détectées plate forme principaux composants notre système suivants ensemble connecteurs données unité analyse multimédia unité géolocalisation centre stockage tionnaire messages fournisseur services connecteurs consomment données provenant différentes sources certaine fréquence fonction configurations prédéfinies interface sources incluent réseaux sociaux twitter facebook citoyens mentant fuites proximité sources médiatiques divers journaux article monde mentionnant incendie informations météo rologiques provenant source conditions climatiques événe spécifique événements organisés extraits fournisseurs source concerts expositions événements sportifs informations profi extraites dbpedia nombre habitants quartier concepts propriétés utilisées rechercher données représentés ontologie formalise différentes relations appartenance détaillée unité analyse médiatique synthétise provenant kafka appuie apache spark analyser temps derniers enregistrés comme événements annotés géolocalisation début ainsi cription filtrer événements pertinents conserver doublons belabbess scouter architecture données approche extraction résumé analyse sentiment combinées extraction résumé analyse texte découvrir occurrences termes ensuite module scoring parti pondérations définies utilisateur entre associées concepts ontologie fournir globale chaque texte analyse sentiment classe catégories positives négatives utili algorithme entropie maximum berger pietra simultanément unité profilage fournit caractéristiques géographiques analysée déter autour emplacement anomalie générant profil donné résidentiel touristique industriel agricole suite étapes annotation scoring événements enregistrés données distribuée orientée documents mongodb résultat final obtenu contex tualisation spatio temporelle temps pouvant expliquer anomalie détectée réseau potable scouter fournit également outil suivi performances grâce panoplie métriques telles temps exécution requêtes durée extraction résumés métriques stockées données orien séries temporelles influxdb permettant accès lecture écriture rapide enfin composant services utilisé configurer système manière conviviale interface media analytics traitement langage naturel cette section détaillons méthodologie collecte données issues férentes sources disponibles ontologie extraction systèmes scrapping reposent généralement fichier configuration réper torie propriétés concepts événements tentera extraire risuriya scouter extraction optimisée améliorée grâce ontologie contextualisation singularités temps construite énumère principaux concepts utilisateur recherche permet organiser différentes relations dimensions hiérarchie verticale concept donné avoir plusieurs concepts incendie brasier explosion alias erreurs orthographe brazier dépendance horizontale concept avoir plusieurs propriétés décrivent spécifique durant période données exemple potable également train avoir couleur odeur spécifique aperçu ontologie extraction combinant concepts propriétés prédicats pouvons créer ontologie expressive telle celle figure utilisée utilisation fuites structure expressif liste classique modularité extensibilité extraction résumés après avoir récupéré événements pertinents différentes sources données basant ontologie concepts propriétés étape suivante consiste extraire résumés significatifs événements suivant processus décrit figure processus extraction résumés prétraitement initial concerne nettoyage texte entrée identification candi potentiels enfin ainsi harmonisation majuscules minuscules fichiers belabbess entrée filtrés régulariser texte déterminer limites phrases inter viennent fractionnement tokens ainsi quelques opérations nettoyage pression apostrophes séparation certaines expression plusieurs ensuite considérons toutes séquences générées pouvoir déterminer celles conviennent phrases complètes compréhensibles augmenter précision utilisons liste français contenant entrées différentes classes syntaxiques conjonctions articles particules trions harmonisons méthode itérée utilisée domaine lovins processus répété jusqu amélioration possible quant traitement principal relatif calcul valeurs distinctes chaque phrase candidate fréquence phrases texte entrée rapport rareté utilisation générale première occurrence correspond distance texte entrée indiquant première apparition phrase valeurs converties données nominales faciliter processus apprentissage automatique table discrétisation chacune valeurs dérivée données entraînement enfin générons modèle donne scores chaque candidat classons utilisant techniques bayésiennes naïves domingos pazzani pertinence résumés plusieurs travaux recherche abordent question résumé automatique ellouze notre avons choisi approche basée similarité distribuée compare contenu entrée résumé considérons résumé devrait caractérisé faible divergence entre distributions probabilité entrée résumé généré forte similitude entrée cette avons utilisé mesures complémentaires divergence kullback leibler divergence jensen shannon abord entrée résumé triés segmentés avant calcul ensuite calculons mesures divergence kullback leibler correspond nombre moyen utilisés codage échantillons appartenant utilisant autre distribution approxima donnée notre distributions probabilités estimées partir texte résumé étant donné divergence symétrique divergences texte entrée résumé introduites mesures outre fectuons lissage simple fonction approximation capture comportements spécifiques excluant bruit autres nuisances faible échelle divergence jensen shannon appuie distance entre distributions éloignée moyenne distances distribution moyenne donnée formule suivante contextualisation singularités temps contrairement divergence divergence symétrique toujours définie calculons versions lissées lissées divergences résultats résumé dernière étape utiliser sortie fonctions classer résumés extraits conserver ayant meilleur résultat résumé divergences faibles processus estimation pertinence résumé analyse sentimentale cours dernière décennie analyse sentiment connu développement ponentiel nombreuses solutions proposées diverses technologies collomb brunie proposons cette section approche simple efficace mêlant outils variés toolkit fourni stanford corenlp manning processus analyse sentimentale avant appliquer modèle devons effectuer plusieurs étapes prétraitement améliorent précision score final sortie trois étapes réalisées kenisation séparation texte séquence tokens division chaque séquence phrases significatives reconnaissance entités inférence informations genre annotation personnes emplacements organisations nombres solution syntaxique recherche dépendances grammaticales utilisation dictionnaire français belabbess après phase prétraitement appliquons modèle composition arbres approche apprentissage profond appuie nœuds arbre binarisé chaque phrase compris particulier racine chaque étant annoté score sentiment saisir sentiment texte entrée modèle réseau neurones récursifs recursive neural tensor network construit fonction caractéristiques phrases entrée cette approche inspirée modèles récursifs profonds développés équipe stanford richard correspondance résumés objectif différentes étapes module analytique extraire événements uniques pertinents annotant résumé expressif système permet éviter stocker événements doublons faisant référence occurrence chaque nement extrait système proposera liste résumés potentiels basés approche bayésienne ensuite résumés seront classés utilisant divergences faibles divergence divergence évaluer précision parmi mieux classés vérifierons disposent sentiment positif neutre négatif résumés sélectionnés cours processus score pertinence sentiment supposons alors réfèrent événement conséquent concluons événements doublons conserverons contenu processus global notre module analytique détaillé figure6 processus correspondance résumés module fournit fonctions puissantes filtrer événements uniques pertinents dimension spatiale nécessaire parfaire contextualisation anomalie détectée cette partie expliquée section suivante profilage géographique pouvoir établir pertinence événements détectés comme origine potentielle anomalie pouvoir ajuster score probabilité attribué scouter système profilage géographique objectif pouvoir déterminer composition secteurs consommation étudiés termes terrain profilage réalisé partir données cartographiques provenant openstreetmap haklay weber projet international licence libre programme établit partir informations secteurs consommation données extraire construisant contextualisation singularités temps architecture système profilage bounding adaptée fichier configuration également fourni programme pouvoir établir quelles grandes catégories terrain considérer quels définis appartiennent catégories chaque attribué correspond pertinence décrire catégorie terrain laquelle appartient fichier format organisé hiérarchie catégories incluant feuille aussi différentes classes définies permet utilisateur créer modifier configuration profilage nécessaire exemple fichier scores usage waves fourni figure amenity tourism entertainment arts_centre casino community_centre fountain gambling planetarium theatre extrait arborescence fichier configuration profilage waves partir informations profilages distincts réalisés partir points intérêt polygones première méthode nodes récupérés ainsi notes attribuées fichier configuration chaque identifié données cartographiques classification fichier ajoute attribuée score total catégorie terrain représente catégories représentées secteur auront ainsi score important suffit ensuite simple calcul proportions finales obtenir répartition finale cette méthode adaptée identifiées zones denses deuxième méthode utilisation polygones établir répartition procédé cédemment identification pertinents toutefois place notes attribuées arbitrairement fichier configuration calcule place surface polygones belabbess répartition finale calcule manière scores chaque catégorie obtenus différemment parfaits secteurs riches polygones chaque méthode profilage convient mieux précis avère elles également complémentaires effet zones géographiques comportant beaucoup souvent dépourvues polygones secteurs divers parcelles terrain uniforme inverses secteurs riches polygones seront également plupart temps pauvres terrains éléments remarquables ainsi certains méthodes peuvent sélectionnées voire adaptées obtenir résultat précis suffit calculer densité proportion kilomètre carré partir données sorte utilisateur choisir profilage adapté besoins densité moyenne pertinent calculer moyenne résultats profilages ajuster proportions ensemble système profilage configurable partir fichier score permet effectuer ajustements ailleurs parfaitement possible réaliser propre méthode combinaison profilage approche générique convient évaluation cette section évaluons performance système plusieurs dimension quantitatives qualitatives media analytics cette expérimentation avons collecté durant heures depuis sources telles facebook twitter feeds agenda dbpedia weather notre cible géographique était agglomération versailles paramètres utilisés chaque source présentés table exemple utilisant streaming twitter récupérons géographique versailles aussi depuis comptes versailles monversailles utilisés requêter tweets associés concepts ontologie lesquels score pertinence associé source fréquence heures pages comptes concepts scores facebook versailles versailles officiel public events meter damage concert spray water blaze wildfire chlore pressure twitter streaming versailles monversailles prefet78 sdis78 agenda weather dbpedia papers parisien versailles sdis78 yvelines sources données scores concepts performance système métriques permettent déterminer performance scouter fonctions demandeuses ressources tableau montre temps moyen nécessaire annoter événements collectés calculé divisant somme temps scoring chacun événements nombre événements lectés montre également temps entraînement algorithme extraction résumés contextualisation singularités temps visant construire modèle approprié pouvons scouter reste performant nombre relativement important événements traités système autant subir panne retard mesure temps millisecondes temps moyen traitement temps entrainement extraction résumés temps traitement scouter qualité événements collectés avons considéré notre utilisation fuites durant année courante notre système tourné durant heures lecter événements diverses sources utilisant ontologie explicitée section scores assignés tableau exploitant réseau potable fourni horodatage emplacement toutes anomalies rapportées dénombrées total partir données avons récupéré événements stockés respondant horodatage emplacement chaque anomalie avons présentés experts domaine chaque événement demandé estimer événement pouvait fournir explication pertinente anomalie signalée contrainte imposée réponse devait borner simplifier interprétation résultats uateur evénements évaluation experts domaine évaluer fiabilité annotation avons utilisé mesure kappa fleiss fleiss mesure statistique visant évaluer fiabilité accord entre certain nombre évaluateurs attribution étiquettes sujets catégoriels cette mesure exprimée équation dessous résultats calculés scénario évaluateurs kappa 5256888889 5256888889 6626686657 après tableau interprétation valeurs kappa landis évaluateurs accord substantiel événements annotés comme pouvant fournir explication pertinente anomalie fuite conséquent scouter assez efficace sélectionner événements pertinents profilage géographique évaluation précision profilage géographique réalisée partir divers échantillons données fournis notre client présentons exemple belabbess versailles france allons présenter résultats méthodes profilage chaque secteur consommation détailler performances chaque méthode programme secteurs consommations versailles superposés données openstreetmap figure donne aperçu organisation système profilage profilage utilise types terrains différents résidentiel agricole naturel industriel touristique certaines zones facilement détectables utilisant autres seront majoritairement représentées polygones résultats finaux présentés tableau avons utilisé technique décrite section réalisons moyenne méthodes densité jugée moyenne ajustements finalisés avons présenté résultats groupe experts domaine recueillir colonne tableau leurs évaluations majoritairement satisfaisantes quelques remarques certains secteurs comportant terrain majoritaire catégorie densité évaluation données profilage polygones laval résidentielle élevé correct 605ms nouvelle moyen correct 282ms 630ms hubies faible nuancé brezin moyen nuancé guyancourt moyen correct louveciennes élevé correct 1118ms 1290ms hubies naturelle faible correct 163ms 180ms clagny résidentielle élevé correct garches touristique élevé correct 205ms gobert naturelle faible correct 105ms satory industrielle faible nuancé 103ms 215ms évaluation résultats finaux dernières colonnes tableau détaillent performances chaque méthode profilage taille données télécharger dépend surface secteur données polygones généralement volumineuses celles quand dernières nombreuses représentation étant complexe ainsi méthode profilage polygone généralement longue celle temps exécution potentiellement zones vastes profilage secteurs dépend contextualisation singularités temps aucune données scouter exécuté ligne impacter performances références collomb costea brunie study comparison sentiment analysis methods reputation evaluation technical report berger pietra maximum entropy approach natural language processing computational linguistics sirisuriya comparative study scraping international research conference domingos pazzani optimality simple bayesian classifier under machine learning ellouze jaoua belguith machine learning approach evaluate multilin summaries proceedings multiling workshop association compu tational linguistics fleiss measuring nominal scale agreement among raters psychological bulletin haklay weber openstreetmap generated street pervasive computing landis measurement observer agreement categorical biometrics profilage sémantique probabiliste zones géographiques lovins development stemming algorithm mechanical translation putational linguistics manning surdeanu bauer finkel bethard mcclosky stanford corenlp natural language processing toolkit association computer linguistics richard recursive models semantic compositionality sentiment treebank association computational linguistics summary anomaly detection feature applications processing singularities using sensor measures guarantee quality detections providing spatio temporal contexts sensor measures needed paper introduce scouter generic helps capturing analyzing scoring storing contextual information given application process depends semantic based approach exploits ontologies score relevancy contextual information paper provides details system architecture describes components evaluates performance based world datasets