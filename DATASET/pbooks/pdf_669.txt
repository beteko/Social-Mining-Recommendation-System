articles assemblage pdfModèle Langue Concepts Recherche Information Lynda Mohand BOUGHANEM Ecole Doctorale Ecole nationale Supérieure Informatique Algérie l_said_lhadj Laboratoire Université Sabatier route Narbonne 31062 Toulouse Cedex France bougha Résumé majorité modèles langue appliqués recherche information repose hypothèse indépendance précisément modèles estimés partir simples apparaissant documents considérer éventuelles relations sémantiques conceptuelles pallier problème grandes approches explorées première intègre dépendances ordre surfacique entre seconde repose utilisation ressources sémantiques capturer dépendances entre modèle langue présentons article inscrit seconde approche proposons intégrer dépendances entre représentant documents requêtes concepts Introduction modèles langue acquis grande popularité Recherche Information étant donné solidité fondement mathématique Ponte Croft modèles modélisent directement notion pertinence Cette dernière comme probabilité conditionnelle requête générée modèle langue document estimée hypothèse indépendance simplifie calcul mathématique Cependant problème majeur représentation documents requêtes comme dénués sémantique pallier problème nouvelle génération modèles langue inscrit intersection modèles langue recherche sémantique information développée Srikanth Srihari cette intersection grandes approches peuvent distinguées approche statistique surfacique prend compte dépendances surfaciques entre approche sémantique basée ressources sémantiques ontologies thésaurus identifier modèle langue présenté article inscrit seconde approche proposons capturer dépendances entre identification concepts auxquels renvoient approche conceptuelle souffre problème silence1 pensons définition modèle langue mixte combinant concepts identifiés ontologie concepts identifiés permet résoudre problème disponibilité ressources conceptuelles complètes générales Modèle langue concepts recherche information reste papier organisé comme présentons modèles langue sémantiques section section consacrée présentation modèle proposé déroulons exemple section Enfin section terminons papier synthèse travail présenté pertinence document requête rapport probabilité requête comme suite puisse générée modèle langue document score pertinence alors donné Score estimer probabilité soient indépendants ainsi hypothèse indépendance problèmes majeurs premier celui données éparses absent document alors nulle autres présents problème résolu techniques lissage3 second problème représentation permet prise compte phénomènes importants savoir polysémie synonymie résoudre plupart travaux proposés jusque utilisé techniques lissage lissage sémantique incorporer ainsi liens entre modèles langue travaux classés catégories approches approche statistique surfacique approche guidée ressources sémantiques approche surfacique tente intégrer relations entre selon considérations statistiques exemple cooccurrences modèle translation statistique Berger Lafferty premiers travaux cette direction considèrent dépendances entre comme variable cachée représentée graphe acyclique orienté modéliser dépendances entre Srikanth Srihari proposé modèle termes ignorent contrainte adjacence ordre imposée modèles grammes Enfin Srikanth Srihari présentent modèle gramme concepts séquences identifiés parseur syntaxique approche guidée ressources sémantiques liens sémantiques extraits ressources sémantiques comme ontologies proposé approche combine modèle indépendance gramme modèle dépendance exploitant techniques lissage dépendances types statistique cooccurrence sémantique relations entre simples WordNet direction proposé modèle langue gramme lissé modèle gramme identifiés système désambiguïsation WordNet résultats approches meilleurs modèle langue gramme Cependant approche surfacique engendre beaucoup bruit nécessite filtrage linguistique voire sémantique pensons également intégration relations écrit représenter probabilité modèle document Attribuer probabilité nulle requête absents document SaidL Boughanem surfaciques sémantiques entre simples simple désambigüisation suffit capturer contenu sémantique implicite documents requêtes pensons concept correspondant exemple entrée ontologie précis isolé isolé Modèle proposé modèle proposons concepts précisément document respectivement requête projeté ontologie exemple WordNet utilisons effet algorithme Baziz détection concepts Ainsi termes ayant entrée ontologie comme éléments document permettent construction arborescence document requête contrairement Baziz proposons garder termes reconnus ontologie descripteur arriver termes renvoient concepts importants propres néologismes considérons requête Document comme concepts Ainsi score pertinence donné estimée comme distinguons correspond aucune entrée WordNet appariement strict Sinon exploite hiérarchie requête document chercher seulement aussi concepts proches lissage interpolation linéaire permet tenir compte ainsi probabilité candidat expansion généré probabilité concept identifié WordNet généré Estimation Quand concept correspond entrée ontologie seulement retrouver documents figure effectivement appariement direct aussi documents figurent concepts relations sémantiques ontologie subsomption appariement indirect combinés lissage interpolation linéaire probabilité concept généré directement probabilité concept généré indirectement Estimation probabilités Elles estimées utilisant formule pondération concepts Concept Frequency proposée Baziz Count retourne fréquence apparition Length représente nombre concept sub_concept nombre concepts algorithme comprend détection groupes désambigüisation pondération Modèle langue concepts recherche information concepts ontologie dérivés Quand correspond aucune entrée WordNet annulons deuxième terme Quand identifié WordNet absent document alors nulle pourquoi lissons utilisant méthode Absolute Discount appliquée concepts modèle collection Estimation intégrer relations entre concepts utilisons modèle Berger Lafferty ensemble concepts requête approches conceptuelles proposées jusque mélangent concepts spécifiques concepts génériques constaté concepts génériques améliorent rappel concepts spécifiques améliorent précision Baziz Zakos modèle séparé tenir compte lissage interpolation linéaire respectivement probabilité conditionnelle généré concept générique respectivement avons contraintes tenir seule quand existent requête Estimation probabilités interprètent distance sémantique entre Elles estimées utilisant mesure similarité basée distance sémantique entre concepts avons choisi mesure Palmer simple mettre œuvre basée principe distance sémantique suivant Soient éléments ontologie similarité entre basée distances séparent racine distance sépare concept subsumant racine alors manière estime probabilité Après remplacement différentes probabilités modèle proposé donné modèle expansion concepts proprement parler SaidL Boughanem Exemple Soient Natural Science Geology Geography Geophysics Globe aeroelastic Earth Natural Science Anatomy Regional earth science geography aeroelastic Application modèle gramme mixte modèle langue mixte donné Sachant correspondent requête application numérique modèle retourne scores pertinences suivants 00225 00125 remarque ainsi pertinent rapport Application modèle concepts proposé projection hiérarchie WordNet retourne représentations conceptuelles Natural Science Geology Geography Geophysics Globe aeroelastic Earth Natural Science Regional anatomy earth science geography aeroelastic Représentations hiérarchiques concepts figure permet distinguer niveaux concepts voici aeroelastic earth science geography earth science natural science science earth science geology geography oceanography geology geophysics geography Après application modèle proposé6 scores pertinence 000003726 000001481040 remarquons alors pertinent rapport résultat diffère premier justifié fréquence élevée Earth celui requête influencé résultat retourné modèle gramme paramètres avons donné poids important modèle expansion capture effectivement sémantique requête document Modèle langue concepts recherche information Conclusion papier avons proposé modèle langue concepts choix intégrer concepts modèles langue justifie aussi résultats prometteurs recherche conceptuelle information performance flexibilité modèles langue intégrer plusieurs sources connaissances effet cette flexibilité permis tenir compte seulement concepts ontologie ainsi leurs liens aussi concepts apparaissent ontologie modèle cours expérimentation résultats permettront approfondir consolider hypothèses combinaison simples concepts ainsi séparation niveaux concepts génériques concepts spécifiques Références bibliographiques Shenghua Zhang Erdong Language Sense Model Information Retrieval Baziz Indexation Conceptuelle Guidée Ontologie Recherche Information thèse doctorat université Sabatier Berger Lafferty Information retrieval statistical translation SIGIR Guihong Integrating Relationships Language Models SIGIR Salvador Brazil August Jianfeng Guangyuan Guihong Dependance Language model information retrieval SIGIR Ponte Bruce Croft Language Modeling Approach Information Retrieval SIGIR Munirathnam Srikanth Rohini Srihari Biterm Language Models Document Retrieval SIGIR Tampere Finland Munirathnam Srikanth Rohini Srihari Incorporating Query Dependencies Language Models Document Retrieval SIGIR Canada Zakos Novel Concept Context based Approach Information Retrieval Doctorate thesis Griffith University Palmer Semantics Lexical Selection Annual Meeting Associations Computational Linguistics Lafferty Study Smoothing Methods Language Models Applied Information Retrieval SIGIR