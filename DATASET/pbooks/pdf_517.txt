egc2010_12p_fr dviApprentissage génératif structure réseaux logiques Markov partir graphe prédicats Quang Thang Matthieu Exbrayat Christel Vrain Université Orléans Léonard Vinci 45067 ORLEANS Cedex France thang matthieu exbrayat christel vrain orleans orleans Université Pierre Marie Curie Paris Place Jussieu 75252 Paris cedex Résumé Réseaux Logiques Markov combinent apport tistique Réseaux Markov logique premier ordre cette proche chaque clause logique affectée poids instantiation clauses permettant alors produire Réseau Markov apprentissage consiste apprendre structure liste clauses logiques autre poids celles proposons méthode appren tissage génératif Réseau Logique Markov Cette méthode repose lisation graphe prédicats produit partir ensemble prédicats apprentissage méthode heuristique variabilisation œuvre produire clauses candidates résultats présentés montrent intérêt notre approche regard Introduction Apprentissage Statistique Relationnel Getoor Taskar consiste biner pouvoir descriptif apprentissage relationnel souplesse apprentissage tistique Diverses approches proposées cours quinze dernières années programmes logiques stochastiques Muggleton PRISM Kameya MACCENT Dehaspe modèles relationnels probabilistes Friedman programmes logiques bayésiens Kersting Raedt seaux relationnels dépendants Neville Jensen réseaux logiques Markov Richardson Domingos constituent approches récentes domaine reposent combinaison logique premier ordre réseaux Markov réseau Markov Pearl graphe nœuds représentent variables aléatoires arêtes expriment dépendances conditionnelles entre riables Chaque clique graphe correspond ainsi ensemble variables conditionnelle dépendantes associe chaque clique poids réseau permet ensuite inférer valeur plusieurs variables réseau logique Markov constitué ensemble clauses logiques pondérées clauses constituées atomes lesquels Apprentissage génératif partir graphes prédicats peuvent comme prototypes construction réseau Markov effet dispose ensemble constantes produire instantiant clauses ensemble atomes constitueront nœuds réseau Markov nœuds issus instanciation clause seront cliques ainsi produites seront affectées poids clause elles dérivent apprentissage décomposé phases consistant apprendre respectivement structure clauses logique premier ordre paramètres poids clauses réseau premières stratégies développées limitaient prentissage paramètres structure étant fournie algorithme considérée comme proposée expert apprise étape antérieure telles approches pouvaient conduire résultats optimaux quand structure fournie reflétait pendances essentielles domaine étudié conséquence travaux récents concentrés méthodes apprentissage globales intègrant étapes apprentissage structure poids Néanmoins cette intégration reste délicate raison vaste espace recherche induit logique premier ordre approches pratiques proposées principales reposant approche Domingos approche bottom Mihalkova Mooney descente gradient Iterated Local Search généralisation hypergraphes Learning Hyper graph Lifting Domingos apprentissage partir motifs Learning using Structural Motifs Domingos propositionalisation Heuristic Generative Structure apprentissage partir réseaux bayésiens Moralized Bayes Khosravi point objectifs réseaux logiques Markov permettent envisager prentissage génératif apprentissage prédicats domaine apprentissage criminant valeur prédicat article concentrerons apprentis génératif notre principale contribution étant algorithme innovant apprentissage génératif structure algorithme construit abord graphe prédicats mettant évidence liens entre atomes partageant arguments thétisant chemins pouvant exister apprentissage construction clauses candidates ensuite graphe Chaque chemin transformé clause biais technique heuristique variabilisation algorithme produit clauses longueur croissante commençant clauses longueur intérêt chaque clause ensuite évalué pertinentes étant ajoutées réseau logique Markov final article organisé comme rappelons introduisons quelques définitions section décrivons notre algorithme section expérimentations leurs résultats présentés section concluons section Préliminaires rappelons quelques définitions logique première ordre seront utilisées article considérons langage premier ordre fonction composé ensemble symboles prédicats ensemble constantes ensemble variables atome expression prédicat variables constantes littéral atome littéral positif négation atome littéral négatif parlerons littéral lorsqu comporte constantes littéral variables quand contient variables clause disjonction littéraux longueur clause notée correspond nombre littéraux littéraux variables connectés partagent moins constante variable clause connectée quand existe ordonnance littéraux chaque littéral existe variable constante apparaissant monde aussi appelé parfois interprétation Herbrand affectation valeur vérité atomes données spécification partielle monde chaque atome implicitement inconnu article utilisons hypothèse monde atome données considéré comme réseau logique Markov ensemble formules premier ordre pondérées Associé ensemble constantes définit réseau Markov Pearl chaque correspond atome chaque caractéristique clique correspond instanciation formule poids clique poids formule produite probabilité observer monde nombre instanciations vraies formule poids associé cette formule constante normalisation ensemble atomes souhaite inférer valeur ensemble atomes valeur connue vraisemblance conditionnelle sachant pseudo vraisemblance monde donnée atome valeur vérité couverture Markov Rappelons couverture Markov atome constituée voisins atome réseau présentent dépendance conditionnelle directe celui cadre génératif cherche ensemble formules pondérées optimisant pseudo vraisemblance technique optimisation courante nomme Pereira repose méthode quasi Newton calcul fortement influencé présence clauses contenant grand nombre variables présentant grand nombre instanciations couramment remplacée mesure dérivée introduite Domingos pseudo vraisemblance pondérée définie ensemble prédicats nombre atomes valeur vérité atome valeur dépend utilisateur expériences avons choisi valeur Domingos Apprentissage génératif structure reposant graphe prédicats cette section présentons notre approche apprentissage structures partir données donné entrée éventuellement Apprentissage génératif partir graphes prédicats définissons abord notion graphe prédicats décrivons ensuite comment utilisons apprendre structure Graphe prédicats considérons ensemble prédicats données composée littéraux construits prédicats Définition atome modèle prédicat expression type1 typen typei indique argument Définition entre atomes modèles liste ordonnée couples positions types arguments position identiques décrit entre prédicats dénoté ensemble liens possibles entre leurs atomes modèles Quand atomes modèles partagent argument existe entre Définition formule satisfaisant forme Visid Vjsjd autres variables toutes distinctes Naturellement définitions formule satisfaisant peuvent pliquées négation exemple entre négation Seuls arguments compte Exemple considérons langage composé prédicats AdvisedBy Professor respectivement comme atomes modèles AdvisedBy person person Professor person argument person apparaît position Professor person positions AdvisedBy person person liens possibles entre encore formule satisfaisant dernier Professor AdvisedBy avons AdvisedBy AdvisedBy Notons plusieurs liens considérés exemple AdvisedBy AdvisedBy conduit formule composée littéraux identiques similaire mesure conduisent formules identiques renommage variables aussi laire Professor Professor Professor Professor Définition graphe orienté données paire ordon composé ensemble nœuds ensemble arêtes Chaque correspond prédicat négation Exemple graphe prédicats Algorithme maxLength Créer graphe prédicats Ajouter toutes clauses unitaires apprendre poids longueur maxLength CreerClausesCandidates Algorithm AjouterClausesCandidatesMlnElaguer Retourner existe entre prédicats alors existe arête entre nœuds laquelle associées étiquettes label contenant exemple cardinalité donnant nombre instantiations vraies formule binaire satisfaisant chaque associé poids défini weight nombre arêtes incidentes cardinalité arête coefficient ajustement Exemple figure donne graphe prédicats langage donné exemple avons Professor AdvisedBy supposons avons instantiations vraies formule correspondante données trouvons alors arête entre Professor AdvisedBy label cardinalité Inver sement avons AdvisedBy Professor Quand poids AdvisedBy AdvisedBy weight nombres instantiations formule correspondant liens autres liens instantiations Structure algorithme Etant données données consistante éventuellement nombre entier maxLength spécifiant longueur maximale clauses présentons maintenant algorithme appelé Generative Structure Learning based graph Predicates permettant apprendre générativement structure Apprentissage génératif partir graphes prédicats approches développées Domingos Huynh Mooney explorent manière intensive espace clauses produisant beaucoup clauses inutiles alors celles développées Mihalkova Mooney Domingos recherchent chemins données nécessitant temps calcul important préférons engendrer clauses potentielle intéressantes fondant observations abord semble inutile considérer clauses connectées instances données Ensuite formule connectée autant utile couvre grand nombre atomes données Autrement clauses intéressantes surtout celles assez fréquentes termes nombre instantiations retrouve ainsi notion traditionnelle couverture évident formule connectée fréquente formules connectées moins aussi fréquentes remarques blables utilisés beaucoup stratégies connues exploration proposons ainsi produire clauses candidates commençant clauses binaires formées atomes connectés clauses atomes peuvent alors formées partir clauses binaires celles atomes ainsi suite problème résoudre alors celui recherche ensemble clauses binaires connectées partir desquelles étendre progressivement ensemble clauses candidates Notre algorithme génération clauses repose graphe prédicats chaque prédicat créons nœuds correspondant prédicat négation graphe contient seulement nœuds nombre prédicats langage nombre nœuds incidents dépend nombre arguments prédicat correspondant nombre relations entre types arguments prédicats reste général raison nable calculons seulement cardinalité liens réduisant ainsi calcul nombre instantiations formules binaires recherche chemins graphe alors efficace recherche chemins ensemble atomes particulièrement quand grand nombre prédicats quand beaucoup arguments partagés atomes données Notons cependant chaque chemin connaissons seulement prédicats apparaissent clause informations variables doivent unifiées Beaucoup clauses variables différentes peuvent construites utilisons technique heuristique expliquée section variabiliser chemin obtenir unique clause chaque étape ayant ensemble candidats longueur notre algorithme ajoute chaque clause courant apprend poids correspondant pseudo vraissemblance alors calculée poids cette clause grand seuil Weight amélioration cette mesure candidat deviendra appris courant Parce ajout clause influencer poids clauses ajoutées précé dentes toutes clauses candidates considérées tentons élaguer clauses clause poids inférieur seuil minWeight enlevée retrait permet augmenter pseudo vraisemblance poids associé formule clause reflète importance cette formule poids élevé grande différence probabilité entre monde satisfait formule monde satisfaisant toutes choses ailleurs étant égales Concernant clauses unitaires contenant prédicat poids Algorithme CreerClausesCandidates Initialisation foreach clause ChercherCheminDansGraphe foreach chemin ariabiliser Evaluer weight minWeight Return schématiquement capture distribution marginale prédicats alors clauses longues modélisent dépendances prédicats raison laquelle utile ajouter abord toutes clauses unitaires première étape notre algorithme appelons premier réseau unitaire présentons structure Algorithme section suivante décrivons détail façon générer ensemble clauses candidates chaque étape notre algorithme Créer clauses candidates Comme mentionné dessus utilisons graphe prédicats construire variabiliser produire clauses Comme beaucoup systèmes cherchons ensemble clauses candidates couverture important considérons seulement arêtes cardinalité grande minimum weight weight Rappelons poids proportionnel moyenne cardinalités arêtes incidentes coefficient apparaissant poids utilisé comme paramètre ajustement telles arêtes ensuite variabilisées clauses connectées binaires construire clauses longues étendons chaque chemin correspondant clause candidate arête incidente moins ensemble conduisant ainsi chemin vérifiant propriété précédente processus répété toutes arêtes clause évaluée apprenant poids composé unitaire cette clause calculant mesure associée mesure grande celle unitaire poids clause grand seuil minWeight consi cette clause comme clause candidate algorithme donne différentes étapes construire clauses candidates Considérons maintenant processus variabilisation Généralement variabilise clause remplaçant constantes variables notre disposons chemin graphe prédicats liste arêtes contenant seules infor mations prédicats positions arguments partagés envisageable construire toutes clauses satisfaisant chemin raisons temps calcul produit clauses temps évaluer apprendre final Apprentissage génératif partir graphes prédicats variabilisons heuristiquement chemin produire unique clause connectée réduire nombre variables distinctes clause traitons prédicats ordre fréquence décroissante chemin chaque prédicat effectuons variabilisation abord variables partagées ensuite variables partagées argument prédicat variabilisé nouvelle variable position apparaît label chemin impliquant prédicat Exemple chemin advisedBy advisedBy advisedBy student conduit clause advisedBy advisedBy student convention entre student advisedBy appliqué premier advisedBy partir chemin correspondant clause cherchons chemin variabilisons créer clause clause ignorée Expérimentations Bases avons utilisé trois données disponibles ligne décrit films comporte atomes construits partir prédicats constantes décrit département académique comporte atomes construits partir prédicats constantes grande trois bases décrit collection citations publications informatique bâtie prédicats constantes comporte 70367 atomes différence bases précédents partie atomes fournissent atomes vrais Systèmes méthodologie comparons méthodes Learning using Structural Motifs Domingos repose observa données relationnelle contient schémas variations mêmes motifs structurels mécanisme parcours aléatoire utilisé identifier objets fortement connectés regrouper relations associées motif recherche clauses ensuite motifs ainsi créés apprendre final ajoute toutes clauses apprend leurs poids optimaux supprime celles poids inférieur seuil donné Heuristic Generative Structure learning approche bottom exemples fournis bâtit ensemble tables booléennes donnant synthétique atomes connectés Chaque colonne telle table correspond littéral variable chaque ligne indique semble connections observées entre littéraux partir atome donné partir tables littéraux dépendants identifiés grâce algorithme Shrink Markov Networks Bromberg regroupés former clauses candidates meilleures clauses candidates intégrées alchemy washington bases valeurs présentées corres pondent moyenne ensemble prédicats trois algorithmes implémentés dessus librairie Alchemy développée équipe Domingos Notons existence troisième algorithme récent nommé Khosravi permettant apprentissage structure cadre génératif Cependant principes jacents algorithme permettent place validation croisée envisageons étudier ultérieurement adaptations possibles permettraient œuvre Chaque données découpé parties effectuer validation croisée chaque algorithme chaque données avons mesuré chaque prédicat vraisemblance conditionnelle courbe précision rappel mesures constituant standard utilisé plupart publications domaine mesure directement qualité probabilités produites prédicat donné correspond moyenne valeurs observées atomes correspondant montre capacité algorithme inférer positifs données calculer prédicat donné varier seuil duquel atome considéré différents couples précision rappel observés valeur atomes cibles inférée algorithme Domingos Enfin avons utilisé fourni Davis Goadrich calculer paramètres utilisés respectivement Domingos nombre maximal littéraux clause algorithmes limiter espace recherche coefficient ajustement limitant ainsi nombre nœuds graphe exploré limitons nombre prédicats semblables chaque clause conserver temps calcul raisonnables apprentissage poids conduits machine disposant processeur mémoire Résultats avons effectué validation croisée apprenant quatre parties inférant chaque littéral atome suivant signe littéral cinquième partie probabilité processus itéré changeant partie utilisée partir valeurs avons calculer chaque prédicat précision rappel avons ensuite moyenné informations seulement différentes parties aussi ensemble prédicats chaque cette pratique étant norme publications domaine Apprentissage génératif partir graphes prédicats tableau présente ainsi temps exécution apprentissage férence trois algorithmes noter concernant résultats diffèrent légèrement Domingos ayons utilisé mêmes mètres explique trois raisons abord concernant auteurs quelques prédicats égalité tandis avons gardé Ensuite avons autorisé clauses comportant jusqu prédicats Enfin comme plaçons cadre génératif poids formules appris seule optimisation globale réseau alors Domingos réapprennent poids chaque prédicat optimiser réseau manière discriminante pouvons observer trois données surpasse termes vérifie seulement valeurs moyennes présen cette table aussi valeurs moyennes prédicat chaque fragment rapide particulièrement concentre effet seules bonnes arêtes correspondant connexions fréquentes seulement clause chaque chemin espace recherche moins vaste celui revanche dernier repose synthétique chaque données appelée hyper graphe généralisé lifted hypergraph hyper graphes généralisés calculés seule apprentissage utilisés ensuite pendant étape validation croisée trouve considérablement accélérée tableau détaille valeurs moyennes chaque prédicat algorithmes constater comporte mieux plupart prédicats revanche produit résultats remarquables prédicats comme SameCourse SamePerson SameProject Notons dicats particuliers atomes vrais seulement leurs arguments identiques processus évaluation occasion mettre avant divers points lesquels envisageons pencher avenir valeur coefficient ajustement impact clair calculé petit considère arêtes clauses généré général grand présente meilleur score global Néanmoins accroissement taille inférence coûteuse voire infaisable algorithmes dispo nibles équilibre entre performance globale termes temps calcul constituera point intéressant étudier structure apprise globalement satisfaisante inférences résulte nettement meilleurs certains prédicats autres compréhension cette variabilité réduction constituent perspective intéressante retrouve fréquement clauses candidates diffèrent seulement variabili sation exemple différent seulement position traitement approprié telles configurations devrait permettre réduire temps exécution Conclusion avons présenté algorithme apprentissage structure Réseau Logique Markov cadre génératif algorithme graphe prédicats PREDICATES ADVISEDBY COURSELEVEL HASPOSITION INPHASE PROFESSOR PROJECTMEMBER PUBLICATION SAMECOURSE SAMEPERSON SAMEPROJECT STUDENT TEMPADVISEDBY YEARSINPROGRAM TAUGHTBY MOYENNE Résultats prédicat utilise technique heuristique variabilisation produire clauses candidates expériences menées tendent montrer supériorité rapport Elles également permis identifier plusieurs directions recherches ultérieures envisageons maintenant appliquer domaines grands riches notamment domaines comportant prédicats exploiter avantages notre approche étudions également possiblités comparer autres algorithmes récents Références Ferilli Esposito Structure learning markov logic networks through iterated local search Press Bromberg Margaritis Honavar Efficient markov network structure disco using independence tests Artif Davis Goadrich relationship between precision recall curves Dehaspe Maximum entropy modeling clausal constraints Springer Verlag Exbrayat Vrain Generative structure learning markov logic networks STAIRS Press Friedman Getoor Koller Pfeffer Learning probabilistic relational models Getoor Taskar Introduction Statistical Relational Learning Press Apprentissage génératif partir graphes prédicats Huynh Mooney Discriminative structure parameter learning markov logic networks Kersting Raedt Bayesian logic programs Technical report Khosravi Schulte Structure learning markov logic networks descriptive attributes Domingos Learning structure markov logic networks Domingos Learning markov logic network structure hypergraph lifting Domingos Learning markov logic networks using structural motifs Omnipress Mihalkova Mooney Bottom learning markov logic network structure Muggleton Stochastic logic programs Press Advances Inductive Logic Programming Neville Jensen Dependency networks relational Computer Society Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible rence Morgan Kaufmann Publishers Domingos Sound efficient inference probabilistic determi nistic dependencies Press Richardson Domingos Markov logic networks Learn Kameya Prism language symbolic statistical modeling Pereira Shallow parsing conditional random fields NAACL Association Computational Linguistics Summary Markov Logic Networks combine Markov Networks first order logic attaching weights first order formulas viewing templates features Markov works Learning decomposed structure weights learning paper present algorithm learn generatively structure Markov Logic Networks algorithm graph predicates which built predicates training database heuristical variabilization technique order produce candidate clauses According first experiments approach appears promising