Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles Rakia Jaziri Mustapha Lebbah YounËs Bennani Université Paris Clément 93430 Villetaneuse Prénom paris13 Institut National Audiovisuel Europe Cedex Marne rjaziri Résumé proposons papier nouvel algorithme classifica supervisée modèle mélange topologique données independently identically distributed nouveau paradigme probabiliste plonge cartes topologiques probabilistes formulation forme chaînes Markov cachées cette formulation généra observation instant donné temps conditionnée états voisins instant temps Ainsi grande proximité impliquera grande probabilité contribution génération approche proposée évaluée utilisant données séquentielles réelles issues bases Institut Nationale Audiovisuel résultats obtenus encourageants prometteurs Introduction Plusieurs techniques classification automatique données séquentielles veloppées dernières années Elles appliquées différents domaines reconnaissance caractères manuscrits reconnaissance parole étude mobilité objets vidéos Buzan analyse séquences biologiques méthode facile traiter données serait plement ignorer aspect temporel traiter observations comme données pendantes independent identically distributed beaucoup applications hypothèse données pauvres perdant information séquentielle beaucoup applications traitement décomposé étapes première étape classification partitionnement données hypothèse deuxième étape résultat classification utilisé construire modèle probabiliste relaxant contrainte naturelles manières faire utiliser modèle Markov cartes topologiques Kohonen intéressantes leurs apports topolo giques classification supervisée leurs capacités résumer manière simple Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles ensemble données multi dimensionnelles Elles permettent comprimer grandes quantités données regroupant individus similaires classes autre projeter classes obtenues façon linéaire carte graphe procédé permet effectuer réduction dimension permettant ainsi visualiser structure données dimensions respectant topologie données sorte données proches espace multi dimensionnel départ aient images proches carte plupart modèles hybrides cartes topologiques utilisées comme traitement quantification vectorielle Hidden Markov Models ensuite utilisés processus transformation avancés prise compte dynamique résultats classification dépendent essentiellement choix paramètres distribution probabilité topologie carte organisation heureusement paradigmes organisation peuvent facilement transférés données Différentes approches développées intégrer informa temporelle carte organisation variété modèles existe cartes récurrentes carte Kohonen temporelle récurrente récursive RecSOM données structurées SOMSD Strickert Hammer Hagenbuchner proposons article nouvelle approche carte topologique probabiliste dédiée données séquentielles multivariées appelons carte organisatrice probabiliste données séquentielles PrSOMS supposons données quentielles générées selon processus markovien modèles Markov cachés gurent parmi meilleures approches adaptées traitements séquences étant donné capacité traiter séquences longueurs variables pouvoir modéliser dynamique phénomène décrit suites événements modélisation graphique probabiliste motive différentes structures graphiques basées Bengio modèles graphiques fournissent formalisme général décrire analy telles structures conséquent important avoir algorithmes capables déduire partir ensemble données séquences seulement probabilité distribution aussi structure topologique modèle nombre états transitions inter connectent Malheureusement cette tâche difficile solutions partielles surmonter limites travaux récents Bouchaffra proposent nouveau original paradigme appelé topolo gical manipule nœuds graphe associé transitions espace Euclidien Cette approche modélise structure locale extrait forme définissant unité information comme forme composée groupe symboles séquence autre modèle souvent présenté comme version probabiliste carte organisation nommé étendu modèle série chronologique variées through Bishop Olier Vellido données structurées Bacciu Récemment Yamaguchi auteur propose extension algorithme Organizing Mixture Model Verbeek séries chronologiques multivariées SOHMMs organizing hidden Markov models Cependant manière modèles réalisent organisation topographique différente celles utilisées algorithmes notre modèle PrSOMS article sommes intéressés problématique analyse données structurées séquences elles soient longueurs fixes variables objectif cette Jaziri proche construire nouveau modèle organisé génératif ensemble données modèle proposé formalisme probabiliste cartes topologiques Anouar Lebbah modèle génératif utilisé conséquent consiste estimer paramètres modèle maximisant vraisemblance ensemble données séquentielles algorithme apprentissage proposons application algorithme standard Espérance Maximisation article organisé comme sections présentent notre approche probabiliste classi fication données séquentielles PrSOMS section décrit dispositif expérimental évaluations Enfin section propose conclusion perspectives Principe modèle génératif PrSOMS notre modèle considérons carte topologique formant grille comme chaîne Markov génération variable observable instant donné temps conditionnée états voisins instant temps Ainsi grande proximité implique grande probabilité contribution génération Cette proximité quantifiée utilisant fonction voisinage formalisme présentons valable toutes structures Supposons séquence observations élément séquence taille principale problématique estimer paramètres modèle apprentissage PrSOMS suppose architecture carte modélisant aussi représentée treillis topologie discrète définie graphe orienté notera nombre cellules noeuds états chaque paire cellules graphe distance définie comme longueur courte chaîne cellules états suppose chaque élément séquence observations généré processus suivant commence associer chaque cellule probabilité vecteur espace données suite sélectionne cellule carte selon probabilité priori chaque cellule sélectionne cellule selon probabilité conditionnelle Toutes cellules instant contribuent génération élément probabilité selon proximité cellule décrite probabilité introduisons notamment variables binaires aléatoires comme variables cachées dimension lesquelles éléments particuliers égaux autres éléments égaux composantes indiquent couple états responsable génération élément observation Utilisant cette notation réécrire probabilité comme introduire processus organisation apprentissage modèle mélange suppose définie manière modèles cartes probabilistes fonction voisinage dépend paramètre appelé température AinsiK définit chaque chaîne Markov région voisinage Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles graphe paramètre permet contrôler taille voisinage influence cellule donnée carte valeur varie entre valeurs ensemble toutes variables cachées chaque ligne associé chaque élément séquence Chaque observation séquence associée couple variables cachées responsables génération ensemble complet données réfère données observables comme incomplètes Ainsi modèle générateur séquence défini manière suivante Puisque distribution simplifier caractéristique importante distributions probabilités variables multiples celle indépendance conditionnelle Luttrel suppose distribution conditionnelle sachant dépend variable cachée Souvent cette hypothèse utilisée modèles graphiques ainsi réécrire distribution marginale comme Paramètres modèle estimation Considérant carte représente modèle Markov ainsi distribution dépend variable latente précédente Cette dépendance représentée probabilité conditionnelle Puisque variables latentes variables naires dimension cette distribution conditionnelle correspond table probabilité éléments connus comme probabilités transition notées écrire distribution conditionnelle explicitement cette forme initial particulier puisqu cellule parente ainsi distribution marginale représentée vecteur probabilités éléments ainsi paramètres modèle complétés définissant distributions conditionnelles variables observées ensemble paramètres définissent distribution connue comme probabilités émission modèle représenter probabilités émission forme Jaziri probabilité jointe variables observables variables latentes expri décrit ensemble paramètres manipulent modèle évident maximiser fonction vraisemblance cause complexité expres utilise algorithme trouver paramètres misent fonction vraisemblance algorithme commence quelques sélections initiales paramètres modèle étape Estimation prend valeurs paramètres trouve distribution posteriori variables latentes Ensuite utilise cette distribution posteriori évaluer espérance logarithme vraisemblance séquences complètes données fonction paramètres obtenir fonction objective définie Ainsi réécrire fonction cette étape introduire quelques notations utiliser noter distribution marginale posteriori variables latentes noter distribution posteriori jointe variables latentes successives Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles observe fonction objective définie comme somme quatre termes premier terme dépend probabilités initiales deuxième terme dépend probabilités transition troisième terme dépend ensemble paramètres probabilité émission quatrième constante maximisation rapport effectuée séparément maximisation probabilités initiales manière modèles probabilistes utilise forme explicite distribution probabilités initiales probabilité initiale ensuite obtenue manière suivante maximisation Probabilités transition Comme traditionnels notre modèle utilise caché valeur discrète distribution multinomiale sachant valeurs précédentes notre modèle modèle premier ordre paramètres calculée manière suivante maximisation probabilités émission ensemble paramètres dépend distribution utilisée présentons plication utilisant gaussienne probabilités émission densité sphérique gaussienne définie moyenne dimension données entrée matrice covariance définie écart matrice identité maximisation fonction fournit expressions connues dimension élément contexte particulier modèle Markov caché utiliser algorithme forward backward Rabiner puisqu utilise structure graphe organiser données séquentielles manière explicite Quelques formules similaires chaînes traditionnelles utilise structure graphe Jaziri Expérimentations analyse séquences audiovisuels comme problème analyse quences multidimensionnelles cette partie allons discuter notre approche données réelles issue avons appliqué aussi autres bases bliques taille article sommes contenté application réelle validation réalisée expert domaine croisement guide programme visualisation segments audiovisuels données composent séquences longueurs variables chaines Chaque séquence représente différents segments diffusés chaine télévision pendant journée Chaque segment composante multidimensionnelle caractérisée variables variables validées expert domaine Elles contiennent descripteurs nombre répétitions intervalle durée autres variables pouvons divulguer papier rappelons problème détecter automatiquement segments quence homogènes reconstruire séquences réunification segments préalablement identifiés comme segment programme autre avons commencé appliquer notre approche PrSOMS données séquence probable obtenue utilisant algorithme Viterbi Rabiner Ainsi permet affecter chaque élément séquence présentons expérimentation évalue pouvoir structuration notre approche figure schématise carte PrSOMS états latents représentés carrés échelle fonction cardinalité éléments toutes séquences modèle capturées affectées algorithme Viterbi lignes rouges représentent chemin Viterbi chaque séquence largeur lignes reflète nombre transition entre états figure schématise profils prototypes associés chaque centre distribution gaussienne Chaque cellule représentée vecteur composantes carte topologique PrSOMS permet visualiser partition Ainsi experts domaine pourraient utiliser cellules analyser certaines caractéristiques audiovisuel effet faisant analyse visuelle segments chaque cellule avons tirer caractéristiques propres chaque chaine distinguer autres utile experts domaine figures permettent analyser toutes séquences chaînes temps effet distribution séquences différentes carte PrSOMS devrait sensiblement différente allons illustrer comparaison entre chemins Viterbi donnés chaines télévision différentes basant illustration principales différences visualisation séquences figures affichent respectivement carte représentant chemin Viterbi calculé chaine chaine représentation correspondant chaine figure compact celle chaine figure étant donnée séquences chaine moins variables celle chaine chaine concentrée partie inférieure gauche droite carte Après analyse chaine1 montre comportement chaine généra chaine comportement chaine information figures montrent chemin Viterbi séquences passant mêmes états identifier similarité entre séquences figures affichent respectivement images extraites cellules droite gauche constatons majorité vidéos capturées correspondent programmes courts météo nouvelles brèves Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles Carte PrSOMS carrés lignes indiquent respectivement cardinalité cellules transitions capturées utilisant algorithme Viterbi profils associés chaque cellule publicités caractéristique chaine figure affiche films capturés gauche chaine chaîne généraliste Notre approche détecte structure jacente résultats montrent précision temporelle conforme vérité terrain meilleure celle indiquée guides programmes fournis chaînes télévision rapprocher partitionnement experts avons appliqué classification ascendante hiérarchique états cellules carte PrSOMS obtenir clusters relaxant contrainte figure montre carte PrSOMS segmentée abord remarquons notre rimentations inter programmes Publicités Jingle météos diffusés moins journée chaines télévisions Deuxièmement seuls environ programmes courts répétés précision programmes extraits également évaluée début respectivement chaque programme extrait rapport démarrage effectif respectivement donné observations réelles suite visualisons chaque cluster carte PrSOMS ensemble prototypes images composantes figure avons remarqué clusters composés contenu homogène effet existe clusters uniquement émissions autres publicités avons constaté aussi segments pétés regroupent ensemble cellules voisines remarquons aussi segments inter programme classés suivant catégorie publicité bande annonce parrainage jingle notre classification automatique parfaitement conforme observations réelles alors indicateurs guide assez décalés résul encourageant difficulté majeure qualité résultats dépend détection répétitions nombre variables utilisées clustering résultats obtenus montrent notre approche effectué bonne segmentation données montre plupart inter programmes diffusés plusieurs programmes courts partagent nombreuse caractéristiques perturbe notre approche Jaziri chaine chaine séquence chaine1 séquence chaine Images vidéos capturées droite carte Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles Images vidéos capturées inférieur gauche carte Quelques images décrivant chaque cluster Jaziri carte PrSOMS segmentée Conclusion Notre approche présente nouvelle approche capturer modéliser information topographique présente données séquentielles approche proposée offre également nouvel outil visualisation topographique ensemble données séquentielles adaptée séquences multidimensionnelles observations tient faible calcul approche proposée évaluée utilisant données réelles issues Institut Nationale Audiovisuel résultats montrent précision temporelle conforme vérité terrain meilleure celle indiquée guides programmes fournis chaînes télévision Comme perspectives voulons appliquer notre approche séquences binaires catégorielles exploiter pouvoir visualisation notre approche Remerciement travail réalisé cadre thèse CIFRE remercions vivement Monsieur Hugues CHENOT remarques pertinentes Références Anouar Badran Thiria organizing probabilistic approach Proceedings Workshop Organizing Espoo Finland Bacciu Micheli Sperduti Compositional generative mapping structured Proceedings International Joint Conference Neural Networks IJCNN Bengio Frasconi input output architecture Classification probabiliste supervisÈe visualisation donnÈes sÈquentielles Bishop Pattern Recognition Machine Learning Springer Science Business Media Spring Street 10013 Bouchaffra Embedding based models euclidean space topological hidden markov models ICPR08 Buzan Sclaroff Kollios Extraction clustering motion trajectories video International Conference Pattern Recognition Hagenbuchner Sperduti Member organizing adaptive processing structured Transactions Neural Networks Kohonen organizing Springer Berlin Lebbah Rogovschi Bennani Besom Bernoulli organizing Luttrel bayesian analysis organizing Neural Computing Olier Vellido Advances clustering visualization series using through Neural Marzal MartÌn Ramos garijo Bleda template based recognition system handwritten characters Journal Information Science Engineering Rabiner tutorial hidden markov models selected applications speech recognition Proceedings Strickert Hammer Neural sequences Proceedings Workshop Organizing Networks Kyushu Institute Technology Verbeek Vlassis Krose organizing mixture models Neurocompu Yamaguchi organizing hidden markov models Mendis Bouzerdoum Neural Information Processing Models Applications Volume Lecture Notes Computer Science Springer Berlin Heidelberg Summary present generative approach learn probabilistic Organizing independent identically distributed model defines dimensional manifold allowing friendly visualizations yield topology preserving model learning behavior advantages probabilistic models paradigm Hidden Markov Models formalism introduces topologi relationships between states allows advantage known classical views associated topographic demonstrate approach world French National Audiovisual Institute