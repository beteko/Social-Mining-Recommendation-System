outil classification visualisation grands volumes données mixtes christophe candillier noureddine mouaddib entreprise géobs avenue thébaudières 44800 saint herblain christophe candillier nantes candillier laboratoire informatique nantes atlantique houssinière 44322 nantes cedex mouaddib nantes sciences nantes résumé avons conçu outil classification données original détaillons présent article outil comporte module création résumés module affichage module création résumés prend charge données mixtes qualitatives quantitatives ainsi grands volumes données utilisant méthode classification incrémentale agglomérative originale module visualisation permet lecture aisée résumés grâce interface graphique évoluée permettant présentation exploration résumés forme hiérarchie profils tableau profils chaque profil donne manière claire informations importantes relatives résumé données correspondant lecture hiérarchie tableau aussi grandement facilitée choix ordre optimal présentation variables résumés introduction discuterons abord algorithme classification utilisé avantages inconvénients intéresserons ensuite visualisation résumés produits comprendra calcul ordre optimal résumés variables ainsi visualisation forme hiérarchie profils résumés forme tableau profils finalement illustrerons fonctionnement outil analyse données socioprofessionnelles paris petite couronne outil classification préliminaires outils classification divers variés regrouper individus semblables classe berkin principales familles méthodes partitionnement méthodes hiérarchiques premières construisent directement partitions cherchent ensuite améliorer dernières peuvent scindées entre méthodes agglomération créent outil classification visualisation grands volumes données mixtes hiérarchie ascendante procédant regroupements successifs méthodes division créent hiérarchie descendante procédant divisions successives familles convient ajouter familles méthodes historiquement récentes recoupent parfois anciennes familles connues méthodes densité groupent objets situés zones fortes densités méthodes grille appliquent grille multi niveaux maillage multi niveaux espace données travaillent cellules créées maillage notre intéressons plutôt stratégies permettant prise compte grands volumes données stratégies principalement suivantes échantillonnage reste méthode utilisée principe simple partir échantillon réalisé utilise méthode classification choix classes déterminées échantillon reste données seule chaque individu étant affecté classe proche utilisé clara kaufman rousseeuw clarans autre stratégie création arbre hiérarchique classes manière descendante incrémentale chaque nouvel individu descend classe racine jusqu classe basse laquelle incorporé descente divers opérateurs sélection création division fusion classes peuvent appliqués méthode utilisée cobweb fisher birch zhang autre technique création bulles données bubbles breunig résumés données chaque bulle possède centre derniers initialisés résumés issus birch échantillon bulles données ensuite remplies individus assignant bulles proches certains individus distants bulles données restent affectés autre technique utilisation graphe voisinage permet emploi méthodes partition graphe rapides telles chameleon karypis cependant principal obstacle vient construction graphe voisinage coûteuse nombre dimensions supérieur trois cadre méthodes classification basées densité dimensions utilisation permet indexation objets spatiaux recherche rapide objets compris certain rayon index spatial utilisent dbscan ester optics ankerst autre stratégie création intervalles chacune dimensions partagée intervalles chaque individu ensuite affecté intervalles auxquels appartient algorithmes classification travaillent alors partir intervalles cette méthode utilisée clique agrawal wavecluster sheikholeslami création cellules extension création intervalles chaque dimension étant partagée intervalles découpage espace cellules chaque individu ensuite affecté cellule laquelle appartient cellules voisines connectées cellules vides supprimées algorithmes classification travaillent alors partir cellules cette méthode utilisée denclue hinneburg sting cependant noter nombre cellules croit exponentiellement nombre dimensions utilisation cette stratégie ainsi limitée faible nombre dimensions stratégies décrites précédemment traitent grands volumes données façons diverses cependant existe certaines catégories intéressantes comme famille christophe candillier noureddine mouaddib méthodes incrémentales celles doivent réceptionner données interdiction conserver mémoire totalité importante données reçues résumer principe respect contraintes suivantes lecture données seule passe individus compte processus instant stoppé redémarré résultat temporaire disponible moment méthodes incrémentales ainsi particulièrement adaptées entrepôts données effet classification effectue ajout nouvelles données avoir recommencer traitement depuis début parmi algorithmes incrémentaux citer birch cobweb précédemment méthode classification ascendante approximative mettre point notre algorithme sommes partis contraintes respecter algorithme incrémental prenant charge grands volumes données avons aussi défini algorithme devait utiliser classes résumés permettant création nouvelles classes individus atypiques ainsi comme défini charikar principal problème résoudre notre algorithme incrémental classes maximum suivant classification classes existantes arrivée nouvel individu classer traduit inclusion individu classe existante création nouvelle classe contenir fusion classes existantes maintenir nombre classes égale nécessité réaliser fusions place notre algorithme catégorie algorithmes ascendants agglomératifs partie classification ascendante hiérarchique étudiant fonctionnement sommes arrivés conclusion algorithme répond notre problème moyennant quelques adaptations ignore distinction entre individus classes individus considérés comme classes ayant individu ainsi ajout nouvel individu choix faire individu incorporé nouvelle classe classes semblables fusionnées cette méthode avantage centrer problème nouvel individu arrivant contre recherche classes semblables coûteuse algorithme algorithme etape liste résumés etape individu classer faire créer nouvelle classe individu ajouter cette classe liste résumés classes alors trouver classes semblables yxdccd fusionner nouvelle classe ajouter nouvelle classe enlever anciennes classes cccrr sinon faire outil classification visualisation grands volumes données mixtes définissons notre algorithme classification ascendante approximative comme indiqué paramètre nombre résumés maximum mesure distance dissimilarité choisie entre classes comme résumer information avons systématiquement utilisé distance représente perte inertie celle étant bonne représentation information ainsi fusionnant classes ayant distance faible minimisons perte information variante algorithme classification ascendante hiérarchique approximative reprend ajoutant ultime étape lorsqu nouveaux individus ajouter continue fusionner classes semblables jusqu obtenir classe contenant individus pendant cette opération mémorise hiérarchie nœuds cette hiérarchie cette dernière étape permet construire arbre classification résumés classique réalisée résumés utilisation distance choix nombre résumés permet paramétrer précision algorithme prenant aussi grand grand nombre individus traiter équivalente effet durant étape aucune fusion chaque résumé contient individu choix offre alternative entre algorithme rapide approximatif algorithme précis fixer valeur sommes partis principe utilisateur souhaite analyse globale totalité données moins classes établissons niveau résumés niveau supérieur prenant résumés obtenir classes acceptables partir résumés gardant traitement rapide informations nécessaires analyse calculées chaque fusion réalisée algorithme chaque variable chaque résumé avons informations suivantes valeur minimum valeur maximum valeur moyenne écart valeur maximum obtient prenant valeur maximum résumés fusionner valeur minimum obtient manière équivalente valeur moyenne écart déduisent simplement somme valeurs somme carrés valeurs poids nombre individus obtenus sommations valeurs respectives résumés fusionner autres informations telles médiane quartiles malheureusement calculables rapidement elles nécessitent réordonner totalité valeurs stocker individus mémoire interdisons cadre algorithme prenant charge grands volumes données complexité algorithme appréhendée matrice distances permettant calculer distance minimale entre résumés cette matrice symétrique taille nécessaire calculer distances ajout nouveau résumé encore distances fusion ailleurs recherche distance minimale nécessite parcours toute matrice accès étapes étant réalisées approximativement durant algorithme complexité temporelle algorithme nkokkno recherche distance minimum étant étape coûteuse complexité temporelle algorithme linéaire rapport nombre individus optimiser recherche distance minimale avons utilisé index arbre binaire équilibré matrice distances distance ainsi recherche distance minimale rapide effectue contrepartie insertion suppression distances index insertions ajout insertions suppressions christophe candillier noureddine mouaddib fusion complexité finale ainsi kkkno loglog4 étapes coûteuses étant accès index complexité finale toutefois moindre suivant nombre résumés lorsque complexité temporelle algorithme standard algorithme standard optimisé lorsque nombre résumés utilisés faible rapport nombre classes réelles subir effet dérive ordre mauvais lequel individus ajoutés traduit certains individus proches autre résumé résumé auquel appartiennent éliminer cette dérive faudrait introduire individus proches entre abord finir individus éloignés malheureusement nécessiterait calculer matrice distances entre individus reviendrait finalement réaliser totalité données cependant solution simple utilisation moyennes traitement permet réduire rapidement nombre individus classés améliorant qualité globale résumés principal inconvénient chaque itération moyennes nécessite parcours totalité individus établirons modalités utilisation moyennes partie expérimentation prétraitements nécessaires premier prétraitement nécessaire disjonction variables qualitatives existe beaucoup types distances données mixtes mazlack coppock cependant pratique courante considérer distance finale comme somme distance adaptée variables qualitatives distance adaptée variables quantitatives adoptons démarche similaire transformant disjonction chaque variable qualitative autant variables quantitatives filles modalités pondérant chacune variables filles ainsi chaque individu valeur variable fille représentant modalité vaudra individu possède cette modalité sinon moyenne variable fille résumé correspond fraction individus possédant cette modalité valeurs minimum maximum résumé peuvent intérêt cette méthode intégrer façon transparente données qualitatives utilisant méthodes uniquement valables données quantitatives cependant variables ayant beaucoup modalités soient avantagées rapport autres variables pondérons chacune variables filles issues variable qualitative poids global variables filles fasse poids seule variable second prétraitement normalisation données consiste simplement centrer réduire toutes données compris variables filles créées partir variables qualitatives devons calculer moyenne écart chaque variable cette opération réalisée préalablement seule lecture totalité données solution alternative calculer informations échantillon moyenne écart chaque variable indiqués réalise normalisation individus lecture outil classification visualisation grands volumes données mixtes expérimentations comparaison moyennes mesurer performances avons comparée algorithme standard moyennes avons comme indicateur inertie perdue résumés après tests effectués données réelles comporter moins moyennes lorsque nombre résumés faible notamment cause effet dérive contre lorsque augmente suffisamment nombre résumés demandés devient alors meilleure moyennes davantage pénalisées mauvais choix partitions départ premiers individus soumis algorithme tandis affranchit rapidement mécanisme fusions autre effet dérive atténue autre alternative corriger effet dérive utilisation moyennes pendant faible nombre itérations entre trois itérations résumés trouvés zones ilots regroupés information statistique paris petite couronne variables socioprofessionnelles seule meilleure moyennes partir données simulées contre meilleure effet dérive faible moyennes pénalisées mauvais choix partitions départ données classes réelles résumés inertie totale itérs itérs itérs itérs paris inconnu 73953 33255 31885 28787 27366 27113 paris inconnu 73953 23111 25282 20201 21087 19552 paris inconnu 73953 16350 20920 14787 17187 14464 paris inconnu 73953 10935 16580 10278 14908 10218 paris inconnu 73953 12144 11301 simulées 500000 11623 73397 11595 73373 11592 simulées 500000 281071 374464 280757 360495 280757 simulées 10000 500000 407919 427235 404225 416338 403617 comparaison qualités résumés outil visualisation méthode optimisation ordre variables résumés optimisation variables optimisation résumés problèmes similaires suite utiliserons terme individu désigner résumés variables selon problème optimisation concerné différentes heuristiques moins efficaces permettent trouver ordre assez basique fonction moyenne valeurs normalisées individu eisen intérêt simple rapide efficace variables toutes corrélées positivement autre méthode selon premier projection analyse composantes principales cependant cette méthode efficace variables toutes corrélées autre façon aborder problème définir façon suivante parmi ordres possibles ordre optimal celui minimise somme distances entre individus consécutifs problème ordonnancement belloni lucena connu sequential ordering problem linear christophe candillier noureddine mouaddib ordering problem variante problème voyageur commerce traveling salesman problem problème étant complet recherche solution exacte nécessite évaluation ordres différents individus réalisable supérieur utilise alors heuristiques toutefois problème simplifié création hiérarchie binaire individus existe algorithmes capables résoudre problème manière exacte joseph complexité temps espace ainsi trouver ordre optimal résumés utilisons directement algorithme hiérarchie résumés utilisant distance outil classification trouver ordre optimal variables serait naturel réaliser classification ascendante hiérarchique conjointe créer hiérarchie variables cependant impossible grands volumes données utilisons place matrice corrélation variables beaucoup compacte calcul matrice corrélation faire seule lecture données normalisées calculons matrice corrélation cours classification données mesure arrivée individus réalisons ensuite matrice corrélation utilisant distance appliquons algorithme recherche ordre optimal hiérarchie variables visualisation hiérarchie résumés représentations utilisées décrire variable quantitative histogramme fréquences inconvénient plutôt volumineux représentation synthétique boîte moustaches donne informations telles valeurs minimale maximale médiane quartiles autre décrire plusieurs variables graphique information possible utiliser représentation coordonnées parallèles siirtola celle correspondre chaque variable vertical lequel représenté information point ligne reliant points proche proche constitue représentation cette information toutes variables graphique cependant raisons lisibilité variables doivent similaires défaut normalisées combinant boîte moustaches représentation coordonnées parallèles ainsi possible visualiser simultanément informations toutes variables avons évoluer cette dernière représentation rendre simple accessible supprimant toutes informations chiffrées graphique résultat final étant profil résumé avons défini chaque origine moyenne générale variable échelle écart général variable ainsi chaque information exprime manière centrée réduite valeur représentant information quelconque moyenne maximum minimum médiane position mgxxp lecture avons réciproquement mggxpx légende globale valeurs indiquées chaque variable profil informations suivantes représentées chaque variable moyenne résumé représentée point couleur écart résumé représenté barres verticales autre point christophe candillier noureddine mouaddib niveaux lecture premier niveau lecture graphique globale rapide permettant connaître moyenne variable classe représentativité moyenne codée forme symbolique cercle rouge plein moyenne écarts types généraux dessus moyenne générale cercle rouge entre cercle blanc entre cercle entre cercle plein dessous taille intervalle écart entre valeur minimum maximum codée symboles suivants valeur moyenne classe considérée comme représentative taille intervalle valeurs inférieure égale écarts types généraux assez représentative entre représentative deuxième niveau lecture donne informations chiffrées moyenne minimum maximum variable classe application analyse catégories socioprofessionnelles paris petite couronne données zones paris département petite couronne départements elles décrites variables quantitatives pourcentage population chaque catégorie socioprofessionnelle variable qualitative indiquant nature habitat administratif divers réalisée résumés chaque pondéré initialement population résidant choisissant regroupement résumés classes obtenons résultats suivants nombre objets étant population résidante hiérarchie classes profil objets objets objets objets objets objets objets objets objets variables poids moyenne générale ecarttype général cadre revmoymen artisan retraite sansact type_iris type_iris type_iris intermed employe ouvrier outil classification visualisation grands volumes données mixtes cette classification évidence types catégories aisées aisées catégories moins aisées rouge remarquera aussi chaque variable chaque classe moyenne représentative taille relativement importante intervalles valeurs conclusions classe seront valables général applicables chaque cette classe interprétation classe rouge foncé suivante cette classe comportent généralement ouvriers employés personnes activité moyenne moins cadres intermédiaires moyenne revenus plutôt faibles géographiquement cette classe surtout présents paris façon dispersée remarque aussi administratif divers habitat discriminant entre classes tableau profils classes carte correspondant classes cadre revmoymen artisan retraite sansact type_iris type_iris type_iris intermed employe ouvrier classe objets classe objets classe objets classe objets classe objets dessous moyenne dessous moyenne proche moyenne dessus moyenne dessus moyenne moyenne représentative moyenne assez représentative moyenne représentative christophe candillier noureddine mouaddib conclusion notre outil robuste nécessite paramètres principal inconvénient actuel paramétrage distance pondérations forcément adaptées pourquoi envisageons remplacer utilisation distance euclidienne pondérée distance mahalanobis calcul distance autre point problématique ordre optimal résumés entraîne forcément ordre optimal classes construites hiérarchie étudions possibilité calculer ordre classes hiérarchie durant exploration références agrawal gehrke gunopulos raghavan automatic subspace clustering dimensional mining applications sigmod international conference management pages ankerst breunig kriegel sander optics ordering points identify clustering structure sigmod international conference management joseph demaine gifford hamel jaakkola srebro clustering optimal ordering expression international workshop algorithms bioinformatics belloni lucena lagrangian based heuristics linear ordering problem metaheuristics international conference juillet berkhin survey clustering mining techniques technical report accrue software diday analysis symbolic springer 66619 breunig kriegel kröger sander bubbles quality preserving performance boosting hierarchical clustering sigmod record special interest group management charikar chekuri feder motwani incremental clustering dynamic information retrieval symposium theory computing pages eisen spellman brown botstein cluster analysis display genome expression patterns proceedings national academy sciences 14863 14868 décembre ester kriegel sander density based algorithm discovering clusters large spatial databases noise international conference knowledge discovery mining fisher knowledge acquisition incremental conceptual clustering machine learning rastogi efficient clustering algorithm large databases sigmod international conference management pages outil classification visualisation grands volumes données mixtes hinneburg efficient approach clustering large multimedia databases noise knowledge discovery mining pages murty flynn clustering review computing surveys jambu méthodes analyse données eyrolles 05256 karypis kumar chameleon hierarchical clustering using dynamic modeling computer special issue analysis mining kaufman rousseeuw finding groups introduction cluster analysis wiley mazlack coppock using computing techniques integrate multiple kinds attributes mining international flins conference computational intelligent systems applied research pages septembre efficient effective clustering methods spatial mining international conference large bases pages septembre sheikholeslami chatterjee zhang wavecluster multi resolution clustering approach large spatial databases international conference large bases pages siirtola direct manipulation parallel coordinates conference human factors computing systems volume interactive posters pages muntz sting statistical information approach spatial mining international conference large bases pages athens greece rundensteiner interactive hierarchical displays general framework visualization exploration large multivariate computers graphics avril zhang ramakrishnan livny birch efficient clustering method large databases sigmod international conference management pages summary conceived original clustering present following article comprises summary module display module summary module handles mixed qualitative quantitative large quantities using original incremental agglomerative clustering algorithm display module allows users understand easily summaries thanks evolved graphic design summaries either described through hierarchy profiles table profiles profile gives clear information about corresponding summary readings hierarchy table eased choice optimal order summaries variables