Apprentissage incrémental anytime classifieur Bayésien pondéré Carine Boullé Vincent Lemaire Orange Lannion avenue Pierre Marzin 22300 Lannion Résumé considérons problème classification supervisée données présentant éventuellement grand nombre variables explicatives classifieur Bayésien révèle alors simple calculer relativement performant hypothèse restrictive indépendance riables conditionnellement classe respectée sélection variables moyennage modèles voies connues amélioration reviennent déployer prédicteur Bayésien intégrant pondération variables explicatives article intéressons estimation directe modèle Bayésien pondéré proposons régularisation parcimo nieuse vraisemblance modèle prenant compte informativité chaque variable vraisemblance régularisée obtenue étant convexe proposons algorithme gradient ligne optimise obtenue déjouer minima locaux expérimentations menées intéressent qualité optimisation obtenue autre performances classifieur fonction paramétrage régularisation Introduction accroissement continu capacités stockage capture traitement données profondément évolué durant dernières décennies désormais courant traiter données comprenant grand nombre variables volumes considérés forcément envisageable pouvoir charger intégralement tourne alors traitement ligne durant lequel données seule contexte considère problème classification supervisée variable catégorielle prédire prenant modalités semble variables explicatives numériques catégorielles intéresse famille prédicteurs Bayésien hypothèse indépendance variables explicatives conditionnellement variable cible modèles directement calculables partir estimations conditionnelles univariées chaque variable explicative instance probabilité prédire classe cible conditionnellement valeurs variables explica tives calcule alors selon formule Apprentissage incrémental anytime classifieur Bayésien pondéré considère estimation probabilités priori probabilités conditionnelles disponibles notre probabilités seront estimées expérimentations discrétisation groupage univarié Boullé probabilités univariées étant connues prédicteur Bayésien pondéré décrit tièrement vecteur poids variables cette famille prédicteurs distinguer prédicteurs poids valeur booléenne parcourant ensemble binaisons possibles valeurs vecteur poids calculer prédicteur savoir prédicteur maximise vraisemblance données apprentissage Cependant quand nombre variables élevé parcours exhaustif devient impossible résoudre parcours optimal espace prédicteurs poids continus prédicteurs peuvent obtenus moyennage modèles précédent pondération propor tionnelle probabilité posteriori modèle Hoeting compression Boullé Cependant bases comprenant grand nombre variables observe modèles obtenus moyennage conservent grand nombre variables modèles coûteux calculer déployer moins interprétables article intéresse estimation directe vecteur poids optimisation vraisemblance régularisée attente principale obtenir cette proche modèles robustes comprenant moins variables performances équivalentes travaux préliminaires Guigourès Boullé montré intérêt telle estimation directe poids suite article organisée façon suivante régularisation cimonieuse proposée présentée section place algorithme gradient ligne anytime budget limité optimisation critère régularisé section sieurs expérimentations présentées section avant bilan présentation perspectives travaux Construction critère régularisé Étant donné données cherche minimiser vraisemblance négative écrit comme problème classique optimisation régularisation vraisemblance opérée ajout terme régularisation terme priori exprime contraintes souhaiterions imposer vecteur poidsW critère régularisé critère forme désigne vraisemblance fonction régularisation poids régulari sation Plusieurs objectifs guidé notre choix fonction régularisation parcimonie favorise vecteurs poids comprenant possible composantes nulles fonctions norme classiquement utili régularisation ajout terme régularisation forme Toutes fonctions croissantes favorisent vecteurs poids composantes élevées fonction norme convexe facilite optimisation attractive Cependant cette convexité minimisation terme régularisation conduit forcément mination variables choix favorable obtention vecteur poids parcimonieux capacité prendre compte coefficient associé chaque variable explica vraisemblance équivalente variables simples soient préférées variables complexes pondérant contrainte norme prise compte coefficient obtient terme pénalisation forme coefficient supposé connu amont optimisation aucune connaissance disponible coefficient utilisé intégrer préfé rences métier entre variables notre décrit préparation variable discrétisation variable numérique variable catégorielle décrits respectivement équations Boullé cohérence critère régularisé prédicteur Bayésien lection binaire variables Boullé critères coïncident valeurs binaires utilise finalement terme régularisation Algorithme optimisation descente gradient perturbation voisinage variable Notons toutes quantités constantes problème optimisation critère régularisé minimiser écrit alors donne contrainte valeurs obtenir modèles inter prétables critère convexe différentiable vecteur poids Apprentissage incrémental anytime classifieur Bayésien pondéré Entrees données profondeur historique conservé évaluer valeur critère taille utilisés poids vecteur initial poids vecteur initial nombre maximal itérations nombre toléré dégradations successives Sorties argminCRD ttotal nombre itérations effectuées while Amélioration critère moins dégradations successives Nombre itérations index itération courante données taille historique données taille terminant données Calcul Calcul valeur critère historique taille amélioration critère mémorisation meilleure valeur incrémentation compteur dégradations successives Algorithme Algorithme descente gradient projeté dérivée partielle gradient vecteur dérivées partielles intéressé minimisation algorithme descente gradient projeté Bertsekas algorithme descente gradient lequel chaque itération vecteur obtenu projeté Plusieurs objectifs guidé notre choix algorithme algorithme ligne structure algorithme adaptée traitement données nécessite traitement intégralité algorithme anytime algorithme interruptible mesure tourner meilleure optimisation étant donné temps calcul budgété préalable algorithme gradient projeté batch procède itérativement mettant vecteur poids chaque itération selon gradient calculé toutes instances pondéré vecteur poids obtenu itération itération effectue selon équation Entrees nombre maximal itérations total TailleVoi taille initiale voisinage Entrees données profondeur historique conservé évaluer valeur critère taille utilisés poids vecteur initial poids vecteur initial nombre maximal itérations optimisation nombre toléré dégradations successives Sorties argminCRD Initialisation Initialisation Initialisation SommeT while SommeT Calcul total SommeT SommeT tmtotal amélioration rapport Mémorisation TailleVoi TailleVoi Random TailleVoi TailleVoi Algorithme Algorithme descente gradient projeté perturbation voisinage variable selon variantes constante scalaire varier selon itérations varier selon composantes vecteur poids projection consiste simplement borner valeurs obtenues poids intervalle Cette approche batch suppose dispose intégralité mesure débuter optimisation variante stochastique intégrant gradient calculé seule instance descente gradient alors révéler chaotique variance gradient instance autre élevée Souhaitant adopter approche ligne avons retenu variante croisée approches batch stochastique savoir approche Dekel consiste orienter descente gradients calculés paquets successifs données taille noterons chemins descente soient comparables lorsque taille varie gradient utilisé rapporté taille algorithme descente gradient adopté résumé algorithme valeur optimale objet nombreuses recherches conduisant algorithmes moins coûteux temps calcul avons méthode Rprop Riedmiller Braun calcul spécifique chaque composante vecteur vecteur dimensionK chaque composante vecteur multiplié facteur grand petit dérivée partielle change Apprentissage incrémental anytime classifieur Bayésien pondéré change signe itération autre terme complexité algorithmique chaque itération nécessite évaluation critère échantillon taille complexité retrouve algorithme batch classique gradient stochastique Etant donné convexité critère optimiser présente général nombreux minima locaux lesquels telle descente gradient converger alors courant lancer plusieurs descentes gradient initialisations aléatoires distinctes approche multi start espérant chemins descente conduise minimum global critère rendre optimisation efficace perdre temps calcul début chacune descentes également possible perturber solution obtenue certain nombre itérations sortir éventuelle cuvette contenant minimum local rendant variable taille voisinage lequel perturbe solution donne moyens sortir minima locaux approche Variable Neighborhood Search Hansen Mladenovic Cette approche notée décrite algorithme remarquer voisinage recouvrant intégralement algorithme revient structure multi start initialisation aléatoire autre précisons tirage aléatoire conduire valeur nulle poids issue start précédent variable apparaître cours lecture algorithme anytime estimation minimum critère disponible première descente gradient ensuite améliorée fonction budget disponible temps calcul interruptible moment complexité totale nombre total itérations autorisées paramétrage permet limiter budget maximal utilisé algorithme Expérimentations premières expérimentations objectif évaluer qualité optimisation obtenue algorithme fonction taille présentés nombre total itérations autorisées étudier qualité intrinsèque optimisation indépendamment performances statistiques prédicteur associé avons poids régularisation revient optimiser directement vraisemblance régularisé seconde partie expérimentations traite performances statistiques classifieur obtenu optimisation critère régularisé ensemble expérimentations paramètres suivants algorithme fixés valeurs suivantes multiplication changement changement signe entre gradient successifs nombre maximal itérations nombre présentés vérifié nombre avait jamais atteint bases testées nombre dégradations successives autorisées considère amélioration critère amélioration moins valeur précédente critère poids inférieurs seuil Abalone Mushroom Adult 48842 PenDigits 10992 Australian Phoneme Breast Satimage Segmentation Shuttle 58000 German SickEuthyroid Glass Sonar Heart Soybean Hepatitis Horsecolic Thyroid Hypothyroid Tictactoe Ionosphre Vehicle Waveform WaveformNoise LED17 10000 Letter 20000 Yeast Caractéristiques bases nombre instances Nombre initial variables Nombre classes Toutes expérimentations menées cross validation bases décrites tableau présentation résultats désigne perfor mance classifieur Bayes moyenné compression Boullé Expérimentations qualité optimisation compression Train Batch compression moyen Train bases abord avons étudié performances algorithme algorithme descente gradient projeté optimisation fonction taille avons choisi comme indicateur qualité optimisation Apprentissage incrémental anytime classifieur Bayésien pondéré Itération batch Chemins convergence critère selon taille cours itérations Phoneme compression mesure logarithme négatif vraisemblance modèle normalisé entropie Shannon proche vraisemblance modèle élevée modèles moins performants modèle aléatoire compression négatif valeur données apprentissage indicateur qualité optimisation obtenue étant donné critère régularisé réduit vraisemblance négative figure présente compression obtenu Train moyenné bases différentes tailles dernier choix revient algorithme batch compression obtenus Train Boullé servent référence résultats obtenus indiquent taille petite qualité optimisation dégrade autre résultats obtenus proches pression Train significativement meilleur batch bases figure présente titre illustratif série valeurs prises critère cours timisation selon valeur Phoneme ensemble bases convergence rapide chaotique lorsque taille diminue avons ensuite comparé qualité optimisation algorithme optimisation algorithme optimisé autre Plusieurs types optimisations testées multi start perturbation aléatoire voisinage variable avoir complexité ordre grandeur celle algorithme traitement univarié savoir nombre total itérations autorisées proportionnel précisément choisi 2PostOptiLevel PostOptiLevel entier permet régler niveau optimisation souhaité chacun types optimisation étudié influence valeur niveau optimisation OptiLevel mesure algorithme optimisé mémorise mesure meilleure solution rencontrée optimisation améliorer compression Train premier temps mesuré cette amélioration était significative optimisation compression Train amélioré manière significative bases niveau optimisation respectivement optimisation compression Train amélioré manière significative bases niveau optimisation respectivement optimisation semble préférable optimisation exploration guidée voisinage taille variable partir meilleur minima rencontré permet recherche fructueuse autres minimas exploration purement aléatoire figure permet illustrer phénomène gaspillage itérations début Itération Chemins convergence critère selon optimisation Phonème niveau optimisation chaque nouveau lancement expérimentations présentées cette section évidence effet taille qualité optimisation cette taille élevée meilleure optimisation intérêt optimisation comparé notamment optimisation retenons suite expérimentations algorithme taille fixée niveau optimisation PostOptiLevel Performances classifieur régularisé présentons performances classifieur fonction paramétrage poids régularisation exposant fonction Trois valeurs testées performances terme prédicteurs régularisés associés valeurs présentées figure référence performances prédicteur régularisé obtenu prédicteur valeur élevée poids régularisation savoir violet figure performances terme dégradées rapport performances obtenues régularisation rouge figure quelle valeur Apprentissage incrémental anytime classifieur Bayésien pondéré Train Lambda SansReg Lambda Lambda Lambda Lambda Lambda Lambda Lambda Lambda Lambda moyenne Train bases fonction poids régularisation vanche autres valeurs poids performances similaires toutes valeurs légèrement supérieures identiques moyenne celle prédicteur régularisation valeurs poids régularisation conduisent performances statistiques équivalentes celle prédicteur régularisé autre étudier parcimonie prédicteurs obtenus figure présente nombre variables retenues somme poids abord faible nombre poids faible également régularisation quadratique celle conduit prédicteurs moins parcimonieux Parmi régularisation valeur absolue celle racine carrée seconde permet réduction importante nombre variables retenues Concernant somme poids riables retenues types régularisation permettent réduire moyenne somme poids autre poids donné régularisation quadratique impact moins portant réduction somme poids autres régularisations performances proches indicateur prenant compte aspects performance statistique parcimonie compro semble favorable dégrader performances prédicteur régularisé permet réduire façon importante nombre variables sélectionnées prédicteur interprétable moins complexe déployer Conclusion avons proposé régularisation parcimonieuse vraisemblance sifieur Bayésien pondéré avons décrit expérimenté algorithme descente gradient intégrant ligne données optimisant poids classifieur exploration moins poussée solution optimale courante fonction budget donné expérimentations menées permis montrer intérêt introduction Poids régularisation Lambda SansReg Lambda Lambda Lambda Lambda Lambda Lambda Lambda Lambda Lambda Poids régularisation Lambda SansReg Lambda Lambda Lambda Lambda Lambda Lambda Lambda Lambda Lambda Nombre variables retenues somme poids moyennés bases fonction poids régularisation Apprentissage incrémental anytime classifieur Bayésien pondéré optimisation autre étude paramétrage régularisation ressortir choix optimal correspondait terme régularisation norme poids permet obtenir prédicteur parcimonieux performant expérimentations données beaucoup volumineux nécessaires tester performances approche réels données feront cadre travaux futurs Références Bertsekas Goldstein Levitin Polyak gradient projection method sactions Automatic Control Boullé Compression based averaging selective naive bayes classifiers Journal Machine Learning Research Boullé Recherche représentation données efficace fouille grandes bases données thesis Ecole Nationale Supérieure Télécommunications Dekel Gilad Bachrach Shamir Optimal distributed online prediction using batches Journal Machine Learning Research Guigourès Boullé Optimisation directe poids modèles prédic bayésien moyenné 13èmes Journées Francophones Extraction Gestion Connaissances Hansen Mladenovic Variable neighborhood search Principles applications European Journal Operational Research Hoeting Madigan Raftery Volinsky Bayesian model averaging tutorial Statis tical Science Riedmiller Braun direct adaptive method faster backpropagation learning Rprop algorithm International Conference Neural Networks Summary consider supervised classification streams number input ables basic naïve Bayes classifier attractive simplicity performance strong assumption conditional independence valid Variables selection models averaging common improve model process amounts manipulate weighted naïve Bayes classifiers article focus direct estimation weighted naïve Bayes classifiers propose sparse regularization model likelihood which takes account information contained input variable sparse regularized likelihood being convex propose online gradient algorithm using batches optimization avoid local minima Experiments study first optimization quality classifier performance according parameterization effectiveness approach