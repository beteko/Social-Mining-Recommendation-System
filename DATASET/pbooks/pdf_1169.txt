Sélection modèles méthodes noyaux classification données séquentielles Trinh Thierry Artières Patrick Gallinari Université Pierre Marie Curie Prénom travail concerne développement méthodes classification discriminantes données séquentielles Quelques techniques proposées étendre séquences méthodes discriminantes comme machines vecteurs supports nature adaptées données dimension Elles permettent classifier séquences complètes réaliser segmentation consiste reconnaître séquence unités phonèmes lettres exemple correspondant signal utilisant correspondance donnée modèle transformons problème apprentissage modèles partir données problème sélection modèles attaqué méthodes machines vecteurs supports proposons évaluons divers noyaux fournissons résultats expérimentaux problèmes classification Introduction Cette étude concerne intégration information discriminante systèmes classification données reposant modèles génératifs spécifiquement mélanges modèles génératifs majorité tâches classification dispose possibilités principales nature approche employer approche discriminante approche générative utiliser modèle discriminant réseau neurones classifieur linéaire machine vecteurs supports apprentissage focalisé différencie différentes classes point probabiliste correspond apprendre probabilités posteriori classes plupart techniques discriminantes adaptées données dimension délicates utiliser données séquentielles taille variable comme parole écriture autre approche consiste modéliser classes indépendamment autres apprendre chacune modèle correspondant densité probabilité modèle gaussien modèle Markov critère Maximum Vraisemblance utilise modèle génératif classe chaque modèle appris indépendamment autres données classe Ensuite théorème Bayes ramener probabilités posteriori construire système classification optimal règle générale approche discriminante performante Cependant avoir intérêt employer mélanges modèles génératifs certaines conditions mélanges modèles particulièrement adaptés lorsque classes fortement multimodales exemple écriture manuscrite écrit différentes façons parle allographes modèles génératifs particulièrement intéressants lorsque données dimension variable dernier correspond Sélection modèles méthodes noyaux toutes données disponibles forme signaux séquences parole écriture intérêt modèles génératifs données séquentielles intérêt faute mieux réside essentiellement inadéquation techniques discriminantes données Certaines techniques proposées apprendre critère discriminant modèles traitant données séquentielles Ainsi noyau Fisher Jaakola permet utiliser machines vecteur support données séquentielles Cependant techniques permet étendre méthodes discriminantes classification séquences complètes segmentation segmentation consiste reconnaître séquences unités phonèmes lettres reconnaissance parole écriture souvent tâche intéressante données séquentielles Notre explorer diverses techniques permettant allier efficacité méthodes discriminantes souplesse mélanges modèles génératifs capables réaliser segmentation données variables cherchons utiliser méthodes discriminantes construire modèles génératifs performants segmentation données séquentielles toute suite considérons souhaitons mettre point modèle génératif classe forme suivante xPwCxP classe vraisemblance donnée modèle classe modèles classe probabilités priori poids composantes mélange vérifient appelé suite taille modèle classe Correspondance donnée modèle Machines vecteurs supports Fonction discriminante Données apprentissage Ensemble modèles Sélection modèles Retour modèles Utilisation machines vecteurs supports apprentissage modèles génératifs génératifs figure présente schématiquement notre approche partir ensemble données apprentissage change espace représentation utilisant correspondance donnée modèle expliciterons chaque donnée ainsi transformée modèle alors utiliser machines vecteurs supports nouvelles données modèles définissant noyaux modèles étant définir noyaux vecteurs supports correspondent modèles soient candidats cette procédure apprendre modèles génératifs obtenant performance classification meilleure possible notamment meilleure apprentissage classique critère comme Maximum Vraisemblance passons abord revue techniques utilisées exploiter méthodes noyaux reconnaissance données séquentielles définissons quelques noyaux modèles avons utilisés expériences Enfin décrivons résultats expérimentaux données dimension ainsi données manuscrites ligne permettant évaluer qualités pertinences différentes méthodes envisagées Utilisation noyaux données séquentielles Depuis quelques années Machines Vecteurs Support Vapnik Vapnik devenues approche classique performante problèmes classification régression classificateur forme générale suivante problème classification classes bxxyxf forme classifier exemples apprentissage classe identifiée label coefficients paramètres fonction représente projection espace dimension élevé représente produit scalaire vecteurs projetés classification signal réalisée déterminant signe certaines conditions nécessaire expliciter fonction projection existe fonction nomme fonction noyau telle équation précédente réécrit notant fonction noyau bxxKyxf partir ensemble apprentissage apprentissage consiste déterminer paramètres fonction meilleurs maximisation marge coefficients solutions problème programmation quadratique ensemble points apprentissage appelle vecteurs supports autres Différentes approches proposées appliquer données séquentielles Jaakola consiste ramener représentation dimension exploitant modèle génératif utilise modèle génératif appris ensemble données défini ensemble paramètres définit nouvelle représentation séquence comme gradient logarithme vraisemblance λxPxU Intuitivement norme importante modèle devrait changé produire signal forte vraisemblance changé espace représentation toutes données séquences apprentissage maintenant Sélection modèles méthodes noyaux représentées espace dimension nombre paramètres utiliser représentations Bahlman utilise système prototypes exemples représentatifs chacune classes définit noyau données séquentielles distance entre séquences utilisée reconnaissance parole Dynamic Warping noyau entre séquences défini ByxdA dtweyxK paramètres système fixés empiriquement techniques similaires discutées Watkins Moreno explore troisième technique consiste utiliser correspondance entre donnée modèle tâche identification locuteur visée chaque donnée signal parole quelques dizaines secondes transformé modèle génératif mélange gaussiennes apprenant modèle donnée partir cette association modèle donnée auteurs proposent utiliser noyau probabiliste entre données traduit différence entre modèles génératifs associés noyau divergence Kullback Leibler entre distributions probabilités définies modèles Cette approche entendu viable modèle utilisé relativement simple appris seule donnée Sélection modèles génératifs machines vecteurs supports travaux inspirent Moreno cherchons utiliser machines vecteurs supports exploitant correspondance donnée modèle Notre travail diffère points abord correspondance donnée modèle intervenir apprentissage exploite information priori fournie concepteur Notre approche sembler moins générique celle Moreno noter apprentissage modèle partir donnée unique généralement faisable information priori Ensuite notre obtenir fonction discriminante performante uniquement sélectionner modèles génératifs élémentaires performants suite suppose dispose association donnée modèle apprentissage réalisé partir données apprentissage modèles associés discutons maintenant noyaux avons utilisés première méthode avons envisagée consiste définir explicitement représenter donnée vraisemblances cette donnée calculée ensemble modèles toutes classes classe notant modèle défini partir donnée apprentissage classe défini 2N211N11 utilise alors noyau exemple gaussien entre données constante simplifier procédure construire partir scores calculés ensemble restreint modèles chaque classe expériences choisissons aléatoirement exemple modèles classe définissons comme vecteur scores nombre classes considérées appellerons cette méthode Noyau avons également utilisé noyau proposé Moreno exploitant divergence symétrisée Kullback Leibler entre modèles génératifs construit partir données nommons cette méthode Noyau divergence symétrisée écrit yxyxyxsym KLKLKL λλλλλλ expériences avons estimé divergences données apprentissage zPzPKL décrit ensemble données apprentissage noyau utilisé défini yxsymeK Enfin avons utilisé noyau extrêmement simple appelé Noyau Proba contient information ordre noyau précédent fruste défini xyyxoba yPxPK terminer noyau Fisher première méthode utilisée fournissons résultats cette méthode raison étant délicate employer nécessite réglage manuel important présentons résultats noyaux KProba Apprentissage discriminant coefficients mélange Notre apprendre modèles génératifs soient discriminant modèles génératifs modèles mélange forme donnée équation utilisation machines vecteurs supports noyaux définis permet sélectionner composantes modèles mélange classes ensuite optimiser coefficients mélange obtenir meilleure discrimination possible algorithme classique consiste apprendre paramètres critère Maximum Vraisemblance apprentissage réalisé indépendamment chaque classe introduit information discriminante niveau proposons chercher coefficients modèles mélange optimisant critère discriminant Sélection modèles méthodes noyaux produit probabilités posteriori données apprentissage étant fixés prenant logarithme cherche maximiser xCPxCPJ réalisé algorithme gradient dérivant critère rapport effets espéré observé certains optimisation critère partie poids convergent comparerons cette méthode apprentissage coefficients apprentissage discriminant optimisant critère Maximum Vraisemblance Expériences données dimension tests réalisés données dimension mettre évidence particularités différentes méthodes envisagées discutons brièvement correspondance donnée modèle décrivons données fournissons enfin résultats expérimentaux données utilisons données extraite pbvowel1 Klautau contenant signaux parole voyelles signaux prétraités représentés après extraction caractéristiques dimensions correspondent premiers formants classes utilisons environ exemples apprentissage reconnaissance plafond donné auteurs données obtenu méthode proches voisins Correspondance donnée modèle données dimension avons utilisé correspondance extrêmement simple modèle associé donnée point gaussienne moyenne point variance proportionnelle identité point modèle associé gaussienne variance modèles fixée empiriquement Résultats tableau montre résultats classification différentes approches envisagées système discriminant obtenu différents noyaux systèmes obtenus sélectionnant modèles déterminant différents schémas apprentissage comparer équitablement méthodes avons réglé détails aldebaro repository paramètres façon obtenir nombre vecteurs support classe identique toutes expériences tableau nombre classe Noyau Proba Noyau Noyau Fonction discriminante classe Modèles génératifs Apprentissage coefficients mélange Maximum Vraisemblance Critère discriminant Performance modèles classe Performances fonctions discriminantes apprises méthodes utilisant sélection modèles constate abord résultats fonctions discriminantes identiques noyau KProba noyau simple donnant meilleures performances examine résultats obtenus modèles génératifs modèles élémentaires sélectionnés aperçoit leurs performances élevées celles correspondantes coefficients mélange soient appris critère discriminant critère discriminant noyaux utilisés adaptés construire fonction discriminante permettent sélectionner relativement efficacement modèles concevoir modèles génératifs constate également quelle méthode utilisée sélectionner modèles systèmes génératifs obtenus après apprentissage coefficients mélange présentent performances similaires Enfin apprentissage discriminant coefficients mélange permet obtenir meilleurs résultats titre référence avons appris critère Maximum Vraisemblance modèles génératifs donné équation tailles variant reconnaissance varient ailleurs rappelons performance plafond Comparativement résultats systèmes génératifs modèles sélectionnés coefficients mélange appris critère discriminant légèrement supérieurs résultat encourageant autant apprentissage discriminant coefficients mélange certains coefficients convergent taille modèles classes réduite notamment noyau Notons tirer conclusions autres expériences lesquelles nombre vecteurs support élevé étudier détail fonctionnement approche visualisant points plutôt modèles apprentissage choisis comme vecteurs support Figure construction modèles génératifs figure montre ensemble données classes ainsi modèles appris méthode Maximum Vraisemblance vecteurs moyens gaussiennes figure montre façon modèles sélectionnés utilisant noyau Sélection modèles méthodes noyaux figures exploitation permet récupérer modèles situent frontière exemples classes alors modèles appris critère Maximum Vraisemblance modèles modélisant zones denses chaque classe Modèles élémentaires appris critère discriminant problème classification voyelles Modèles élémentaires sélectionnés noyau problème classification voyelles Expériences données séquentielles présentons tests réalisés signaux écriture manuscrite ligne signal écriture manuscrite ligne signal temporel constitué coordonnées successives stylo capturé tablette digitale stylo électronique présentons données utilisée discutons correspondance donnée modèle fournissons résultats expérimentaux données avons travaillé partie UNIPEN Guyon internationale référence domaine écriture ligne expériences portent signaux correspondant chiffres usuels écrits environ scripteurs utilisons 16000 exemples apprentissage Chaque résultat expérience résultat moyen obtenu expériences faisant varier tiers données utilisées apprentissage Correspondance donnée modèle signal écriture étant variable existe nombreux allographes tracer caractère usage modèles mélanges systèmes basés prototypes tracés typiques extrêmement répandu caractère souvent modélisé mélange modèles exemple modèles Markoviens gauche droite chacun modèles correspondant allographe apprentissage modèles nombre allographes ainsi topologie modèles Markoviens modélisant doivent fixés certain nombre travaux menés apprendre complètement modèles caractères partir données Artières Marukatat basés construction partir tracé exploitent représentation tracés forme séquence codes directionnels avons repris cette étude procédure proposée Artières décrivons brièvement construire partir tracé manuscrit ligne originel donnant forte vraisemblance tracés ressemblant tracé originel faibles vraisemblances tracés différant modèle associé tracé figure présente schématiquement cette procédure signal manuscrit abord segmenté système markovien ergodique lequel chaque représente tracé élémentaire existe tracés élémentaires représentés figure droites uniformément réparties entre ainsi tracés légèrement convexes concaves résultat premier traitement représentation tracé originel forme séquence tracés élémentaires figure partir cette représentation construit gauche droite trois états droite figure chaque associée probabilité émission dérivée tracé élémentaire correspondant exemple premier troisième modèle Figure tracé élémentaire idéal tracé émission donne fortes probabilités directions proches direction tracé faibles directions différentes Sélection modèles méthodes noyaux Construction gauche droite partir tracé signal manuscrit ligne segmenté séquence temporelle tracés élémentaires constitue codage directionnel tracé modèle Markov gauche droite construit partir cette séquence Ensemble tracés élémentaires utilisé représenter formes dimensions Artières gauche droite tracés droits notés convexes concaves Résultats expérimentaux tableau résume performances différentes méthodes envisagées classification signaux chiffres remarque genre phénomènes données dimension performances fonctions discriminantes variables puisqu elles jusqu presque suivant noyau utilisé noter performance obtenue fonction noyau meilleure performance classification données notre connaissance ailleurs systèmes génératifs lesquels modèles élémentaires sélectionnés apprentissage coefficients mélange discriminant permet atteindre résultats supérieurs noyau KProba avoisinant noyaux performances fonctions discriminantes noyau utilisé performance systèmes génératifs proches ordre apprentissage discriminant ordre apprentissage discriminant Ainsi noyaux forcément aussi adaptés construire fonction discriminante semblent équivalents conception systèmes génératifs terminer comparer résultats résultats obtenus système Marukatat inspiré travail reconnaissance écriture ligne système obtient performance plafond ordre données rapport performances systèmes obtenus réduisent erreur systèmes génératifs système noyau Méthode KProba Modèles génératifs Apprentissage poids Vraisemblance Critère discriminant Performance Taille modèles Fonction discriminante Performance classe Méthode référence Marukatat Taille modèles Taille modèles Taille modèles Performance méthodes utilisant comme fonction discriminante sélection modèles élémentaires schémas apprentissage coefficients mélange système référence Marukatat discriminant Conclusion avons étudié papier méthodes discriminantes apprentissage mélanges modèles génératifs modèle particulièrement utilisé données séquentielles généralement appris façon discriminante avons envisagé possibilité transformer problème façon utiliser machines vecteurs support automatiquement sélectionner modèles pertinents classification avons également proposé apprendre coefficients mélange modèles critère discriminant performances obtenues montrent résultats intéressants systèmes génératifs obtenus performants modèles appris critère discriminant présentent souplesse derniers réaliser tâches segmentation signaux Références Artières Gallinari Stroke level handwriting recognition International Workshop Frontiers Handwriting Recognition Bahlmann Haasdonk Burkhardt Handwriting Recognition using Support Vector Machines kernel approach Workshop Frontiers Handwriting Recognition Chang Chung library Support Vector cjlin libsvm Guyon Schomaker Plamondon Liberman Janet UNIPEN project exchange recognizer benchmark International Conference Pattern Recognition Sélection modèles méthodes noyaux Jaakkola Diekhans Haussler Exploiting generative models discriminative classifiers Advances Neural Information Processing Systems Mateo Jaakkola Diekhans Haussler Using Fisher kernel method detect remote protein homologies International Conference Intelligent Systems Molecular Biology Klautau Classification Peterson Barney vowels using Technical report citeseer klautau02classifcation driven design toplogy handwriting recognition International Journal Pattern Recognition Artificial Intelligence Marukatat Sanparith approche générique reconnaissance signaux écrits ligne Thèse doctorat Université Paris Moreno Pedro Purdy Vasconcelos Generative Model Based Kernel classification Multimedia applications Vapnik Siegelmann Support Vector Method Hierarchical Clustering Advances Neural Information Processing Systems Vapnik Nature Statistical Learning Theory Springer Verlag Watkins Chris Dynamic Alignment Kernels Neural Information Processing Systems Vancouver Canada Summary paper investigates development discriminant methods sequential techniques proposed adapt discriminant models Support Vector Machines These techniques allow handling complete sequence classification perform segmentation tasks recognizing sequence units characters phones correspond signal Based association between models transform problem learning training problem selecting appropriate generative models enabling Support Vector Machines generative models learning compare kernels report experimental results different classification problem