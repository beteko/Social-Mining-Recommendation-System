complémentarités représentations vectorielles similarité sémantique julien cruys philippe muller fabrice popineau benamsili octopeek général gaulle 95880 enghien bains france octopeek noetzlin 91190 yvette france université toulouse sabatier route narbonne 31062 toulouse france centralesupélec joliot curie 91192 yvette france résumé tâche similarité sémantique textuelle consiste exprimer tomatiquement nombre reflétant similarité sémantique fragments texte chaque année depuis campagnes semeval déroulent cette tâche similarité sémantique textuelle article présente méthode ciant différentes représentations vectorielles phrases objectif amélio résultats obtenus similarité sémantique notre hypothèse férentes représentations permettraient représenter différents aspects séman tiques extension améliorer similarités calculées principale ficulté étant sélectionner représentations complémentaires cette tâche notre système système vainqueur campagne ainsi notre méthode sélection complémentarité résultats obtenus viennent confirmer intérêt cette méthode lorsqu comparés résultats campagne introduction nombreux travaux récents intéressent similarité sémantique entre entre groupes depuis syntagmes jusqu documents complets passant similarité entre phrases rapprocher phrases permet utiliser traits sémantiques modèles évitant dispersion inhérente taille vocabulaire espace phrases possibles plupart travaux calculent similarités entre représentations construites bases distributionnelles similarité dérive similarité contextes apparition hypothèse énoncée harris complémentarités représentations vectorielles représentations prennent forme vecteurs matrices tenseurs distributionnels turney pantel dimensions correspondent occurrences lexicales syntaxiques baroni lenci transformations contextes réduction dimension pennington intermédiaire apprentissage réseaux neurones mikolov évaluation modèles vectoriels repose tâches externes mesures intrinsèques fondées échantillons similaires groupes similaires récemment notion similarité sémantique textuelle motive représentations vectorielles seuls représentation phrastique construire compo sition représentations lexicales mitchell lapata cruys encore construite intermédiaire apprentissage réseau neurones mikolov méthodes appliquées reconnaissance paraphrases similarités appuient représentations vectorielles différentes façons combiner utilisent aussi appariements éléments phrase sultan problème crucial représentations vectorielles existence nombreux hyper paramètres construction combinaison définition similarité textuelle existe travaux tentent combiner différentes représentations tirer parti éventuelles complémentarités choix effectués amont présentons travail combinaison représentations vectorielles plorer potentiel association représentations différentes échelles revenons section suivante détail tâche similarité sémantique textuelle campagne semeval section présentera motivations pothèses justifieront cette recherche complémentarité représentations vectorielles détaillerons ensuite notre méthode recherche complémentarité travers rithmes optimisant critères différents section enfin discuterons résultats obtenus comparant résultats campagne année similarité sémantique textuelle revenons brièvement modèles vectoriels phrases discuter place tâche spécifique mesure similarité textuelle modèles vectoriels représentation texte espace vectoriel technique utilisée dernières années plusieurs disciplines comme analyse sentiment traduction automatique mikolov cette représentation permet rapprocher phrases manière générale texte passer représentation précise éléments représentation vectorielle permet ainsi construire forme pouvant positionner rapport autres chaque étant réduit vecteur nombres possible calculer similarité mesures simples telle similarité cosinus construction représentations vectorielles possible apprentissage supervisé larges corpus différentes techniques construction vecteurs tiennent compte chaque leurs voisins texte proches phrase contexte vecteurs prêtent également différentes méthodes composition sible composer plusieurs construire vecteur phrase faisant moyenne éléments vecteurs autres méthodes attaquent direc tement représentation phrases passer composition vecteurs mikolov approches supervisées permettent également représentation phrases grâce notamment réseaux neurones profonds recursive networks recur networks encore convolutional networks tâche semeval tâche propose mesurer quantitativement similarité sémantique phrases échelle valeur signifiant phrases strictement identiques sémantique chaque paire phrases disposition depuis évaluée plusieurs annotateurs humains rémunérés plate forme microworking amazon mecha nical éliminant annotateurs fiables phrases mesure montre variance chaque paire phrases correspond moyenne donner valeurs réelles intervalle total paires phrases mises disposition cadre cette tâche depuis score système calculé corrélation pearson entre leurs résultats données annotées année courante chaque équipe proposer trois systèmes entraîner données années antérieures manière générale participants tâche abordent comme problème régres supervisée utilisent descripteurs classiques comme nombre commun séquences différentes longueurs commun mesures similarité basées alignement mesures provenant domaine traduction automatique aussi utilisées trois participants vainqueurs exploité nouvelles techniques représentations vectorielles phrases toutes basées architectures réseaux neurones profonds comme rychalska utilisé recursive neural motivations hypothèse comparaison basique entre phrases consisterait calculer similarité topicale lexicale autres méthodes provenant sémantique distributionnelle permettent quant elles représenter ensemble phrase espace sémantique commun toutes phrases corpus donné méthodes demandent prétraitement corpus métrer algorithme apprend représenter chaque phrase article proposons méthode tente représenter finement phrases restreindre métrage précis occurrence pensons humain capable cibler différents aspects sémantiques objectif comparer morceaux texte plusieurs plans complémentarités représentations vectorielles différents aspects peuvent exemple sujet traité topic action phrase mouvement entités impliquées informations spatio temporelles cherche général trouver meilleur algorithme génère représen tations vectorielles essaye optimiser paramètres certains algorithmes notre connaissance aucun travail tente détecter automatiquement aspects sémantiques permettraient comparaison textuelle optimale niveau humain proposons faire grâce variation certain nombre paramètres prétraitement phrases construction vecteur représentatif évaluons notre méthode tâche semantic textual similarity campagnes semeval capturer automatiquement aspects sémantiques permettant comparaison optimisons sélection différentes représentations vectorielles critère complémentarité tâche cette variation permettre cibler différents aspects phrase ainsi faire jugement pertinence proche humain notre article définissons suite représentations vectorielles plémentaires possibles comme suite représentations associées permettent meilleurs résultats tâche exploitant diversité affectation mètres prétraitement corpus construction vecteurs proposons utiliser extension word2vec mikolov communé appelée doc2vec mikolov permettant représenter document semble phrases espace sémantique avons utilisé implémentation doc2vec gensim sojka variation paramètres notre contribution fonde hypothèse combinaison différentes représen tations vectorielles améliorer qualité score similarité calculé représenta tions suffisamment complémentaires concrètement pensons variation paramètres prétraitement corpus paramètres construction vecteurs diversifier représentation sémantique obtention représentations complé mentaires mesure orienter calculs similarité différents aspects sémantiques paramètres compte notre système suivants entre 10000 indique taille vecteurs générés phrase représen dimensions dimensions portera indices sémantiques différents granularité indices aspects topicals aspects sémantiques removepunct indique suppression ponctuation chaque phrase structure phrase variera selon valeur paramètre virgule diquera séparation entre parties phrase point interrogation indiquera phrase interrogation demande informations énoncées probablement factuelles revanche supprimer toute ponctuation produira vecteurs focalisant leurs voisins window entre définit taille fenêtre contexte chaque fenêtre grande contexte constitué voisins éloignés phrase gauche comme droite exemple phrase piano window correspondra fenêtre phrases centré prendra compte action verbe jouer alors window considérera aussi applique action piano tolowercase indique conservation majuscules présentes phrase majuscules peuvent exemple permettre différencier commun personnel suppression permet réduire personnels forme commune celle existe permet également différencier début phrase removestopwords indique vides supprimés chaque phrase vides informatifs permettent informatifs structurer phrase suppression permettra focaliser analyse sémantiquement riches contraire conserver permettra mieux représenter enchaînement sémantiquement riches lemma indique phrase lemmatisée phrase lemmatisée information sémantique puisque exemple supprimant conjugaison entre verbe sujet affaibli lemmatisation rendra taille cabulaire beaucoup moins grande phrases seront proches accentuera similarité thématique autres paramètres directement génération vecteurs algorithme utilisé aussi variations alpha sample negative min_count entre certains hyper paramètres constitution vecteurs résultat final simple terminer proposons méthode optimisation cherche combinaisons paramètres complémentaires figure schématise notre méthode trois étapes fondamentales premier temps optimisation paramètres permet obtenir ensemble modèles classés selon performance variation paramètres décrite section conjointe cette optimisation générons modèles paramètres choisis hasard ensuite algorithme sélection topdesc associe modèles suffisamment différents courant classement étape enfin algorithme sélection topdelta analyse séries obtenues étape associe modèles complémentaires présentent forte value ensemble séries mettons disposition source python plateforme github combinaison modèles complémentaires optimisation paramètres commençons générer modèles cherchant paramètres optimaux méthode optimisation recherche locale paramètres cités section utilisés chaque modèle modèle généré grâce outil doc2vec doc2vec github stsseries définissons modèle comme étant ensemble représentations vectorielles phrases générées outil doc2vec ayant mêmes paramètres prétraitement corpus lemmatisation vides construction vecteurs nombre dimensions taille fenêtre complémentarités représentations vectorielles génération combinaisons paramètres corpus phrases prétraitement données entraînement modèles combinaisons optimales combinaisons aléatoires séries aléatoires séries topdesc séries topdelta comparaison algorithme sélection topdelta3 algorithme sélection topdesc2 optimisation paramètres1 schéma illustrant trois étapes optimisation algorithmes sélection topdesc topdelta prend corpus ensemble phrases prétraité partie paramètres prend lement paramètres propres exécution outil outil donne sortie chaque phrase vecteur représentatif similarité chaque paire phrases culée grâce vecteurs autres modèles générés affectations paramètres aléatoires modèles générés cette première étape moitié procédure optimisation autre moitié grâce affectation aléatoire mètres chaque modèle permet obtenir représentations vectorielles différentes toutes phrases représentations vectorielles apprises doc2vec données tâche ainsi brown corpus total environ phrases environ provenant tâche objectif obtenir score chaque modèle reflétant performances tâche intégrons chaque modèle système proposé sultan calcul similarité sémantique appris régression linéaire ridge système constitué descripteurs premier correspond score alignement entre phrases score obtenu fonction nombre aligneur réussi relier entre phrases selon différentes métriques dictionnaire synonymes distance levenshtein second descripteur correspond similarité cosinus reprenant vecteurs baroni lorsque intégrons modèle signifie prenons chaque vecteur chaque phrase représentations vectorielles issues modèle entraîné certaine graphique rassemblant trois types séries combinaison paramètres calculons similarités cosinus entre chaque paire phrases similarités ajoutées descripteur système suite utilisation plusieurs modèles produira plusieurs descripteurs notre première phase optimisation utilisons modèle descrip additionnel évaluons modèle défini affectation paramètres performance système descripteur modèle enfin chaque modèle classé selon performances évaluation croisée toutes données rieures celles année algorithme sélection topdesc classement obtenu utilisons algorithme topdesc sélectionner séries modèles algorithme topdesc prend entrée description parcourt définissons série modèles comme étant ensemble ordonné modèles ayant différents paramètres prétraitement corpus construction vecteurs complémentarités représentations vectorielles exemple description série topdesc modèles générés phase optimisation ordre performant moins performant sélectionne itérativement ignore modèles retourne sortie série modèles correspondant description donnée algorithme sélectionne modèles critères modèle sélectionné suffisamment différent autres celui performant lorsqu utilisé description série compose plusieurs éléments paramètres différencier différence minimum modèles sélectionnés chaque paramètre différencier nombre modèles sélectionner figure montre exemple description première ligne signifie algorithme sélectionner modèles window variant respectivement moins ensuite modèles variant quelques paramètres prétraitement ainsi suite autres paramètres entrent différenciation seront paramètres optimaux trouvés phase optimisation figure montre ensemble scores différentes descriptions séries générées aléatoirement chaque point correspond score série fonction certain nombre modèles utilisés série points situés droite associent modèles points gauche permet observer évolution performances série abscisse correspond nombre modèles sélectionnés ordonnée correspond score système comprenant descripteurs additionnels similarités cosinus modèles série courante chaque point graphique appartient courbe étant nombre similarités cosinus descripteurs provenant modèles série score ordonnée correspond système nombre descripteurs additionnels appartenant série séries prolongent score correspondra alors système mêmes descripteurs composé similarité modèle sélectionné série objectif cette seconde étape était observer modèles pouvoir améliora différentes séries dizaine séries testées faisant varier descrip tions semblables figure raisons lisibilité seulement entres elles représentées figure sélection topdesc permet environ système meilleur modèle toujours présent début chaque série marquerons séries générées aléatoirement permettent surpasser séries topdesc suffisamment modèles utilisés contrebalancer effet introduisons rithme topdelta algorithme sélection topdelta algorithme sélection topdesc permet créer séries modèles performants comme avons constater permet important performance algorithme cherchant modèles complémentaires capable discriminer automatiquement paramètres influent moins diversité représentations exemple pouvons supposer certains paramètres comme nombre itérations construction vecteurs directement performances modèle variation paramètres permettra nécessairement orienter représentation aspects sémantiques variés sélection faisait critère performance prenait compte apport concret calcul similarité modèle parmi semble descripteurs algorithme topdelta permet quant combiner modèles pouvoir complémentarité hypothèse jacente modèles complémentaires améliorant série algorithme consiste attri bution score potentiel complémentarité chaque modèle utilisé séries topdesc aléatoires série topdelta correspondra suite modèles ayant meilleurs scores plusieurs séries différentes peuvent générées fonction affectations paramètres équation calculant score potentiel allons détailler concrètement pouvons intuitivement considérer modèles plémentaires pouvant faire partie série tiennent compte facteurs performance modèle score dehors série autre pouvoir complémentarité différence moyenne améliora performances entre modèle ensemble modèles prédécesseurs possibles apparaissent avant série facteur score correspond scores obtenus notre première étape deuxième facteur compliqué mesurer effet pouvoir complémentarité moyen modèle tenir compte idéal toutes associations possibles celui prédécesseurs puisqu modèle améliorer série uniquement grâce mauvaises performances prédécesseurs chaque modèle utilisé moyenne trois corres moyenne trois points figure possible faire moyenne différences entre score série score série combiné modèle courant score topdelta défini ainsi potentiel paramètre permet régler balance entre influence score celle complémentarité complémentarité différence moyenne entre modèle courant auquel voulons attribuer score ensemble prédécesseurs existants ensemble séries topdesc générées modèle également modèle présent séries générées aléatoirement paramètre normalisé entre valeur minimale maximale parmi toutes séries lisibilité cette normalisation incluse équations modèle série faible probabilité améliore cette série constat général plupart tâches apprentissage automatique complémentarités représentations vectorielles descripteur indépendamment performant performances associées plusieurs descripteurs correspondront somme leurs performances individuelles pertinent introduire bonus permet augmenter delta fonction position modèle série bonus introduit équation normalisé borne supérieure bonus défini selon informations nombre prédécesseurs puisque modèle prédécesseurs moins chance améliorer série score série niveau modèle puisque score série élevé rapport autres séries moins modèle chance améliorer globalement série simplifier équations cette information considérée comme malisée titre bonus modèle série courante correspond équation nbancestors nombre prédécesseurs réglant influence informations réglant importance bonus grand bonus pourra grand nbancestors nbancestorsmax sseriei figure montre séries topdelta générées optimisées méthode recherche locale paramètres meilleures affectations représentées courbe topdelta haute graphique marquerons facteur complémentarité important score modèle comme montre affectation affectation montre bonus aussi impor delta enfin affectation priorise information nombre prédécesseurs rapport score série figure raisons complexité espace mémoire avons prolongé uniquement série topdelta série aléatoire choisissant meilleure parmi celles générées considère score référence comme étant système modèle généré aléatoirement alors série topdelta permet prochaine partie destinée tester expérimentalement meilleure série topdelta évaluant notre système données mises disposition expérimentations campagne semeval agirre équipes participé tâche total essais scores globaux allaient médiane baseline campagne correspond similarité basée similarité cosinus représentations paires phrases obtenu score tableau montre meilleure série topdelta effectivement amélioré significati vement performances système grâce sélection automatique modèles complémentaires notre système obtenir score dessus médiane scores campagne système score baseline 51334 69797 topdeltaserie 73408 score comparaison conclusion perspectives travers notre travail expérimental recherche complémentarité avons montrer possible sélectionner représentations vectorielles suffisamment plémentaires guider calcul similarité différents aspects sémantiques cependant raisons temps calcul avons entraîné modèles corpus restreint composé brown corpus données semeval antérieures suite pensons améliorer notre méthodologie puisse appliquer larges corpus permettrait prendre compte autres paramètres faire varier comme ciblage entités nommées conserver phrases entités nommées forment vocabulaire large pensons elles jouent particulier similarité sémantique textuelle méthodes supervisées montré obtenir meilleurs résultats rychalska tâche pourront aussi utilisées notre recherche complémentarité autres méthodes représentation vectorielle pourront combinées doc2vec objectif capturer aspects sémantiques différents potentiellement complémen taires références agirre banea gonzalez agirre mihalcea rigau wiebe semeval semantic textual similarity monolingual cross lingual evaluation proceedings international workshop semantic evaluation semeval diego california association computational guistics baroni kruszewski count predict systematic comparison context counting context predicting semantic vectors baroni lenci distributional memory general framework corpus based semantics computational linguistics curran distributional semantic similarity thesis university edinburgh harris distributional structure mikolov distributed representations sentences documents proceedings international conference machine learning china complémentarités représentations vectorielles mikolov corrado efficient estimation representa tions vector space proceedsings workshop mitchell lapata vector based models semantic composition proceedings annual meeting association computational linguistics columbus pennington socher manning glove global vectors represen tation proceedings conference empirical methods natural language processing emnlp october qatar meeting sigdat interest group english sojka software framework topic modelling large corpora proceedings workshop challenges frameworks letta malta publication 884893 rychalska pakulska chodorowska walczak andruszkiewicz poland semeval necessity diversity combining recur autoencoders wordnet ensemble methods measure semantic similarity ceedings international workshop semantic evaluation semeval diego california association computational linguistics sultan bethard sumner sentence similarity gnment semantic vector composition proceedings international workshop semantic evaluation semeval denver colorado association computational linguistics turney pantel frequency meaning vector space models semantics journal artificial intelligence research cruys poibeau korhonen tensor based factorization model semantic compositionality conference north american chapter association computational linguistics naacl summary semantic textual similarity automatically quantify semantic similarity snippets since organized yearly basis semeval evaluation campaign paper presents method combine different sentence based vector representations order improve computation seman similarity values hypothesis combination different representations allows pinpoint different semantic aspects which improves accuracy similarity computations method difficulty selection complementary representations which present optimization method final system based winning system evaluation campaign augmented complementary vector representations selected optimization method equally present evaluation results campaign which confirms benefit method