étude algorithmes construction architecture réseaux neurones multicouches norbert tsopzé1 engelbert mephu nguifo gilbert tindo université 62307 cedex tsopze mephu artois département informatique université yaoundé yaoundé tsopze norbert gmail gtindo uycdc uninet résumé problème choix architecture réseau neurones multi couches reste toujours difficile résoudre processus fouille données papier recense quelques algorithmes recherche architectures réseau neurones tâches classification présente également analyse théorique expérimentale algorithmes travail confirme difficultés choix paramètres apprentissage modèle nombre couches nombre neurones couches apprentissage algorithme apprentis communs processus construction réseaux neurones difficultés choix paramètres propres certains algorithmes introduction réseau neurones ensemble neurones interconnectés communiquent entre extérieur réseau neurones présente comme graphe noeuds différentes unités réseau représentent connexions entre unités nombre couches nombre neurones couche interconnexions entre différentes unités réseau définissent architecture encore appelée topologie celui neurone appelé unité cellule comme système apprentissage supervisé systèmes apprentissage supervisé réseaux neurones fonctionnent phases phase apprentissage consiste construire partir observations exemples présentés forme représente observation fonction système capable approximer fonction expression analytique facile trouver phase classement utilise modèle construit phase apprentissage produire décisions prédire nouvel exemple faisait partie observations apprentissage définir structure réseau système tâche évidente hamber cornuéjols miclet effet existe aucune méthode permettant définir justifier structure réseau neurones hamber service coopération action culturelle ambassade france yaoundé cameroun financé séjour premier auteur pendant réalisation travail travail partiellement financé ministère français affaires étrangères etude algorithmes construction architecture réseaux neurones définition architecture réseau neurones multicouches résolution problème donné reste problème ouvert outre méthodes génétiques curran problème souvent résolu utilisant approches première consiste ajouter successivement neurones connexions petite architecture deuxième quant consiste supprimer neurones connexions architecture initiale maximale approches souvent comme inconvénient temps apprentissage élevé imprévisible domaines application réseaux neurones multiples dreyfus biologie moléculaire analyse séquences shavlik towell prédiction classification traitement images génie logiciel estimation coûts logiciel mbarki aucune explication justifie notre connaissance définition architec tures utilisées problèmes classification particulier plusieurs méthodes développées proposées littérature parekh 1997b classer méthodes catégories celles construisent archi tecture utilisant ensemble connaissances domaine exemple kbann shavlik towell autres définissent cette architecture aucune connaissance parekh 1997b parekh algorithmes construction réseaux neurones artificiels avons rencontrés littérature produisent réseaux ayant caractéristiques suivantes parekh 1997a architecture minimale habile trouver compromis entre mesures performances telles temps apprentissage habilité généraliser méthodes constructives neurones diffèrent facteurs suivants parekh 1997a restriction entrées données entrée circonstances ajout nouvelle unité initialisation poids connexion cette unité apprentissage travail notre intérêt porte méthodes recherche architecture seaux neurones multicouches forward informations circulent entrées sorties retour résolution problèmes classification principaux mètres mesure performance traités taille réseau nombre neurones nombre couches complexité temps capacité généralisation certaines méthodes recherche architecture réseaux neurones évaluées données taille lativement petite qualité résultats varie ensemble données autre parekh 1997a autre comparaison théorique algorithmes notre connaissance faite notre étude portera essentiellement comparaison rithmes après mesures performances citées dessus résultats expérimentaux données tirées newmann opérations supplémentaires prétraitement données telles projection binarisation normalisation autres seront abordées cette étude reste papier organisé comme section suivante présente réseaux neurones multicouches quelques notions définitions apprentissage liées réseaux neurones multicouches troisième section recense algorithmes construction chitecture neuronale analyses expérimentales théoriques feront objet quatrième section tsopzé réseaux neurones multicouches algorithmes prentissage généralités réseaux neurones architectures vérifient propriétés suivantes cellules neurones unités réparties façon exclusive différentes couches première couche couche entrée composée cellules entrée respondent variables entrée généralement nombre unités nombre attributs exemples couche cachée composée unités effectuent calculs intermédiaires entre entrées sorties composée plusieurs autres couches dernière couche celle décision avoir aussi nombre unités nombre classes poids connexion réseaux multicouches généralement modifiés rétropropa gation rumelhart 1986a algorithme produit résultats lorsque archi tecture appropriée également utilisé lorsque architecture réseau reste statique curran riordan outils witten frank stuttgart neural network simulator offrent utilisateurs possibilité définir struc réseau outils apprennent réseaux retropropagation difficulté jeure trouver cette architecture cornuéjols miclet algorithme apprentissage poids connexion apprentissage systèmes neuronaux faire unité couche cipal algorithme apprentissage unités neuronales perceptron lorsque données linéairement séparables remplacé variantes pocket racket barycentric apprentissage couche généralisé toutes faire suivant principe winner montrer influence algorithme apprentissage complexité système présentons tableau entrer détails complexités temps algorithmes détails algorithmes apprentissage présentés parekh gallant frean 1992a théoriquement algorithmes utilisent manière similaire espace moire tableau présente complexités temps algorithmes apprentissage lequel variables désignent nombre objets nombre classes nombre attributs nombre itérations algorithme apprentissage algorithmes construction cette section présente résumé algorithmes mtiling mtower gallant parekh 1997a mupstart parekh 1997b distal 2simulateur réseaux neurones disponibles adresse internet informatik tuebingen etude algorithmes construction architecture réseaux neurones algorithmes complexité temps perceptron pocket racket modification thermal perceptron barycentric correction complexité temps algorithmes apprentissage convergence algorithmes démontrée parekh parekh 1997a algorithme mtiling mtiling adaptation algorithme tiling mezard nadal classifi cation multiclasses cette méthode construit réseau neurones multicouches lequel unités niveau couche reçoivent unités niveau inférieur immédiat mauvais classement réseau courant procédure détermine neurone ayant erreurs augmente certain nombre neurones auxillaires couche sortie courante augmente également nouvelle couche neurones réseau connecte entrées cette couche sorties unités couche sortie cette couche ajoutée devient nouvelle couche sortie neurones ajoutés appris individuellement algorithmes pocket variante nombre maximal couches cachées spécifié utilisateur couche sortie fonctionne suivant principe assurer seule classe active exemple donné algorithme mtower parekh 1997a cette méthode construit réseau forme comme méthode originale tower gallant architecture finale réseau telle neurones sommet sortie connectés neurones entrée neurone couche reçoit informa neurones couche immédiatement connectés couche construite ajoutant successivement couches unités réseau unités apprises variantes perceptron couche ajoutée complètement connectée couche sortie couche entrée cette couche devient ainsi nouvelle couche sortie modification réseau répétée jusqu obtention précision classement fournie utilisateur algorithme aussi arrêter nombre maximal couche teint couche sortie fonctionne aussi suivant principe méthode mpyramid parekh 1997a semblable mtower seule différence couche ajoutée reçoit information toutes couches précédentes algorithme mupstart parekh 1997b mupstart version algorithme upstart frean 1992b classification ticlasses comme upstart algorithme correction erreur produite tsopzé réseau courant réseau couche entrée unités couche sortie unités gauche augmenté wrongly droit wrongly parle wrongly wrongly lorsque sortie obtenue alors désirait obtenir plutôt neurone ajouté corriger erreur produite cette étape appris algorithme perceptron variantes neurones ajoutés appris sorties attendues définies tableau tableau représente sortie désirée sortie obtenue sortie attendue apprentissage unités correction différence entre tableau présentant sorties attendues unités ajoutées algorithme distal algorithme calcul distance entre exemple entre attribut distance euclidienne autre cette procédure calcule abord distances entre exemples attributs stocke matrice procédure distal construit partir matrice distances couche cachée réseau cette couche cachée construite manière suivante neurone ajouté apprendre exemples indice classe ayant grande plage exemples consécutifs appartenant classe neurones couche cachée distal seuil sphérique neurone actif θhigh inactif sinon résultat sortie poids connexion entre unité ajoutée unités entrée initialisés attributs exemple partir duquel trouvé étape suivante poids réseau entre couche cachée celle sortie multipliés unités couche fonctionnent suivant principe complexités evaluations théoriques algorithmes recherche architectures réseau neurones peuvent classés suivant approche construction réseau grands groupes approche descendante réseau final obtenu élimination neurones connexions architecture initiale comporte nombre suffisant neurones connexions capables classer exemples approche ascendante réseau obtenu ajout neurones connexions ajouts lorsque réseau considéré produit erreurs architecture initiale etude algorithmes construction architecture réseaux neurones comporte nombre assez réduit neurones généralement couches couche entrée couche sortie algorithmes présentés précédemment construisent réseau approche ascen dante mtiling permet combiner approches recherchant approche ascendante architecture maximale architecture ayant maximum couches ensuite élaguant cette architecture asymptotiquement algorithmes avons présentés comportement blable complexité algorithmes calculée présentée tableau distal exemple général nombre exemples toujours supérieur nombre attributs justifie temps maximal obtenu lorsque distance entre exemples considérée tableau présente complexités temps espace mémoire algorithmes tandis tableau présente autres aspects négligeables rithmes projection consiste ajouter attribut supplémentaire ayant valeur somme carrés autres attributs exemple particularités algorithmes algorithmes complexité temps complexité espace distal mupstart mtiling mtower mpyramid cascade complexité temps espace mémoire algorithmes nombre nombre neurones opération couches cachées couche cachée suplémentaire distal distance mupstart projection mtiling projection mtower projection mpyramid projection perceptron cascade projection recapitulatif certains aspects suivantes particularité algorithme mtiling correction erreur ajout couche neurones maitres cette couche devient nouvelle couche sortie ajout neurones couche cachée immédiatement connectée couche sortie connexions complètes entre couches adjacentes mtower ajoute simplement nouvelle couche neurones cette couche devient également nouvelle couche sortie complètement connectée couche entrée tsopzé distal spécifique autres méthodes construit réseau ayant seule couche cachée chaque neurone couche cachée permet séparer ensemble exemples réseau obtenu garantit classement exemples appris méthode mupstart quant ajoute chaque correction neurone appris individuellement connectée neurone ayant produit grand nombre erreurs expérimentations algorithmes testés données newmann tributs ensembles numériques valeur manquante expérimentations précédentes présentées parekh 1997a expériences porté algorithmes distal mupstart mtiling perceptron cascade mpyramid expériences étaient validées validation croisée ordre cross validation expériences distance choisie aucune justification entre exemples distance euclidienne unités ajoutées cours construction réseau autres appris perceptron racket modification résultats obtenus après classement avons obtenus aussi meilleurs présentés newmann données nombre exemples nombre attributs nombre classes spambase pendigits opdigits description données utilisées expérimentations cours expériences attention surtout portée chaque algorithme aspects suivants temps nécessaire construction réseau nombre total neurones réseau construit capacité généralisation expérimentations classées groupes distal seule méthode construit réseau faire apprentissage cette avons évalué séparément autres cette méthode présente résultats expérimentaux présence données petite taille lorsque données atteignent certaine taille distal produit résultat besoin excessif mémoire figure montre évaluation distal fonction taille données autres algorithmes construisent réseau neurones modifiant simultanément structure celui poids connexion leurs évaluations expérimentales teuses temps apprentissage unités ajoutées construction réseau algorithme perceptron racket modification itérations maximum choix arbitraire neurones couches précision apprentissage haitée temps affiché secondes représente temps obtenu meilleur système validation croisée etude algorithmes construction architecture réseaux neurones espace mémoire utilisé pendant éxécution nombre patterns pendigits optdigits spambase temps éxécution fonction taille données nombre patterns pendigits spambase optdigits nombre neurones couche cachée fonction taille données taille données pendigits spambase optdigits evaluation expérimentale algorithme distal tableaux présentent résultats expérimentaux algorithmes spambase respectivement comme nombre maximum couches cachées tableaux montrent influence nombre couches généralisation effet malgré temps élevé obtenons bonne capacité généralisation lorsque nombre couches élevé tableaux présentent résultats obtenus expérimentation données présentées nombre maximal couches cachées choisies cours périences choix arbitraire permis avoir résultats temps relativement raisonnable symbole signifie expérience échoué générale débordement nombre maximal couches atteint précision inférieure celle souhaitée notons données pendigits optdigits particularité rapport spambase exemples appartiennent comportement algorithmes données dessus appréciable saurait généralisé toute classification multiclasses tableaux montrent amélioration capacité généralisation lorsque nombre maximum couches passe explique influence nombre couches capacité généralisation discussion conclusion méthodes construction réseaux neurones presque semblables gorithmes initialisent réseau structure minimale modifient réseau ajout tsopzé algorithmes nombre neurones temps capacité couche cachée généralisation cascade mpyramid mtiling mtower mupstart expérimentations algorithmes données spambase nombre maximal couches algorithmes nombre neurones temps capacité couche cachée généralisation cascade mpyramid mtiling mtower mupstart expérimentations algorithmes données spambase nombre maximal couches unités mesure erreurs surviennent question savoir comment diriger choix concepteur problème architecture réseau rones choix architecture réseau neurones devra orienté satisfaction utilisateur final système choix devrait aboutir production réseau neurones ayant bonne capacité généralisation après notre étude pouvons séparer algorithmes catégories gorithmes construisent réseaux ayant seule couche cachée cette catégorie classé distal algorithme aussi avantage faible intervention utili sateur besoin énorme espace mémoire autres algorithmes construisent réseaux pouvant avoir couches cachées utilisateur fournir algorithme nombre maximal couches cachées précision souhaitée algorithme apprentis partir tables remarque variation généralisation fonction nombre maximal couches également noter mauvais comportement gorithmes données pendigits optdigits pouvons généraliser toutes grandes bases données généraliser devons tenir compte influence taille réseau répartition données quelque méthode construction réseaux neurones choix mètres modèle réseau nombre couches nombre neurones couches définition connexions apprentissage reste toujours problématique méthode etude algorithmes construction architecture réseaux neurones algorithmes nombre neurones temps capacité couche cachée généralisation cascade mpyramid mtiling mtower mupstart expérimentations algorithmes données pendigits nombre maximal couches cachées algorithmes nombre neurones temps capacité couche cachée généralisation cascade mpyramid mtiling mtower mupstart expérimentations algorithmes données pendigits couches distal trouver influence choix calcul distance entre exemples entre tributs expériences confirment comportement appréciable données petite taille mauvais comportement grands volumes données cette défaillance besoin énorme espace mémoire méthodes mupstart mtiling mtower pyramid perceptron cascade trouver influence algorithme apprentissage unités ajoutées choix nombre maximal couches cachées reste problématique dions actuellement application méthodes grandes bases données aussi envisageable trouver moyen traiter données résidentes disque algorithmes références cornuéjols miclet apprentissage artificiel concepts algorithmes eyrolles curran riordan applying evolutionary computation designing neural networks study state department information technology galway dreyfus samuelides martinez gordon badran thiria hérault réseaux neurones méthodologie applications eyrolles tsopzé algorithmes nombre neurones temps capacité couche cachée généralisation cascade mpyramid mtiling mtower mupstart expérimentations algorithmes données optdigits maximum couches algorithmes nombre neurones temps capacité couche cachée généralisation cascade mpyramid mtiling mtower mupstart expérimentations algorithmes données optdigits maximum couches frean 1992a thermal perceptron learning neural computation frean 1992b upstart algorithm method constructing training forward neural networks neural computation gallant perceptron based learning algorithms transactions neural networks hamber datamining concepts techniques morgan kauffman publishers parekh honavar distal inter pattern distance based constructive learning algorithm intell mezard nadal learning forward network tiling algorithm newmann hettich blake repository machine learning databases inform comput california irvine mldbrepository parekh 1997b mupstart constructive neural network learning algorithm multi category patterns classification proceedings international conference neural networks innsš97 parekh honavar constructive neural networks learning algorithms etude algorithmes construction architecture réseaux neurones multi category classification report department computer science state university describes package natbib parekh honavar 1997a constructive neural network learning algorithm multi category valued pattern classification report department computer science state university describes package natbib parekh honavar comparison performance variants single layer perceptron algorithms separable datasets neural parallel scientific computations parekh honavar constructive neural network learning algorithms pattern classification transactions neural networks rumelhart hinton williams 1986a learning internal representations error propagation parallel distributed processing cambridge press rumelhart hinton williams 1986b learning representations propagating errors nature shavlik towell kbann knowledge based articial neural networks artificial intelligence mbarki abram application réseau neurones estimati mayion coûts logiciel conférence africain recherche informatique witten frank mining practical machine learning tools niques edition morgan kaufmann francisco parekh mtiling constructive neural network learning algorithm multi category pattern classification proceedings world congress neural networks summary choice neural network architecture remains tremendous neural paper describes different algorithms build forward multilayer neural networks architecture these algorithms differ features paper presents oretical study these algorithms experimental study using cross validation datasets taken repository these experiments point difficulties choose networks parameters learning algorithm learning accuracy number layers