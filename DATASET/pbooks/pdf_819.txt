Discretization Continuous Features Resampling Taimur Qureshi Zighed University Avenue Pierre Mendès France 69676 Cedex France taimur qureshi abdelkader zighed lyon2 Résumé arbres décision largement utilisés générer classi ficateurs partir ensemble données processus construction partitionnement récursif ensemble apprentissage contexte tributs continus discrétisés alors chaque variable discrétiser trouver ensemble points coupure papier montrons recherche points coupure méthode échantillonnage comme BOOTSTRAP conduit meilleurs résultats avons testé cette approche méthodes principales discrétisation comme MDLPC FUSINTER CONTRAST Merge résultats systématique meilleurs utilisant bootstrap exposons principaux résultats ouvrons nouvelles pistes construction arbres décision Introduction process knowledge discovery first preprocess remove noise handle missing fields transformation reduction number variables discretization attributes defined continuous often performed which later provided mining algorithm important complex issues mining related transformation process discretization which consists converting numerical symbolic discrete emphasized quality knowledge discovery enhanced discretization because knowledge discovery techniques sensitive terms complexity choice discretization technique important consequences induction model addition numerical value ranges enough evaluation functions handle nominal domain example original versions popular machine learning gorithms could categorical Quinlan transform conti nuous discrete values decision learner world classification algorithms solve unless continuous attributes discretized termine intervals discretization numerical attributes infinite number candidates simple discretization procedure divides range continuous variable equal width intervals equal frequency intervals Fayyad suggested class pendent algorithm which reduce number attributed values maintaining relationship between class attribute values classified discretization methods different viewpoints supervised unsupervised static dynamic global local bottom direct incremental Unsupervised methods class information discretization process while supervised methods utilize class infor mation available unsupervised discretization method possible Dynamic methods perform discretization continuous values during classification process while static methods preprocess discretization before classification process Local methods local region instance space while global methods entire space methods FUSBIN MDLPC CONTRAST start interval split intervals process discretization based mostly binarization within subset training While bottom methods FUSINTER Merge split completely continuous values attribute merge intervals process discretization article focus these types strategies determining better discretization points providing comparisons terms quality prediction rates produce better discretization points Previously various studies estimate discretization points samples Significantly learning samples approximate discretization points whole lation argue learning sample approximation whole population optimal solution built single sample necessarily global terpretation leads resampling approach determine better distributions discretization points where point probability exact discretization point towards whole population doing attempt improve quality discreti zation better estimation discretization points entire population treating discretization problem statistical results paper performing resampling using bootstrap determine better estimate discretization point distribution entire population which shown improving prediction achieved discretization Moreover further improve quality predic obtained resampling applying discretization point selection protocol protocol selects points according criteria entropy resampling bootstrap frequency point distribution obtained resampling times improves further prediction Furthermore compare prediction rates different bottom strategies using resampling section framework discreti zation define calculations illustration results applying methodology example tailed Breiman dataset compare several bottom strategy based criteria merge based Statistical FUSBIN FUSIN based uncertainty principle MDLPC based information CONTRAST takes account homogeneity classes point density conclude observations deductions proposals future Definitions Notations Framework Formulation attribute value example learning value taken attribute attribute called endogenous variable class usually symbolic example belongs class suppose known learning sample build model denoted ideally discretization consists splitting domain continuous attribute intervals denote called discretization points which determined taking account particular attribute Prediction measure quality discretization taking account prediction which calculated follows denote prediction resulting discretization obtained applying method sample applying sample article different First small individuals corresponding class problem shown figure second large comparisons results Breiman waveform dataset having individuals attributes correspond three class problem boundary points sample classes Results Comparisons Illustration using Example Figure Consider figure individuals having classes perform FUSBIN discretization random bootstrap sample generate samples Figure gives discretization point distribution bootstrap random samples discretization achieved bootstrap little generalized defined small intervals While random sampling point distribution poorly defined large region values further argue difference increases becomes larger which shall Breiman calculated prediction estimating values above samples found rates bootstrap random sampling respectively showing bootstrap samples performed better difference further increasing added complexity population shown subsection Discretization point distribution random bootstrap samples improve quality prediction introducing notion discretization point selection protocol protocol selects discretization points given point quency distribution having higher probability occurrence splits those points certain criterion entropy illustrate figure highest bable point point split population certain criterion FUSBIN entropy continue process obtained splits manner until criterion allows further splitting points frequency distribution ready chosen applied protocol bootstrap random samples selec discretization points frequency point distributions respectively calculated prediction bootstrap random pling demonstrating better quality discretization achieved selection bootstrap further argue sampling gives variation prediction rates bootstrap samples prediction varies difficult obtain generalized estimate discretization points original population protocol achieves defined discretization points better estimate original discretization points Analysis Results using Breiman Waveform section Breiman waves generated bootstrap random samples points sample points taken sample vector components denoted label repeated process described above waveform variable generated bootstrap samples above performed FUSBIN bootstrap random samples obtained prediction rates respectively showing better formance bootstrap sampling applied discretization point selection protocol point distribution obtained selected points using FUSBIN criterion sampling methods found prediction points obtained bootstrap distribution lesser value random sampling showing significant amount improvement prediction using resampling bootstrapping Finally compare FUSINTER FUSBIN CONTRAST MDLPC Merge sampling according following procedure methods compare First obtain discretization points bootstrap samples create frequency point distribution variable using selection protocol MDLPC ChiMerge CONTRAST FUSBIN FUSINTER MDLPC ChiMerge CONTRAST FUSBIN FUSINTER Computed Results Difference Prediction select discretization points those point distribution frequencies applying criterion respective method which initial discretization points obtained compute prediction rates selected discretization points method relation whole sample difference prediction rates obtained conclude better significantly superior Table presents comparison terms difference means prediction rates riables Positive values indicate method better method column Aside Merge method whose results relatively other methods relatively smaller differences However among those methods MDLPC seemed lesser complexity FUSBIN FUSINTER smaller complexity comparison CONTRAST which quadratic complexity which taken account number examples becomes Conclusion learning sample approximation whole population optimal discreti zation built single sample necessarily global optimal Resampling gives better estimate discretization point distribution terms achieving defined distribution Applying discretization point selection protocol frequency distribution achieved resampling significantly improves quality discretization prediction nearing global optimal solution Moreover protocol applied frequency point distribution random samples achieved lesser improvements prediction compared bootstrap applied protocol after resampling various methods Except Merge other methods provide small variations terms prediction rates MDLPC performs FUSBIN achieves complexity which point dealing examples future shall apply discretization approach context decision trees whether improves global performance carrying approach needs answer other questions complexity apply potential discretiza points context fuzzy discretization decision trees Références Zighed Rabaséda Rakotomalala Discretization Methods Supervised Encyclopedia Computer Science Technology vol40 Breiman Friedman Olshen Stone Classification Regression Trees Wadsworth International Francisco Wehenkel Information Quality Based Decision Pruning Method Proceedings International Conference Information Processing Management certainty Knowledge Based Systems IPMUŠ92 Kerber Discretization Numeric Attributes Proceedings Tenth National Confe rence Artificial Intelligence Press Cambridge Zighed Rakotomalala Rabaséda Discretization Method Continuous Attributes Induction Graphs Proceeding European Meetings Cyberne System Research Fayyad Irani Multi interval Discretization Continuous Valued Attributes Classification Learning Proceedings International Joint Conference ficial Intelligence Morgan Kaufmann Mateo pp1022 Merckt Decision Trees Numerical Attribute Spaces Proceedings International Joint Conference Artificial Intelligence Morgan Kaufmann Mateo Mooney Duval Bootstrapping Nonparametric Approach Statis tical Inference University Paper series Quantitative Applications Social Sciences Newbury Kusiak Feature transformation methods mining Trans Electronics Packaging Manufacturing Hussain Discretization enabling technique Mining Knowledge Discovery Quinlan Improved continuous attributes Journal Artificial Intelli gence Research Flach Discretization Enhance Continuous Decision Induction Integrating Aspects Mining Decision Support Learning pages workshop notes September Summary Decision induction widely generate classifiers training through process recursively splitting space training continuous valued associated attributes discretized advance during learning process generate discretization points performing resampling original produce selection discretization points using resampling selection proto generate discretization points using ordinary random sampling calculate prediction discretization points obtained using sampling resampling techniques process repeated using different discretization strategies mentioned above paper observe whether resampling technique better discretization points which opens paradigm construction decision trees