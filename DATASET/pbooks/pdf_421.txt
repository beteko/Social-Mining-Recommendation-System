algorithme exploratoire subspace clustering Sylvain Dormieu Nicolas Labroche Paris place Jussieu 75005 Paris France sylvain dormieu gmail nicolas labroche Résumé article propose nouvel algorithme problème space clustering dénommé Contrairement approches descendantes classiques repose hypothèse localité permet affectation donnée plusieurs clusters espaces différents expéri mentations préliminaires montrent notre approche obtient meilleurs sultats algorithme COPAC référence appliquée données réelles Introduction méthodes classification supervisée clustering classiques synthétisent formation construisant groupes données données souvent définies ensemble attributs clusters résultants déterminés également attributs Plusieurs méthodes comme pondération sélection attributs métriques adaptées permettent modifier limiter supprimer influence certains tributs ensemble clusters généralement défini espace Cependant certains groupes peuvent pertinents ensemble attributs ensemble attributs caractéristiques appelé espace cluster donnée appartenir plusieurs clusters définis espaces différents Comme indiqué Kriegel objectif méthodes subspace clustering découvrir clusters espaces exemple données représentant objets différentes formes férentes couleurs possible déterminer plusieurs espaces évidents basés couleur forme enfin attributs exemple carré rouge devrait pouvoir appartenir cluster rouge espace limité attribut couleur cluster carré limité espace forme papier organisé comme section rappelle principaux travaux conduits domaine subspace clustering section décrit algorithme section présente résultats comparatifs algorithme COPAC données artificielles illustre résultats données réelles issues Machine Learning Reposi section conclut article présente perspectives algorithme exploratoire subspace clustering Travaux existants subspace clustering domaine assez récent Parsons Kriegel déterminer conjointement clusters leurs espaces associés Contrai rement approches classiques clustering lesquelles phase partitionnement précédée phase sélection pondération attributs subspace tering dissocie définition espace celle groupe données conséquence donnée théoriquement appartenir plusieurs clusters définis espace propre subspace clustering défini principaux travaux Parsons Kriegel clarifient terminologie distinguent subspace clustering autres domaines proches comme biclustering coclustering distingue plusieurs méthodes subspace clustering fonction mécanisme sélection attributs construction clusters Certains algorithmes reposent mécanismes pondération attributs autres recherchent espaces potentiels manière ascendante espaces dimension espace contenant toutes dimensions inversement descendante méthodes reposent mécanisme sélection pondération attributs appar tiennent domaine subspace clustering Gustafson Kessel Candillier principale méthodes affecter poids chaque attribut utiliser optimisation alternée rechercher maximum local fonction objectif existe toutefois nombreuses limitations méthodes définition nombre clusters priori garantie convergence optimum global affectation éventuellement floue chaque donnée plusieurs cluster défini unique espace autres approches reposent exploration systématique espaces gibles partant espaces petits approches ascendantes basées mécanisme recherche itemsets fréquents exemple algorithme CLIQUE Agrawal intègre mécanisme agrégation ensembles denses basse dimen sionnalité retrouver ensembles denses haute dimensionnalité Toutefois complexité algorithme grande rapport nombre attributs inverse méthodes ascendantes méthodes descendantes commencent étudier ensemble attributs avant déterminer sélectionner attributs caractéristiques réduire nombre dimensions algorithme efficace lorsque répartition données vérifie hypothèse localité définie Kriegel sélection locale données suffit estimer orientation locale données Cette définition localité repose calculs proches voisins lisent ensemble attributs définir voisinage local Cette hypothèse semble pertinente pratique espace grande dimension nombreux attributs caractéristiques affectent calcul voisinage choix attributs caractéristiques nombreux algorithmes utilisent heuristique proches voisins Achtert Friedman Meulman Plusieurs paramètres estimés locale chaque cluster comme orientation voisinage utilisés ensuite agréger cluster données vérifiant relation proximité Toutefois comme précédemment algorithmes affectent donnée unique cluster espace Enfin algorithme Achtert diffère approches descendantes précé dentes repose hypothèse localité modèle clusters modéli hyperplans espace hyperplans contenant minimum donnée divisé Dormieu Labroche grille parcouru déterminer quels hyperplans contenant nombreuses données réitération calcul modélisation hyperplan permet construire espaces Cette méthode possède cependant complexité rédhibitoire décrivons section suivante modèle algorithme comme gorithme repose hypothèse localité possède complexité moindre Algorithme algorithme détermine chacune itérations cluster espace associé Chaque itération indépendante précédentes repose processus étapes principales génération aléatoire cluster potentiel détermination hyper propre cluster potentiel calcul densité données chacun attributs extension hyper partir densité obtenir cluster maximal Génération aléatoire cluster potentiel Contrairement approches déterminent clusters partir voisinage seule donnée notre approche sélection aléatoire plusieurs données former graine premier cluster potentiel Cette sélection plusieurs données amène robustesse détermination attributs caractéristiques cluster contrairement voisinage local permet considérer plages valeurs importantes moins sensible variations locales densité attributs points aberrants Enfin chaque itération distribution aléatoire initiale points favorise émergence attributs caractéristiques différents assure bonne couverture espace solutions Comme recherche modèle reliant données données mutuellement proches Détermination hyper cluster potentiel définit hypercube potentiel comme produit intervalles chacun attributs espace initialRm Chaque intervalle attribut cluster défini comme intervalle minimal englobant ensemble valeurs points attribut aussi désigne valeur attribut point cette étape ajoute cluster potentiel ensemble données contenues hypercube Calcul densité Cette étape déterminer densité locale cluster potentiel chaque attribut définit séquence comme ensemble ordonné valeurs attribut intervalle densité ensuite simplement défini comme distance maximale observée attribut entre valeurs consécutives désigne élément séquence algorithme exploratoire subspace clustering Détermination cluster hypercube maximal déterminé attribut notre algorithme agrège itérativement cluster potentiel points coordonnées situées distance inférieure frontières hypercube attributs hypercube associé cluster ensuite processus agrégation nouveaux points réitéré jusqu aucun candidat puisse ajouté cluster alors maximal Paramétrage algorithme repose paramètres fixés utilisateur premier nombre maximal itérations permet optimiser couverture qualité clusters leurs espaces associés rapport temps calcul second paramètre nombre données sélectionnées générer graines clusters potentiels petite valeur diminue temps calcul grande valeur permet meilleure estimation densité attributs cluster potentiel obtenir meilleures performances conjonction paramètre Expérimentations Cette section présente expérimentations conduites valider notre algorithme première expérimentation propose comparaison notre algorithme thode COPAC Achtert mesure moyenne mesures découverts pertinents utilisons implémentation COPAC fournie framework Achtert bases données utilisées premier données artificiel référence nommé Parsons Parsons clusters rapprochés peuvent poser problème méthodes comme COPAC reposant hypothèse localité données modifié séparant clusters favorable méthode COPAC reposant hypothèse localité seconde expérimentation évaluer capacité méthode produire clusters interprétables données réelles nommée Quinlan Résultats comparatifs Parsons utilisons méthode évaluation consiste partir ensemble clusters cibles connus intégrer score total chaque meilleur cluster généré rapport chaque cluster cible liste clusters générés algorithme clusters cibles chaque cluster cible cluster ayant meilleur score rapport proposons comme score général algorithme moyenne meilleurs scores rapport chaque cluster cible Score Discussion résultats Parsons Parsons modifiée meilleurs résul obtenus approche COPAC expérimentations résultat proximité clusters effet calcul voisinage inclut attribut espace conséquent calcul orientation voisinage intègre données clusters différents faussant résultat final quand sensible hypothèse Dormieu Labroche Score Parsons fonction rapport COPAC clusters générés localité quand génère clusters candidats aléatoirement score Parsons modifiée COPAC obtient score proximité clusters cause mauvaise performance COPAC premier données obtient score légèrement supérieur COPAC données réelles Contrairement données Parsons existe étiquettes clusters théoriques permettant évaluation objective résultats proposons image démarche suivie Candillier étudier cette réelle pertinence interprétabilité clusters découverts produisant grand nombre clusters avons retenu expérimentalement clusters parmi denses espace conduire notre interprétation densité clusters calculée partir nombre données cluster divisé volume cluster longueur intervalles hypercube bornée calcul minimum lancé comme valeurs paramètres 30000 clusters ayant effectif inférieur supprimés clusters restants triés ordre densité décroissante premiers clusters classement relativement homogènes analyse rapportée tableau illustre profils principaux clusters ainsi découverts Discussion résultats après tableau premier cluster représente segment petites voitures légères économiques deuxième cluster celui grosses tures puissantes Toutefois remarque accélération année origine attributs caractéristiques premier cluster tandis deuxième cluster seule lération semble caractéristique manière inattendue après accélération caractéristique importante voitures identifées comme puissantes algorithme exploratoire subspace clustering Conclusion perspectives article avons présenté algorithme subspace clustering repose hypothèse localité expériences montré amélioration résultats rapport COPAC pertinence résultats données réelles Références Achtert David Kröger Zimek Robust clustering arbitrarily oriented subspaces International Conference Mining Achtert Kriegel Kröger Zimeck Robust complete ficient correlation clustering International Conference Mining Achtert Kriegel Zimek Software System Evaluation Subspace Clustering Algorithms Scientific Statistical Database Management Scien tific Statistical Database Management SSDBM Berlin Heidelberg Agrawal Gehrke Gunopulos Raghavan Automatic subspace clustering dimensional mining applications SIGMOD Candillier Tellier Torre Bousquet Statistical Subspace Clustering Machine Learning Mining Pattern Recognition International Conference Machine Learning Mining Pattern Recognition Friedman Meulman Clustering objects subsets attributes Royal Statist Series Statistical Methodology Gustafson Kessel Fuzzy clustering fuzzy covariance matrix Volume Kriegel Kröger Zimek Clustering dimensional survey subspace clustering pattern based clustering correlation clustering Trans Knowl Discov MacQueen methods classification analysis multivariate observa tions Berkeley Mathematical Statist Probability Parsons Haque Subspace clustering dimensional review SIGKDD Explor Newsl Quinlan Combining instance based model based learning Machine Proceedings Tenth International Conference Morgan Kaufmann Summary article introduce subspace clustering algorithm called Unlike approaches assume locality assumption assign several clusters different subspaces Preliminary experiments approach provide better results COPAC algorithm reference dataset