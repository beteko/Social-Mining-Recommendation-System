Régression logistique classification images grande échelle Thanh François Poulet Université Vietnam dtnghi Université Rennes IRISA Campus Beaulieu 35042 Rennes Cedex France francois poulet irisa Résumé présentons nouvel algorithme parallèle régression logis tique classification images grande échelle posons plusieurs extensions algorithme original régression logistique classes développer version efficace grands ensembles données images plusieurs centaines classes présentons nouvel algorithme BBatch descente gradient stochastique gression logistique batch équilibré apprentissage parallèle approche contre reste multi classes multiples cœurs résultats expérimen ensembles données ImageNet montrent notre algorithme efficace comparés algorithmes classification linéaires Introduction classification images assigner automatiquement catégorie prédéfinie image Parmi nombreuses applications citer reconnaissance caractères nuscrits reconnaissance empreintes digitales reconnaissance visages nombre images stockées différentes bases données cesse croître exemple cebook contient aujourd milliards images estimé utilisateurs moyens appareils photos numériques prendront environ photos internet volume données chargées correspond images catégorisation images contenu véritable challenge importance aujour approches performantes dernières années utilisent modèle visuels Words construits descripteurs locaux images visuels adaptation utilisé catégorisation textes ensemble documents chaque document contenant ensemble modèle correspond nombres occurrence documents visuels commence calculer descripteurs niveau points particuliers image populaires SIFTs Scale Invariant Transform SURFs Speeded Robust Features DSIFTs Dense SIFTs Bosch méthodes extraction descripteurs Régression logistique classification images grande échelle basées apparence objets points particuliers image invariantes changements échelle rotations occlusions variations luminosité suite utilise méthode quantification descripteurs exemple appliquant algorithme clustering comme means MacQueen chaque cluster alors considéré comme visuel finalement obtenir distribution SIFTs chaque image ensemble clusters obtenus modèle visuels conduit ensembles données ayant grands nombres dimensions Support Vector Machine Séparateurs Vaste Marge Vapnik utilisés classification ensembles données Cependant ensembles images geNet millions images 21841 classes rendent tâche classification complexe challenge conduits développer nouvel rithme efficace temps exécution précision classification présentons extensions algorithme descente gradient stochastique Shalev Shwartz Bottou Bousquet créer nouvel algorithme parallèle régression logistique multi classes classification grands ensembles données images nombre important classes contributions comprennent algorithme descente gradient stochastique régression logistique équilibré batch équilibré BBatch classification ensembles données grand nombre classes apprentissage parallèle classifieurs machines multi cœurs expérimentations menées ensembles données issus ImageNet montrent notre algorithme efficace comparé algorithmes reste article organisé manière suivante section présente vement algorithme descente gradient stochastique régression logistique problème classification classes section décrit comment étendre gorithme plusieurs classes paralléliser section présente résultats expérimentations avant conclusion travaux futurs Régression logistique classification classes problème classification classes points dimen étiquettes classe correspondantes régression logistique essaye apprendre modèle données vecteur dimension maximise vraisemblance probabilité appartenance point classe positive probabilité appartenance point classe négative probabilités peuvent réécrire forme suivante Poulet vraisemblance régression logistique maximiser vraisemblance marge conduit problème formule régression logistique utilise fonction perte logistique solution problème convexe aussi obtenue méthode gradient chastique Shalev Shwartz Bottou Bousquet descente gradient stochastique régression logistique notée algorithme effectue calcul itérations apprentissage chaque itération utilise point calculer gradient mettre comme algorithme utilisant décrit algorithme Régression logistique grand nombre classes présentons plusieurs extensions régression logistique classes multi classes classes approches peuvent classées catégories première considère multi classes comme problème optimisation Akiva Lerman Weston Watkins Guermeur seconde décompose problème multi classes série problèmes classes incluant approches contre reste Vapnik contre Kreßel graphe décision acyclique orienté Platt pratique approches contre contre reste utilisées simples mettre œuvre problème classes stratégie contre reste Régression logistique classification images grande échelle Algorithme Algorithme input training dataset positive constant number epochs output hyperplane begin randomly datapoint training dataset update ytxt1 return construit classifieurs ieclassifieur sépare classe reste stratégie contre quant construit classifieurs utilisant toutes paires possibles classes classe ensuite prédite majoritaire Lorsque traite ensembles ayant grand nombre classes plusieurs taines stratégie contre devenir coûteuse mettre œuvre calculer plusieurs milliers classifieurs stratégie contre reste alors préférée notre algorithme multi classes aussi utiliser cette stratégie contre reste effectuant apprentissages conséquence algorithme apprentissage contre reste conduit problèmes suivants algorithme faire classes déséquilibrées construire classi fieurs binaires algorithme coûteux temps exécution effectuer apprentissage séquentiel proposons améliorations création nouvel algorithme capable traiter nombre classes temps raisonnable première construction classifieurs équilibrés méthode échantillonnage classe joritaire seconde parallélisation mécanisme apprentissage machines multi cœurs Batch équilibré régression logistique approche contre reste tâche apprentissage algorithme essayer séparer classe positive autres classes classe négative Lorsque grands nombres classes classes conduit déséquilibre entre classe positive classe négative problème algorithme vient ligne algorithme traiter centaines classes probabilité avoir point positif faible exemple rapport forte probabilité avoir Poulet point classe négative exemple algorithme concentre alors essentiellement erreurs classe négative ainsi difficultés séparer classe positive classe négative Différentes solutions aussi niveau données niveau algorithme déséquilibre classes présentées Japkowicz Weiss Provost Ralescu encore Lenca niveau données algorithmes modifient distribution classes échantillonnage classe minoritaire Chawla échantillonnage classe majoritaire Ricamato niveau algorithme solution modifier équilibre classes jouant coûts mauvaise classification classes Notre algorithme balancé batch BBatch appartient mière catégorie séparer classe positive reste classe négative probabili fortement déséquilibrées exemple probabilité appartenance classe sitive classes ImageNet ailleurs échantillonnage étant relativement élevé proposons algorithme BBatch utili échantillonnage classe majoritaire avons modifié algorithme batch équilibré unique point ligne effectuer chaque itération avons aussi modifié manière suivante ηtλwt ηtλwt cardinal classe positive cardinal classe négative algorithme BBatch Algorithme sépare classe positive reste classe négative utilisant échantillonnage classe négative batch équilibré dification Parallélisation apprentissage BBatch algorithme BBatch puisse traiter grands ensembles données temps raisonnable avantage possibilités architectures multi cœurs machines actuelles algorithme BBatch effectue apprentissages indépen dants classification classes propriété intéressante parallélisation améliorer temps exécution algorithme BBatch allons effectuer apprentissage classifieurs binaires parallèle plusieurs machines multi cœurs programmation parallèle principalement basée modèles Message Passing Interface Forum OpenMP Multiprocessing OpenMP Architecture Review Board système standardisé mécanisme envois messages systèmes mémoire distribué actuellement système utilisé parallélisation Cependant défaut nécessiter ensemble données mémoire effectuer apprentissage signifie utilise classifi cation classes parallèle besoin quantité mémoire charger ensembles données inutilisable contexte allons utiliser Régression logistique classification images grande échelle Algorithme Algorithme BBatch input training positive class training negative class positive constant number epochs output hyperplane begin creating balanced batch Bbatch sampling without replacement dataset datapoint dataset Bbatch update using return OpenMP besoin autant mémoire parallélisation moins timisée celle apprentissage parallèle algorithme BBatch décrit algorithme Algorithme Apprentissage parallèle BBatch input training dataset classes output model Learning pragma parallel class training BBatch Evaluation évaluer performances notre nouvel algorithme parallèle régression logistique multi classes classification grands ensembles images grand nombre classes avons algorithme utilisant bibliothèque Library Bottou Bousquet comparaisons porteront précision temps apprentissage avons choisi algorithmes récents classifi cation LIBLINEAR library large linear classification Poulet optimized cutting plane algorithm Franc Sonnenburg réputés efficaces classification linéaires LIBLINEAR utilisés leurs paramètres défaut rithme effectue apprentissage batch équilibré descente gradient stochas tique régression logistique utilisant itérations terme régularisation Toutes expériences effectuées Linux Fedora Intel cœurs mémoire Ensembles données algorithme conçu classification grands ensembles images grand nombre classes allons évaluer trois ensembles données suivants ImageNet10 ensemble données contient grandes classes ensemble ImageNet images chaque classe images servent apprentissage commençons construire visuels utilisant chaque image descripteurs DSIFTs Dense SIFTs vocabulaire Ensuite featuresmaps Vedaldi Zisserman représentation haute dimension image dimension 15000 Cette méthode donné résultats classification images classifieurs linéaires obtenons finalement ensemble données apprentissage ImageNet100 ensemble données contient grandes classes ensemble ImageNet 183116 images chaque classe prenons images prentissage restants manière construisons visuels utilisant DSIFTs vocabulaire visuels utili featuresmaps obtenons finalement entrée algorithme apprentissage ensemble ILSVRC2010 ensemble données contient grandes classes ensemble ImageNet million images apprentissage 50000 images validation 150000 images utilisons visuels fournit méthode décrite encoder chaque image vecteur dimensions 21000 retenons images classe ensemble apprentissage nombre total images apprentissage 887816 Toutes données ensemble utilisées tester modèle obtenu Résultats classification abord intéresse performances concerne temps exécution avons varier nombre threads OpenMP threads chaque exécution notre algorithme raison architecture utilisée Intel Régression logistique classification images grande échelle Temps apprentissage ImageNet10 Algorithme thread threads threads LIBLINEAR Temps apprentissage ImageNet100 Algorithme thread threads threads LIBLINEAR cœurs expérimentations algorithme rapide threads tellement threads petit ensemble images multi classes ImageNet10 temps apprentissage algorithmes tableau montre notre algorithme threads rapide rapide LIBLINEAR tableau figure présentent temps apprentissage ensemble données ImageNet100 grand nombre classes encore notre algorithme procure significatif temps exécution utilisant threads rapide rapide LIBLINEAR ensemble données ILSVRC possède grand nombre images million grand nombre classes achever calcul prentissage après plusieurs semaines LIBLINEAR besoin minutes jours heures apprentissage modèle classification ensemble 16h45 threads Notre algorithme threads quant besoin minutes effectuer cette tâche apprentissage montre notre algorithme rapide LIBLINEAR concerne précision classification résultats présentés tableau figure petit ensemble données ImageNet10 moyen ImageNet100 algorithme obtient meilleur bonne précision concerne comparaison LIBLINEAR performances quasiment identiques Notre algorithme meilleure précision ensembles ImageNet10 ILSVRC2010 Temps apprentissage ILSVRC Algorithme thread threads threads LIBLINEAR Poulet Temps apprentissage ImageNet100 Temps apprentissage ILSVRC Précision classification Algorithme ImageNet ImageNet ILSVRC LIBLINEAR Régression logistique classification images grande échelle Précision classification précision moindre ImageNet100 ILSVRC2010 grand ensemble données million images classes challenge obtenir classification algorithmes particulier ensemble données fournit compétition ILSVRC2010 meilleurs algorithmes obtenus précision ordre Notre algorithme obtient meilleure précision contre représente amélioration relative comparé LIBLINEAR obtient amélioration relative remarquer algorithme beaucoup rapide LIBLINEAR ayant précision équivalente Enfin aussi gourmand mémoire puiqu nécéssite mémoire contre LIBLINEAR résultats montrent notre algorithme bonnes qualités traiter intégralité ensemble données ImageNet taille ensemble données augmente rapport entre temps apprentissage entre notre algorithme LIBLINEAR favorables passe facteur rapide ImageNet10 rapide ILSVRC2010 Conclusion travaux futurs avons présenté nouvel algorithme parallèle régression logistique multi classes permet classification efficace grands ensembles données images grand nombre classes avons utilisé algorithme batch équilibré descente gradient stochastique régression logistique apprentissages classes cadre classification multi classes nouvel algorithme permet notamment sification grands ensembles données images architectures multi cœurs performances algorithme évaluées grandes classes semble données ImageNet ensemble données ILSVRC2010 Notre algorithme présente gains significatifs concerne temps exécution facteur Poulet ensemble données petit grand ayant précision similaire résultats montrent ensemble données traiter grand rapport autres algorithmes important serait doute encore important avions utilisé machine disposant cœurs effet pratiquement différences entre threads extensions travaux prévoient version hybride OpenMP algorithme parallèle régression logistique classifier efficacement grands ensembles données multi classes permettre classification totalité ensemble données ImageNetsur ensemble machines multi cœurs Références Tuytelaars Speeded robust features Computer Vision Image Understanding Akiva Lerman Discrete Choice Analysis Theory Application Travel Demand Press Large scale visual recognition challenge Technical report Bosch Zisserman Muñoz Image classification using random forests ferns International Conference Computer Vision Bottou Bousquet tradeoffs large scale learning Platt Singer Roweis Advances Neural Information Processing Systems Volume Foundation books Chawla Lazarevic Bowyer Smoteboost improving diction minority class boosting Proceedings Principles Knowledge Discovery Databases classifying image categories European Conference Computer Vision Chang Hsieh LIBLINEAR library large linear classification Journal Machine Learning Research Franc Sonnenburg Optimized cutting plane algorithm large scale minimization Journal Machine Learning Research Guermeur multiclasses théorie applications Japkowicz Workshop Learning Imbalanced Number Report Kreßel Pairwise classification support vector machines Advances Kernel Methods Support Vector Learning Lenca Lallich comparison different centered entropies class imbalance decision trees Pacific Confe rence Knowledge Discovery Mining Springer Verlag Exploratory undersampling class imbalance Transactions Systems Cybernetics Régression logistique classification images grande échelle Distinctive image features scale invariant keypoints International Journal Computer Vision MacQueen methods classification analysis multivariate observa tions Proceedings Berkeley Symposium Mathematical Statistics Probability Berkeley University California Press Forum message passing interface standard OpenMP Architecture Review Board OpenMP application program interface version Lenca Lallich Using local information decision trees coupling local decision centered International Conference Mining Vegas Nevada CSREA Press Platt Cristianini Shawe Taylor Large margin multiclass classifi cation Advances Neural Information Processing Systems Ricamato Marrocco Tortorella based balancing techniques skewed classes empirical comparison Shalev Shwartz Singer Srebro Pegasos Primal estimated gradient solver Proceedings Twenty Fourth International Conference Machine Learning Vapnik Nature Statistical Learning Theory Springer Verlag Vedaldi Zisserman Efficient additive kernels explicit feature Transactions Pattern Analysis Machine Intelligence Ralescu Issues mining imbalanced review paper Midwest Artificial Intelligence Cognitive Science Dayton Weiss Provost Learning training costly effect class distribution induction Journal Artificial Intelligence Research Weston Watkins Support vector machines multi class pattern recognition Proceedings Seventh European Symposium Artificial Neural Networks Power large scale visual classification Computer Society Conference Computer Vision Pattern Recognition Summary present parallel multiclass logistic regression algorithm aiming classifying large number images dimensional signatures classes extend class logistic regression algorithm several develop multiclass efficiently classifying large image datasets hundreds classes propose balanced batch stochastic gradient descend logistic regression BBatch training class classifiers versus strategy multiclass problems parallel training process classifiers several multi computers numerical results ImageNet datasets algorithm efficient compared state linear classifiers