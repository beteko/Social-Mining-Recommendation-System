Evaluating Bayesian Networks Sampling Simplified Assumptions Saaid Baraty Simovici University Massachusetts Boston Computer Science Department Boston Massachusetts 02125 sbaraty Abstract common fitness evaluation Bayesian networks presence Cooper Herskovitz criterion technique involves massive amounts therefore expansive computations propose cheaper alternative evaluation method using simplified assumptions which duces evaluations strongly correlated Cooper Herskovitz crite Introduction investigate problem constructing Bayesian network composite nomenon where discrete random variables representing state assignment attributes accomplish start multiset where tuple instance event refer multiset evidence short number assumptions necessary deriving measure evaluating fitness Bayesian network structure training Stronger hypotheses evaluationmoremanageable other model obtained under weaker assumptions better capable conforming underlying distribution problem directed acyclic graph having vertices edges which captures direct probabilistic dependencies among these variables collection parameters which quantifies joint probability distribution specified denote possible assignments random variable notion domain extended variables using Cartesian product parent nodes descendants nodes excluding descendants clear context subscript satisfies local Markov condition where probability distribution specified model Bayesian network satisfies local Markov condition chain Therefore θijri joint probability distribution specified Evaluating Bayesian Networks Sampling Posterior based Score Reduced Assumptions Cooper Herskovitz introduced probability measure assessing fitness probabilistic model Since constant across different networks space probability distributions structure Recall collection distributions vectors satisfy itself collection these random vector variables treated random variable conditional probability function given conditional density function given structure prior probability function structure evaluate integral number assumptions introduced Cooper Herskovits independence assumes tuples independent given network structure local global independence assumption requires conditionally independent given structure Based assumption space possible collections written assumption Cooper Herskovits replace above product Equality assume distribution uniform refer assumption second order uniform probability Heckerman introduce metric which posterior based measure similar metric assumption three other assumptions second order Dirichlet probability suggested Cooper Herskovits parameter modularity multinomial sample assumption generalization assumption which states follows Dirichlet distribution multinomial sample assumption asserts define ordered where denotes restriction tuple state assign consistent Later assumption replaced other assumptions likelihood equiva lence structure possibility which imply assumption every probabil function follows Dirichlet distribution which requires parameters specify parameters makes approach impractical overcome difficulty Heckerman encoded prior knowl single Bayesian network referred prior network Dirichlet parameter corresponding probability distribution component ParGpr whereN given parameter which Baraty Simovici refer equivalent sample choice values collection without observing arbitrary sampling which enable shape distribution posterior probability vectors evaluation prior Cooper Herskovits assumed uniform prior distribution other assumptions based parameters arbitarily specified Sampling enables substitute strong assumptions domain knowledge determining parameters second order probability distribution prior probability disjoint samples evaluate measure fitness structure Since depend specific instead compute chain sample consistently across different structures constant dropped Therefore adopt relative measure fitness structures repeat process sampling extend measure where samples fromD where refer measure sample validation structure denote SAMPk disjoint samples first SAMPk written topological order nodes which represents expert prior knowledge domain Denote number occurrences tuple Since attributes discrete λlijr where first equality chain second equality assuming assumption λlijr λlijr otherwise Since λlijr second right Equality Evaluating Bayesian Networks Sampling assume theSOUP hypothesis posterior probability conditioned presence sample shown Equality approach different Cooper Herskovits where hypothesis applied directly without intervention sample Equality result Jeffreys Jeffreys pages reference previous equalities where Euler function Combining Equalities obtain approximate quantity slight variation measure called distribution distortion introduced Baraty Simovici evaluate conditional independency captured local Markov condition according assess degree conditions holds where frequency function relative sample achieve measure divergence probability distributions probability distributions Definition local Markov divergence structure according sample denoted LMDGS number where extends Kullbach Leibler divergence between probability distributions Shannon entropy partitioned according values conditional Shannon entropy partitioned according values conditioned partition according assignment attributesW Baraty Simovici Theorem LMDGS Theorem Theorem implies LMDGS Theorem smaller value closer structure satisfy local Markov condition according Therefore Markov condition closer satisfied according sample another closer divergent Baraty Simovici probability distributions every LMDGS haveHS means prediction capability perfect predication capability possible Bayesian structures attributes Define LMDGS Using previous evaluations SAMPk written LMDGS2q consistently sample across different structures constant entities respect assuming LMDGS2q SAMPk Experimental Results Conclusions conducted experiments three known structures domains Alarm Diagnosis2 Neapolitan Cancer nodes respec tively first structures randomly generated corresponding probability tables based probability distributions introduced generated sizes 80000 100000 respectively corresponding literature missing values randomly generated number structures different complexities number edges these structures ranged respectively Figures strong correlations between score score various values derived measure cheaper compute since works samples smaller entire introduced measure based posterior probability measuring fitness Bayesian network structure based conclusion sampling based scoring viable cheaper alternative score sampling reduce assumptions strong correlation between measure confirms uniform distribution assumptions distort search Evaluating Bayesian Networks Sampling Alarm Diagnosis2 Neapolitan Cancer comparison diagram Correlations between SAMPk needed computing SAMP1 scores References Baraty Simovici evaluation Bayesian network structures Proceedings Australian Mining Conference Melbourne Cooper Herskovits Bayesian method induction probabilistic networks Technical Report Stanford University Knowledge System Laboratory Heckerman Geiger Chickering Learning Bayesian networks combination knowledge statistical InMachine Learning Jeffreys Jeffreys Dirichlet Integrals Cambridge Cambridge versity Press Résumé évaluation qualitative connue réseaux Bayesiens présence données critère Cooper Herskovitz Cette technique implique quantités massives données conséquent nombreux calculs proposons méthode évaluation efficace utilisant suppositions simplifiées produit évaluations fortement corrélées critère Cooper Herskovitz