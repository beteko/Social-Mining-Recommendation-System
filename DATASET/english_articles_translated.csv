,series,booktitle,year,title,abstract,authors,pdf1page,pdfarticle,lang,content_translated
1,Revue des Nouvelles Technologies de l'Information,EGC,2018,A two level co-clustering algorithm for very large data sets,"La classification croisée (co-clustering) est une technique qui permet d'extraire la structuresous-jacente existante entre les lignes et les colonnes d'une table de données sous forme de blocs. Plusieurs applications utilisent cette technique, cependant de nombreux algorithmes de co-clustering actuels ne passent pas à l'échelle. Une des approches utilisées avec succès est la méthode MODL, qui optimise un critère de vraisemblance régularisée. Cependent, pour des tailles plus importante, cette méthode atteint sa limite. Dans cet article, nous présentons un nouvel algorithme de co-clustering à deux niveaux, qui compte tenu du critère MODL permet de traiter efficacement de données de très grande taille, ne pouvant pas tenir en mémoire. Nos expériences montrent que l'approche proposée gagne en temps de calcul tout en produisant des solutions de qualité.","Marius Barctus, Marc Boullé, Fabrice Clérot",http://editions-rnti.fr/render_pdf.php?p1&p=1002372,http://editions-rnti.fr/render_pdf.php?p=1002372,en,"Un algorithme de co-clusters à deux niveaux pour très grandes quantités de données Bartcus Marius, Marc Boullé, Clérot Fabrice Orange Labs prenom.nom@orange.com Résumé. Co-classification est une technique d'exploration de données qui vise à identifier la structure sous-jacente entre les lignes et les colonnes d'une matrice de données sous la forme de blocs homogènes. Il a de nombreuses applications du monde réel, mais de nombreux algorithmes co-regroupement en cours ne sont pas adaptés à de grands ensembles de données. Une des approches de grands ensembles de données de co-dispersion est utilisé avec succès la méthode de classification co- MODL qui optimise un critère basé sur une probabilité régularisée. Cependant, on rencontre des difficultés avec d'énormes ensembles de données. Dans cet article, nous présentons un nouvel algorithme de co-regroupement à deux niveaux, étant donné le critère MODL al mugissement de traiter efficacement avec de très grands ensembles de données qui ne correspondent pas à la mémoire. Nos expériences, sur des données mondiales réelles et simulées, montrent que l'approche proposée réduit considérablement le temps de calcul sans froissement de manière significative la qualité de dé- la solution de co-regroupement. 1 Introduction Co-regroupement (Hartigan, 1972), le regroupement de blocs a également nommé (Govaert et Nadif, 2008) ou le regroupement bimode (Mechelen et al., 2004) est une technique d'exploration de données. Elle vise à identify- ing la structure sous-jacente entre les lignes et les colonnes d'une matrice de données sous la forme de blocs homogènes. Considérant que, le principe de regroupement norme est de regrouper des individus similaires (observations) par rapport à un ensemble de fonctionnalités, la tâche de co-regroupement est de simultané- groupe tanément individus similaires par rapport à des variables et des variables similaires par rapport aux observations, ainsi l'extraction de la structure de correspondance entre les objets et caractéristiques. Un autre avantage de la co-agrégation par rapport aux techniques classiques de clustering est sa capacité de réduction de la matrice, où une grande table de données peut être réduite dans une plus petite de manière significative tout en ayant la même structure que la matrice d'origine. En effet, cette technique trouve son utilisation dans de nombreuses applications comme dans les télécommunications, l'extraction de texte (Guigourès et al, 2015). (Dhillon et al., 2003; Li et Abe, 1998), l'exploitation minière de graphique (. Guigourès et al, 2015), etc. Plusieurs approches co-regroupement ont été proposées dans la littérature (Bock, 1979; Dhillon et al., 2003; Govaert et Nadif, 2008). Ces procédés diffèrent principalement en fonction du type de données analysées (catégoriques ou numériques), l'hypothèse sous-jacente, le procédé d'extraction et les résultats escomptés. Plusieurs familles d'approches ont alors été proposées pour effectuer des co-regroupement. Govaert et Nadif (2013, 2008) ont étudié les modèles probabilistes avec l'utilisation de variables latentes dans les modèles de mélange. Des difficultés surgissent lors de l'initialisation, un grand nombre de para- mètres d'estimer et l'efficacité de calcul, donc de grandes données sont difficiles à gérer. En effet, - 95 - Un algorithme de co-clusters à deux niveaux pour des ensembles de données très volumineux quelques méthodes capables de grandes quantités de données co-munitions ont été proposées dans la littérature. Par exemple, Pa- padimitriou et Sun (2008) a développé un outil, nommé DisCo mettre en œuvre un pré-traitement et co-regroupement des données distribuées en utilisant Hadoop et une mise en œuvre cartographique réduire. DisCo peut bien évoluer et d'analyser efficacement des ensembles de données extrêmement importantes, cependant, il a besoin d'une grande bué dis- infrastructure. Une autre méthode de co-regroupement, qui exploite des modèles probabilistes pour deux ou plusieurs variables de tout type (numérique ou catégorique) est basée sur l'approche MODL (Boullé, 2011). Les principaux avantages de la co-regroupement MODL est qu'il est facile à utiliser paramètre- gratuit et bénéficie d'algorithmes avec la complexité du temps sous-quadratique w.r.t. le nombre d'instances, ce qui permet de traiter de grands ensembles de données. Selon les avantages précédemment men- tionné nous nous concentrons sur l'approche co-regroupement MODL. En effet, la co-clustering MODL peut traiter de grands ensembles de données atteignant jusqu'à des millions de cas et des dizaines de milliers de valeurs par variable, avec une com- plexité temps sous-quadratique. Cependant, je t peut difficilement être utilisé avec des données très importantes, jusqu'à des milliards de cas et des variables ayant des millions de valeurs. Par exemple, cette limite est atteinte dans le cas de la yse Anal- de Enregistrement d'appel Retard à l'échelle du pays, lorsque la granularité étudiée va de l'antenne niveau (application dimensionnement du réseau) aux clients individuels (application de marketing avec l'identification des grains fins les communautés et la personnalisation de l'expérience client). Dans cet article, nous nous concentrons sur l'extension des algorithmes d'optimisation co-regroupement à ces grandes quantités de données, compte tenu du critère de co-clustering MODL. Malgré le fait que MODL peut faire face à de nombreuses variables numériques ou catégoriques et même mixtes, dans cet article, nous examinons le cas de deux variables. Ce document est organisé comme suit. Tout d'abord, pour des raisons d'auto-retenue, la section 2 rappelle les principes de la méthode de co-classification en utilisant le critère MODL (Boullé, 2011) qui accouple esti- la distribution conjointe entre deux variables. Ensuite, la section 3 présente le algorithme proposé à deux niveaux pour les grandes données co-regroupement. L'article 4, donne des résultats expérimentaux et évalue l'approche proposée sur des données réelles et simulées. Enfin, la section 5 est consacrée aux discussions et aux remarques finales. 2 Le co-regroupement MODL pour deux variables Soient X et Y deux variables avec des valeurs ensembles VX = {} VXI, avec V X = | VX | et VY = {} vYj, avec V Y = | VY |. Soit D = {(xn, yn), xn ∈ VX, VY yn ∈, 1 ≤ n ≤ N} un ensemble withn instances de données. Un exemple de cette représentation des données est donnée à la Fig. 1, dans lequel VX = {a, b} avec V = X 2, VY = {A, B, C} avec V Y = 3 et N = 4. FIG. 1 - Exemple de représentation des données. - 96 - I. Bartcus et al. L'ensemble de données D, représentée par cette table de contingence (voir la figure 1), peut être résumée en utilisant une partition de la valeur de chaque variable en grappes / groupes. Le produit croisé des deux partitions de taille I × J forme une (I × J) co-agrégation avec une cellule par paire de pièces de valeur. Notez que cette méthode diffère de la co-classification traditionnelle (Govaert et Nadif, 2013) qui considère la partition d'observations et de variables. Afin de choisir le « meilleur » modèle de co-regroupement M (compte tenu des données) de l'espace modèle M, nous utilisons une approche Maximum A Posteriori bayésien (MAP). Nous explorons l'espace de modèle tout en minimisant un critère bayésien, appelé coût. Le critère de coût des outils d'un compromis entre la sous-montage et sur-raccord et est définie comme suit: c (M) = - log P (M | D) α - p (M) log - log p (D | M) (1) où p (M) est l'avant et p (D | M) est la probabilité que les données indiquées le modèle de co-agrégation. Les détails sur le critère des coûts et l'algorithme d'optimisation (appelé KHC) sont disponibles dans Boullé (2011). La clé dispose de garder à l'esprit sont: (i) KHC est paramètre- libre, à savoir, il n'y a pas besoin de régler le nombre de groupes / groupes par dimension; (Ii) KHC fournit une solution localement optimale efficace pour la construction du modèle de co-agrégation, dans la complexité du temps quadratique sous-O (N √ N logN) plus précisément O (N * √ N * log *), où N * = Σ VX i = 1 ΣVY j = 1 1 {nij> 0} nij est le nombre de paires de valeurs réelles rencontrées au moins une fois. Comment- jamais, certains ensembles de données sont potentiellement jusqu'à des milliards de cas et des variables ayant des millions de valeurs. Ces ensembles de données ne peuvent pas être analysées à l'aide KHC, à moins d'utiliser des machines équipées de centaines de Go de RAM et d'attente encore des jours de calcul. 3 Scaled co-Clustering MODL Notre objectif est d'étendre les algorithmes d'optimisation co-regroupement à ces données à grande échelle, étant donné le critère co-regroupement MODL, tout en tenant compte des contraintes de mémoire suivantes pour la co-regroupement Scaled MODL. - l'algorithme peut stocker toutes les VX, VY valeurs dans la mémoire, - le N * paires réelles de valeurs ne peuvent pas être stockées dans la mémoire; et ils ne peuvent être stockés sur le disque, - nous pouvons exécuter notre algorithme de co-clusters sur des matrices de taille au plus I2maxi N *. Enfin, l'optimisation co cluster ing modèle doit tenir en mémoire avec la complexité de mémoire O (V X) + O (V Y) + O (I2max). Supposons, les données observées D peut difficilement être co-regroupés en raison d'une ou deux raisons sui- vants. En premier lieu, le nombre d'instances N peut être très grand et, deuxièmement, le nombre de valeurs sur chaque dimension V X V ou Y peuvent être trop grand pour être manipulé par des algorithmes de co-classification actuelle. Pour gérer cela, nous considérons les contraintes de mémoire et de proposer un algorithme des deux co-niveau qui permet KHC de produire des modèles co-regroupement plus rapide avec la plus petite diminution possible de leur qualité. L'algorithme est organisé en deux phases. La première phase consiste dans la phase de Split donnée par les deux étapes suivantes. 1. étape Cloisonnement: vise à obtenir des données sous-ensembles de l'ensemble des données, telles que les futurs co-regroupement sur chacun d'entre eux répondent aux contraintes de mémoire. 2. étape de co-regroupement fine: construit une co-classification de chaque ensembles de données sous en utilisant l'outil KHC. - 97 - Un algorithme de co-agrégation deux niveaux pour les ensembles de données très volumineux La deuxième phase consiste dans la phase d'agrégation avec les deux étapes suivantes. 3. étape Amalgamate: consiste à construire un co-classification globale sur l'ensemble de données initial (large), en fusionnant les co-clusterings obtenues à partir des ensembles de données secondaires. 4. étape de post-optimisation: améliore le modèle par les pistes suivantes. clusters de fusion première et seconde valeurs de déplacement entre les clusters. En conséquence, notre algorithme à deux niveaux proposé est un processus en quatre étapes, qui sont décrites plus précisément. 3.1 Phase de Split Dans notre méthode, nous adoptons une approche diviser pour mieux régner, qui commence par la phase de séparation. 3.1.1 étape de partitionnement FIG. 2 - Exemple de co-classification grossière du jeu de données D, avec I (c) x J (c) secondaires co-clusters. Un grand ensemble de données, avec au moins une violation de contrainte de mémoire, nous conduit à la première étape de notre algorithme proposé, l'étape Cloisonnement. Au sein de la terminologie co-regroupement nous nommons co-regroupement grossier. Cette première étape consiste à co-classification grossière D pour obtenir I (c) des grappes de gros à base de VX et J (c) des agrégats grossiers à base de VY. On obtient G (c) = I (c) x J (c) secondaires co-clusters. Soit, VXα et VYβ les ensembles de valeurs pour les clusters de grossières respectivement α et ß, tels que VX = ⋃i (c) α = 1 VXα et VY = ⋃J (c) β = 1 VYβ. Nous vous proposons un algorithme de partitionnement aléatoire, qui fonctionne comme suit. Tout d'abord, nous mélanger les valeurs de variablesX et Y. Deuxièmement, nous partitionner les valeurs de variables respectivement mélangées dans I (c) et des parties de taille égale J (c). Étant donné que nos valeurs variables sont mélangées, cette solution initiale est susceptible d'être aveugle aux modèles d'information. Ainsi, en utilisant une telle solution et continuer à les prochaines étapes peuvent produire un résultat de co-regroupement non informative. Pour contourner ce problème, une étape de pré-optimisation, similaire à Boullé (2011), est utilisé. Cette étape de pré-optimisation consiste à améliorer le rapport coût MODL (1) en déplaçant les valeurs entre les grappes, améliorant ainsi la solution co-classification initiale en déplaçant les frontières. Theg (c) = I (c) x J (c) grossier co-grappes sont en fait liées à des ensembles de données sous qui sont en outre plus facile à analyser en conséquence un plus petit volume de données. Note Dαβ = {(x, y) ∈ D, x ∈ VXα, y ∈ - 98 - I. Bartcus et al. VYβ} l'intégralité des ensembles de données sous de D, où 1 ≤ α ≤ I (c) et 1 ≤ β ≤ J (c). En outre, D = ⋃ αβ Dαβ. Chacun de ces ensembles de données de sous est adaptée aux contraintes de la mémoire. Fig.2 montre un exemple de co-classification grossière sur les données D. La complexité de cette étape de l'ISO (I (c) J (c) (V X + VY) / 2), ainsi le temps de calcul croît linéairement avec I ( c) et J (c). 3.1.2 étape de co-regroupement fine figure. 3 - Exemple d'amende co-classification pour chaque donnée de sous ensemble Dαβ en I (f) αβ x J (f) co-grappes aß. Cette étape consiste à faire passer KHC sur chacun des ensembles de données de sous précédemment obtenus. Nous nommons l'étape de co-regroupement fine. Sur la base de l'amende co-regroupement pour toutes les données sous-ensembles Dαβ, ∀α = 1,. . . , I (c), ∀β = 1,. . . , J (c), on obtient I (f) le nombre de grappes de αβ fines à base de VXα et J (f) nombre αβ de grappes fines à base de VYβ. Pour résumer, nous HAVEG (f) = αβ i (f) αβ x J (f) aß fines co-groupes pour chaque sous-données définies Dαβ. Fig. 3, montre un exemple de belle co-regroupement pour l'ensemble des données définies D. Notez que cette étape produit de différentes tailles fines co-clusterings, avec différentes clusterings fines, pour chaque ensemble de données sous, donc nous devons combiner tous les résultats de la co-classification pour l'ensemble de jeu de données obtenues D. La complexité de cette étape est O (N √ N / I (c) J (c) logN / I (c) J (c)). Observez que, contrar- ily à l'étape de partitionnement, un grand nombre de partitions diminue le temps de calcul de l'étape de co-regroupement fine, la section 3.3 est donc dédié à montrer comment nous choisissons un nombre optimal de pièces. 3.2 Dans la phase d'agrégation cette phase, nous agrègent les résultats de la phase de séparation. 3.2.1 étape fusion de l'étape fusionnent, qui commence la phase de l'agrégation de l'algorithme à deux niveaux, constitué en grappes de calcul pour l'ensemble de données entier grand D en combinant tous les amas de fines obtenues des ensembles de données secondaires. Dans cette étape, nous faisons référence aux grappes obtenues comme micro clusters. - 99 - Un algorithme de co-agrégation des données à deux niveaux très grands ensembles FIG. 4 - Exemple d'amalgame de l'ensemble SetD de données, avec I (m) × J (m) micro co-clusters. Succédant à l'étape amalgame, on obtient I (m) sur la base des grappes micro VX et J (m) des groupements à base de micro-VY. Fig. 4 illustre un exemple de l'étape amalgame sur l'ensemble du jeu de données. 3.2.2 étape de post-optimisation figure. 5 - Exemple de post-optimisation de l'ensemble des données établies D. En ce qui concerne l'étape amalgamé, nous avons besoin de rappeler la contrainte de mémoire qui dit que notre algorithme co-cluster peut fonctionner sur des matrices de taille au plus I2maxi. Cependant, l'étape consistant à amalgame peut éventuellement produire un trop grand nombre de groupements de micro avec I (m)> Imax ou J (m)> Imax. Avant de procéder à l'étape de post-optimisation, certains résultats fusionner pourrait être nécessaire de réduire I (m) et J (m) de telle sorte que I (m) ≤ Imax et J (m) ≤ Imax. Nous vous proposons une méthode d'échantillonnage qui consiste à regrouper de façon aléatoire les grappes micro dans le nombre maximum de groupes possibles Imax. Cela fonctionne comme suit pour chaque dimension. Tout d'abord, nous mélanger les grappes micro et les regrouper en grappes égales Imax. En second lieu, pour améliorer le modèle que nous déplaçons les grappes micro entre les groupes. Il en résulte un modèle de co-regroupement aléatoire qui est encore être amélioré par l'étape de post-optimisation. - 100 - I. Bartcus et al. Boullé (2011) a proposé deux types de post-optimisation: la fusion exhaustive et le post-optimisation gourmande. Nous utilisons des approches post-optimisation similaire à la fusion des clusters et puis déplacer les valeurs entre les clusters. grappes de fusion: la fusion consiste en grappes jusqu'à ce que le modèle nul est observée. Le meilleur modèle de co-regroupement est alors retenu. mouvement Valeur: déplace les valeurs entre les grappes alternativement pour chaque variable. Fig. 5 illustre un exemple d'optimisation de post sur l'ensemble du jeu de données. 3.3 Le choix du nombre optimal de pièces Un principal problème rencontré dans notre algorithme proposé co-regroupement à deux niveaux est de choisir la taille optimale des partitions I (c) et J (c) dans l'étape de partitionnement. Nous mettons en évidence le fait que pour les petites données, outil KHC est plus efficace que l'algorithme de co-clusters à deux niveaux. En effet, le comportement de temps sur l'ensemble des processus de notre algorithme est plus grand que le comportement temporel d'un processus KHC directement sur l'ensemble des données. D'après ce que nous supposons que chaque sous ensemble de données, nous devons avoir au moins 200 valeurs par variable et 104 cas. Soit T = TS + TA soit le temps d'exécution globale de notre approche de co-regroupement à deux niveaux, où TS est le temps de calcul de la phase Split et TA est le temps de calcul de la phase d'agrégation. Nos expériences montrent que TA est pas impacté par le nombre de partitions, donc nous nous concentrons sur TS pour minimiser T. Rappelons que TS est composé du temps de calcul de l'étape de partitionnement, qui augmente avec la taille de partition, et le Compu temps tational de l'étape de co-classification fine, ce qui diminue avec la taille de la partition. Par conséquent, pour en déduire une proposition théorique de choisir le nombre de partitions, nous utilisons une approche heuristique qui égalise la complexité temporelle de l'étape de partitionnement O (I (c) J (c) (VX + VY) / 2) avec la complexité du temps de l'amende co-classification stepO (N √ N / I (c) J (c) logN / I (c) J (c)) et suppose que la taille des partitions sur X et Y (I (c) et J ( c)) sont proportionnelles à leur nombre respectif de modalités (Vx et Vy). Nous obtenons: J (c) * =   c * √ VY / VX (2N √ N logN + VX VY) 1 3   (2) I (c) * = ⌈√ VX / VYJ (c ) * ⌉ (3) où c * = 1/4 est un facteur constant ajusté de nos expériences. 4 expériences Nous effectuons des expériences à la fois sur des données réelles et simulées afin d'évaluer notre pro- posé algorithme co-regroupement à deux niveaux. Dans ces expériences, nous courons l'approche co-regroupement MODL sur les ensembles de données mondiales ont été et réelles et les comparer avec notre algorithme co-regroupement à deux niveaux, donné par 2L-KHC. En effet, les pistes co-regroupement MODL à la mode à tout moment, jusqu'à ce qu'aucun changement significatif sont observées et sorties des solutions intermédiaires. There- avant, nous montrons les résultats du poing (KHC (1)) et les dernières solutions (KHC) de la co-regroupement MODL. Le but de ces expériences est d'obtenir une bonne et simple résumé des données - 101 - Un algorithme de co-clusters à deux niveaux pour les ensembles de données très grand ensemble. Nous évaluons la qualité du modèle de co-regroupement à partir du coût normalisé, calculé par 1 c (M) c (M0), où c (M) est le coût du modèle estimé et c (M0) est le coût du modèle nul. Ce coût normalisé peut être interprété comme un taux de compression. En outre, afin de montrer l'efficacité de l'algorithme proposé, nous fournissons le temps de calcul pour chaque approche. 4.1 Les expériences sur les données simulées Dans cette expérience, nous avons d'abord générons nos ensembles de données D avec deux variables X et Y. Pour générer les données, on utilise la distribution de probabilité suivante: p (i = xn, yn = j) α 1 - ||| | (i-j) | V ||| b, où (i, j) sont la possible les valeurs nominales pour les variables X et Y, respectivement; V = V X = V Y est le nombre de valeurs de la variable X ou Y, qui pour la simplicité sont considérés comme étant égaux; et b est un paramètre qui commande la concentration des données simulées sur la diagonale de la matrice de données. Nous varions mélanges de données et sparsity en générant trois familles de type de données. Ceux-ci sont sous forme uniforme, les familles biaisées et rares. Fig. 6 montre un exemple de ces trois familles de types de données. Tout d'abord, nous générons des familles de données uniformes et respectivement asymétriques. Les valeurs de X sont donnés a = 1,5, b = 1 a = 1, b = 1 a = 1, b = 0,01 FIG. 6 - Exemple de parcelles asymétrique (à gauche), uniforme (centre) et des ensembles de données éparses (à droite). par i = dU * V e et les valeurs de Y sont données par j = dw * V e, où (u, w) sont des variables aléatoires tirés indépendamment dans l'axe de loi de puissance (a-1), avec aa paramètre de forme con- pêche à la traîne de l'équilibre dans nos données. Pour a = 1 la famille de données uniforme est générée, alors que pour une plus grande famille de données en biais sont générés. Pour nos expériences, nous fixons un = 1,5 pour générer des données biaisées. Pour une meilleure compréhension de la différence entre uniforme et des ensembles de données asymétriques, les données de travers génère moins de données pour les première générés (i, j) des paires de valeurs. En outre, la fixation d'un petit b = 0,01 concentrés les données dans la diagonale, en obtenant ainsi la famille des données éparses. Pour montrer l'efficacité de notre algorithme de co-clusters à deux niveaux que nous générons six types d'ensembles de données en faisant varier le nombre d'instances et valeurs par variable. Le tableau (1) résume les ensembles de données générés. Jeu de données J1 J2 J3 J4 J5 J6 N 106 106 106 107 107 107 V 200 2000 20000 200 2000 20000 TAB. 1 - Generated ensembles de données. - 102 - I. Bartcus et al. Tout d'abord, nous courons nos expériences sur des ensembles de données uniformes. Le tableau 2, montre le coût normalisé obtenu et le temps de calcul, pour KHC et notre approche de co-regroupement à deux niveaux. Rappelons que KHC exécute d'une manière quelconque de temps fournissant de façon intermédiaire lutions. Dans nos résultats, nous présentons la première et les dernières solutions récupérées notées par respectivement KHC (1) et KHC. Notre approche de co-regroupement à deux niveaux est donnée par 2L-KHC. Observer que, lorsque le nombre de valeurs est faible, V = 200 (D1, D4), l'approche 2L-KHC obtient une meilleure solution que celle de KHC (1) donnée en même temps d'optimisation. La solution finale de KHC est améliorée de 0,5%, alors que notre approche est plus rapide dix fois. Pour D2, D3, D5 et D6 lorsque le nombre de valeurs par variable sont V = 2000, 20 000, nous pouvons voir que l'approche 2L-KHC obtient une meilleure solution que celle de KHC (1) à environ 15-50 moins de temps. En outre, il est d'environ 70-150 fois plus rapide que KHC, tout en obtenant des modèles de qualité comparable. coût normalisé Temps (s) de données 2L-KHC KHC (1) KHC 2L-KHC KHC (1) KHC D1 0,005354 0,005311 0,005381 8 10 84 D2 0,003270 0,003127 0,003282 277 3.885 20.324 D3 0 0 0 361 18113 18113 D4 0,005534 0,005525 0,005537 11 11 116 D5 0,003792 0,003718 0,003793 1036 32015 204137 D6 0,002533 0,002447 0,002538 4056 196974 602534 TAB. 2 - Les résultats co-classification obtenue sur les ensembles de données uniformes. En second lieu, nous courons nos expériences sur des ensembles de données rares. Le tableau 3, montre les résultats obtenus sur nos rares données. Notez que, pour V = 200 (D1, D4), l'approche 2L-KHC obtient la même qualité de co-regroupement comme celui de la KHC (1) et KHC, tandis que les temps de calcul pour toutes ces approches sont assez petites. Cependant, quand on a V = 2000 valeurs par variable (D2, D5), on constate que notre approche de co-regroupement à deux niveaux est 2- 10 fois plus rapide que KHC (1) et 15-60 plus vite que KHC, tout en ayant une normalisée coût 5% pire que celle de KHC. Enfin, avec V = 20000 (D3, D6), notre approche de co-regroupement donne une solution légèrement mieux être 40 fois plus rapide que KHC (1) et 80 fois plus rapide que KHC. coût normalisé Temps (s) de données 2L-KHC KHC (1) KHC 2L-KHC KHC (1) KHC D1 0,08474 0,08474 0,08474 48 35 342 D2 0,01535 0,01484 0,01587 2282 3939 34326 D3 0,0041 0 0 427 21586 21586 D4 0,08951 0,08951 0,08951 43 23 262 D5 0,01750 0,01749 0,01750 2409 29449 142887 D6 0,01076 0,01045 0,01046 4966 193273 405939 TAB. 3 - Les résultats co-classification obtenue sur les ensembles de données éparses. En raison du manque d'espace, les ensembles de données asymétriques résultats ne sont pas présentés dans ce document. Cependant, les résultats obtenus sont similaires à celles des données uniformes. - 103 - Un algorithme de deux co-regroupement niveau des ensembles de données très volumineux Pour conclure, l'approche proposée co-regroupement à deux niveaux surclasse KHC dans le temps de mise en Compu sans dégrader considérablement la qualité des solutions de co-regroupement. 4.2 Les expériences sur les données réelles Nous effectuons des expériences sur des données réelles qui nous permet d'évaluer notre approche de clustering coopération à deux niveaux sur des données avec une distribution plus complexe par rapport aux données générées. 4.2.1 Les données Nous menons des expériences sur plusieurs séries de données réelles: 20 Newsgroups (Mitchell, 1997), Web- Spam et Netflix (Bennett et Lanning, 2007) (avec 1% et 10% ran- (Castillo et al., 2008) les utilisateurs domly choisis), dont les caractéristiques sont résumées dans le tableau 4. Les données des variables définies Co-regroupement NVXVY 20 Newsgroups 2.047.830 19.464 11.315 texte × mots webspam 13.068.666 390,130 400,000 site de la source × site cible Netflix (1%) 960.327 16,235 4,649 utilisateurs × films Netflix (10%) 17,764 48,068 10.049.248 utilisateurs de films TAB. 4 - ensembles de données du monde réel. L'ensemble de données 20 Newsgroups est devenu populaire pour les expériences dans les applications texte de l'apprentissage des techniques de la machine, telles que la classification de texte et le regroupement de texte. Il se compose d'une collection d'environ 20.000 documents de newsgroup. Ces données comprend 2.047.830 observations, 19.464 textes et 11.315 mots. L'ensemble de données Webspam vient d'un défi de détection du site de type spam. Les données se compose d'un extrait du graphique web avec 13.068.666 liens de 390.130 sites sources et sites cibles 400.000. L'ensemble de données Netflix se compose de 100 millions d'observations, correspondant aux évaluations des utilisateurs 480.000 liés aux films 18,000. Pour t o avoir des résultats plus rapides, nous avons choisi d'enquêter sur environ 1% et 10% des utilisateurs choisis au hasard. On obtient ainsi deux ensembles de données. La première contient 1% d'utilisateurs choisis au hasard, comprenant des observations avec 960.327 4.649 16.235 utilisateurs et les films. La seconde contient 10% d'utilisateurs choisis au hasard et se compose de 10.049.248 observations avec 48.068 17.764 utilisateurs et les films. 4.2.2 Résultats Le tableau 5 montre le résultat obtenu sur les ensembles de données réelles, où nous évaluons le coût normalisé et le temps de calcul. Tout d'abord, observer les résultats sur les 20 groupes de discussion. On peut voir que notre algorithme à deux niveaux est deux fois plus rapide, avec un coût normalisé est d'environ 10% pire que celle de la première solution KHC, KHC (1). Ensuite, nos résultats sur l'ensemble de données Webspam, montre que notre algorithme de co-clusters à deux niveaux est deux fois plus rapide que KHC (1) ou 15 fois plus rapide que KHC, avec un coût normalisé à 10% de celle du KHC. - 104 - I. Bartcus et al. coût Normalisée Temps (s) de données 2L-KHC KHC (1) KHC 2L-KHC KHC (1) KHC 20 Newsgroups 0,0153 0,0163 0,0170 6510 12840 597600 Webspam 0,2160 0,2331 0,2427 43130 84859 716552 Netflix (1%) 0,0184 0,0190 0,0191 1529 5534 78399 Netflix (10%) 0,0199 0,0202 0,0202 29523 354888 3548888 TAB. 5 - Les résultats co-classification obtenus sur les données du monde réel. Enfin nos résultats sur les deux ensembles de données Netflix montrent une bonne performance pour notre 2L-KHC approche pro- posée. Nous pouvons voir que pour le Netflix avec 1% des utilisateurs, le calcul de l'algorithme de co-clusters à deux niveaux est trois fois plus rapide que la KHC (1) et 50 fois plus rapide que KHC, obtenant 4% pire coût normalisé. En outre, pour le Netflix avec 10% des utilisateurs, nous voyons que nos deux niveaux algorithme co-clusters est d'environ 12 fois plus rapide que KHC (1) et environ 120 fois plus rapide que KHC, perdant seulement 2% de la qualité de co-regroupement. Pour conclure, des expériences sur des données réelles montrent que, nos solutions duces plus rapide pro- approche de co-regroupement à deux niveaux sans diminuer considérablement la qualité des résultats co-regroupement. Ceci est particulièrement remarqué sur des données qui a besoin d'heures de calcul avec notre approche au lieu de jours de calcul avec KHC. En outre, il est à noter que nos deux niveaux co-classification des utilisations des ap- proche beaucoup moins de mémoire que KHC. Par exemple, le Netflix avec 10% des utilisateurs choisis au hasard, nécessite une machine avec au moins 10 Go de RAM pour exécuter KHC, alors que notre approche proposée co-regroupement à deux niveaux peut fonctionner sur la machine avec environ 1 Go de RAM. 5 Conclusions et perspectives Dans cet article, nous avons présenté un algorithme de co-clusters à deux niveaux en utilisant le critère MODL, qui permet de traiter de grands ensembles de données qui ne correspondent pas à la mémoire. Le premier niveau, qui est la phase Split, consiste en la séparation et les étapes consistant à co-classification de fines, tandis que le second niveau, qui est la phase d'agrégation consiste en la fusionner et les étapes postérieures à optimiser. Nous avons étudié chaque étape de notre algorithme de co-clusters à deux niveaux. Pour mettre en évidence les performances de notre algorithme de co-clusters à deux niveaux, nous avons effectué des expériences sur des données réelles et simulées. Nous notons que pour les petits ensembles de données, l'outil KHC est favorable, mais pour des données plus importantes, l'approche proposée est plus approprié si l'on veut obtenir une solution plus rapide, sans diminuer considérablement la qualité des solutions co-regroupement. Enfin, dans notre travail futur, nous allons nous concentrer sur notre co-classification algorithme ameliora- tion à deux niveaux, par exemple par parallélisation il. En outre, des expériences sur des ensembles de données plus importantes seront étudiées. Références Bennett, J. et S. Lanning (2007). Le prix de netflix. Dans Actes de l'atelier Coupe KDD 2007, New York, pp 3-6.. ACM. Bock, H. (1979). regroupement simultané des objets et des variables. Dans E. Diday (ed) Analyse des Données et Informatique, pp. 187-203. INRIA. - 105 - Un algorithme de co-clusters à deux niveaux pour ensembles de données très grand Boullé, M. (2011). modèles de grille de données pour la préparation et la modélisation dans l'apprentissage supervisé. En I. Guyon, G. Cawley, G. Dror, et A. Saffari (Eds.), Travaux Pratiques Motif Reconnaissance: Les défis en matière d'apprentissage machine, volume 1, pp 99-130.. Microtome Publishing. Castillo, C., K. Chellapilla et L. Denoyer (2008). le spam Web challenge 2008. 4ème Atelier interna- tional sur la recherche d'information accusatoire sur le Web (Airweb), Beijing, Chine. Dhillon, I. S., S. Mallela et D. S. Modha (2003). co-regroupement des informations-théorétique. Dans Proc. de l'ACM Neuvième SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données, KDD '03, New York, NY, USA, pp. 89-98. ACM. Govaert, G. et M. Nadif (2008). regroupement bloc avec des modèles de mélange Bernoulli: fils de différentes approches comparai-. Informatique Statistiques et analyse des données 52 (6), 3233-3245. Govaert, G. et M. Nadif (2013). Co-Clustering (1re éd.). Wiley-IEEE Press. Guigourès, R., D. Gay, M. Boullé, F. Clérot et F. Rossi (2015). analyse exploratoire à l'échelle du pays des enregistrements détaillés des appels à travers la lentille de modèles de grille de données. Dans Actes du ECML / PKDD, pp. 37-52. Springer International Publishing. Hartigan, J. A. (1972). Direct Clustering d'une matrice de données. Journal de l'American Statistical Association 67 (337), 123-129. Li, H. et N. Abe (1998). cluster Word et homonymie à partir des données de cooccurrences. Dans Proc. de la 17e Conférence internationale sur la linguistique informatique - Volume 2, COLING '98, Stroudsburg, PA, USA, pp 749-755.. Cul. Comp. Linguistique. Malines, I. V., H. H. Bock et P. D. Boeck (2004). méthodes de classification à deux modes: un aperçu struc- Tured. Méthodes statistiques dans la recherche médicale 13 (5), 363-394. Mitchell, T. M. (1997). Machine Learning (1 ed.). New York, NY, USA: McGraw-Hill Companies, Inc. Papadimitriou, S. et J. Sun (2008). Disco: co-cluster réparti sur la carte-reduce: Une étude de cas vers pétaoctet échelle de bout en bout l'exploitation minière. En ICDM, pp. 512-521. IEEE Computer Society. La classification CV Croisée (co-classification) technique is juin Qui d'Përmet la sous la structure Extraire-Entre les existante jacente et les lignes d'colonnes Une Table de sous forme de Données blocs. applications several technique this utilisent, de cependant de co Nombreux algorithmes Clustering Actuels ne Passent pas à l'échelle. Une des approaches utilisées with is the succès MODL méthode, Qui optimize un critere de vraisemblance régularisée. Cependent, verser des tailles en plus importantes, sa reached this méthode limite. Dans this article, un Présentons NOUS de co nouvel algorithme-regroupement à deux levels, Qui du compte Tenu MODL Përmet de critère EFFICACEMENT de Données Traiter de très grande taille, ne pas Pouvant tenir en mémoire. Nos expériences montrent Que l'approach proposed en temps de gagne tout en calcul des solutions de produisant qualité. - 106 -"
10,Revue des Nouvelles Technologies de l'Information,EGC,2018,Big Data for understanding human dynamics: the power of networks,,Fosca Giannotti,http://editions-rnti.fr/render_pdf.php?p1&p=1002363,http://editions-rnti.fr/render_pdf.php?p=1002363,en,"Sur les méthode ensembles basée répandrai l'apprentissage approximatifs en présence des incrémental déséquilibrées Sarra Bouzayane Données * ***, Saad * Inès ** * Université de Picardie Jules Verne, Amiens {sarra.bouzayane, ines.saad}@u-picardie. fr ** Ecole Supérieure de commerce, Amiens *** Institut Supérieur d'Informatique et de Multimédia, Sfax CV. Ce papier propose juin sur la méthode basée des ensembles AP- théorie proximatifs et à l'apprentissage dédiée supervisez Dans un incrémental de Données déséquilibrées Contexte. Consiste en THIS method phases: la Trois construc- tion d'Une Table de décision, L'Inference D'un Ensemble de rêgles de decisión et La classification de l'action each Potentielle Dans l'des cours de l'UNE de décision. Prédéfinies La MAI2P is Validée méthode in the des MOOCs (Contexte Massive Cours en ligne ouvert). 1 Introduction when les exemples d'Fournis de apprentissage Sont Manière séquentielle, l'apprentissage verser prix Une incrémental de décision s'avère Une obligation (Greco et al., 2004). ment GENERALE-, la phase de d'apprentissage par les traitee is techniques de l'ordinateur conventionnelles apprentissage. Cependant, les techniques CÉS au demeurent sensibles des Données DES- Problème Qui resulte de équilibrées la Entre les Répartition des cas inégalé des cours de décision. This la inégalité affecte considérablement la qualité de décision en il s'agit Particulier de Quand Don- Nées massifs. Ce may Problème, toutefois, par l'Être surmonté approach DRSA (base Dominance- rugueux approche Set) (Greco et al., 2001) sur les repos Qui Preferences et l'expertise des Décideurs la construction répandrai Humains d'un ensemble d » Apprenticeship de garantir La AFIN REPARTITION EGALE des instances Sur l'Ensemble de cours de décision. Ce travail propose Une méthode MAI2P (multicritère approche de la Per- incrémentale iodique prévision) sur l'approche basée DRSA la classification multicritère répandrai et periodic incrémentale. La Méthode MAI2P soi Compose de phases trois. La première à étau Construire une Table de décision et le repos sur trois étapes: l'identification d'un ensemble d'apprentis- sage; La construction d'Une Famille coherente de critères Pour la CARACTERISATION des actions; Et la classification de l'action Chaqué d'apprentissage des Dans l'UNE de cours de décision. La phase de deuxiéme is sur notre basée DRSA incrémentiel algorithme (Bouzayane et Saad, 2017) pour l'inférence et la mise à jour de l'ensemble de rules de décision. La third Consiste à la classification des « Actions PotentiELLES », en les rules Utilisant inférées précédemment. L'approach MAI2P is Validée sur le MOOCs (des Contexte Massive Cours en ligne ouvert). - 305 - sur DRSA Méthode basée la répandrai Le papier prédiction is incrémentale Comme costume structuré: La section 2 les notions de Définit la base de l'approche DRSA. La section 3 un état de present l'art. La section 4 Detaille la MAI2P méthode. La section 5 les Résultats de discute l'expérimentation. La section 6 le papier conclut. 2 Préliminaires: Set grossière basée Dominance- L'approche par approach DRSA developpee Greco et al. (2001) au EST dédiée de tri en Problème aide multicritère à la décision et de la Inspirée des ensembles théorie approximatifs. Elle Përmet de comparateur des actions Ë Une relation de Travers domination, des Preferences rendant compte d'un Décideur, d'inférer les AFIN rules de décision. This approche Définit Une table d'information par un 4-uplets S = <A, F, V, f> tells Que: Un ensemble d'un est fini des actions de référence; F is Une famille coherente de critères; V is a ensemble des facts des critères; possibles et f: A × F - → V is Une fonction d'information tel Que f (x, g) ∈ Vg, ∀ x ∈ A, ∀g ∈ F. l'action de each is référence à Une Seule affectée Clt classe; t ∈ {1, N}. Relation de dominance: La relation de domination DP is definie Comme suit: ∀ (x, y) ∈ A2, XDP y ⇔ f (x, gj) <f (y, gj) ∀gj ∈ P ⊆ F. ∀x ∈ A, es t associé un ensemble, P- dominant, les actions D'dominant x et un ensemble de P-Domine, D'actions dominées par x. Union Inférieure (supérieure): Cl≤n = ∪s≤nCls (Cl≥n = ∪s≥nCls); n = {1. . . N}: L'union Inférieure (supérieure) de Cln Que signifié « x Appartient au maximum de (minimum) à la classe Cln bien à ous Une classe au better (moins) also bonne Que CLN ». P Approximation Inférieure (Cl≥n) (ou P (Cl≤n)): les actions REGROUPE TOUTES l'en- Semble Do not P-dominante (P-Dominé) Est Certitude AVEC Ë affecté des MoiNs les classes (de) also Mieux bonnes Que Cln. En revanche, l'approximation REGROUPE Toutes les supérieure actions l'afféterie Do not is d'Une Manière réalisée possible. Règles de décision: L'ensemble de rules de décision de l'Est Appelé modèle Preferences. CES rules are générées à partir de l'approximation Inférieure et se presentent sous la forme: Si f (x, g1) ≥ r1 ∧ ... ∧ f (x, gn) ≥ rn Alors x ∈ Cl≥t tel QUE (r1 , ..., rn) ∈ (Vg1 × ... Vgn). 3 Travaux antérieurs approaches Dynamiques Quelques were Dans la littérature proposées répandrai la mise à jour des rules de incrémentale décision Suite à la variation de l'ensemble d'apprentissage. Les auteurs Dans (Greco et al., 2004) un de have Glance Proposé Algorithme Appelé. CeT al- gorithme Est basons des Négatives Sur les actions. En effet, each règle d'union Une IMPERATIVEMENT ne Doït Donnée pas x Satisfaire, si x n'appartient pas à l'union this, but also ne may Elle Satisfaire l'action x Aucune ET CE CAS la DANS sans supports ELLE Demeure. sans les supports de CÉS non Sont Dites et robustes l'algorithme Fait also dit non robuste. L'Glance Stocke Dans Algorithme la mémoire les rules de UNIQUEMENT et pas les décision d'exemples et apprentissage il is Fait par rapport à économe l'utilisation de l'espace mémoire. La Complexité de l'algo- rithme is si le linéaire le Considère d'actions et Nombre Elle Est en exponentielle le Nombre de Considérant critères. Les Les Dans mes (Li et al., 2013) un de have de mise Proposé à Algorithme jour des approximations incrémentale et inférieures de l'approach supérieures DRSA LORs de l'ajout (ou la suppression) D'une seule l'action Dans le Système D » information. La méthode - 306 - S. Bouzayane et I. Saad Nécessite: premièrement, la mise à jour des syndicats des Supérieures ET inférieures des classes de DE- Cision, Deuxiemement, la mise à jour des ensembles P-dominante et P-Dominé de l'action each Dans le Système d'information et la mise à enfin jour des approximations et inférieures des syndicats rieures supé- des cours de décision. L'algorithme le temps de réduire Proposé calculation l'action Entre OU lorsqu'une Quitte le Système d'information sans la AFFECTER Qualité des Règles de décision inférées. D'inférer des AFIN rules de décision robustes, nous choisissons de l'algorithme presented généraliser Dans Li et al. (2013) de considerer L'AFIN plats principaux Simultanée D'un d'ensemble des actions. D' 4 MAI2P: Méthode de classement section This Présente incrémentale la méthode MAI2P nous Qué proposed for the Avons prédiction et periodic de incrémentale la classe de décision Cli à l'action x Une Laquelle susceptible d'EST Appartenir. This method phases SE Compose de trois (cf. figure 1). FIGUE. 1 - Description générale de la méthode MAI2P (Pi = period i; T = Nombre de Périodes) 4,1 Phase 1: Construction de la Table de décision de la period Pi This is phases composée de trois étapes: 4.1.1 Etape 1: Construction d « ensemble un des « Actions de référence » Cette éTAPE Consiste à definir l'ONU d'ensemble d'apprentissage süffisant Nombre de postes ONU Contenant d'exemples représentatifs des cours répandrai de Chacune de décision. prédéfinies De la AFIN acception Dans l'terminologie Utilisée approach DRSA, nous les appelons d'exemples apprentissage, « Actions de référence ». La construction de s'effectue par ensemble CET un OU several deurs en fonction déci- de their expertise et their expérience. D'un point de vue de psychologique (Miller, 1956), un Décideur se caracterise par humain Une ca pacité cognitive representative la su- périeure à limite il may Laquelle des ses Réponses aux Associer des stimuli Qui lui Accordes are. AINSI, Pour la construction de l'ensemble d'apprentissage, Il Est süffisant Qué les actions Sélectionnées - 307 - sur DRSA Méthode basée la répandrai prédiction et incrémentale de représentants Soient qualité, quel Que their Soit effectif. L'intervention des experts Pour la construction de l'ensemble d'apprentissage d'Përmet obtain des sous-ensembles des « Actions équitables de référence » et de le Surmonter Problème de déséquilibrées Données. La MAI2P Doït Être méthode sur les Systèmes appliquée d'informations Qui Evoluent Dans le temps, l'ensemble des where « Actions de référence » d'Varie juin period à enchainee. AINSI, each period Pi, le Décideur ensemble Doït un nouvel Aï définir des « Actions de référence » qui se à l'ensemble rajouté des « Actions de référence », Ai-1, de les toutes ses Précédentes Périodes. 4.1.2 2 Etape: (. Mousseau et al, 1996) Construction d'Une famille coherente de Comparons un critères attribut, un critere de Doït permettre les Preferences des Mesurer Décideurs point un SELON de personnel Vue. Dans CE travail, l'approche nous Que is adoptons Consiste à Qui ascendante Construire une famille de criteria partir d'Une liste Indicators d'influenceur l'sujets sensibles opinion des Décideurs concerning La CARACTERISATION des actions. Salle de bains, des Réunions directes doivent Être le Décideur Avec menées d'obtain SES AFIN informations sur each préférentielles critère. D'appliquer les AFIN des points de vue préfé- tielles, nous adoptons qualitative Une échelle. 4.1.3 3 Etape: Classification de l'ensemble des « Actions de référence » Cette étape Consiste à la construction d'Une table de décision Di de la period Pi. C'est une matrice Do not les representent les colonnes « p » Critères d'évaluation et Fi Dans Contenus les Do not Forment ensemble un lignes de « m » « Actions de référence » Dans Aï contenues. Le contenu de la matrice is the d'évaluation fi fonction (Aj, i, gk) de l'action each Aj, i ∈ Aï sur each gk ∈ Fi critère tel i ∈ {Que 1..T}, j ∈ {1 ..M} et {k ∈ 1..p}. Les variables T, m et p le respectively Sont Nombre de considerer à pendentif Périodes le Processus de Prédiction, la taille | Aï | de l'ensemble des « Actions de référence » à la Définit et la period ième de l'ensemble taille | Fi | de la famille de critères. La derniére Colonne de la table de CONTAINS La AFFECTATION de en Décision D'each « Action de référence » Dans l'UNE des N cours de décision. 4.2 Phase 2: Mise à jour incrémentale des approximations de DRSA This Appliqué phase notre algorithme DRSA-incrémental (Bouzayane et Saad, 2017) sur la Table de décision Di Construite pendant la period Pi AFIN d'en inférer un modèle de la préférence, MPi, susceptible de classer action each Dans l'des cours de l'UNE de décision. prédéfinies la phase This is appliquée des que la Table de décision is complète. Elle l'ensemble des Considère « Actions de référence », Ai-1, de toutes ses les Précédentes et l'Périodes ensemble des « Actions de référence », Aï, de la period Pi. L'algorithme DRSA-incrémental EST l'insertion déclenché Dès de l'ensemble Aï Dans la Table de décision. Il est Couleur de composé: 1. Quatre ÉTAPES CALCULER les syndicats de Supérieures ET inférieures des cours de Chacune décision Cli-1. 2. les ensembles Calculer et Domines Dominantes Action pour each x + ∈ insérée Aï. 3. Mettre à jour les ensembles dominants et Domines Action pour each Aj, i-1 ∈ Ai-1. 4. Mettre à jour les approximations de syndicats de des Chacune des classes de décision. La sortie de la phase 2 is a Modele de de VIA Préférence permettant les « Actions Classer PotentiELLES » Pendentif la period Pi + 1. - 308 - S. Bouzayane et I. Saad 4.3 Phase 3: Classification des « Actions PotentiELLES » de la period Pi + 1 La phase third Exploité les rules de décision inférées précédemment d'attribuer AFIN des « Actions Chacune PotentiELLES » une L'Dans cours des N de décision Pré . définies Une « action potentielle » l'action is Une sensible d'être CLASSEE DANS L'UNE des cours de décision. Pendentif s'exécute phase de This la period Pi + 1 tout au long du Processus de tel Que i prédiction ∈ {2,. . . , T}. Elle commence par L'ÉValuatioN de « Actions Toutes Les PotentiELLES » sur l'ensemble de critères construits. Salle de bains, IL s'agit D'les rêgles de appliquer inférées pendentif en Décision La Periode Pi de les AFFECTER AFIN Dans Les cours de la décision prédéfinies. La MAI2P s'exécute méthode tout au long périodiquement du Processus de Prédiction: la première et la phases se déroulent Deuxième pendentif Pi toutes ses les tel Qué Périodes i ∈ {1,. . . , T -1} Alors that the third se deroule pendant la Période Pi; tel i ∈ {Que 2,. . . , T}. 5 Expérimentation et Evaluation de la MAI2P Nous Avons Méthode le Traité d'un MOOC CAS (formation en ligne et gratuite) Français Qui a Duré 5 et Accéde semaines nominale 2360 apprenants. L'Objectif is the de la prédiction hebdomadaire de classe à décision un apprenant Laquelle appartiendra: Cl1 des « Apprenants en osée d'abandonner »; Cl2 des « Apprenants en difficulté » mais Qui sont Actifs; et Cl3 des « dirigeants Apprenants ». - La phase 1. Nous Avons Construit, l'aide de Avec l'équipe pédagogique, des ensembles Quatre « Apprenants de référence » Aï tel i ∈ {Que 1, 2, 3, 4} et | Aï | = 30. Ensuite, Une famille coherente de 11 critères was definie Dont 8 are Statiques (exp. Niveau d'études) et 3 dynamiques d'are (exp. Le Nombre hebdomadaire de messages). Enfin, à la fin de each semaine Une Table de décision is Construite. - Phase 2. This étape was à la fin appliquée de each Si semaine Une Fois Que la Table de décision Di is i Que tel complète ∈ {1, 2, 3, 4} en l'appliquant DRSA incrémentiel Algorithme la mise à verser jour des incrémentale Règles de décision. - Phase3. This étape au Était appliquée Début de each Si du MOOC semaine en AP- pliquant le modèle de inféré à la Préférence fin de la semaine Si-1 pour la classification de l'ensemble d'Potentiels tel Que apprenants i ∈ {2, 3, 4, 5}. FIGUE. 2 - Qualité de la prédiction (F-mesure) les semaines du Durant MOOC La Figure 2 a rencontré l'accent sur-la variation de La F-des mesure de cours Trois décision Cl1, Cl2 et Cl3 D'une semaine à enchainee. - La F-Mesure de la classe CL1, des « Apprenants de risque », AUGMENTE au cours du temps. En effet, le MOOC par les EST rôdeurs Connu. CÉS RESTENT apprenants au bout de Actifs la première semaine en Mais l'intention Une Ayant d'abandonner la préalable la formation. - 309 - sur DRSA Méthode basée la répandrai Ce de type prédiction d'incrémentale dévalorise la prestation apprenants du modèle de prédiction Qui est basons sur le profil et le comportement de l'apprenant et non pas sur l'intention de fils. - La F-Mesure de la classe CL3, des « leaders Apprenants » AUGMENTE au cours du progressivement temps. En effet, D'une semaine à enchainee, les apprenants Ce Qui donne multiplient their forum au participation, plus d'informations Une ample sur Leur profils. Also, les EVALUATIONS par le MOOC proposées de plus de en Sont ainsi que des complexes d'Une semaine à enchainee Ce Qui d'Përmet Une vision, plus Précise sur les des Compétences apprenants. 6 Conclusion Dans papier CE, nous Avons Une méthode de Proposé classement multicritère et incrémentalement conte MAI2P sur l'approach basée DRSA Pour la periodic de la prédiction de décision à classe Une mesure is Laquelle susceptible d'Appartenir. La MAI2P is méthode de phases composée Trois: la construction d'Une table de décision; l'un d'inférence de modèle en Preferences l'appliquant DRSA incrémentiel Algorithme et la de la classe prédiction de décision à l'action each Laquelle appartiendra. Les Expérimentations de la MAI2P sur un méthode MOOC Fran- Cais Une qualité en Ontario de démontré prédiction Qui reached Une satisfaisante F = 0,66-mesure. Bouzayane Références, S. et I. Saad (2017). algorithme de mise à jour incrémentale des approximations dans DRSA pour traiter les sys d'information dynamique tèmes de moocs. Lors de la conférence internationale sur la gestion des connaissances, l'information et les systèmes de connaissances (KMIKS), 55-66. Greco, S., B. Matarazzo, et R. Slowinski (2001). La théorie des ensembles rugueux pour l'analyse de décision multicritère. EJOR 129 (1), 1-45. Greco, S., R. Slowinski, J. Stefanowski, et M. Zurawski (2004). Incrémentale par rapport à la règle mentale nonincre- induction pour la classification multicritère. Transaction Rough Sets II, 33-53. Li, S., T. Li, et D. Liu (2013). entretien dynamique des approximations dans l'approche définie grossière basée dominance dans la variation de l'ensemble d'objets. Int. J. Intell. Syst. 28, 729-751. Miller, G. A. (1956). Le nombre magique de sept, plus ou moins deux: certaines limites à notre capacité de traitement de l'information. Psychological Review 63 (2), 81. Mousseau, B. R., VINCENT, et B. Roy (1996). Un cadre théorique d'analyse de la notion d'importance relative des critères. J. multicritère Decis. Anal 5, 145-159. Roy, B. (1985). .. Methodology multicritère d » aide à la décision Economica Résumé Cet article propose une méthode basée sur la théorie des ensembles rugueux et dédié à l'apprentissage supervisé tal incrémentalement dans un contexte de données asymétriques Cette méthode consiste en trois phases:. La construction d'un . table de décision, la conclusion d'un ensemble de règles de décision, et la classification de chaque action potentielle dans l'une des classes de décision prédéfinis la méthode MAI2P est validée dans le cadre de MOOC (Massive cours en ligne ouvert) -. 310 -"
14,Revue des Nouvelles Technologies de l'Information,EGC,2018,Community structure in complex networks,,Santo Fortunato,http://editions-rnti.fr/render_pdf.php?p1&p=1002362,http://editions-rnti.fr/render_pdf.php?p=1002362,en,"La structure des communautés dans les réseaux complexes Santo Fortunato Centre pour les réseaux complexes et les systèmes de recherche (CNETS) Ecole d'informatique et de l'informatique. Indiana University santo.fortunato@aalto.fi Résumé Les systèmes complexes présentent généralement une structure modulaire, sous forme de modules sont plus faciles à assembler que les unités individuelles du système, et plus résistant aux pannes. Dans le sentation des systèmes complexes, des modules ou des communautés de réseau, apparaissent comme des sous-graphes dont les noeuds ont une probabilité sensiblement plus pour être connecté à l'autre que pour les autres noeuds du réseau. Dans cet exposé, je vais répondre à trois questions fondamentales: Comment est la structure communautaire générée? Comment détecter? Comment tester les performances des algorithmes de détection de la communauté? Je vais montrer que les communautés émergent naturellement dans la croissance des modèles de réseau favorisant que triadique ClO-, un mécanisme nécessaire pour mettre en œuvre pour la production de grandes classes de systèmes, comme par exemple réseaux sociaux. Je parlerai des limites de la classe la plus populaire des algorithmes de regroupement, ceux basés sur l'optimisation d'une fonction de qualité globale, comme la maximisation de la modularité. algorithmes Le test est probablement la plus importante question de détec- tion communautaire du réseau, car elle implique implicitement le concept de communauté, ce qui est encore controversée. Je vais discuter l'importance d'utiliser des graphiques de référence réalistes avec intégré la structure communautaire. Biographie Santo Fortunato est le directeur du Centre pour les réseaux et systèmes complexes de recherche (CNETS) à l'Université de l'Indiana et un directeur scientifique de l'Institut des sciences de l'Indiana University Network (Iuni). Auparavant, il était professeur de systèmes complexes au Département des sciences de l'Université ordi- nateur Aalto, en Finlande. Le professeur Fortunato a obtenu son doctorat en physique théorique des particules à l'Université de Bielefeld en Allemagne. Il a ensuite déménagé dans le domaine des sys- tèmes complexes, par un stage postdoctoral à l'École d'informatique et en informatique de l'Université d'Indiana. Ses domaines d'intérêt actuels sont la science des réseaux, en particulier la détection de communautés dans les graphiques, les sciences sociales et de la science informatique de la science. Ses recherches ont été publiés dans des revues, y compris Nature, Science, PNAS, Physical Review Letters, Avis sur la physique moderne, les rapports de physique et a recueilli plus de 21.000 citations (Google Scholar). Son article de revue détection communautaire sous forme de graphiques (physique Rapports 486, 75-174, 2010) est l'un des plus connus et les articles les plus cités dans la science des réseaux. Il a reçu le Prix du jeune scientifique pour Socio et éconophysique 2011, un prix décerné par la Société allemande de physique, pour sa contribution exceptionnelle à la physique des systèmes sociaux. - 5 -"
39,Revue des Nouvelles Technologies de l'Information,EGC,2018,Long-range influences in (social) networks,,Ernesto Estrada,http://editions-rnti.fr/render_pdf.php?p1&p=1002361,http://editions-rnti.fr/render_pdf.php?p=1002361,en,"influences à long terme dans (sociaux) Ernesto Estrada réseaux ERNESTO ESTRADA Département de mathématiques et de statistique de l'Université de Strathclyde, 26 Richmond Street Glasgow G1 1xH, Royaume-Uni ernesto.estrada@strath.ac.uk Résumé Dans cet exposé, je présenterai quelques problèmes qui motivent la nécessité de considérer (non aléatoires) influences à long terme dans les interactions sociales. Cette motivation sera développée sur la base de la diffusion des innovations dans les réseaux sociaux et quelques exemples seront fournis. Ensuite, je vais développer un cadre mathématique qui permet de généraliser l'opérateur laplacien sur les réseaux et de proposer une équation de diffusion généralisée sur les graphes. Je prouverai analytiquement que dans les cas d'une ou deux dimensions de ce nouveau système donne lieu à des comportements superdiffusive sur les réseaux. Je vais montrer comment étendre ce modèle à un multi-trémie aléatoire à appliquer dans les réseaux du monde réel. Enfin, je vais revenir au problème des systèmes sociaux montrant des implications du nouveau modèle pour la sélection des dirigeants, l'influence des dirigeants cohésion et la diffusion des innovations. Biographie Ernesto Estrada a un leader international réputation pour l'élaboration et le développement de l'étude des réseaux complexes. Son expertise se situe dans les domaines de la structure du réseau, la théorie des réseaux algébrique, les systèmes dynamiques sur les réseaux et l'étude des modèles aléatoires de réseaux. Il a une expérience remarquable de publications de haute qualité, qui a attiré plus de 10.000 citations. Son index h (nombre de documents avec au moins citations h) est 56. Ses publications sont tions les domaines de la théorie du réseau et de ses applications aux problèmes sociaux, écologiques, ingénierie, physiques, chimiques et biologiques problèmes réels. Le professeur Estrada a publié deux livres de texte sur les sciences du réseau à la fois publié par Oxford University Press en 2011 et 2015, respectivement. Il a fait preuve d'un leadership international continue dans son domaine où il a été invité et conférencier en séance plénière lors des grandes conférences en sciences du réseau et les mathématiques appliquées. Il est le rédacteur en chef du Journal des réseaux complexes (Oxford Uni- sité Press) ainsi qu'un éditeur de SIAM Journal de mathématiques appliquées et des travaux de la Royal Society A - 3 -"
41,Revue des Nouvelles Technologies de l'Information,EGC,2018,Mean-shift : Clustering scalable et distribué,"Nous présentons dans ce papier un nouvel algorithme Mean-Shift utilisantles K-plus proches voisins pour la montée du gradient (NNMS : NearestNeighbours Mean Shift). Le coût computationnel intensif de ce dernier a longtempslimité son utilisation sur des jeux de données complexes où un partitionnementen clusters non ellipsoïdaux serait bénéfique. Or, une implémentationscalable de l'algorithme ne compense pas l'augmentation du temps d'exécutionen fonction de la taille du jeu de données en raison de sa complexité quadratique.Afin de pallier, ce problème nous avons introduit le ""Locality SensitiveHashing"" (LSH) qui est une approximation de la recherche des K-plus prochesvoisins ainsi qu'une règle empirique pour le choix du K. La combinaison de cesaméliorations au sein du NNMS offre l'opportunité d'un traitement pertinentaux problématiques du clustering appliquée aux données massives.","Gaël Beck, Hanane Azzag, Mustapha Lebbah, Tarn Duong",http://editions-rnti.fr/render_pdf.php?p1&p=1002420,http://editions-rnti.fr/render_pdf.php?p=1002420,en,"Mean-Shift: Clustering évolutive et Beck Gaël Distribué, Hanane Azzag, Mustapha Lebbah, Tarn Duong, Christophe Cerin Laboratoire d'Informatique de Paris Nord (LIPN) Université Paris Nord - Paris 13, F-93430 Villetaneuse, France Email: {beck, prénom.nom}@lipn.univ-paris13.fr CV. CE papier de un de nous Présentons Mean-nouvel algorithme Maj les K uti- lisant-plus PROCHES Pour la voisins Montee du gradient (NNMS: les plus proches voisins de décalage moyenne). Le computationnel Coût Intensif de CE un long un dernier partition- en grappes temps nement utilisation de fils Limité sur des jeux de complexes where non Données ellipsoïdaux Bénéfique Serait. Ou, Une évolutive de l'Implémentation ne COMPENSE pas Algorithme l'augmentation du temps d'exécution en fonction de la taille du jeu de en raison de Données sa QUADRA Complexité tique. De Pallier AFIN, nous CE Problème le Avons introduit ""Localité Hashage sensible"" (LSH) Qui est Une approximation de la recherche des K-plus proches voisins AINSI répandrai Qu'une règle empirique le choix du K. La combinaison de bureaux au sein Améliorations du NNMS l'offre d'un treatment Opportunité pertinente aux Problématiques du regroupement aux Données massifs appliquée. 1 Introduction L'Objectif de la recherche non supervisee is d'un label Affecter à des points de non labélisés where le et l'emplacement Nombre des grappes Sont Inconnus. Nous nous Concentrés sur un Sommes de regroupement modale algorithme where le Nombre de grappes en defini is modes de terme de la fonction Locaux de density de genere les Qui Probabilité Données. Le plus CONNU des grappes de modales Algorithmes Est Le k-means. Comme CE sur basons is dernier la répartition de chiné normalien, il EST à Pressothérapie des contraint groupes ellipsoidaux Ce Qui can be verser des jeux inapproprié de complexes Données. Le Mean-shift is Une du k-généralisation des moyens en raison de sa capacity à des grappes de Calculer topologie Definis Comme les aléatoire d'attractions des bassins modes par la Generes Locaux de gradient de montée (Fukunaga et Hostetler, 1975). De les AFIN Calculer Chemins de la montée de gradient, les k, plus proches voisins voiture Sont appropriés s'adaptent à la NIT locale des Données topologie. La Version actuelle des K plus proches de Mean-Shift contains des Goulots d'étranglement par Poses Une grille de recherche multiple for the d'un choix Nombre de et par optimal Voisins le calcul des k exactes, plus PROCHES voisins. Nous proposons ici un nouvel algorithme Qui résout SCÉ Gouffres computationnels: (a) Une échelle normale Efficace du choix du Nombre des plus de proches voisins Qui Evite la recherche en grille, (b) le lieu hash sensible (LSH) Qui est juin la version approximée des k, plus proches voisins et (c) Une Implémentation MapReduce distribuée. - 415 - Mean-shift: clustering évolutives AVEC Les plus proches voisins approximés 2 Méthode 2.1 Le moyen de décalage Le moyen par Shift introduit (Fukunaga et Hostetler, 1975), genere point d'écoulement non x de dimension d une séquence de points de Qui suivent le chemin en montée de la densité de gradient en Utilisant la relation de récurrence: xj + 1 = 1 k Σ Xi∈k-nn (xj) Xi (1) where X1,. . . , Xn is un échantillon aléatoire Obtenu D'une fonction de densité f commune, les k voisins, plus proches de x k-nn are (x) = {Xi: ‖x-Xi‖ ≤ δ (k) (x)} tel Que δ (k) (x) is the Distance du k-ème, plus proche voisin. x0 = x. L'équation (1) au moyen fils donne Shift NOM because DU DEPLACEMENT des ITERATIONS de successif xj Vers la moyenne de Ses K plus PROCHES Pour la Voisins xj + Prochaine 1 itération. La convergence de la SEQUENCE {x0, x1,. . .} vers un mode Locale pour la Version à de l'équation noyau (1) par un etablie Été (Comaniciu et Meer, 2002) pour Une grande classe de sur des noyaux fenêtres fixées. Convergence This Validé Reste Quand la fenêtre fixe is Replaced par la distance la plus de des décroit Qui proches voisins Avec l'augmentation du Nombre d'Itérations. Le chemin de gradient v Montée de ers les modes Locaux produit par l'équation (1) forme les bases de l'Algorithme 2.1 (NNMS), ainsi que notre méthode des moyennes proches de décalage Voisins. Les plats principaux du NNMS les Échantillons de are X1 Données,. . . , Xn et les points de nous Candidats Que clusteriser x1 souhaitons,. . . , Xm (ILS PEUVENT Être X1,..., Xn Mais CE Ne EST PAS UN prérequis). Les paramêtres de règlage Sont les Suivants: - le Nombre de plus de proches de k - le seuil sous Lequel la convergence des Itérations is considérée Comme being sufisante ε1 - le Nombre maximal d'Itérations jmax - le seuil sous Lequel deux itérés finaux Sont considérés COMME being du same groupe Membres ε2 - la cardinalité MINIMALE des grappes Cmin Les formes les étiquettes Sorties des Sont grappes Punti {c Candidats (x1),. . . , C (xm)}. Il y a trois sous-routines à L'Algorithme 2.1. Les 1-6 correspondant à lignes la formation des Chemins de la montée de gradient Dans l'équation 1 Qui sont itérés Jusqu'a Ce que la distance de de la Dernière INFERIEURE à itération ε1 Soit ous le Qué d'un maximum Itérations Nombre jmax reached Soit . Les sorties de bureaux les itérés lignes Sont x * 1 finaux,. . . , X * m. Les 7-8 lignes la fusion des concernent itérés Dans le same finaux groupe when la distance de les séparantes is sous le seuil ε2, creant un CECI initial des x Regroupement * 1,. . . , X * m. Les lignes 9-13, plus il un facteur déterminant SI les petits amas Ont Une cardinalité supérieure à smin SINON sur fusionné les grappes Concernés with their voisin Le plus proche verser Produire c (x * 1). . . , C (x * m). La ligne 14 les étiquettes de assigne grappes aux bureaux x1 originalExcellent Données,. . . , Xm. 2.2 Choix du Nombre de plus de proches voisins suivant Une échelle de normalien Le règlage critique Paramètre for the mean shift is le Choix du Nombre de plus de k de proches. . Le de travaux pionniers (Loftsgaarden et Quesenberry, 1965), (Fukunaga et Hostetler, - 416 - G. Beck et al algorithme 1 NNMS - Plus de Mean-proches changement Avec les K plus proches exactes de {X1: Entrées,.. ., Xn}, {x1, xm}, k, ε1, ε2, jmax, smin Sorties:...... {c (x1), c (Xm)} / * Calcul du chemin de montee de gradient * / 1: pour `: = 1 à m faire 2: j: = 0; x`, 0: = x`; 3: x`, 1: = moyenne de k-nn de x`, 0; 4: tandis que ‖x`, j + 1, x`, j‖> ε1 ou j <jmax faire 5: j: = j + 1; x`, j + 1: = moyenne de k-nn de x`, j; 6: x * `: = x`, j; / * Création des grappes fusions nominale des itérés finaux * / 7: pour` 1, '2: = 1 à m faire 8: si ‖x * `1 - x *` 2‖ ≤ ε2 puis c (x * `1) = c (x *` 2); / * Fusion des petits amas * / 9: C *: = grappe with a cardinalité minimale; 10: alors que la carte (C *) <smin faire 11: C ': = plus proche grappe à C *; 12: pour x * `∈ C * au c (x *`): = c (C'); 13: C *: = grappe with a cardinalité minimale; 14: pour `: = 1 à m do c (x`) : = C (x * `); 1973) l'erreur établissent des Sélecteurs quadratique optimale et Globaux Locaux Pour Les estimateurs de density des plus de PROCHES voisins, Qué bureaux Sachant ne considèrent pas auteurs les Sélecteurs Basés sur les Données. Une grille de recherche sur les basée à minimiseur Données Cherche les indices de qualité de recherche non supervisee Silhouette Comme l'indice par Considéré (Wang et al., 2007). Notre proposition d'échelle normale verser le sélecteur is: kns = v0 [4 / (d + 4)] d / (d + 6) n6 / (d + 6) (2) where v0 = nD / 2Γ ((d + 2 ) / d)) is l'hyper-sphère de volume d'juin unitaire d-dimentionnel. La dérivation de l'équation (2) is Dans Donnée (Duong et al., 2016). Elle suit l'affirmation Que la Sélection des paramêtres de reglages Basés sur le gradient de densité sur la Que Plutôt density elle-same is, plus verser le quart de travail moyen adéquat (Chacón et Duong, 2013). La Complexité de KNS is O (1) Ce Qui contraste Avec le O (n) de la grille de Recherche pour selectionner le Nombre optimal de Plus proches voisins k Sachant Que le Nombre de recherches de facts possibles is usuellement régle Pour Etre proportional à n . 2.3 De plus proches approximés with the voisins Localité sensible Hashage La Tache calculatoire la, plus intensive Dans NNMS is le calcul des k, plus proches Plutot Que la Voisins Sélection du Nombre de plus de PROCHES voisins. En effet, le point d'écoulement each candidat, ACDE le requiert et le tri calcul de la distance de ‖xi - xj‖, i = 1,. . . , N, j = 1,. . . , M, Qui est O (mn log n). Dans les m where No CAS EST usuels du same de grandeur ordre Qué n, ACDE application fils empèche verser des jeux de Donnees Importants. Une approche de réduction de Complexité - 417 - moyenne équipe: Clustering évolutive Avec les plus de approximés proches voisins sur le TIENT prometteuse des approximés plus de calcul proches voisins sur les Qué Plutôt, plus PROCHES exactes voisins. Parmi CELLES to vary, le lieu pair introduit de hachage sensible, EST Une approche probabiliste basée sur la projection Une scalaire aléatoire de points de multivariés x L (x, w) (Datar et al., 2004) (Datar et al., 2004) = (ZTX + U) / w Où Z ~ N (0, Id) Variable is Une aléatoire normale d-variée et U ~ Unif (0, w) la variable aléatoire is juin prix uniforme sur [0, w), w> 0. Une table de hashage Dont les blocs Sont Basés sur des Valeurs Entières bL (Xi; w) c, i = 1,. . . , N is Alors Construite. En raison de la de Propriétés statistiques de distribution normale, les points de dans l'Espace proches de départ multidimensionnel à tomber auront tendance Dans les blocs scalaires et Mêmes les Points des Dans distants blocs tomberont Comme verifie Dans Différents (Slaney et Casey, 2008). D'facts de w IMPORTANTES impliqueront de blocs Avec Moins, plus de la précision Dans des characteristics de préservation Xi, TANDIS Que de facts de w petites entraineront en plus de blocs Avec Moins de précision. Nous paramétriser le Avons préféré LSH par le M blocs Nombre de de la Table de hashage. NOUS Avons fixe w = 1 sans perte de généralité Li ≡ L (Xi; 1). projections CES scalaires Sont bains triées Dans Leur ordre statistique w = (L (n) - L (1)) / M ou Ij = [L (1) + w (j - 1), L (1) + wj], j = 1,. . . , M. La Valeur de hashée x is l'indice de l'intervalle Dans Lequel L (x, 1) tombe H (x) = j1 {L (x, 1) ∈ Ij} (3) ou 1 {·} is la fonction d 'indication. De les AFIN approximés, plus Chercher PROCHES Voisins, le réservoir des Potentiels, plus proches voisins à la régle HNE du bloc Valeur la Valeur de Contenant hashage. Ce Élargi si is réservoir par concaténation Avec Nécessaire les blocs voisins. Les approximés k, plus proches voisins de x k, plus Sont Les proches voisins Contenus Dans le réservoir Réduit R (x): k-NN (x) = {Xi ∈ R (x): ‖x -Xi‖ ≤ δ (k) ( x)} where δ (k) (x) la distance de l'Est, plus des SEUIL à x proches voisins. L'erreur d'approximation DANS LES comprises proches voisins à x induite par la recherche Dans R (x) Plutôt Que Dans Toutes Les Données is probabilistiquement Contrôlée, voir (Slaney et Casey, 2008). L'Algorithme 2 NNLSH is Une approximation de la recherche des plus de proches voisins Avec le LSH et la fonction de hashage par l'équation Fourni (3). Les plats principaux les Échantillons de are X1 Données,. . . , Xn. Dans les lignes 2 à 6, point d'écoulement each candidat x`, les approximés k-plus proches de k-NN (x`) Sont plus de calculs à partir du réservoir R (x`). La proposition where le NNLSH au NNMS is intégrée was faité par (Cui et al., 2011), Ce Qui la Complexité à Réduit O ((mn / M) log (n / M)). Le M Nombre de blocs de l'Est un reglage essentiel Paramètre. Un fort intêret du Malgré le verser LSH (Har-Peled et al., 2012), il Ne existe pas de verser selectionner méthode optimale le de blocs Nombre, nous heuristiques des examinerons de Fait des performances Dans la section Prochaine. Implementer les plus les proches voisins approximatifs NNMS de Manière un Avec distribuée et N Processus maître Processus la Complexité Esclaves à Réduit O (mn / (MN) log (n / (MN))). C'est notre proposition, le DNNMS Dans l'3. Les plats principaux Algorithme et les sorties are For the Que Mêmes 1. Pour la Algorithme j-ème iteration, les Chemins de gradient de montée xj = [x1, j; . . . ; Xm, j] are Collectes Dans Une matrice m × d. Aux lignes 1 à 6, sur itère JUSQU'A Une convergence globale ‖xj + 1 - xj‖≤ 2 ≡ ‖x1, j + 1 - x1, j ‖,. . . , ‖Xm, j + 1 - xm, 2 ou j‖≤ Nombre maximal d'Jusqu'au Itérations jmax. CERTAINS Calculs when redondants Sont effectués des x` CERTAINS, j Converge Déjà en Ontario, Mais this forme calculation is Nécessaire POUR UNE - 418 - Beck G. et al. parallélisation efficace en MapReduce (Dean et Ghemawat, 2008). Le Paradigme MapReduce is, plus si les algorithmes Efficace en repensés série are, D'une passant sur each itération à Une candidat sur l'ensemble itération des Candidats simultanément. Les 7-14 lignes la fusion des décrivent reprise de l'regroupements 1 sans modification Algorithme majeure being Donné Que le MapReduce Ne est pas ici Requis. Algorithme 2 NNLSH - Approximés k, plus proches voisins Avec LSH Entrées: {X1,. . . , Xn}, {x1,. . . ,} Xm, k, M: {k Sorties-ppv (x1),. . . , K-NN (xm)} / * Création des tables de hashage with blocs M * / 1: pour i: = 1 à n faire Salut: H = (Xi); / * Recherche des approximés en plus proches voisins Dans les blocs adjacents * / 2: pour `: = 1 à m faire 3: R (x`): = {Xi: Salut = H (x`), i ∈ {1,. . . , N}} 4: tandis que la carte (R (x`)) <k do 5: R (x`): = R (x`) ∪ adjacent bloc; 6: k-NN (x`): = k-nn de R (x`) à x`; Algorithme 3 DNNMS - Plus DISTRIBUE moyenne Voisins-proches quart de travail, approximés k Avec en plus en proches voisins le LSH Utilisant Entrées: {X1,. . . , Xn}, {x1,. . . , Xm}, k, ε1, jmax, ε2, smin, M Sorties: {c (x1). . . , C (xm)} / * Calcul des chemins de montée de gradient * / 1: j: = 0; x0: = [x1,0; . . . ; Xm, 0]; 2: x1: = moyenne de k-ppv de {X1,. . . , Xn} pour x0 3: tandis que ‖xj + 1 - xj‖> ε1 ou j <jmax do 4: xj + 1: = moyenne des k-NN de {X1,. . . , Xn} pour xj; / * Cf Algorithme 2 * / 5: x *: = [x1, j; . . . ; Xm, j]; 6: aux lignes 7-14 Identique Dans l'Algorithme 2.1; 3 Influence 3.1 Résultats des expérimentaux de paramêtres parallélisation 3.1.1 Nombre de Sur observateur noeuds sur la may figure 1a Que notre Scala / Spark de la montée Implémentation de gradient Avec les k-plus proches voisins is évolutive, le temps d'exécution diminue Efficace - ment les Avec Nombre de noeuds esclaves. Des enquêtes après Lacunes de notre première mise en œuvre, nous observateur de l'étape Que Avons de LABELISATION ne etait pas extensible sur la Montré Comme la figure 1b. Cependant, en réutilisant les LSH de segmenteur les AFIN Comme Tâches - 419 - Mean-Shift: clustering évolutives AVEC Les plus proches voisins approximés algorithme 4 LABELISATION les k AVEC distribuée, plus PROCHES approximés Entrées voisins: {x * 1,. . . , X * m}, M2, 2, 3 Sorties: {c (x1),. . . , C (xm)} / * Création des tables de hashages Ë blocs M2 * / 1: pour i: = 1 à m faire Salut: = H (x * i); / * Labelisation DES DONNEES * / 2: pour `: = 1 à m faire 3: R (x *`): = {x * i: Salut = H (x * `), i ∈ {1,. . . , M}} 4: 1 pour `,` 2: = 1 à la carte (R (x * `)) do 5: si ‖x *` 1 - x * `2‖≤ ε2 puis c (x *` 1) : = c (x * `2); / * Calcul des barycentres * / C1, ..., Cj: = barycentre des grappes / * Fusion Des plus proches se regroupent * / 6: pour C1, C2: = 1 à j 7 faire: si ‖C1 - C2‖≤ 3 puis Fusion de C1 et C2 decrit Dans l'algorithme 4, nous observons Avons juin Scalabilité de la solution representee sur la figure 1c. La third à fusionner étape les cohérente petits groupes, plus their Avec se deroule proches voisins sur le noeud localement maître, les coordonnées Elle des Prend barycentres et la cardinalité du groupe qu'associé verser LABELISATION Une finale sortir Qui sera appliquée en parallèle. En si les ValeurS pratique 2 et 3, bien Sont choisies this is étape IMMEDIAT, un mauvais choix de bureaux facts may la génération d'agent d'entraînement un grand amas de et Nombre faire le temps d'exploser exécution. 5 10 15 20 25 500 1000 1500 Nombre d'esclaves Te m ps d 'ex a c © cu tio n (s) (a) les plus proches voisins de remontée gradient 2 3 4 5 6 7 8 20 40 60 80 100 120 Nombre d' Te m ps esclaves d » ex éc ut io n (s) (b) Première Implémentation de labelisation 2 = V 1 = V 2 2 2 3 4 5 6 7 8 6 8 10 12 Nombre d'esclaves Te m ps d 'ex éc ut io n (s) (c) Nouvelle labelisation 2 = V 1 = V 2 2 FIGUE. 1 - de Scalabilité Amélioration. (A) Montée de gradient KNN. (B) Première labellisation. (C) Nouvelle mise en œuvre. 3.1.2 Influence du blocs Nombre de LSH du Notre pneu nouvelle Implémentation du LSH Avantage la phase de de Durant de gradient montée pendant l'Mais aussi de LABELISATION étape. Un sérum clé Paramètre du Nombre de Celui blocs M1, M2 Dans le LSH. Si on laisse CE constante dernier sur la presented Comme la figure 2, sur CONSTATE Que la Complexité de la montée quadratique de gradient par les k plus de Persiste tout proches voisins Comme l'répandrai de LABELISATION étape. Cependant, sur observateur may sur la Figure 3 - 420 - Beck G. et al. 0,2 0,4 0,6 0,8 1 · 107 0 1000 2000 Taille du jeu de Donné Te m ps d » ex éc ut io n (s) (a) Montée de gradient with 1000 blocs 0 1 2 3 4 · 106 0 500 1000 Taille du jeu Te de Donné m ps d » ex éc ut io n (s) (b) labelisation with 13 blocs FIG. 2 - Influence de la taille du jeu de à Données de constante Nombre bloc. (A) Sur la Mon- de gradient tée. (B) Sur la LABELISATION. 0 2000 4000 6000 8000 0 2000 4000 6000 Nombre de blocs M1 Te m ps d » ex éc ut io n (s) (a) Montée de gradient Avec les K plus de proches 4 Esclaves 6 esclaves 8 esclaves 0 50 100 150 200 250 20 40 60 80 100 Nombre de blocs M2 Te m ps d » ex éc ut io n (s) (b) labelisation FIG. 3 - Influence du blocs Nb de verser un jeu banque de points taille fixe. (A) Sur la Mon- de gradient tée. (B) Sur la LABELISATION. le temps d'exécution decrease RAPIDEMENT, en fonction du blocs Nombre de, verser Puis ralentir un SEUIL Qui Atteindre du dépendra Nombre de d'entrées Données. Une observation Intéressante concerns l'augmentation du temps d'linéaire exécution un fixe lorsqu'on d'élément constant Nombre par bloc l'augmentation de Avec la taille du jeu de sur Données Comme illustré la figure 4. Ce de Përmet Résultat Version notre conforter du décalage en moyenne qu'algorithme Tant évolutive pleinement. 3.2 Application à la segmentation d'images La Résurgence d'Intérêt Dans l'algorithme Mean-Shift à l'application is dûe fils à la segmentation d'image (Comaniciu, 2003) where l'image Une is transformée Dans un espace colorimétrique each Dans Lequel correspondent à des grappes des régions segmentées de l'image originale. de L'allure 3 dimentionnel de L * u * v * de couleur (Pratt, 2001) is a choix commun. Sachant image Qu'une is un tableau de pixels bidimentionnel, disons QUE (x, y) Sont Les indices de lignes et de colonnes d'un pixel. Les informations spatiales et colorimétriques d'pixel d'un PEUVENT Fait Être concaténées en un vecteur 5-dimensionnel (x, y, L *, u *, v *) Dans la jointure des Domaines - 421 - Mean-shift: Clustering évolutive with Les plus proches voisins approximés 0 0,2 0,4 0,6 0,8 1 1,2 1,4 · 108 0 0,5 1 1,5 · 104 Taille du jeu de Données Te m ps d » ex éc ut io n (s) (a) Montée de gradient 1000 éléments blocs nominale de 2000 éléments par blocs 5000 éléments blocs pair 0 0,4 0,6 0,8 0,2 1 1,2 1,4 · 108 0 500 1000 Taille du jeu de Données Te m ps d » ex éc ut io n (s) (b) blocs nominale labelisation with 20k éléments Fig. 4 - Influence de la taille du jeu de Données. (A) Sur la montée de gradient. (B) Sur la LABELISATION. Spatio-. colorimétriques D'ACDE AFIN illustrer, l'image de l'# NOUS prenons 171 du jeu d'entrainement de Berkeley Segmentation coloré DataSet et référence 1. Dans la Fig.3 (ab) sur les pixels Retrouvé 481 × RGB originaux 321 de l'image JPEG et le diagramme de dispersion des n = 154401 jointure des coordonnées spatio-colorimétriques (X, Y, L *, u *, v *). (A) RGB (b) FIG distance spatiale. 5 - Représentations des couleurs de l'image. (a) l'image RVB de 481 × 321 pixels. (B) nuage de points Avec n = 154401 Transformé en espace (x, y, L *, u *, v *). Un algorithme de segmentation d'images basons sur le moyen-quart à noyau Été introduit Dans Avait [2] nous Qué Pour un adapted Avons utilisation NNMS AVEC. Les paramêtres de reglages verser NNMS et DNNMS KNS = 2463 are, ε1 = 0,005 Fois la marge maximale de la portée des Données, jmax = 100, ε2 = 10ε1, smin = 1544. Nous exécutons le DNNMS-M = M Avec 200, 500, 1000 blocs. Les temps d'exécution 20 respectively Sont, 13 et 10 minutes, en juin improvement significative à la nuit comparaison calculation for the NNMS Nécessaire sur un ordinateur de bureau standard. Quants choix à la Qualité de La segmentation d'images Dans la Fig.4 (a) Pour le NNMS with the PROCHES, plus exactes voisins. This l'image segmentée Une offre de la réduction considérable de l'image Complexité, tout en les centres des agent de détection Fleurs de détails Incluant CERTAINS granularité bien. Les contours, les bords des Pétales, 1. http CERTAINES: Ombres //www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds - 422 - G. Beck et al. Que Feuillages AINSI d'arrière Différents plan en la preuve are. En raison de la nature de la projection aléatoire du LSH verser approximer Les plus proches voisins in the DNNMS, les clusters compacts bureaux et Sont en plus Moins Diffus du NNMS Que Ceux where les projections LSH ne pas Sont utilisées. Pour le DNNMS-200 Dans la Fig.4 (b) avec les approximés, plus proches voisins M = 200 Avec blocs, CERTAINS détails Plus ou moins Sont en l'absence visibles du LSH. Pour le DNNMS-500 et le DNNMS-1000 Dans les Fig.4 (c-d), les répandrai les centres des fleurs jaunes reVu delimités are Pétales Moins, et il y a de considerables épanchements des Pétales sur le feuillage. Nous observons Que M = 200 blocs is un choix fortuit Comme c'est le also in the Fig.2 No CAS et de Plus amples Requises les enquêtes de verser un optimal choix général. (A) NNMS (1 nuit) (b) DNNMS-200 (20 min) (c) DNNMS-500 (13 min) (d) DNNMS-1000 (10 min) FIG. 6 - Segmentation d'images par l'intermédiaire colorée Les plus Mean-shift Voisins proches. (A) NNMS Avec le plus de moyen exact proche voisin de décalage en série. (B-d) DNNMS-M Avec les approximatifs, plus de proches l'Distribué Mean-shift with M = 200, 500 et 1000 blocs. Les temps d'exécutions are respectively une nuit, 20, 13 et 10 minutes. Le Berkeley Segmentation Dataset et Benchmark Une segmentation d'fournit l'image images de humaine their Ë des nageoires de Comparaisons. Dans la Fig.5 (a-b) se trouvent deux Détections de par l'bordures Faites # 1107 et Utilisateur # 1123. L'# 1107 se Utilisateur CONCENTRE sur la segmentation du plan de feuillage d'arrière et le contour de la forme des fleurs, tout en ignorant les détails des Pétales des fleurs. L'# 1123 Utilisateur Ã lui se Quant CONCENTRE sur la segmentation des Pétales Dans le plan de individuelles de premier ordre. Nous portons notre attention ici sur le NNMS et le DNNMS-200 (Fig. 5 (c-d)). Le DNNMS-500 et le DNNMS-1000 (Fig. 5 (e-f)) donne Une qualité insuffisante de la détection des bords. Le NNMS et le DNNMS-200 de segmenteur Sont en capables Une Seule un unique, with exécution de jeu paramêtres de règlage, le feuillage d'simultanément le plan et la arrière des Pétales de forme plan de premier ordre. Sur une zone Une segmentation automatique AINSI Combinant le de deux experts Résultat se focalisant sur Humains de l'image Différentes. - 423 - Mean-shift: Clustering évolutive with Les plus de proches les approximés (a) Utilisateur # 1107 (b) Utilisateur # 1123 (c) NNMS (1 nuit) (d) DNNMS-200 (20 min) (e) DNNMS- 500 (13 min) (f) DNNMS-1000 (10 min) FIG. 7 - Détection de l'image d'segmentée bordure. (A-b) Deux experts Humains: # 1107 et Utilisateur # 1123. (C) NNMS en les Avec série, plus PROCHES exactes voisins. (D-f) DNNMS-M Distribué Avec les approximatifs, plus proches voisins LSH with M = 200, 500 et 1000 blocs. 4 Conclusion Nous Avons several introduit à l'algorithme Améliorations des plus de changement de dire- proches. La première du EST Une heuristique OFFERTE Choix d'une valeur optimale du Nombre de plus de PROCHES voisins. La seconde is l'emploi d'approximation Une des plus de par le proches voisins localité hash sensible Pour la phase de gradient de montée de Mais aussi la phase de verser de LABELISATION. La third is juin Dans un ecosystem Implémentation Distribué. Nous Avons c Que démontré es le drastiquement Améliorations temps diminuent d'exécution tout en la qualité du Maintenant vis-à-vis du Regroupement des PROCHES, plus exactes voisins. CÉS application possible Améliorations de rendent l'dU Mean-Shift verser le regroupement au Appliqué Big Data Dans un futur proche. RESTENT CERTAINES Améliorations à faire sur cependant les paramêtres de reglages cruciaux, i.e. le plus de Nombre de proches voisins that the AINSI Nombre de blocs Dans le localité hash sensible les approximés plus de verser proches voisins is Requis. Références Chacón, J. E. et T. Duong (2013). estimation de la densité axée sur les données, les applications de cluster non paramétrique et la chasse bosse. Revue électronique de la statistique 7, 499-532. Comaniciu, D. (2003). Un algorithme de sélection de bande passante guidée par les données. IEEE Transactions sur le modèle d'analyse et de l'intelligence artificielle 25, 281-288. Comaniciu, D. et P. Meer (2002). mean shift: une approche solide vers l'analyse de l'espace caractéristique. IEEE Transactions on analyse et de renseignement machine modèle 24, 603-619. - 424 - Beck G. et al. Cui, Y., K. Cao, G. Zheng, et F. Zhang (2011). Un algorithme de changement de vitesse moyenne adaptatif basé sur lsh. Procedia ingénierie 23, 265-269. Datar, M., N. Immorlica, P. Indyk,, et V. S. Mirrokni (2004). des tables de hachage Localité sensibles en fonction des distributions stables p. Actes du XX e colloque annuel sur la géométrie computationnelle, 253-262. Dean, J. et S. Ghemawat (2008). MapReduce: traitement des données simplifiées sur les grands groupes. Communications de l'ACM 51, 107-113. Duong, T., G. B. H. Azzag, et M. Lebbah (2016). Les plus proches voisins des estimateurs de dérivés de densité, avec application à dire le regroupement de changement de vitesse. Motif Lettres de reconnaissance 80, 224-230. Fukunaga, K. et L. Hostetler (1973). Optimisation des estimations de densité k-plus proche voisin. IEEE Transactions on Théorie de l'information 19, 320-326. Fukunaga, K. et L. Hostetler (1975). L'estimation du gradient d'une fonction de densité, avec des applications dans la reconnaissance des formes. IEEE Transactions on Théorie de l'information 21, 32-40. Har-Peled, S., P. Indyk, et R. Motwani (2012). Approximer voisin le plus proche: vers la suppression de la malédiction de la dimensionnalité. Théorie de l'informatique 8, 321-350. Loftsgaarden, D. O. et C. P. Quesenberry (1965). Une estimation non paramétrique d'une fonction de densité multivariable. Annale de la statistique mathématique 36, 1049-1051. Pratt, W. K. (2001). Traitement de l'image numérique: PIKS intérieur. Slaney, M. et M. Casey (2008). hashing Localité sensible pour trouver les plus proches voisins. IEEE Signal Processing Magazine, 128-131. Wang, X., W. Qiu, et R. H. Zamar (2007). Une méthode de classification non paramétrique sur la base de rétrécissement local. Informatique Statistiques et analyse des données 52, 286-298. Résumé Nous présentons une mise en œuvre efficace de distribution voisin le plus proche regroupement de décalage moyenne (NNMS). La nature intensive de NNMS informatiquement a jusqu'à présent restreint son application aux ensembles de données complexes où un regroupement flexible avec des groupes non-ellipsoïdales serait bénéfique. Une mise en œuvre parallèle des NNMS série algorithme standard sur ses propres apporte des gains de performances insuffisantes pour que nous introduisons deux nouvelles améliorations algorithmiques: un choix échelle normale (NS) du nombre optimal de voisins les plus proches, et la localité hash sensible (LSH) pour se rapprocher le plus proche voisin recherches. La combinaison de ces améliorations dans un algorithme unique distribué DNNMS offre la possibilité d'une méthode efficace pour Big Data Clustering. - 425 -"
56,Revue des Nouvelles Technologies de l'Information,EGC,2018,Recommendation-based Keyword Search over Relational Databases,"Récemment, la recherche par mots-clés dans les bases de données relationnelles a suscité un intérêtgrandissant en raison de sa facilité d'utilisation. Bien que des recherches approfondies fussentdernièrement effectuées dans ce contexte, la plupart de ces recherches non seulement nécessitent unaccès préalable aux données, ce qui restreint leur applicabilité si cette condition n'est pas vérifiée,mais aussi renvoient des réponses très génériques. Cependant, fournir aux utilisateurs des réponsespersonnalisées est devenu plus que jamais nécessaire en raison de la surabondance de données quipeut déranger l'utilisateur. Le défi de retourner des réponses pertinentes et personnalisées qui satisfontles besoins des utilisateurs demeure. Inspiré par l'application réussie de la technique de filtragecollaboratif dans les systèmes de recommandation, nous proposons une nouvelle approche baséesur les mots-clés pour fournir aux utilisateurs des résultats personnalisés basés sur l'hypothèse queseulement une information sur le schéma de la base de données est disponible.","Haithem Ghorbel, Nouha Othman, Rim Faiz",http://editions-rnti.fr/render_pdf.php?p1&p=1002400,http://editions-rnti.fr/render_pdf.php?p=1002400,en,"Recommandation basée sur les mots-clés Recherche sur les bases de données relationnelles Haithem Ghorbel *, Nouha Othman * et Rim Faiz ***, * Université de Tunis, l'Institut Supérieur de Gestion de Tunis, LARODEC, Tunisie gho.haithem, othmannouha @ gmail.com *** Université de Carthage IHEC Carthage, LARODEC, Tunisie rim.faiz@ihec.rnu.tn~~V~~singular~~3rd Résumé. Récemment, il y a eu un intérêt croissant pour la recherche mot-clé dans les bases de données relationnelle en raison de sa facilité d'utilisation. Bien que des recherches approfondies ont été récemment fait dans ce contexte, la plupart de cette recherche exige non seulement un accès préalable aux données qui limite considérablement leur applicabilité si cette condition est pas vérifié, mais renvoie aussi des réponses très génériques. Cependant, fournir aux utilisateurs des réponses personnalisées est devenu plus que jamais nécessaire en raison de la surabondance de données qui peut être gênant pour l'utilisateur. Le défi de revenir personnalisé et des réponses pertinentes que les informations des utilisateurs reste la satisfaction des besoins. Inspiré par l'application réussie de la technique de filtrage collaboratif dans les systèmes recommender, nous proposons une nouvelle approche basée sur les mots clés pour fournir aux utilisateurs des résultats personnalisés en fonction de la pothèse hy- que seules les informations sur le schéma de base de données est disponible. 1 Introduction Au fil des décennies, une quantité explosive de données structurées ont été stockées dans des bases de données relationnelles (BDR) s. Ces derniers ont été largement utilisés en raison des informations qu'ils fournissent riches, y compris les relations entre les différentes entités du DB. L'élaboration de méthodes d'interrogation efficaces pour les utilisateurs de référentiels facilement recherche énorme et complexe sans le besoin d'expertise technique est devenu l'un des plus grands défis de la communauté de base de données (Agrawal et al., 2002; Aditya et al., 2002). L'émergence de moteurs de recherche Web a mot-clé recherche la technique de recherche le plus couramment utilisé. La force de ce dernier est qu'il permet aux utilisateurs d'exprimer facilement leurs besoins d'information par quelques mots-clés sans avoir besoin de connaître le schéma de base de données ou les langages de requêtes structurées. Néanmoins, une telle technique nécessite un accès avant le contenu de base de données afin de construire les indices qui Pinpoint les différents tuples associés aux mots-clés au moment de l'exécution (Bergamaschi et al., 2011). Ceci est une lacune considérable car elle limite son applicabilité si un accès préalable aux données est impossible. Une autre limitation importante est que les inter-dépendances entre les mots-clés de la requête ont été ignorés. En fait, le sens de chaque mot-clé dans la requête d'un utilisateur dépend aussi de la signification des autres. D'autre part, avec le formidable développement de l'information tech- nologie, la quantité de données a connu une croissance exponentielle. Ainsi, trouver les informations souhaitées dans une base de données massive est devenue un élément crucial, mais aussi une tâche difficile. Recommandation Systems (ORS) sont de puissants outils pour filtrer les données, fournissant uniquement ce que l'utilisateur est le plus susceptible chercher. Dans cet article, nous proposons une tentative réussie de combiner des techniques et RDB pour rSS surmonter la limitation de la recherche par mot clé. Nos objectifs d'approche proposée au retour des réponses personnalisées quand on n'a pas accès avant les données réelles stockées dans la base de données. Le reste du document est structuré comme suit: Dans la section 2, nous passons en revue les principaux travaux en cours sur l'interrogation BRD. Nous décrivons ensuite notre - 347 - Recommandation basée sur les mots-clés Recherche sur Relational approche BDs dans la section 3. Par la suite, nous présentons à la section 4, nous présentons notre évaluation expérimentale et les résultats. Vers la fin, nous concluons et présentons nos perspectives. 2 Travaux connexes sur Interrogation bases de données relationnelles On peut dire que, recherche par mot clé est devenu la norme pour la recherche d'informations sur le Web, car il permet à l'utilisateur de formuler facilement des requêtes avec quelques mots-clés. Cependant, sa simplicité est livré avec un prix; mots-clés sont très ambigu et leurs besoins de sens destinés à explorer plus loin (Wang et al., 2008). Au fil des années, les approches avancées pour le mot clé effectuez des recherches docu- ments ont été proposés pour retourner des réponses pertinentes à l'utilisateur. Ces approches, cependant, ne renvoient pas de bons résultats avec des systèmes RDB (BRD), comme la recherche de style IR considère tuples comme données non structurées, alors que dans BRD, les informations récupérées sont réparties entre les tables. Contrairement à des documents textuels, les tuples sont liés par les contraintes de clés étrangères primaires. Ainsi, les chemins clés étranger primaires connexion tuples qui contiennent des mots clés, représentent un élément essentiel pour résoudre une requête sur une base de données mot-clé. Définition des modèles de représentation des bases de données pour récupérer ces chemins est cru- cial. Pour ces raisons, l'application directe des approches par mots clés à des bases de données relationnelles, où l'information est fragmentée dans de nombreux tableaux, est ni efficace, ni efficace (Bergamaschi et al., 2014). En effet, plusieurs systèmes ont été proposés dans la littérature, où les plus populaires sont des banques (Aditya et al., 2002), banksii (Kacholia et al., 2005), DBXplorer (Agrawal et al., 2002), DISCOVER (Hristidis et Papakonstantinou, 2002) et SQAK (Tata et Lohman, 2008) et le plus un récent sont KEYMANTIC (Bergamaschi, Domnori, Guerra, Trillo Lado et Velegrakis, 2011), KEYRY (Bergamaschi, Guerra, Rota et Velegrakis, 2011) et SEMINDEX (Chbeir et al., 2014). L'objectif de ces systèmes est de mieux couvrir une requête mot-clé, afin de retourner des réponses qui correspond à l'intention de l'utilisateur. Ces approches peuvent être classées en deux grandes catégories: schéma basé et les approches tuple. Les approches fondées sur le schéma modèle, le schéma de base de données sous forme de graphique, dans lequel les noeuds expriment des rapports de base de données et les bords expriment entre in- clés interdépendance primaire et à l'étranger. De telles approches peuvent répondre à une requête mot clé par l'utilisation des informations de schéma pour générer des requêtes SQL dans BRD, comme dans DBXPLORER, DISCOVER, PRECIS, SQAK, KEYMANTIC et systèmes KEYRY. approches fondées sur le tuple tels que les banques et les banques II, le modèle de la base de données sous forme de graphique de données, dans lequel les noeuds représentent les tuples et des arêtes représentent les relations entre une paire d'uplets, tels que clés ou clé primaire de- pendances étrangère. La particularité d'un graphe de données est que les noeuds et les arêtes sont généralement pondérés, ce qui offre aux utilisateurs plus d'informations sur la façon dont les objets sont reliés entre eux. KEYMANTIC et KEYRY ont abordé la question de la recherche par mot clé sur BRD différemment; ils peuvent fournir des réponses à la requête de l'utilisateur sans la nécessité d'un accès préalable aux données stockées dans la base de données pour construire des indices qui localisera les tuples. 3 Solution proposée L'idée de base de notre approche, appelée DeepRec, est de combiner la recherche par mot clé sur RDB avec quelques techniques utilisées dans les systèmes recommender. vise DeepRec à intégrer des concepts de recommandation et des bases de données pour obtenir de meilleures réponses personnalisées à une simple requête basée sur mot-clé posté par un utilisateur. Il fournit des recommandations et des réponses fortuites, même lorsqu'il n'y a pas accès avant la base de données est autorisée, en se fondant sur les deux schémas et informations utilisateurs. Les différentes composantes de l'approche donnée sont détaillées ci-dessous. - 348 - H. Ghorbel et al. 3.1 Termes de schéma ne correspond à la première phase appelée schéma de calcul du poids consiste à déterminer les mots clés correspondant à des termes de schéma (attributs, relations) à partir d'une requête et les informations de schéma de la base de données. Les attributs et les relations sont considérées comme des métadonnées. Afin d'estimer la distance attribut / relation mot-, nous avons opté pour la mesure Levenshtein qui calcule le nom- bre minimal d'insertions, suppressions et remplacements nécessaires pour transformer une chaîne X en une chaîne Y. Cependant, une simple similitude de chaîne entre le mot-clé et le terme de schéma ne suffit pas en raison de l'hétérogénéité du vocabulaire de l'utilisateur. En fait, un utilisateur peut utiliser différents mots qui ne figurent pas dans les informations de schéma de la base de données. Pour tis fin, nous employons WordNet pour Word Sense homonymie (WSD), de sorte que chaque mot-clé utilisé est comparé à tous les synonymes, hyponymes et hy pernyms de chaque terme de schéma pour garder celui ayant la plus forte similitude. La deuxième phase appelée schéma poids personnalisation consiste à mettre à jour le schéma poids (SW) Matrice en utilisant les informations recueillies auprès des profils des utilisateurs. L'idée principale est d'ajouter ici le concept de filtrage collaboratif (CF) de l'ORS; profils de construction pour les utilisateurs et d'utiliser leur historique de recherche, ainsi que l'histoire des utilisateurs similaires à personnaliser les résultats. On calcule le schéma personnalisé Poids (PSW) qui utilise les informations des sessions pour mettre à jour la matrice SW. Des sessions similaires sont répertoriées dans une table. , On calcule ensuite combien de fois chaque terme de schéma a la valeur maximale, pour toutes les requêtes des sessions similaires. Nous stockons le résultat de ce calcul dans sa colonne spécifique dans la matrice SW. Chaque valeur de cette colonne est associée à chacun dans sa colonne analogique dans le SW pour obtenir les nouvelles valeurs de PSW dans leurs cellules correspondantes. En d'autres termes, nous pondérons les valeurs de la SW de chaque colonne par une variable qui affecte les premières valeurs en fonction du nombre de fois cette colonne a la valeur maximale. Le calcul de la meilleure adéquation possible des mots-clés à des conditions de base de données est connu comme le problème d'affectation. Les Munkres populaires, hongrois a.k.a., algorithme (Munkres, 1957), est une solution possible à ce problème, mais il ne fournit que la meilleure correspondance. Bergamaschi, Domnori, Guerra, Trillo Lado et Velegrakis (2011) adapté cet algorithme à notre contexte, de ne pas arrêter après la génération du sommet d'un mappage, mais continuer à générer les autres meilleurs. En outre, la matrice de poids est dynamiquement mis à jour chaque fois une mise en correspondance d'un mot-clé à un terme de base de données est décidée lors du calcul. 3.2 Valeur Poids contextualisation Le calcul de la matrice Valeur Poids (VW) est réalisée de la même manière que dans (Bergamaschi, Domnori, Guerra, Trillo Lado et Velegrakis, 2011). Le calcul se fait principalement dans les informations de domaine d'attributs. KEYMANTIC a utilisé une distance sémantique pour estimer le degré de parenté de deux concepts. Ainsi, chaque cellule de matrice dans la VW contient une valeur comme un indicateur de l'admissibilité et la pertinence du mot-clé avec le domaine d'attribut. Les mots-clés qui ont déjà été mis en correspondance avec schéma dans l'étape précédente s'attribué 0 dans chaque cellule de leurs lignes pour assurer qu'ils ne seront pas recalculées dans la VW. Après le calcul de la meilleure correspondance des termes de schéma Mi et la valeur matrice de pondération, la matrice de VW est mis à jour selon les conditions mises en correspondance avec le schéma. Dans les requêtes de mots-clés, un mot-clé peut se référer soit à un terme de schéma ou à une valeur dans un terme de schéma. Nous contextualiser la matrice de poids de valeur selon les termes mis en correspondance en termes de schéma de la PSW, en tenant compte des positions des mots-clés dans la requête. L'utilisateur peut utiliser plus d'un terme pour décrire un concept. L'intuition de base derrière notre méthode est de vérifier pour chaque mot-clé k si elle correspond à un terme de schéma x de ceux mis en correspondance. Ensuite, si k correspond à un mot-clé mis en correspondance avec un terme de schéma x, deux situations peuvent se présenter: si x est une relation R, on ajoute un Ω de poids à tous les attributs de R pour tous les mots-clés adjacents A (k) ∪ B (k ), sinon, si x est un attribut a d'une relation R, on augmente le poids de cet attribut et les attributs liés (avec dépendances fonctionnelles) en Ω pour tous les a (k) ∪ mots-clés B (k) k voisins. - 349 - Recommandation basée sur les mots-clés de recherche relationnelle DB A (k) et B (k) sont deux fonctions qui récupèrent les mots clés suivants et précédents voisins respectivement k. Ω est une variable, sa valeur est proportionnelle à la distance entre les mots-clés. La sortie de cette étape est une contextualisée Valeur Poids Matrice Vj (Mi). Encore une fois, nous allons utiliser l'extension de l'algorithme hongrois, cette fois-ci sur Vj (Mi), pour obtenir les meilleures affectations. 3.3 Génération des réponses de requêtes personnalisées: Le Vj (Mi) avec son associée Mi est une correspondance complète des mots-clés à termes DB, produisant ainsi une nouvelle combinaison nommée Aij. Le score de chaque c ombinaison est la somme des poids des Vj (Mi) et son Mi. associé requêtes SQL peuvent être obtenues avec la possession des combinaisons de première partition. Pourtant, cette dernière vient se trouver sur les mots-clés correspondent à leurs conditions de base de données adéquates, sans définir les relations entre les termes. Les travaux de ce processus sous la même hypothèse d'une absence d'accès avant des données telles que (Bergamaschi et al., 2011) ne considèrent la similitude entre les mots clés-attributs / domaine des similitudes, les attributs qui ne sont pas toujours approprié. Pour faire face à cette limitation, nous prenons avantage des profils construits par notre approche, de personnaliser les réponses. Donner un utilisateur actuel faisant une requête dans sa session en cours, on calcule la similitude de cette requête avec toutes les requêtes précédentes dans toutes les sessions en utilisant la similitude cosinus. La réponse que suscité un intérêt de l'utilisateur est crédité même sa requête associée est assez similaire à la requête en cours de l'utilisateur. 4 Evaluation expérimentale 4.1 Configuration expérimentale Dans nos expériences, nous avons utilisé 5 MySQL comme système de gestion de base de données relationnelle et WordNet 3.0 1 comme base de données lexicale. Nous lui avons expliqué le contenu de la base de données à aucun utilisateur technique sans exposer ses informations de schéma. Nous leur avons demandé de proposer des requêtes de mots-clés et de décrire ce qu'ils attendent que des réponses à leurs questions. Ensuite, un expert formule une requête SQL à cet effet. Nous avons comparé les résultats générés par notre approche avec ceux obtenus par l'expert. Nous avons utilisé une partie de la base de données MovieLens 2 100K. Lors de nos tests, 18 utilisateurs ont été impliqués, les requêtes de nous regroupons chaque utilisateur en une seule session. Le nombre de sessions est comprise entre 1 et 3 pour chaque utilisateur. Nous avons utilisé 105 requêtes distribuées entre les sessions des utilisateurs. Pour évaluer DeepRec, nous étions basés sur le nombre de mots clés et des sessions. Pour l'initialisation, nous avons utilisé les vues INFORMATION_SCHEMA fournies par MySQL qui permet de récupérer les métadonnées sur les objets dans la base de données. 4.2 Résultats expérimentaux et discussion La figure 1 montre une comparaison entre KEYMANTIC et DeepRec en termes de pourcentage de la 1ère réponses de position, le pourcentage de la non premières réponses de position et le pourcentage des réponses non trouvées qui indique les réponses pertinentes que le système ne parvient pas à revenir. Les expériences ont montré une amélioration du pourcentage des réponses 1er position sur la non premières réponses de position. Cependant, les pourcentages des réponses non trouvées pour les deux approches sont assez similaires. Les résultats du taux de réussite en fonction du nombre de séances sont données à la figure 2. Pour le nombre de mots clés et son impact sur le taux de réussite, nous avons calculé le pourcentage des réponses qui ont été considérées comme pertinentes pour les utilisateurs et retourné comme la première réponse tout en changeant le nombre de mots-clés dans la requête comme le montre la figure 2. 1. www.wordnet.princeton.edu 2. www.MovieLens.com - 350 - H. Ghorbel et al. FIGUE. 1 - Taux de réussite pour Généré DeepRec et KEYMANTIC Nous avons remarqué qu'aucun changement n'a été remarqué pour les réponses non trouvées. Certaines requêtes peuvent générer la réponse attendue, mais la plupart du temps, pas dans la première position, à l'exception de ceux qui ont été « aimé » avant par l'utilisateur. La variabilité des temps de traitement dépend du nombre de tables liées aux requêtes. Des résultats intéressants sont présentés pour les interactions des utilisateurs, y compris des sessions et des requêtes. Le nombre de réponses qui répondent aux attentes des utilisateurs a augmenté. Comme tout CF sur la base FIG. 2 - Taux de réussite selon Généré aux sessions et système Nombre Mots-clés, plus les utilisateurs que nous avons et les interactions plus qu'ils font avec le système, mieux sa performance est. Les expériences montrent qu'il n'y a pas un nombre optimal de mots-clés. Cependant, le nombre de mots clés peut augmenter soit la précision ou un heureux hasard selon les termes employés par l'utilisateur. Le problème de démarrage à froid de la technique CF est résolu par DeepRec. En outre, l'étape de personnalisation permet de prendre en compte chaque résultat qui correspond à la reque de l'utilisateur st. Ceci est très utile lorsqu'un utilisateur recherche une information qui a déjà recherché. Cependant, avec KEYMAN- TIC, quel que soit le nombre de fois que l'utilisateur interagit avec le système, il recalcule toujours les réponses en ignorant ce que l'utilisateur est probablement attend. 5 Conclusion Cet article a abordé le problème du traitement des requêtes par mot-clé sur BRD sous l'hypothèse que l'accès ne préalable aux données est possible. Notre contribution consiste à fournir aux utilisateurs des résultats sonalized per- dans ce contexte spécifique en élargissant une approche existante avec de nouveaux composants et des ressources à savoir, la personnalisation de la matrice de poids de schéma et les réponses, ainsi que les utilisateurs et les informations relatives à leurs interactions avec les système. Au-delà de simplement revenir réponses génériques, nos résultats indiquent que DeepRec fournit des résultats personnalisés qui correspondent mieux aux intention des utilisateurs. Avant la génération des réponses, nous avons profité du profil de l'utilisateur actuel à d'autres réponses de personnalisation basées sur la technique des FC de ORS; favoriser une réponse déjà aimé par l'utilisateur. Apporter des réponses personnalisées en l'absence de l'accès aux données avant est possible, rend utilisable DeepRec dans les bases de données Web et certains systèmes où la construction d'indices spécialisés ne sont pas une option possible. - 351 - Recommandation basée sur les mots-clés Recherche sur Relational Références DB Aditya, B., G. Bhalotia, S. Chakrabarti, A. Hulgeri, C. Nakhe, P. Parag et S. Sudarshan (2002). Banques: Navigation et recherche par mots clés dans les bases de données relationnelles. Dans Actes de la 28ème conférence internationale sur les très grandes bases de données, pp. 1083-1086. Agrawal, S., S. Chaudhuri et G. Das (2002). Dbxplorer: Système A pour la recherche par mots-clés sur les bases de données relationnelles. En CIED, p. 5-16. Bergamaschi, S., E. Domnori, F. Guerra, R. Trillo Lado et Y. Velegrakis (2011). Recherche par mots clés sur les bases de données relationnelles: Une approche de métadonnées. Dans Actes du 2011 SIGMOD Conférence internationale sur la gestion des données, p. 565-576. Bergamaschi, S., F. Guerra, S. Rota et Y. Velegrakis (2011). Une approche de modèle de Markov caché à la recherche par mots-clés sur les bases de données relationnelles. Dans la modélisation conceptuelle ER 2011, pp. 411-420. Bergamaschi, S., F. Guerra, et G. Simonini (2014). Recherche par mots clés sur les bases de données relationnelles: Sues, iS approches et défis ouverts. Dans un pont entre la recherche et l'information des bases de données, pp. 54-73. Chbeir, R., Y. Luo, J. Tekli, K. Yetongnon, C. R. Ibanez, A. J. Traina, C. Traina Jr, et M. Al Assad (2014). Semindex: index inversé sémantique-courant. En progrès dans les bases de données et systèmes d'information, pp. 290-307. Hristidis, V. et Y. Papakonstantinou (2002). Découvrez: les mots-clés de recherche dans les bases de données relationnelles. Dans Actes de la 28ème conférence internationale sur les très grandes bases de données, pp. 670-681. Kacholia, V., S. Pandit, S. Chakrabarti, S. Sudarshan, R. Desai et H. Karambelkar (2005). l'expansion bidirec- tionnel pour la recherche par mot clé sur les bases de données graphiques. Dans Actes de la 31ème Conférence internationale sur des bases de données très volumineux, pp. 505-516. Munkres, J. (1957). Les algorithmes pour les missions et les problèmes de transport. Journal de la Société de mathématiques appliquées et industrielles (1), 32-38. Tata, S. et G. M. Lohman (2008). Sqak: faire plus avec des mots-clés. Dans Actes de la conférence internationale SIGMOD 2008 sur la gestion des données, pp. 889-902. Wang, H., K. Zhang, Q. Liu, T. Tran et Y. Yu (2008). Q2semantic: Une légère interface de mot-clé pour la recherche sémantique. Dans le Web sémantique: Recherche et Applications, pp 584-598.. Résumé recemment, la recherche par mots-clés les bases de Dans Données un relationnelles un in- Teret suscité Grandissant en raison de sa facilité d'utilisation. Bien Que des Recherches Approfondies EFFECTUEES Dans fussent dernièrement CE Contexte, la Plupart de bureaux non Recherches un only nécessitent aux Accès préalable Données, Ce Qui RESTREINT si their this état Applicabilité Ne est pas vérifiée, Mais aussi des Réponses très renvoient Génériques. Cependant , Provide aux Personnalisées Utilisateurs des became Réponses Plus que jamais en raison de Nécessaire la de surabondance Données-qui-peut l'déranger Utilisateur. Le Défi de RETOURNER des Personnalisées et Réponses pertinentes Qui font les needs satisfaisant des Utilisateurs demeure. Par l'application de Inspiré l'de la technique réussie de les Dans filtrage collaboratif Systèmes de recommendation, nous proposons Une nouvelle sur les approach basée Mots-Clés pour provide aux Personnalisés Utilisateurs des Basés sur Résultats l'hypothèse Que l'information sur juin only le schéma de la Base de Données est disponible. - 352 -"
58,Revue des Nouvelles Technologies de l'Information,EGC,2018,Reframing for Non-Linear Dataset Shift,"Les modèles de classification discriminante supposent que les données de formation et dedéploiement ont les mêmes distributions d'attributs de données. Ces modèles donnent des performancestrès variées lorsqu'ils sont déployés dans des conditions variées avec différentesdistributions de données. Ce phénomène est appelé Dataset Shift. Dans cet article, nous avonsfourni une méthode qui détermine d'abord s'il y a un changement significatif dans les distributionsd'attributs entre les ensembles de données d'apprentissage et de déploiement. S'ilexiste un changement dans les données, la méthode proposée utilise ensuite une approche deHill climbing pour cartographier ce décalage, quelle que soit sa nature, c'est-à-dire (linéaireou non linéaire) à l'équation pour la transformation quadratique. Les résultats expérimentauxsur trois jeux de données réels montrent de forts gains de performance obtenus par la méthodeproposée par rapport aux méthodes précédemment établies telles que le reconditionnement etle recadrage linéaire.","Md Shadman Rafid, Mohammad Mazedul Islam, Md Naimul Hoque, Chowdhury Farhan Ahmed",http://editions-rnti.fr/render_pdf.php?p1&p=1002375,http://editions-rnti.fr/render_pdf.php?p=1002375,en,"Recadrer pour Dataset non-linéaire Maj Md Shadman Rafid, Mohammad Mazedul Islam, Md Naimul Hoque, Chowdhury Farhan Ahmed Département du CST, Université de Dhaka, au Bangladesh shadmanrafiddeep@gmail.com mazidmailbox@gmail.com naimul.et@easternuni.edu.bd farhan@du.ac.bd Résumé. modèles de classification discriminants présupposant que les données de formation et de déploiement ont les mêmes distributions d'attributs de données. Ces modèles donnent des performances significativement varié quand ils sont déployés dans diverses circonstances des distributions de données différentes. Ce phénomène est appelé Dataset Shift. Dans cet article, nous avons fourni une méthode qui détermine d'abord s'il y a un changement important dans les distributions d'attributs entre les ensembles de données de formation et de déploiement. S'il existe un changement dans les données de la méthode proposée utilise alors une approche d'escalade Hill pour cartographier ce changement, quelle que soit sa nature à savoir (linéaire ou non linéaire) à l'équation de transformation quadratique. Résultats tal sur trois ensembles de données expéri- réelles montrent des gains importants de performance obtenus par la méthode proposée par rapport aux méthodes précédemment établies telles que le recyclage et recadrée linéaire. 1 Introduction Les principales préoccupations de l'apprentissage machine supervisé est d'apprendre un modèle pour la classification, la régression ou de toute autre fonction à l'aide d'un ensemble de données de formation et l'application de ce nouveau modèle appris aux données de déploiement. Lors du déploiement d'un modèle de données particulier, il est implicitement supposé que la formation et à la fois des données d'essai suivront les mêmes distributions. Mais dans de scénarios réels de la vie, il est naturel pour les distributions des attributs de données et les fonctions de décision de changer en particulier lorsque les données de formation sont recueillies dans un contexte alors que les données de déploiement est utilisé dans un contexte de dif- férents par exemple (les données de formation sont recueillies l'été saison tandis que le modèle est déployé sur les données pour la saison de l'automne). Un tel ""Dataset Shift"" [Han et al. (2012)] sinon dédommagés pour peut réduire considérablement l'efficacité des résultats fournis par le modèle appris. Une solution consiste à recycler l'ensemble du modèle sur les données de déploiement. Mais ce n'est pas une option réalisable souvent que la collecte et l'étiquetage des données dans le déploiement peut devenir coûteux. Une autre approche récente est Redéfinir les données de manière à rendre le modèle appris compenser le décalage des données de déploiement en temps réel et fournir des résultats efficaces. Nous nous concentrons principalement sur les changements des attributs continus des données de tests à déploiement ensemble de données. Par exemple, la consommation alimentaire quotidienne des habitants d'une ville en Amérique du Nord peut varier considérablement de celle des habitants d'une ville similaire en Amérique du Sud. - 131 - Recadrer pour non-linéaire Dataset Maj Afin de traiter l'ensemble de données quart plusieurs travaux de recherche [LACHICHE et Flach (2003)], [Charnay et al. (2013)], [Zhao et al. (2011)], [Hernandez-Orallo (2013)] ont été publiés en proposant plusieurs méthodes d'utilisation des méthodes apprises dans différents environnements de déploiement en ajustant les valeurs de sortie. Reframing est un procédé de transformation des valeurs d'entrée au modèle appris en temps réel au cours du déploiement et ainsi compenser le décalage. Considérons un vrai scénario de vie où l'on considère deux villes A et B. On voit que dans la ville A la plupart des gens achètent une voiture quand ils sont au moins 35 ans et avoir un revenu minimum de 5000 $, tandis que dans la ville B la plupart des résidents achètent une voiture une fois qu'ils atteignent l'âge de 40 ans et ont un revenu d'au moins 4500 $. Considérons que nous utilisons les données recueillies de la ville A que les données de formation pour créer le modèle et déployer le modèle créé sur les données de la ville B afin de déterminer les résidents de la ville B sont susceptibles d'acheter des voitures. Comme les données des deux villes ne suivent pas les mêmes distributions d'un simple changement d'échelle de l'âge et les paramètres revenus des données permettra au modèle existant de classer correctement les données de la ville B sans nécessiter de recyclage ou des ajustements de la sortie. Ceci est un exemple très simple de reframi ng qui est nécessaire à chaque fois que la fonction de décision, en cas de déploiement est différente des fonctions de décision en cas de formation. Il est également très efficace lorsque seulement quelques données étiquetées est disponible dans les conditions de déploiement. Ce type d'ensemble de données Maj où les valeurs d'entrée sont décalés est connu comme covariable Shift [Moreno-Torres et al. (2012)], [Shimodaira (2000)]. Covariable Shift est la situation lorsque les données de formation et de test suivent différentes distributions alors que la distribu- tion de probabilité conditionnelle reste le même [Moreno-Torres et al. (2012)]. Il y a aussi d'autres types de Dataset Concept facteurs, comme la dérive où la distribution des données reste la même, mais les changements de la fonction de décision [Moreno-Torres et al. (2012)]. Différents types d'approches [Sugiyama et al. (2007)], [Bickel et al. (2009)], [Gretton et al. (2009)] ont été proposées pour faire face à changement covariable. Mais la plupart de ces approches nécessitent un recyclage remarquable des modèles créés de clas- sification et par conséquent nécessitent également une grande quantité de données étiquetées pour être disponibles dans le cadre du déploiement. Ces approches ne sont pas très applicable dans de véritables scénarios de vie où la disponibilité des données marqué est très faible et très peu de temps est présent pour classer les données et en tant que telles les approches mentionnées ci-dessus ne sont pas appropriés reframing de BOF. Le Reframing avec escalade Stochastique Hill (RSHC) [Ahmed et al. (2014)] approche ne compense pour le décalage covariable temps réel jalonnement des données de déploiement, mais il ne peut compenser les décalages linéaires dans les données et ne peut pas différencier entre les changements linéaires et non-linéaires dans les données. Notre approche proposée peut bien résoudre le problème de recadrant les données d'entrée dans le déploiement ensemble de données pour les linéaires et les changements de jeu de données non linéaires. Notre approche fait cela dans un processus en deux étapes dans lequel il détectez s'il y a un changement dans les données de l'essai à l'ensemble de données de déploiement. S'il existe changement dans les données, elle transforme alors le changement dans les données de l'équation de transformation quadratique qui compense les changements non linéaires dans les données, mais peut également compenser le décalage linéaire dans les données en éliminant la partie non linéaire. Le reste de ce document est organisé de la manière qui suit. Dans la section 2, nous discutons des travaux connexes. Dans la section 3, nous offrons notre approche proposée pour recadrant en cas de changement et ensemble de données dans la section 4 résultats expérimentaux sont fournis pour plusieurs ensembles de données de la vie réelle. Enfin les conclusions sont tirées à la section 5. - 132 - M. S. Rafid et al. 2 Contexte de l'étude Les différents types de méthodes à base de score ont été proposées pour probable- classification gérer SLE dans différents scénarios de déploiement. Par exemple, l'algorithme de classification binaire de [LACHICHE et Flach (2003)] génère des scores d'être négatif positif par rapport et définir un seuil de diviser ces scores pour prédire la frontière entre deux classes. En fonction de l'environnement de déploiement, généralement une matrice de coûts et la classification erronée proba- bilité avant des classes, ce seuil peut être réglé et le modèle est adapté pour la prédiction de la classe. La recherche a également été fait pour gérer ce problème pour la classification multi-classe. 2.1 l'ensemble de données de décalage SHIFT du jeu de données est le phénomène lorsque les éléments de données dans le déploiement (test) jeu de données subissent un changement dans la distribution d'un attribut unique ou plusieurs attributs ou un changement dans les limites définissant la classe résultant dans les données de déploiement présentant un comportement différent à celui de l'ensemble de données de formation. Formation Dataset: L'ensemble de données de formation est l'ensemble des éléments de données avec des étiquettes de classe bien définies et les valeurs d'attributs dont le modèle de classification qui est en cours de construction peut facilement apprendre la gamme des différents attributs pour chaque limite de classe. Déploiement Dataset: Le jeu de données de déploiement est le jeu de données où les éléments de données ont aucune étiquette de classe, mais ont les mêmes attributs que les éléments de données de l'ensemble de données de formation. Le modèle de classification est utilisé sur cet ensemble de données pour faire des prédictions des classes des éléments de données. Le modèle de classification des études les valeurs d'attributs de chaque élément de données et utilise les connaissances acquises suite à l'ensemble des données de formation pour faire des prédictions sur la classe des éléments de données. Ces contextes sont souvent différents d'une manière non négligeable. Par exemple, un modèle peut être construit à partir des données recueillies de formation dans un certain laps de temps et dans un pays donné, et déployé des données dans un temps futur et / ou dans un pays différent. 2.1.1 Les types de jeux de données Shift - covariable vitesse: décalage covariables [Shimodaira (2000)] se réfère à des changements dans la distribu- tion des variables d'entrée-à-dire des changements dans les attributs des éléments de données d'entrée. Covariable changement est le type le plus étudié du jeu de données quart de travail. Il est également connu sous le nom ""Population Drift"" [Hand (2006)], [Kelly et Adams (1999)]. - Avant Probabilité Shift: Les modifications apportées aux distributions de la variable de classe à savoir la distribution des classes [Webb et Ting (2005)] est connu comme préalable Probabilité Shift. - Concept Shift: Il est surtout connu comme ""Concept Drift"". Il est également défini comme « Les modifications apportées aux définitions des classes » [Hand (2006)]. Dans cet article, nous allons surtout mettre l'accent sur la détection des Covariable Shift et la reframing des variables d'entrée dans le contexte du déploiement en cas de Covariable Shift. 2.2 Polyvalent Arbre de décision La décision Versatile Arbre (VD) algorithme T propose différentes approches de la décision de construire des arbres en présence de changements d'observation covariables [Al-Otaibi et al. (2015)] à savoir cet algorithme est uniquement applicable dans les cas où les changements se produisent covariables dans le déploiement - 133 - Recadrage pour les données Dataset non linéaire Shift. Cet algorithme fait deux contributions majeures. La première contribution est qu'elle propose une approche nouvelle et imprévue de dts construire en utilisant des centiles. FIGUE. 1 - Deux types de modèles; la figure du haut est le modèle en utilisant un seuil fixe tandis que le chiffre inférieur est le modèle en utilisant percentiles. Pour chaque contexte de déploiement, l'arbre de décision est déployée de manière à ce que les instances de déploiement sont divisés sur les feuilles dans les mêmes quantités percentile de 63 pour cent et 37 pour cent. L'idée principale proposée dans cet algorithme est d'apprendre un DT classique, puis de remplacer les seuils de décision internes avec centiles qui peut alors faire face aux changements monotones dans les données de déploiement. La deuxième contribution de cet algorithme est qu'il propose une manière plus géné- ral Modèle polyvalent (VM) qui déploie des stratégies différentes (y compris les percentiles) de mettre à jour le seuil DT pour chaque ie contexte de déploiement pour chaque autre jeu de données de déploiement conformé- ing aux changements observés dans les données. Les changements sont identifiés par l'application d'un test statistique non paramétrique. 2,3 Reframing avec Colline Stochastic escalade de la Reframing avec l'algorithme d'escalade Stochastic Hill (RSHC) les cartes de tout changement dans les instances de données à partir de la formation de l'ensemble de données de déploiement de l'équation de transformation linéaire: y = aX + β En cartographiant le déplacement des données vers linéaire l'algorithme de transformation RSHC essaie de compenser le décalage de com- qui se produit dans le cas de déploiement en transformant la répartition des attributs des données de déploiement à celle qui est représentée par les données d'apprentissage. - 134 - M. S. Rafid et al. Cet algorithme présente le concept recadrant pour lutter contre le changement pour l'entrée en continu des attributs des éléments de données dans l'ensemble de données de déploiement et propose un processus simple et efficace de l'apprentissage des valeurs des paramètres optimaux de l'attributs de classification décalés. Les attributs d'entrée des éléments de données de déploiement seront transformées par ces valeurs de paramètres de transformation linéaire et sera appliqué au modèle de classification construit avec la base de données de formation pour fournir des résultats précis à savoir la liste des éléments de données sont correctement classés de l'ensemble de données de déploiement [Ahmed et al. (2014)]. 3 Notre approche proposée 3.1 RNLDS: Notre approche proposée Nous proposons une nouvelle Alg orithm Reframing pour l'ensemble de données non linéaire Maj (RNLDS) qui permet de détecter changement dans les données et la carte sera choisie à une transformation non linéaire. Notre algorithme proposé est une amélioration de la Reframing avec Stochastique Climbing Hill (RSHC) algorithme proposé dans [Ahmed et al. (2014)]. L'algorithme RSHC une approche d'escalade la colline afin de déterminer les valeurs les plus appropriées des paramètres de l'équation de transformation linéaire. Dans notre algorithme, nous avons utilisé l'approche d'escalade la colline afin de déterminer les valeurs les plus appropriées des paramètres de l'équation de transformation non linéaire que nous utilisons. Dans notre algorithme proposé, nous avons utilisé l'équation quadratique carte changement non linéaire dans les données de déploiement à dire que nous avons utilisé l'équation: y = αx2 + ¬x + γ Mais dans le déploiement de la vie réelle l'une des équations de transformations non linéaires peuvent être utilisés à la carte décalage ensemble de données. En cas d'utilisation d'une équation différente du nombre de la colline pas d'escalade dans l'algorithme doivent être changé au nombre de valeurs de paramètres de l'équation. 3.2 Exemple de scénario Supposons que nous avons 3 ensembles de données. Chacune d'entre elles contient les données sur les élèves d'une école à savoir les trois ensembles de données contient des informations sur les élèves de trois écoles différentes. Nous partons du principe que chacune de ces écoles restent séparées les unes des autres i.e. ils sont à différents endroits géographiques et les informations sur les élèves de chacune de ces écoles ont été recueillies à différents intervalles de temps. Laissez les ensembles de données contiennent 3 types de données sur chaque élève de chacune des écoles, leur présence, les résultats des tests pour un certain terme et leur performance parascolaire exprimées en valeurs numériques entre 1 et 100. Soit nous devons classer les élèves en quatre non les classes numériques de: Bad, modérée, bonne et excellente en fonction de leurs scores. Laissez les trois ensembles de données sont les suivantes. L'information des élèves des élèves de l'école 1 reste marquée avec les étiquettes de classe de la classe à laquelle appartient chaque élève en fonction de leurs scores aux tests. Les informations suivantes sur les élèves de l'école 2 et l'école 3 restent également marquées, mais il appartiendra au classificateur pour déterminer leurs classes et vérifier avec la classification initiale. Les ensembles de données suivants ont moins d'informations que le premier. - 135 - Recadrer pour Dataset non linéaire Décalage Std. ID Présence d'essai Note Extracurricular Per- formance Étiquette Classe 1 50 23 80 Bad 2 62 30 50 Bonne 3 57 27 90 Modéré 4 67 40 67 Bon 5 70 48 75 Excellente 6 66 56 70 Bonne 7 20 13 33 Bad 8 70 60 87 Bon 9 80 77 40 10 60 39 Excellente Bonne 72 11 50 42 60 Moyenne 12 72 63 85 13 47 29 Excellente 63 Bad 14 70 37 50 15 75 47 Excellente Excellente 92 TAB. 1 - la performance des élèves d'un terme particulier à l'école 1. Std ID de présence Score parascolaire test formance Classe Étiquette Per- 1 90 88 22 Excellent 2 75 67 80 62 52 Bonne 3 50 4 77 59 Modérée 46 Modérée 5 57 38 88 60 6 Bad 69 62 7 83 72 Modéré Bon 65 8 97 92 45 9 Excellente 65 52 75 10 40 32 Bad Bad 69 TAB. 2 - la performance des élèves d'un terme particulier à l'école 2. On voit que si il reste plus ou moins le même pourcentage d'étudiants mauvais, modérée, bonne et d'excellentes performances académiques, leurs performances ne sont pas au même niveau. Donc, il y a un changement des données du 2e et 3e ensembles de données à partir du 1er jeu de données. Maintenant, si nous essayons de cartographier le changement des données du 2e et 3e ensembles de données à l'équation de transformation quadratique par exemple y = αx2 + ¬x + γ on voit que les valeurs des paramètres de l'équation-à-dire α = 0,015, β = 0,1 et γ = 7, nous pouvons compenser le décalage dans les données du 2e set et faire suivre la même distribution que les données contenues dans le 1er jeu de données. - 136 - M. S. Rafid et al. Std ID de test de présence Score Extracurricular Per- formance Classe Étiquette 1 45 37 62 Bad 2 63 62 50 Bonne 3 67 70 33 Excellent 4 40 23 80 Bad 5 70 47 25 Modéré 6 23 39 90 Bad 7 90 77 75 Excellente 8 75 52 52 modérée 9 31 21 77 Bad 10 80 59 47 Bonne TAB. 3 - Le rendement des élèves d'un terme particulier à l'école 3. Fig. 2 - fonction de la décision de trois écoles différentes. Bien que le calcul des valeurs de paramètres précis pour l'équation quadratique pour compenser le décalage dans les jeux de données 2 et 3 du jeu de données 1, nous utilisons le dessus de l'algorithme donné. La nation détermination des valeurs des paramètres est une tâche très complexe que le changement pour les différentes tuples de données peut être différent et nous devons déterminer une valeur unique pour chaque paramètre de l'équation quadratique qui donne la meilleure précision résultat par exemple possible tout en compensant pour le déplacement des données lors de la classification des données de l'ensemble de données de déploiement à l'aide du modèle de classification. Nous utilisons l'algorithme RNLDS donné à la figure 3 pour effectuer la tâche complexe de détermi- les valeurs ing paramètres de l'équation quadratique pour compenser le changement de jeu de données dans les données de la vie réelle de type. - 137 - Reframing pour l'ensemble de données non linéaire Maj 1: Entrée: C, des moyens de modèle de base; Peu de données de déploiement Td, les données de test TTST; La précision de réglage p> 0 2: Débit: Précision du classificateur sur datadset de test. 3: 4: param ← [1, 1, 0] 5: pour i ← 0 à 3 do 6: param ← chooseAlphaBetaGama (param, i, p) 7: fin de 8: 9: Classifier TTST en appliquant les nouveaux paramètres de décalage ( param) et la précision de retour. 10: 11: procédure CHOOSEALPHABETAGAMMA (param, index, p) 12: pour i ← 1 ou 2 font 13: paramt ← param 14: while true do 15: newAccr ← classifierAccr (Td, paramt) 16: si shouldContinue (newAccr) = vrai, alors 17: paramt [index] ← paramt [index] + p [i]; 18: autre 19: pause; 20: end if 21: fin tandis que 22: saveParam [i] ← paramt 23: saveAccr [i] ← newAccr 24: fin de 25: param ← saveParam [(saveAccr [0] ≤ saveAccr [1] 0: 1)] )) 26: Retour 27 param: procédure de fin figure. 3 - Le choix α, β et γ en utilisant Climbing Hill 4 Résultats expérimentaux Nous avons effectué plusieurs expériences sur des ensembles de données de synthèse et de la vie réelle pour montrer l'FICIENCY et l'efficacité de EF- notre approach.In cette section, nous allons comparer la précision de la classification des données fournies par notre approche tout en classant les données d'un ensemble de données particulier contre la précision de la classification fournie par les autres approches tout en classant les données du même ensemble de données que précédemment. Nous fournirons la comparaison graphique des exactitudes des différentes approches tout en classant les données d'un ensemble de données particulier pour faciliter la comparaison. Dans notre approche, nous avons utilisé le classificateur bayésien Naive pour classer l'ensemble des données que nous voulons utiliser dans les données de déploiement. Nous avons comparé nos résultats avec les résultats fournis par le classificateur de base, approche de recyclage et l'approche linéaire à savoir Reframing avec Stochastique Climbing Hill. - 138 - M. S. Rafid et al. 4.1 Dataset 1: Maladie rénale chronique Nous au premier terme de notre algorithme sur l'ensemble de données de la maladie rénale chronique disponible dans l'apprentissage automatique UCI référentiel de données. Cet ensemble de données contient l'ensemble des informations des patients qui ont été admis à l'hôpital Apollo dans Karaikudi, Tamil Nadu sur une période de deux mois. Il peut être utilisé pour prédire quel groupe de personnes dans une région peut être affectée par une maladie rénale chronique à une certaine période de time.We diviser les données en utilisant la valeur d'âge à savoir nous avons séparé les patients de différents groupes d'âge dans différents jeux de données. Parmi les différents groupes d'âge les données des patients appartenant au groupe d'âge de 0-30 est considéré comme les données de formation et les données des patients appartenant au groupe d'âge de l'âge de 70 ans et plus est considéré comme les données de test. FIGUE. 4 - Courbe d'apprentissage pour Reframing pour la transformation non linéaire, pour la transformation linéaire Reframing, modèle de base et Rééducation. 4.2 Dataset 2: Recensement de revenu Dans notre deuxième expérience, nous courons notre algorithme sur l'ensemble de données sur le revenu de recensement disponible dans l'apprentissage automatique UCI référentiel de données. Cet ensemble de données contient les informations de recensement d'un grand nombre de personnes de plusieurs pays. Il contient des informations sur leur sexe, la famille, l'éducation, la profession, etc. Nous avons divisé la DataSet en fonction du pays à savoir que nous considérons l'information des personnes de E.U.A. comme l'ensemble de données de formation et l'information sur le peuple de la Jamaïque comme l'ensemble de données de déploiement. Maintenant, le modèle de classification construit en utilisant les données de formation peuvent prévoir en fonction de leur information si une personne vivant en Jamaïque gagne un salaire inférieur ou plus de 50 000. - 139 - Recadrage pour Dataset non linéaire Maj Fig. 5 - Courbe d'apprentissage pour Reframing pour la transformation non linéaire, pour la transformation linéaire Reframing, modèle de base et Rééducation. 4.3 Dataset 3: ILP (patient de foie indien) Nous courons maintenant notre algorithme sur le ILP (patient indien du foie) ensemble de données disponibles dans l'apprentissage automatique UCI référentiel de données. Cet ensemble de données contient l'ensemble des informations d'une collec- tion de personnes dont beaucoup ont été touchés par des maladies du foie et d'autres non. Il a été recueilli à partir du nord-est de l'Andhra Pradesh en Inde. Il peut être utilisé pour prédire quel groupe de personnes dans une région peut être affectée par une maladie rénale à une certaine période de temps. Nous avons divisé les données selon le sexe du peuple à savoir nous avons séparé les gens en deux groupes différents hommes et les femmes. L'ensemble de données des mâles est considéré comme l'ensemble des données de formation alors que celle des femmes est considéré comme l'ensemble de données de test. Les résultats expérimentaux dans ces ensembles de données réelles démontrent que notre approche est tout à fait capable d'apprendre les valeurs des paramètres de décalage optimal pour l'équation de transformation quadratique en utilisant des données presque pas marquées au déploiement dans un environnement réel où la nature d'un changement est inconnu de source déploiement. Ces résultats révèlent également l'applicabilité de changement non linéaire dans les domaines de la vie réelle en évidence sa force ex- pression pour faire face à ces changements de jeu de données inconnues entre une formation et des contextes différents de déploiement. 5 Conclusion Nous avons prouvé nos recherches que les changements non linéaires se produisent dans la vie réelle datasets.In cet article, nous avons proposé une nouvelle approche de recadrant les valeurs des attributs d'entrée en continu. De plus, nous avons conçu un algorithme efficace pour apprendre les valeurs des paramètres optimaux pour les attributs d'entrée décalé en continu en cas de classification des données non marqué. Notre algorithme a la capacité d'adaptation à différents types de changements aux distributions des attributs de données et rendant ainsi le modèle existant utilisable dans différents environnements de déploiement. Même sans données étiquetées disponibles dans le scénario de déploiement, il peut fournir le paramètre optimal requis - 140 - M. S. Rafid et al. FIGUE. 6 - Courbe d'apprentissage pour Reframing pour la transformation non linéaire, pour la transformation linéaire Reframing, modèle de base et Rééducation. valeurs afin de l'adapter à l'évolution du jeu de données réelles. Avec des résultats expérimentaux étendus nous avons montré que notre approche est meilleure que l'approche actuelle en utilisant uniquement recadrant décalage linéaire des données attribue quant à la précision. Dans le pire des cas, les résultats de notre approche est équivalente aux résultats fournis par l'approche pour reframing déplacement linéaire qui est possible uniquement si l'ensemble de données n'a qu'un décalage linéaire dans le scénario de déploiement. De plus, il est tout à fait adapté pour les environnements où le recyclage est pas applicable à apprendre les fonctions de décision. Enfin, nous avons présenté l'existence d'ensemble de données changement de trois ensembles de données réelles en tenant compte des différences d'âge, zone géographique et le sexe respectivement comme la différence entre les jeux de données. Nous avons démontré la capacité de notre approche de rapprocher ces changements ensemble de données réelles inconnues avec précision. Références Ahmed, C. H., N. LACHICHE, C. Charnay et A. Braud (2014). Reframing attributs d'entrée en continu. IEEE ICTAI, 31-38. Al-Otaibi, R., R. B. Prudencio, M. Kull et P. Flach (2015). Polyvalent arbres de décision pour l'apprentissage sur plusieurs contextes. Apprentissage et découverte des connaissances dans les bases de données, 184-199. Bickel, S., M. Bruckner, et T. Scheffer (2009). l'apprentissage discriminante sous COVAR iate décalage. Journal of Machine Learning Research 10, 2137-2155. Charnay, C., N. LACHICHE et A. Braud (2013). Optimisation des classificateurs par paires bayésiens pour l'apprentissage des coûts sensibles multi-classe. IEEE ICTAI, 499-505. Gretton, A., A. Smola, J. Huang, M. Schmittfull, K. Borgwardt et B. Scholkopf (2009). changement Covariable par correspondance moyenne noyau. Dataset Changement dans l'apprentissage machine, 131-160. - 141 - Recadrer pour non-linéaire Dataset Maj Han, J., M. Kamper, et J. Pei (2012). Les données des concepts et techniques d'exploration. 225 Wyman Street, Waltham, MA 02451, USA: Morgan Kaufmann Publishers. Main, D. J. (2006). la technologie classificateur et l'illusion du progrès. Statistical Science 21 (1), 30-34. Hernandez-Orallo, J. (2013). Les courbes ROC pour la régression. Reconnaissance 46 (12), 3395- 3411. Kelly, M. G., D. H. et N. Adams (1999). L'impact des populations sur l'évolution des formance per- classificateur. Compte rendu de la cinquième SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données 58 (1), 367-371. LACHICHE, N. et P. A. Flach (2003). Amélioration de la précision et le coût des classificateurs probabilistes deux classes et multi-classe en utilisant des courbes ROC. ICML, 416-423. Moreno-Torres, J. G., T. Raeder, R. Alaiz-Rodriguez, N. V. Chawla et F. Herrera (2012). Une vue d'unifier le jeu de données changement de classification. Reconnaissance 45, 521-530. Shimodaira (2000). L'amélioration de l'inférence prédictive sous décalage covariable par la pondération de la fonction de vraisemblance logarithmique. Journal de la planification statistique et Inference 90 (2), 227-244. Sugiyama, M., M. Krauledat et K.-R. Muller (2007). Covariable adaptation de changement de vitesse par impor- tance validation croisée pondérée. Journal of Research Machine Learning 8, 985-1005. Webb, G. et K. Ting (2005). Sur l'application de l'analyse pour prédire la performance Roc de classification dans différentes distributions de classe. Machine Learning 58 (1), 25-32. Zhao, H., A. P. Sinha et G. Bansal (2011). Une méthode de réglage étendue de la régression sensible aux coûts et à la prévision. Decision Support Systems 51 (3), 372-383. Les CV de classification discriminante Modèles Que les supposent de formation et Données de deployment les Ontario distributions d'attributes Mêmes de Données. CÉS des per- Modèles perfor- donnent très VARIÉES Dans lorsqu'ils des are conditions déployés VARIÉES de distributions Avec Différentes Données. Ce phénomène is Dataset Maj Appelé. Dans this article, nous Avons Une méthode Qui Fourni déterminons d'y ABORD un non Se il changement Dans les dis- significatif tributions d'attributes Entre les ensembles de Données d'apprentissage et de deployment. EXISTE un changement Se il les Dans Données, la méthode proposed uTiliseRa ensuite de juin approach colline escalade POUR CE décalage cartographier, Quelle nature sa Que Soit, c'est-à-dire (non linéaire OU linéaire) à l'équation Pour la transformation quadratique. Les Résultats Sur expérimentaux de Donnees Trois Jeux Bobines de forts gains montrent de la performance par la méthode obtenus par rapport aux proposed methods précédemment le Qué Telles établies et le Reconditionnement recadrage linéaire. - 142 -"
63,Revue des Nouvelles Technologies de l'Information,EGC,2018,Temporal hints in the cultural heritage discourse: what can an ontology of time as it is worded reveal?,"Dans le champ des sciences patrimoniales, la dimension temporelle de l'information joueun rôle à l'évidence majeur tant pour l'interpréter et l'analyser que pour relier des faits isolés. Mais la façon dont cette dimension est verbalisée pose des problèmes de formalisation non triviaux. Pourtant, cette verbalisation, que l'on associe souvent au terme-chapeau d'incertitude, peut être lue en dissociant d'une part le caractère mal connu d'un fait documenté, irréductible, et les choix faits par le producteur de l'information pour la relativiser. Dans cette contribution nous proposons un modèle formel permettant d'observer et d'analyser de façon systématique cette couche de verbalisation. L'expérience est menée sur des données fortement hétérogènes, souvent d'origine citoyenne, documentant le petit patrimoine matériel et immatériel. Ce cas d'étude est donc limité, mais il apparait néanmoins comme portant une question de fond allant au-delà du cas d'espèce. La contribution détaille d'abord la grille d'analyse d'indices temporels proposée, puis relate l'expérimentation concrète associée (ontologie OWL). Il n'est pas fait état d'une quelconque prétention à un résultat généralisable stricto sensu, mais cette expérience peut contribuer à nourrir de façcon pragmatique un débat nécessaire sur la formalisation d'indices temporels dans les sciences historiques.","Gamze Saygi, Jean-Yves Blaise, Iwona Dudek",http://editions-rnti.fr/render_pdf.php?p1&p=1002370,http://editions-rnti.fr/render_pdf.php?p=1002370,en,"Conseils temporels dans le discours du patrimoine culturel: ce qui peut une ontologie de temps telle qu'elle est formulée? révéleront Gamze Saygi, Jean-Yves Blaise, Iwona Dudek UMR CNRS / MCC 3495 Modeles ET simulations pour l'Architecture et le Patrimoine Campus CNRS Joseph Aiguier - Bat. Z », 31 chemin Joseph Aiguier 13402, Marseille Cedex 20, France. (Gamze.saygi, jean-yves.blaise, iwona.dudek) @ map.cnrs.fr http://map.cnrs.fr Résumé. Le temps est un élément indispensable de l'information CH: implement- modèles de connaissances ING appropriés portent une importance cruciale afin de fournir une meilleure compréhension de l'évolution des éléments du patrimoine, à Concurrences exposons, et de peser les facteurs de qualité. Il est une tâche difficile mais en raison des caractéristiques incertaines de données temporelles, et le libellé du temps dans le discours CH. modèles KR existants sont soit pas conçus pour ces particularités, aspects spatiaux ou ont tendance à éclipser la dimension temporelle. Cette recherche vise à proposer déchiffrage et une représentation formelle de la façon dont des notes temporelles sont formalisées dans les récits historiques. Une ontologie OWL est introduite qui fournit un mécanisme de support de coeur pour permettre une représentation sémantique des états temporels, et pour l'analyse structurelle. L'objectif est de faciliter le contre-interrogatoire des indices temporels dans et à travers les collections CH afin que les spécialistes peuvent avoir des possibilités étendues de lecture d'information sur le patrimoine. 1 Introduction Comme l'a souligné Jurisica et al. (2004), avec de plus en plus des morceaux lisibles par ordinateur d'information, les analystes ont besoin aujourd'hui de repenser leurs stratégies d'extraction de connaissances. Gies Ontolo- offrent des capacités importantes pour la gestion des connaissances, en particulier dans les grands volumes d'information (Davies et al., 2003) en fournissant des vocabulaires contrôlés et uniformes définis comme un ensemble de primitives de représentation (types d'informations, leurs propriétés et relations) cohérentes avec les significations et les contraintes dans un domaine de la connaissance (Gruber, 1993). D'autre part, le temps est une caractéristique qui apparaît dans de nombreuses informations (et Faucher al., 2010), et ontologies de temps peut être source de préoccupation pour les différentes disciplines. Dans cette recherche, nous nous concentrons sur la notion de temps dans le discours du patrimoine culturel (CH): aspects temporels sont là un inséparable et central rôliste pour l'analyse historique, et dans toutes les tâches de raisonnement effectué sur l'évolution, la transformation, la réutilisation, le statut des biens du patrimoine. Mais dans le discours CH événements passés ou faits ancrés dans le temps dans une grande variété de formes (par exemple, [...] il remonte à la deuxième moitié du 13ème ou 14ème siècle, [...] après la Révolution, etc. .). Ces formulations ne rentrent pas dans les systèmes de quantification « classiques » tels que les formats de date dans les systèmes de gestion DB (par exemple, « 19/03/1942 »). - 71 - Conseils temporels dans le discours du patrimoine culturel afin de parvenir à une représentation plus précise de la dimension temporelle des récits historiques, il est important d'analyser avec soin le libellé même des indices temporels. Une seule fois cette analyse a été réalisée et confronté à la réalité des ensembles de données historiques, peut-on espérer construire un modèle formel générique, fournissant des moyens interopérables pour déchiffrer et représenter des structures temporelles. L'idée derrière la recherche est que, comme les analystes de preuves historiques, et avant toute mesure d'interprétation, nous avons d'abord besoin de comprendre et de décrire de manière structurée et partageable la nature des données que nous traitons, en particulier des états temporels. Un tel objectifs de formalisation de raisonnement tâches sur les habitudes de verbalisation: les mettant en corrélation les types de fournisseurs d'information, à des périodes historiques qu'elles concernent, à des zones géographiques ou culturelles particulières, aux auteurs particuliers, etc. L'objectif de fond de cette recherche est de faire une analyse complète de les indications temporelles dans le CH du discours. Pour ce faire, nous introduisons une pratique scénario d'application, qui englobe l'extraction des indices « réel » f rom données « réelles » et une mise en œuvre expérimentale utilisant le OWL (Web Ontology Language) / Suite techno- logique Protégé (Musen, 2015). Une telle ontologie permet non seulement d'atteindre un modèle formel de temps telle qu'elle est formulée, mais Réitère également les spécialistes du patrimoine façon d'extraire différentes interprétations des données qu'ils traitent. Nous faisons cependant aucune réclamation que nous avons mis au point un cadre ontologique générique qui serait adapté aux sciences historiques en général. Nous reconnaissons le fait que temporalités, et la façon dont ils sont formulés, peuvent dépendre des paramètres distinctifs tels que la région, TY logie ou d'une collection. Néanmoins, nous mettons en évidence des tendances particulières du libellé dans le discours CH et se concentrer sur des défis typiques tout en acquérant et en formalisant informa- tions temporelles dans le domaine. Dans la section 2, nous tirons les grandes lignes du contexte de la recherche d'une manière double; d'abord en décrivant la façon dont le concept de temps est traitée dans le contexte général de la recherche, et la seconde en se concentrant sur ce qui a été expérimenté jusqu'à présent dans le contexte spécifique du domaine CH. La section 3 présente notre approche de la représentation du temps car il est rédigé: concepts, notions et leurs interrelations. L'article 4 traite de la mise en œuvre technique et son évaluation expérimentale. Dans la dernière section, nous listons les résultats et les lacunes à ce stade de la recherche, et quelques enseignements tirés de l'expérience. 2 L'état de l'art des normes utiles, des définitions, des spécifications et des recommandations existent déjà visant à diverses préoccupations et les étapes de traitement de l'information temporelle. Le SIS de données axées sur le temps est une préoccupation au sein de nombreuses communautés de recherche, comme la communauté de temps qui traite des théories temporelles, logiques, langages de représentation, le raisonnement et ontologies (Ermolayev et al., 2014). ISO 8601 décrit une méthode normalisée de dates et heures présentant, alors que l'ISO 19108 établit les normes de technologie de l'information pour l'information temporelle interchanger des. ISO TimeML vise une préoccupation cruciale; tâches Traitement du langage naturel (NLP) pour la création d'expressions temporelles contrôlées de texte non structuré. Il ne prend pas seulement en compte les quantités, mais aussi les opérateurs sémantiques pertinents. OWL-temps, ce qui est une recommandation du W3C candidat, vise à fournir un vocabulaire pour exprimer des faits sur les relations entre topologiques et instants intervalles de temps. Il a été récemment étendu (Cox, 2016) pour soutenir le codage des systèmes de référence temporels autres que le calendrier grégorien. Il y a un intérêt croissant sur l'extension de concept de temps régulier à une perspective plus large non absolue, - 72 - G. Saygi et al. à savoir, la recherche traitant des ensembles de données sales. Par exemple, Tao et al. (2010) développer une ontologie appelée Cntro pour représenter l'information temporelle dans les récits cliniques comme RDF (Resource Description Framework) triplets soutien des requêtes orientées de temps dans le web sémantique. Anagnos- Topoulos et al. (2013) attirer l'attention sur la fréquence des expressions qualitatives dans les expressions temporelles (par exemple, avant, après), et de développer un raisonneur nommé CHRONOS pour découvrir les relations temporelles. Or et Shaw (2016) faciliter la tâche de relier entre les ensembles de données temporelles fines périodes de- différemment. Poveda-Villalón et al. (2014) mettent en évidence l'importance d'intégrer des événements récurrents, alors que Diallo et al. (2015) considèrent différentes granularités en ad- dition à récurrences. Faucher et al. (2010) expérimenter un pipeline de bas en haut, à savoir, pour l'acquisition de connaissances temporelles à partir de textes afin de peupler un modèle calculable contraint. Néanmoins, l'incertitude, l'imprécision et l'imprécision dans le libellé des indices temporels (par exemple, fin des années 1980, la fin de Novembre) restent difficiles à représenter officiellement. Par ailleurs, aucune de ces recherches mentionnées travaux prennent encore en compte les notations contradictoires ou des formulations qui peuvent alter- native être combinés, même dans une seule source (par exemple, au 14ème siècle, probablement autour de 1380), bien que ces façons de dire sont communs dans e discourir e CH. Dans le domaine CH, le CIDOC CRM, également connu sous le nom ISO 21127, est un modèle ontologique de base visant à créer la colle sémantique entre les différentes sources d'information, telles que celles publiées par les musées, les bibliothèques et les archives. Dans le modèle CRM spatialité et temporalités vont de pair. Notamment aux expérimentations basées CRM, certains chercheurs ont représenté tem- poral sur des périodes de 4 volumes dimensions (Papadakis et al., 2014), étant associé à spatio-temporel (Hiebel et al., 2016). limites approximatives, définis et indéfinis de périodes sont considérées. Bien, ils affirment l'approche pourrait être intégrée dans un système d'information tels que les SIG, il n'y a pas de cas de test, mais solide. De plus, Papadakis et al. (2014) soulignent que leur modèle ne permet pas représentatives des périodes qui retraite au même endroit plusieurs fois (à savoir, des événements récurrents) ou se produire à des endroits disjoints (par exemple, des événements festifs). Ceci est un problème tout à fait critique dans CH, compte tenu par exemple des ensembles de transformations / ajouts / extensions de mise en forme de durée de vie d'un artefact du patrimoine. Ils mettent l'accent sur la modélisation de la réalité en utilisant des preuves que du matériel sur les périodes passées ou des événements tirés de l'observation des traces. Cependant, bien que le temps peut laisser des traces physiques sur les biens du patrimoine matériel ne fait pas toujours, en particulier dans le cas du patrimoine immatériel (par exemple, les pratiques, les traditions, les événements festifs). La liaison (2010) adoptent des entités CRM et propriétés pour vocabulaires contrôlés, et démontrer une méthode temporelle raisonne- ment pour la modélisation des relations temporelles pour les enregistrements archéologiques. Ils utilisent des accords conventionnels pour les subdivisions temporelles. Cela permet d'aligner les enregistrements de données avec des périodes de temps connues et représentant les limites approximatives inférieures et supérieures des périodes de temps avec des valeurs numériques. Par exemple, ils se séparent des siècles en années de 01-32 si cela est indiqué au début, 33-66 à la mi ou 67-100 pour la fin en se référant aux conseils reçus de l'anglais Her- itage. Comme indiqué à la section 3.1 de notre contribution réutilise ce concept de cartographie classique (accords), mais il étend en termes de granularités et propose un niveau de flexibilité qui permet une cartographie classique spécifique à l'utilisateur choisi par l'utilisateur ou. Kauppinen et al. (2010) soulignent l'imprécision de l'information temporelle dans le CH dis- cours. Ils traitent des limites floues et limites exactes des intervalles de temps, et formalisent chaque intervalle temporel lucratif en limitant les premiers et les derniers début possibles et les dates de fin. Néanmoins, ils ne tiennent pas compte des indications de fin ouvertes comme avant et après les déclarations. En outre, ils analysent uniquement le potentiel de chevauchement des intervalles. Nurminen et Heimburger (2012) discutent la représentation et la récupération de l'information temporelle incertaine - 73 - Conseils temporelles dans les bases de données du musée discours du patrimoine culturel avec un accent particulier sur des intervalles de temps ancrés. Ils soulignent le passage du point centrée (à savoir, structuré autour d'entités physiques) au catalogage centrée sur l'événement (à savoir, en se concentrant sur divers événements) d'objets de musée. Il est particulièrement important lorsque des événements de Sidering tions que les clusters reliant les entités du patrimoine aux actions culturelles des êtres humains et le contexte social. D'une manière générale un grand nombre de travaux de recherche sur la construction de raisonnement temporel sur les concepts introduits par Allen (1983, 1991), et il ne fait aucun doute que ces efforts sont les bienvenus, par exemple, dans le contexte des applications d'intelligence artificielle. Mais il est assez loin entre la façon dont des notes historiques sont effectivement rédigés, et une bonne application des relations d'Allen mal lucratif. Notre recherche unique dans cet écart: les états temporels (les valeurs quantitatives et modificateurs lexicales) doivent d'abord se soumettre à un processus d'extraction afin de découvrir leur struc- ture, et une analyse à grain fin afin que les scientifiques puissent avoir une compréhension critique de la façon dont doutes imprègnent leurs processus de raisonnement. Ce défi majeur dans les sciences historiques a été prise dans la suite poste de données du patrimoine contre-interrogatoire et la visualisation Blaise et al. (2016), mais une enquête approfondie de la façon dont il peut être traité dans le reste du contexte de KR à effectuer. raisonnement temporel efficace, basé sur les relations de Allen ou non, ne sera possible que si elle a été mis en place un modèle formel plète com- des formulations de spécifiques des historiens. Afin de remplir cet aspect, une approche de base pourrait être les traces d'une ontologie temporelle standard (tel que OWL temps) en étendant ou en réutilisant l'ontologie spatio-temporelle CIDOC CRM. Néanmoins, le temps de voie est verbalisée dans le discours historique, il faudrait un mouvement très icant signif- loin des versions originales de ces normes. L'approche présentée dans cet article est de ne pas travailler sur le concept de temps lui-même, ou sur des entités spatio-temporelle, mais d'essayer et d'évaluer le potentiel de valeur ajoutée d'une représentation formelle des états temporels tel qu'il est formulé dans le discours CH. 3 Analyse et représentation des états temporels Dans cette section, nous discutons du modèle formel des états temporels. Pour commencer, nous définissons les notions principales, l'organisation de niveau supérieur et les relations et les détails les concepts liés à temps classification. 3.1 notions principales et de l'organisation générale du modèle formel Un TemporalStatement est une séquence de mots qui nous disent quand quelque chose hap-PENED et / ou combien de temps quelque chose a duré dans une référence temporelle. champ tic seman- d'un TemporalStatement est la notion de temps seul, il ne couvre pas la notion de propriété spatiale / physique qui affecte ou fait référence. Tout TemporalStatement est composé d'un ou plusieurs concepts liés au temps (points, intervalles, etc.), qui peut être accompagné d'un ou plusieurs LexicalOperators (pendant, avant, fin, etc.). Il y a un ensemble d'éléments de base mais importants que nous associons à une déclaration temporelle (figure 1). Tout d'abord, la granularité décrit la mise en correspondance de temps en unités conventionnelles sur décision humaine. Cette cartographie vise essentiellement à traiter avec le temps d'une manière plus facile et peut être spécifiée de multiples façons (en unités plus ou moins) en fonction des besoins de l'analyste (Aigner et al., 2011). Dans un TemporalStatement comme « Les premières dates de cotation dos au 20 Avril 1687 », la granularité temporelle peut être fixé à « un jour ». Mais le jour peut être subdivisé en - 74 - G. Saygi et al. petits segments tels que 24 heures ou 1440 minutes (et, accessoirement, la « normalisation » des dates de temps en 1884, avec la définition d'une heure « moyenne » en réponse à la multiplicité et la variabilité des heures solaires, d'où les goulots d'étranglement d'interprétation graves lors de l'observation telle déclarations aujourd'hui). L'unité non-décomposable à une granularité donné est appelé chronon, un terme inventé par Levi (1927). Par exemple, en Java, la classe de date utilise millisecondes chronon. Bien entendu dans le discours historique, il peut y avoir un décalage entre la granularité intrinsèque d'une déclaration et la temporalité du fait qui est rapporté. Par exemple, dans une déclaration temporelle comme « Un grand incendie a endommagé le bâtiment abondamment à l'hiver 1920 », la granularité de la déclaration est une saison alors que le « grand feu » mentionné dans la déclaration la plus probablement duré quelques jours au plus ou même quelques heures. La notion de UnfoldableTimePoint (voir la section 3.1.3) est une réponse pragmatique à cette préoccupation. 1855-1860 1858-1860 Concepts liés au temps ConventionalMapping (Accord) lexicales Opérateur Nommée timeperiod Quantifié TemporalStatement TemporalStatement fin de 1850 isPartOf isPartOf isTranslatedByUsing IntervalInterval l'utilisateur isUsedforExpressing A isInterpretedBy RefersTo utilisateur RefersTo B composé TemporalStatement figure. 1: Les composants d'un TemporalStatement et leurs interrelations. Une différence est faite entre les systèmes temporels de référence (protocoles calendriers, une partie de OWL-temps) et la notion de périodisation de l'histoire, qui peut être utilisé comme référence dans la formulation de conseils tem- poral, mais ce n'est pas un sy discrétisation stematic de temps (par exemple, gothique apparaît avant Re- dans des styles birth architecturaux européens, néanmoins nous ne pouvons pas préciser les limites exactes temporelles de ces tendances). Cette notion est mise en correspondance dans un concept appelé NamedTimePeriod. Par conséquent, le modèle proposé est organisé comme suit: - Un concept LexicalOperator correspondant aux « modificateurs verbaux » (autour, avant, etc.), - Un concept utilitaire appelé ConventionalMapping utilisé pour interpréter les expressions qualitatives en cas de besoin, et les transformer en quantités exploitables, - les classes qui représentent les concepts liés à la fois présents dans les TemporalStatements. 3.1.1 LexicalOperator lexicales de raffinage dans la mesure qualitative du temps et de l'appui de l'ancrage des états Temporal- sont définis comme LexicalOperators (tableau 1). Ces éléments qualitatifs de l'indice déterminent la « mesure » d'un TemporalStatement. - 75 - conseils dans le temps du discours du patrimoine culturel Type d'utilisation par les exemples quantité Définit ExplicitOperator couvre les mots qui n'ont pas d'impact de la quantité formulée. Montrant en || sur || à partir de délimitation à || entre la fin de chaque fréquence || Définir annuellement ImpreciseOperator agrandit ou prolonge la quantité formulée. Ancrer à un point médian autour || A propos de || vers Bounding à une direction avant || après || depuis RefinementOperator se rétrécit dans la mesure du TemporalStatement en créant une sous-section. Commande Subdivisions début || latebeginning de || la fin du milieu des subdivisions naturelles saisons || marées TAB. 1: Les types de LexicalOperators agissant en tant que composants potentiels de TemporalStatements. 3.1.2 ConventionalMapping Avant toute conversion d'une déclaration temporelle contenant un opérateur lexical en une quantité en mesure Travail, devrait être mis en place une convention ad hoc: il est le rôle joué par ConventionalMapping (accords). La quantification de tout LexicalOperator est soumise à conven- tionalMapping (CM). Par exemple, « la fin du 15ème siècle » peut être traduit en un intervalle de temps quantifié par l'utilisateur choisi comme « 1480-1500 », et les analystes ont besoin de formaliser de manière durable une telle décision. 3.1.3 Les concepts liés à temps concepts Time-connexes sont représentés par trois grandes classes / hiérarchie de classes: NamedTimePeriod, QuantifiedTemporalStatement, CompoundTemporalStatement. Dans cette sec- tion, nous traitons leurs définitions, propriétés et spécifications. NamedTimePeriod. NamedTimePeriods correspondent à TemporalStatements qui fournissent une référence commandée plutôt que d'un système de coordonnées temporelles (par exemple, après le règne de plutôt qu'entre X et Y). La séquence de NamedTimePeriods peuvent se chevaucher les uns les autres. Il peut être lié au cadre de diverses références telles que les mouvements d'art (par exemple, Art Nouveau), les événements politiques (par exemple, 30 guerres an) ou des faits naturels. A NamedTimePeriod, si elle est accompagnée d'une LexicalOperator ou non, implique l'utilisation de CM. Le concept correspond à la notion de périodisation dans les analyses historiques. QuantifiedTemporalStatement. A QuantifiedTemporalStatement est un indice temporel exprimé en nombre (ou avec lexicales universellement acceptés comme une dizaine d'années). Il peut représenter un point temporel, un intervalle ou un UnanchoredDuration. A QuantifiedTemporalStatement a deux propriétés: un système de référence temporel, et un chronon. Bien que la déclaration QuantifiedTemporal- est la partie chiffrée d'un indice, sa valeur doit toujours être interprétée en relation avec tout présent LexicalOperator dans la déclaration, par exemple, « avant 1650 » ne pointe pas à « l'année de 1650 » lui-même, mais une « tranche de temps qui précède 1650 ». - 76 - G. Saygi et al. Timepoint: définition et sous-classes. Au niveau conceptuel un point temporel représente un instant, avec une longueur de zéro. En fonction de la granularité, une timepoint de fait peut avoir une portée dans le temps. On introduit trois concepts de raffinage de la classe timepoint classique (tp) pour traiter les états temporels qui sont verbalisées comme timepoints (tableau 2). FuzzyTemporalStatement (TPF) est un w ording qui fait référence à un événement confiné de manière ambiguë à un point temporel. Le manque de netteté de l'information délivrée par un tel TemporalStatement provient d'un doute quant à l'alignement de la granularité peut-être nécessaire pour analyser l'événement et ce du timepoint. g ≥ g W E g E g W ... TPF (granularité formulée) (granularité de l'événement) Exemple Elle was reconstruite au milieu du 18e siècle. L'édifice a été construit entre 12e et 13e siècles. Extraction [tpf] = milieu du 18ème siècle et [gW] = un siècle ≥ [gE] = un an UnfoldableTemporalStatement (Tpu) est une formulation qui fait référence à un événement exprimé en timepoint. Contrairement au cas de FuzzyTemporalStatement la durée de l'événement est là pour vous plus court que le chronon correspondant au timepoint. ... g ≥ g WE EW TPF (granularité de libellé) (granularité de l'événement) gg Exemple La toiture was emportée par l'avalanche de 1978. Le toit a été emporté par l'avalanche de 1978. Extraction [TPU] = l'avalanche de 1978 et [gW] = année> [gE] = une minute RecurrentTemporalStatement (Cvr) est dédié à TemporalStatements péri- odique / cyclique occurrences. Sa structure est la même que le point de temps, mais avec des descripteurs de fréquence (f). tp R f f f f t Exemple Aujourd'hui, Notre-Dame-de-Vie fait Toujours [...] annuel le 15 août. Aujourd'hui, Notre-Dame-de-Vie est toujours [...] un pèlerinage annuel le 15 Août. Extraction [Cvr] = 15 Août et [f] = TAB annuel. 2: de timepoint Sous-classes. - 77 - conseils dans le temps du patrimoine culturel discourent Intervalle: définition et sous-classes. Au niveau conceptuel un intervalle de temps représente un segment de temps avec une durée. Six concepts sont introduits pour faire face aux déclarations temporelles qui sont verbalisées comme des intervalles de temps (tableau 3). Ils partagent des caractéristiques communes: elles sont définies par un ou deux points de temps et sont ancrés dans le temps. ProperIntervalStatement (IPR) est un libellé correspondant à un intervalle exprimé par 2 points dans le temps (à partir de limite: TPB, et à la fin de limite: TPE). Ces deux points de temps sont reliés par un ExplicitOperator. d tp tp B E E tp tp B iPR opérateur lexical, par exemple, «de ... à» t occurrence intervalle Exemple [...] Bâti en pierre Entre 1815 et 1825. [...] construit en pierre entre 1815 et 1825. Extraction [TPB] = 1815 et [tpe] = 1825 implique d = ([tpe] - [TPB]) + 1 = 11 ans OnePointAnchoredIntervalStatement (POIA) est une formulation qui fait référence à un événement exprimé par un timepoint, et accompagné d'un LexicalOperator tels que « autour ». L'effet de la LexicalOperator est de créer deux durées égales (d) sur les deux côtés du point. La valeur de la durée est choisie par l'utilisateur (de ConventionalMapping). d tp M i OPA opérateur lexical (par exemple, «autour») d M tp t Exemple Bâtiment Autour de actuel 1645. édifié bâtiment actuel est construit autour de 1645. Extraction [TpM] = 1815 et [d] est lié à CM OneSideBoundedIntervalStatement (iOSB ) est une formulation qui fait référence à un événement exprimé par un timepoint (tp), et accompagné d'un LexicalOperator comme « avant BE- » ou « après ». Le point de temps agit comme une frontière (début ou fin) et l'effet de l'opérateur Lexical- est de créer une durée définie sur un côté. Une valeur classique pour la durée peut être définie par ConventionalMapping. tp iOSB opérateur lexical (par exemple, «avant») d 'opérateur lexical (par exemple, «après») t t Exemple Elle was Construite APRES 1720 par publique de souscription. Il a été construit après 1720 par souscription publique. Extraction [tp] = 1720 et [d] est en relation avec CM (suite à la page suivante.) - 78 - G. Saygi et al. RelativeIntervalStatement (iR) est une formulation où timepoint est utilisé pour ancrer un événement situé à une distance dans le passé ou dans l'avenir du timepoint. L ' « écart poral tem- » entre le point temporel et l'événement est exprimé, mais la durée de l'événement n'est pas. tp opérateur lexical iR (par exemple, «x années avant») exploitant lexical (par exemple, «x ans après») t t d l'exemple 1 en 706: la chapelle Sainte-Anne [...], ainsi que de bâtie 60 DEPUIS ans. en 1706: la chapelle Sainte-Anne [...], construit il y a plus de 60 ans. Extraction [tp] = 1706 et [d] = plus de 60 ans PertinentIntervalStatement (Ipé) est une formulation où l'on ne se définit que limite, et par une instance de NamedTimePeriod ( « Seconde Guerre mondiale », « la grande peste », etc.). Le concept est utilisé pour affiner iOSB et iR lorsque l'ancrage ne peut pas être fait par un timepoint, mais se fait par une instance de NamedTimePeriod. i PNT PE opérateur lexical (par exemple, «x années avant») opérateur lexical (par exemple, «x ans après») t t (période nommée) d Exemple [...] après la Révolution remaniements. [...] rearrengements après la Révolution. Extraction [PNT] = la Révolution et ([d] et [PNT]) sont liés au CM UnfoldableIntervalStatementt (iu) est un libellé exprimé avec deux points dans le temps, et où la durée de l'événement est sûr plus courte que la distance séparant les deux Points de temps. tp tp B E tp opérateur lexical B par exemple, «de ... à» i U tp E ttd Exemple 1160-1164 [...] seigneurs [...] y verser Construire une Abbaye acte signé [...] 1160-1164 seigneurs [...] pour construire une abbaye [...] acte signé [...] Extraction [TPB] = 1160 et [tpe] = 1164 implique [d] = 5 ans> [Dévent] TAB. 3: Intervalle de Sous-classes. UnanchoredDuration. Un UnanchoredDuration représente toute déclaration temporelle que hommes- tions un segment de temps avec une durée, mais sans limites explicites (ancré dans le temps). Ce concept est utilisé pour représenter des conseils tels que « la rénovation a duré 40 jours. » - 79 - conseils dans le temps du discours du patrimoine culturel CompoundTemporalStatement. Il est assez fréquent d'avoir plusieurs indications temporelles au sein d'un TemporalStatement, par exemple, « il remonte au 13ème ou 14ème siècle ». CompoundTem- poralEntity représente les cas où la déclaration contient deux ou plusieurs autres QuantifiedTemporalEntities, peuvent se chevaucher ou non, et qu'ils soient cohérents en termes de formulation ou non. 4 OWL mise en œuvre, les résultats et les limites Nous avons mis une solution open-source, en utilisant OWL 2, construit sur la norme RDF dans lequel les données sont représentées par des ensembles de « triplets ». Nous avons peuplé l'ontologie des informations extraites des ressources en ligne publiées par les différentes parties prenantes (par exemple, les archives publiques, les sociétés cultu- relles, etc.) récoltées dans le cadre d'une initiative de recherche 1 sur le patrimoine matériel et immatériel mineur. Ces ensembles d'information, étant verbalisées par les différentes parties, correspondent à la nature heterogonous, imprécis et incertain de l'information temporelle souvent rencontré dans le domaine CH. Nous avons choisi des déclarations et 1576, en plus de types de données standard OWL, nous resservir temps OWL types de données comme le temps: generalDay, temps: generalMonth. A ce stade encore précoce de la recherche, l'effort de formalisation jette un éclairage sur certains modèles de verbalisation importants, mais aussi là où les besoins d'effort à consolider. Par exemple, 90 TemporalStatements correspondent au concept de CompoundTemporalStatement mais leur classement en tant que telle est motivée par des indications contradictoires soit « [...] du XIème OU datées du XII ème siècle ») ou par une sorte de prudence dans la formulation ( « [. ..] Doït au remonter (XVIIe siècle vers 1668-1670) »). Soit dit en passant, l'utilisation de formulations classées comme CompoundTem- poralStatement devient apparemment plus fréquente lorsque les événements sont situés plus loin dans le passé, ce qui est un modèle plutôt attendu. La majorité des déclarations générales considérées entrent dans une classe: la classe FuzzyTemporalStatement. Il en est plutôt attendu puisque l'alignement de granularité entre un indice et un événement est tout à fait souvent hors de portée en sciences historiques, et en particulier lorsqu'ils traitent avec Patrimoine mineur. Quelques CIES tenden- plus question d'ouverture peuvent être observées aussi, par exemple en observant la variété des figures linguistiques utilisées pour verbaliser une même situation: limitant le « commencement » d'un événement. Mais il faut dire clairement tha tendances t observées pourraient très bien être lié à l'ensemble de données particulier, nous avons travaillé sur, ou à des biais introduits dans le modèle lui-même, et par conséquent, ils doivent être pris pour ce qu'ils valent. Notre but est certainement de ne pas tirer de ces premières observations a-sciences historiques liées conclusion générale, mais au contraire d'utiliser les observations comme un moyen de remettre en question le modèle. En outre, l'un des enseignements tirés de l'expérimentation était que quelques indices « contemporains » que l'on trouve dans les e-sources de citoyens a donné naissance récoltées sur le net, comme « l'édifice a été reconstruit en dernières années », sont loin d'être le plus facile à traiter. En bref a dit, nos efforts de formalisation mettent en évidence la diversité extrême de la formation in- temporelle dans le discours CH, mais aussi découvrir des modèles importants comme une déconnexion par rapport entre les opérateurs lexicales et granularité temporelle (opérateurs utilisés quelle que soit la granularité). En conséquence, il apparaît clairement que ce que la recherche souligne également le montant des non-dits dans les raisons pour lesquelles telle ou telle modalité de verbalisation a été choisi par un fournisseur d'information. 1. L'ontologie est disponible sur le site web du projet Territographie (http://map.cnrs.fr/territographie/), portail d'un projet de recherche exploratoire sur la science des citoyens et mineur du patrimoine menée en coopération avec MuCEM (Musée d'Europe et les civilisations méditerranéennes) et financé par la région des autorités Provence-Alpes-Côte d'Azur. - 80 - G. Saygi et al. 5 Conclusions et travaux futurs Dans cet article, nous avons analysé les états temporels imparfaits utilisés dans le discours CH, et introduit une ontologie pour permettre et le codage des connaissances temporelles. Notre contribution, petit mais il peut être, vise à faciliter le contre-interrogatoire par les analystes de la preuve historique de la preuve temporelle elle-même, avant les étapes d'interprétation. Nos résultats expérimentaux montrent que l'ontologie porte le potentiel de faire la lumière sur l'information temporelle efficace (mal définie). Néanmoins, nous reconnaissons la nécessité de valider l'ontologie sur des ensembles de données plus importants, et de tester sa capacité d'extraction dans les collections patrimoniales où les paramètres régionaux et typologiques sont largement circonscrites. De plus, nous reconnaissons la nécessité d'inspecter l'ontologie proposée par les experts principaux DO- externes. Toutefois, les services attendus, ainsi que les fournisseurs d'information, correspondent à une variété de domaines allant de la linguistique ( « libellé en tant que telle ») à la muséologie, logie ethnol-, histoire de l'art (si sciences non historiques en général), etc. D'où une validation robuste aidant à recalibrer la proposition nécessitera une enquête multidisciplinaire, et est sans aucun doute partie du programme de recherche que nous avons devant nous. travaux futurs comprendront une analyse critique de l'applicabilité de l'approche au-delà du corpus initial (règles d'affectation en particulier), un effort de visualisation, et une attention plus approfondie sur la question interdisciplinaire de explicitation: non seulement comment une déclaration temporelle est formulée, mais aussi Pourquoi. Références Aigner, W., S. Miksch, H. Schumann et C. Tominski (2011). Visualisation des données axées sur le temps. Springer Science & Business Media. Allen, J. F. (1983). Le maintien des connaissances sur des intervalles temporels. Commun. ACM 26 (11). Allen, J. F. (1991). Maintes et maintes fois: Les nombreuses façons de représenter le temps. International Journal of Intelligent Systems 6 (4), 341-355. Anagnostopoulos, E., S. Batsakis, et E. G. Petrakis (2013). Chronos: Un moteur de raisonnement pour l'information temporelle qualitative hibou. Procedia Computer Science 22 (Supplément C), 70 - 77. 17 Int. Conf. dans et fondée sur la connaissance intelligente Inf. et Eng. Systèmes. Reliure, C. (2010). La mise en œuvre des périodes de temps archéologique Utilisation CIDOC CRM et SKOS, pp. 273-287. Berlin, Heidelberg: Springer. Blaise, J.-Y., I. Dudek, W. Komorowski et T. Weclawowicz (2016). Mations architecturaux transforma- sur la place du marché à Cracovie - Un catalogue visuel systématique. Cracovie: Oficyna Wydawnic za AFM. Cox, J. S. D. (2016). Temps Ontologie étendu pour les applications Calendrier non grégoriens. Web sémantique Journal 7 (2), 201 - 209. Davies, J., D. Fensel et F. Van Harmelen (2003). Vers le web sémantique: la gestion des connaissances axée sur l'ontologie. John Wiley & Sons. Diallo, P. F., O. Corby, I. Mirbel, M. Lo et S. M. Ndiaye (2015). Huto: un temps humain Ontologie pour les applications Web sémantique. En 2015 Ingénierie des Connaissances, Rennes. Ermolayev, V., S. Batsakis, N. Keberle, O. Tatarintseva et G. Antoniou (2014). Ontologies de temps: études et tendances. IJCSA 11 (3), 57-115. - 81 - conseils dans le temps du patrimoine culturel discourent Faucher, C., C. Teissèdre, J.-Y. Lafaye et F. Bertrand (2010). Temporelle Connaissance Acquisi- tion et de la modélisation, pp. 371-380. Berlin, Heidelberg: Springer Berlin Heidelberg. Or, P. et R. Shaw (2016). Nanopublication au-delà des sciences: la période de periodo gazetteer. Peerj Comp Science 2, E44. Gruber, T. R. (1993). Une approche de traduction aux spécifications de l'ontologie portables. L'acquisition des connaissances 5 (2), 199-220. Hiebel, G., M. Doerr et Ø. Eide (2016). Crmgeo: Une extension spatio-temporelle de CIDOC-crm. Int Journal sur les bibliothèques numériques, 1-9. Jurisica, I., J. Mylopoulos et E. Yu (2004). Ontologies pour la gestion des connaissances: un point de vue des systèmes d'information. systèmes d'information et de connaissances 6 (4), 380-401. Kauppinen, T., G. Mantegari, P. Paakkarinen, H. Kuittinen, E. Hyvönen et S. Bandini (2010). pertinence détermination des intervalles temporels imprécises pour information sur le patrimoine culturel trieval re. Int. J. Comput-Hum.. Goujon. 68 (9), 549-560. Levi, R. (1927). Théorie de l'activité et discontinuer universelle. J. Phys. Radium 8 (4), 182-198. Musen, M. A. (2015). Le projet protégé: un regard en arrière et regarder en avant. AI importe 1 (4). Nurminen, M. et A. Heimburger (2012). La représentation et la récupération de l'information temporelle incertaine dans les bases de données du musée. Dans la modélisation et l'information Bases connaissances XXIII. Papadakis, M., M. Doerr et D. Plexousakis (2014). fois flous sur les volumes d'espace-temps. Dans eChallenges e-2014, 2014 Conférence, pp. 1-11. IEEE. Poveda-Villalón, M., M. C. Suarez-Figueroa, et A. Gómez-Pérez (2014). Un modèle pour péri- intervalles odiques. Web sémantique Journal Oct14, 1-10. Tao, C., W.-Q. Wei, H. R. Solbrig, G. Savova et C. G. Chute (2010). Cntro: une ontologie web sémantique pour relation temporelle dans les récits cliniques d'inférence. Dans AMIA Actes du colloque annuel, Volume 2010, p. 787. American Medical Informatics Association. Dans le champ résumé des sciences Patrimoniales, la dimension de l'Information temporelle un rôle à joue L'Évidence majeur Tant pour l'analyseur l'interprète et Qué verser des Faits Isolés relier. La Façon Dont Mais this dimension is verbalisée pose des Problèmes de formalisation non tri Viaux. POURTANT, this verbalisations, Que l'sur Associe au terme-Souvent chapeau d'incertitude, en Lue may Être dissociant D'une part le caractère mal d'un fait Connu Documente, irréductible, et les par le choix Faits producer de l » informations Pour la relativiseur. Dans this contribution NOUS un modèle Formel proposons d'observateur et permettant d'analyseur de Façon couche de this systématique verbalisations. L'expérience sur des EST Menée FORTEMENT Hétérogènes Données, d'origine Citoyenne Souvent, documentant le petit et patrimoine matériel immatériel. Ce d'study is No CAS Fait Limite, il apparait Mais néanmoins question Comme de juin portante Allant aime au-delà du d'species CAS. La contribution Detaille d'ABORD la grille d'analyse d'indices tempo- rels proposed, concernent l'expérimentation Puis concrète Associée (OWL ontologie). Il fait état Ne EST PAS D'une à un quelconque prétention généralisable Résultat stricto sensu, Mais THIS riences rience may Ë Contribuer de façcon Pragmatique nourrir ONU-sur-la nécéssaire Débat Formalisation D'indices les sciences temporels historiques la DANS. - 82 -"
71,Revue des Nouvelles Technologies de l'Information,EGC,2017,A Hybrid Approach for Detecting Influencers in Social Media,"La détection d'influenceurs dans les réseaux sociaux s'appuie généra-lement sur une structure de graphe représentant les utilisateurs et leurs interac-tions. Récemment, cette tâche a tenu compte, en sus de la structure du graphe,du contenu textuel généré par les utilisateurs. Notre approche s'inscrit dans cettelignée : des informations sont extraites du contenu textuel par des règles linguis-tiques puis sont intégrées dans un système d'apprentissage automatique. Nousmontrerons le prototype développé et son interface de visualisation qui facilitel'interprétation des résultats.","Ioannis Partalas, Cédric Lopez, Pierre-Alain Avouac, Matthieu Osmuk, Domoina Rabarijaona, Dana Popovici, Frédérique Segond",http://editions-rnti.fr/render_pdf.php?p1&p=1002325,http://editions-rnti.fr/render_pdf.php?p=1002325,en,"67-RNTI-E33 une approche hybride pour influenceurs Detecting dans les médias sociaux Ioannis Partalas *, Cédric Lopez *, Pierre-Alain Avouac *, Matthieu Osmuk * Domoina Rabarijaona *, Dana Popovici *, Frédérique Segond * * Viseo R & D Grenoble, 38000, France firstname.lastname@viseo.com, http://www.viseo.com CV. La Influenceurs d'Dans détection les réseaux sociaux s'appuie sur Generators Une structure de souillure de graphè representative les et their Utilisateurs interac- tions. Recemment, un this Tenu compte Tâche, en sus de la structure de du graphè, du contenu Textuel par les Généré Utilisateurs. Notre approche s'inscrit this Dans Lignée: des informations du contenu Sont extraites Textuel par des rules linguis- tiques Intégrées Dans Puis un are Système d'apprentissage automatique. Nous le prototype montrerons et l'interface fils Développé de visualisation Qui l'interprétation des Facilite Résultats. 1 Introduction Un influenceur est une personne ou une chose qui a le pouvoir d'influer sur les personnes, les actions ou les événements. La détection de in- fluencer concerne le problème de déterminer quels utilisateurs ont le plus d'influence dans un certain réseau social. Ces informations sont cruciales dans de nombreuses études de recherche, comme dans les domaines de so- ciology et de gestion de l'information. De plus, avec la croissance frénétique des données lable avai- dans le réseau social en ligne, être capable d'analyser et de détecter les utilisateurs influents devient crucial car ils sont susceptibles d'exprimer leurs idées plus fortement que d'autres personnes. Par exemple, ces informations peuvent être utilisées dans les campagnes de marketing afin de maximiser leur propagation (Richardson et Domingos, 2002). Formellement, la tâche pour détecter les utilisateurs dans un réseau influent social, traite d'un graphe G = (V, E) où V représente les utilisateurs du réseau et E les interactions entre eux. Outre les informations structurelles, nous supposons que chaque utilisateur produit des informations que le contenu textuel. Un tel contenu peut induire de nouvelles interactions entre les utilisateurs par le biais du nouveau contenu textuel. Nous considérons donc la tâche de détecter les influenceurs après deux façons: analyser la structure des réseaux sociaux, ainsi que leur contenu textuel. Notre méthode combine riches informations linguistiques ainsi que des propriétés structurelles afin d'alimenter un modèle d'apprentissage de la machine pour les utilisateurs de notation. Le travail présenté fait partie du projet Eurostars SOMA 1 qui concerne l'amélioration des tèmes de gestion de la relation client avec des capacités d'analyse des médias sociaux. 1. http: //www.somaproject.eu/, SOMA programme Eurostars 9292/12/19892 - 465 - Influence de détection dans les médias sociaux 2 Contexte et travaux connexes Habituellement, la détection d'influence est traitée en analysant la structure, principalement en utilisant la théorie des graphes où une pléthore de mesures existent. Dans ce contexte, les mesures de centralité utilisent l'information structurelle afin d'identifier les nœuds les plus importants dans un réseau (Bonacich, 1987). mesures indicatives sont centralité betweenness et PageRank. Une autre ligne de travail, utilise des modèles de propagation qui tentent de préciser comment les actions sont propagées à travers le réseau social (Kempe et al., 2003). Par exemple, ces actions pourraient être les retweets d'un poste sur Twitter. Enfin, plusieurs méthodes tentent de combiner le contenu des informations structurelles dans les travaux Net- sociaux. Weng et al. (2010) modifie PageRank afin de favoriser certains utilisateurs selon un sujet. Plus récemment Katsimpras et al. (2015) ont proposé une approche de marche aléatoire supervisé vers des nœuds influents sujets sensibles. Récemment, Biran et al. (2012) ont commencé à explorer les caractéris- tiques de communication pour la détection d'influence, l'adoption d'une approche d'apprentissage automatique basée sur des fonctionnalités telles que la persuasion, un accord / désaccord, les modèles de dialogue et sentiments. Cette approche a nécessité l'annotation manuelle des weblogs de LiveJournal et des forums de discussion de Wikipedia. (Cossu et al., 2016) présentent une vue d'ensemble des fonctions qui sont utilisées pour cha- ract Erize utilisateurs influents sur Twitter. L'originalité de notre démarche est d'utiliser des règles linguistiques afin d'extraire l'information à grain fin dans le discours entre les utilisateurs. Ensuite, ces informations sont utilisées comme attributs dans un modèle d'apprentissage de la machine. 3 Influence outil de détection Le cas d'utilisation du béton pour la boîte à outils de détection d'influence se déroule comme suit: 1) L'utilisateur définit une source de médias sociaux (un forum, un réseau social, etc.) à analyser, 2) Les collectes système structurées (par exemple informations comportementales) et des données non structurées (textes), 3) Un score d'influence est attribuée à chaque utilisateur social. Le système développé est composé de trois parties principales en ce qui concerne les différents aspects: la consommation de données et de pré-traitement, l'analyse des données et de visualisation. Les données Wrangling En ce qui concerne la collecte de données à analyser l'outil peut être branché avec les médias sociaux (par exemple les forums, blogs, Twitter, Facebook, etc.) avec des collecteurs appropriés et consommer les données d'intérêt. concernent des données recueillies de structure (par exemple le genre, l'emplacement), comportementaux (par exemple nombre de tweets) et des informations textuelles. une partie textuelle des données recueillies de la pré-traitées par des outils linguistiques pour extraire la structure morpho. Analyse des données En ce qui concerne l'identification des influenceurs, le principal défi est le développement d'un système hybride, la fusion des descripteurs linguistiques et non linguistiques. Une grande partie de cette tâche est généralement assurée par la prise en information non linguistique compte qui a déjà tré démons- bons résultats. L'utilisation des informations linguistiques pour influenceurs de détection repose sur l'hypothèse selon laquelle un influenceur a un comportement spécifique qui se traduit en termes linguistiques. Ceci est une considération récente de la communauté des chercheurs en linguistique, et obtenu des résultats sont encourageants (Rosenthal, 2015). - 466 - I. Partalas et al. En ce qui concerne les informations non-linguistique, nous utilisons les informations statistiques pertinentes bien connues dans la littérature, comme la longueur des messages, le nombre de messages postés par un auteur, le nombre d'adeptes, etc. En ce qui concerne l'information linguistique, nous nous concentrons sur la analyse des différentes dimensions du discours: l'intonation, le style d'écriture, la rhétorique, l'argumentation, actes de parole, relation entre le texte et le contexte, etc. Tous les attributs extraits sont utilisés pour alimenter un modèle d'apprentissage de la machine qui permettra aux utilisateurs de notation en fonction de leur influence . À cette fin, nous utilisons l'état de l'art des forêts aléatoires principalement pour leur capacité à tirer parti des interactions non linéaires entre les caractéristiques, ainsi que pour leur interprétation. Visualisation Le module de visualisation est basée sur le Web qui permet straight-forward accessi- bilité. Plus précisément, un segment peut les utilisateurs sociaux selon les termes de clés ou des sujets d'intérêt qui permettront une vue à grains fins sur l'ensemble des influenceurs. Bien sûr, on peut avoir vue globale des influenceurs détectés avec différents types de visualisation. La figure 1 présente un écran du module de visualisation où les 20 utilisateurs sont présentés sous forme de bulles selon leur score d'influence. La figure 2 présente l'interaction entre les utilisateurs dans un graphique pour une certaine discussion. Les utilisateurs ayant obtenu une note plus élevée d'influence sont représentés avec de plus grands cercles. FIGUE. 1 - Visualisation des utilisateurs à l'aide de bulles. Remerciements Ce travail a été soutenu par le programme Eurostars SOMA 9292/12/19892. Références Biran, O., S. Rosenthal, J. Andreas, K. McKeown, et O. Rambow (2012). Détection influenceurs dans les conversations en ligne écrites. Dans Actes du deuxième atelier sur la langue dans les médias sociaux, pp. 37-45. Association de linguistique informatique. Bonacich, P. (1987). Puissance et Centralité: Une famille de mesures. American Journal of Sociology 92 (5), 1170-1182. - 467 - Influence de détection dans les médias sociaux figure. 2 - Le graphique d'interaction des utilisateurs tout au long de la discussion. Cossu, J.-V., V. Labatut, et N. Dugué (2016). Un examen des caractéristiques de la discrimination des utilisateurs de Twitter : Application à la prédiction d'influence hors ligne. Analyse des réseaux sociaux et des mines 6 (1), 25. Katsimpras, G., D. Vogiatzis, et G. Paliouras (2015). Déterminer les utilisateurs influents avec des promenades au hasard supervisait. Dans Actes de la 24e Conférence internationale sur le World Wide Web, WWW Companion '15, New York, NY, USA, pp. 787-792. ACM. Kempe, D., J. Kleinberg, et E. Tardos (2003). Maximiser la propagation de l'influence à travers un réseau social. Dans Actes de la neuvième ACM SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données, KDD '03, pp. 137-146. ACM. Richardson, M. et P. Domingos (2002). Exploitation minière sites de partage de connaissances pour le marketing viral. Dans Actes de la huitième conférence internationale SIGKDD sur la découverte des connaissances et l'exploration de données, p. 61-70. ACM. Rosenthal, S. (2015). Détection influenceurs dans les discussions des médias sociaux. Ph. D. thèse, Université Co Lumbia. Weng, J., E.-P. Lim, J. Jiang, et Q. Il (2010). Twitterrank: Trouver twitterers influents sujet sensible. Dans Actes de la troisième Conférence internationale sur ACM Web Recherche et Data Mining, WSDM '10, pp. 261-270. ACM. Résumé influenceurs Detecting dans les réseaux sociaux en général repose sur une structure graphique représentant les utilisateurs et leurs interactions. prendre en compte récemment approché, en plus de la struc- ture du graphique, le contenu textuel généré par les utilisateurs. Le long de cette ligne, notre utilisation d'approche informations extraites du contenu textuel par des règles linguistiques et intégrées dans un système d'apprentissage de la machine. Dans ce travail, nous présentons le prototype développé avec la visualisation utilisée afin de faciliter l'interprétation des résultats. - 468 -"
89,Revue des Nouvelles Technologies de l'Information,EGC,2017,Deep Dive on Smart Cities by Scaling Reasoning and Interpreting the Semantics of IoT,"Modern cities are facing tremendous amount of information, captured from internal in-frastructures and/or exogenous sensors, humanincluded. This talk presents how big and het-erogenous city data has been captured, represented, unified to serve one of the most pressingcity objective: improving quality of city, in particular how understanding and reducing traf-fic congestion. We will also present lessons learnt from the deployment of our system andexperimentation in Dublin (Ireland), Bologna (Italy), Miami (USA) and Rio (Brazil).",Freddy Lécué,http://editions-rnti.fr/render_pdf.php?p1&p=1002264,http://editions-rnti.fr/render_pdf.php?p=1002264,en,"04-RNTI-E33 Deep Dive sur les villes intelligentes de mise à l'échelle Raisonnement et interpréter les Sémantique de IdO Freddy Lécué * * Accenture Technology Labs, l'INRIA freddy.lecue@accenture.com, http://www-sop.inria.fr/members/ Freddy.Lecue / Résumé Les villes modernes sont confrontées à quantité énorme d'informations, capturées à partir frastructures in- internes et / ou des capteurs exogènes, humanincluded. Cet article présente la taille et talk Het- données de la ville érogène a été capturé, représentés, unifiés pour servir l'un des plus pressants objectif ville: améliorant la qualité de la ville, notamment la façon dont la compréhension et la réduction de la congestion. Fic DU TRAFFIC Nous allons également des enseignements actuels tirés du déploiement de notre système et de l'expérimentation à Dublin (Irlande), Bologne (Italie), Miami (Etats-Unis) et Rio (Brésil). Biographie Dr Freddy Lecue (PhD 2008, Habilitation 2015) est un scientifique principal et la recherche ager Homme- à grande échelle dans le raisonnement des systèmes Accenture Technology Labs, Dublin - Irlande. Il est également associé de recherche à l'INRIA, en WIMMICS, Sophia Antipolis - France. Avant Accenture ING à Join Junuary 2016, il a été chercheur et chercheur principal dans les grands systèmes de raisonnement à grande échelle chez IBM Research - Irlande. Son domaine de recherche est à la frontière de l'apprentissage, les systèmes de raisonnement et Internet des choses en mettant l'accent sur l'entreprise intelligente et applica- tions de la ville (à Dublin - Irlande, Bologne - Italie, Miami - Etats-Unis et Rio - Brésil). Ses recherches ont reçu une reconnaissance interne d'IBM: Prix de la division de recherche d'IBM en 2015 et IBM récompense Accomplissement technique en 2014. Sa recherche a reçu une reconnaissance externe: prix du meilleur papier de ISWC (Conférence internationale Web sémantique) en 2014, et l'ESWC (Extended Web sémantique Conférence) en 2014, ainsi que des prix de défi Web sémantique de ISWC en 2013 et 2012. - 7 -"
93,Revue des Nouvelles Technologies de l'Information,EGC,2017,Enhanced user-user collaborative filtering recommendation algorithm based on semantic ratings,,"Wen Zhang, Raja Chiky, Manuel Pozo",http://editions-rnti.fr/render_pdf.php?p1&p=1002318,http://editions-rnti.fr/render_pdf.php?p=1002318,en,"60-RNTI-E33 algorithme de recommandation de filtrage collaboratif utilisateur-utilisateur améliorée en fonction des notes sémantiques Wen Zhang *, Raja Chiky ** Manuel Pozo ** * Université de Stanford zhangwen@cs.stanford.edu, ** ISEP LISITE Lab prénom.nom Paris- @ isep.fr 1 introduction Ce document présente un algorithme de recommandation de filtrage collaboratif qui emprunte des idées à partir de modèles basés sur le contenu en tenant compte à la fois les évaluations et les préférences de l'utilisateur pour les attributs de l'élément. Ceci est réalisé en remplaçant chaque note avec son correspondant « note sémantique », qui combine la partition originale et le niveau de préférence historique de l'utilisateur pour les attributs de l'élément (Pozo et al., 2016). Cet algorithme peut obtenir de meilleurs résultats qu'une contrepartie de filtrage collaboratif pur sans se soucier des propriétés intrinsèques de chaque élément. Nous basons notre travail sur une expérience (Pozo et al., 2016), qui propose la « équation sémantique » comme une transformation d'une note originale à son correspondant note sémantique, capturant ainsi la préférence d'un utilisateur pour les attributs d'un élément. Comme il est seulement une transformation des notations, cette technique peut être utilisée dans différents types d'algorithmes de filtrage collaboratif. (Pozo et al., 2016) ont appliqué l'équation sémantique sur le dessus d'un algorithme de factorisation de la matrice et ont trouvé que les approches sémantiques ont donné de meilleurs résultats en termes de précision, de rappel, et de la diversité intra-liste. Dans cet article, nous appliquons la même idée à une méthode simple de voisinage basée sur des corrélations de Pearson. Nous validons (Pozo et al., 2016) et montre l'effet plus profond de ces « transfomation sémantique » en analysant les « modifications sémantiques » seulement. 2 algorithmes sémantiques Les algorithmes sémantiques que nous proposons sont tous basés sur l'équation sémantique, qui transforme une cote ru, i dans sa note sémantique modifiée ru, i (Pozo et al., 2016). Pour un utilisateur u et un élément i, on définit leur « modification sémantique », qui capte la préférence historique de l'utilisateur pour les attributs de l'élément, comme suit: Δu, i = rú · | | | Σ a∈A (i) Cu, AWT (a) | | | | S (u) | , (1) où Cu, a = | {i ∈ S (u): a ∈ A (i)} | indique le nombre de fois attribuer une affiche dans tous les éléments notés par l'utilisateur u, et wt (a) désigne le « poids » du type de l'attribut a. Notez que Δu, i - 443 - filtrage collaboratif utilisateur-utilisateur améliorée doit être entièrement calculé à partir de l'ensemble de la formation, bien qu'il ne soit pas nécessaire que l'utilisateur u ont évalué l'article i. Le poids en poids d'un type d'attribut t reflète sa pertinence et est constante tout au long du calcul: un type d'attribut qui est plus pertinent de faire des prévisions devrait être un poids plus élevé. (Notez que les poids appartiennent à des types d'attributs au lieu des attributs individuels.). Les coefficients de pondération sont les mieux choisis en fonction de chaque grade de type d'attribut de pertinence obtenu à partir de l'APC (Pozo et al., 2016). L'équation sémantique peut être exprimée comme suit: ru, i = ru, i + Δu, i, (2) où ru, i peut être soit une cote initialement présent dans l'ensemble de données, ou être une prédite par un algorithme. Par conséquent, il y a plus d'un endroit où l'on peut appliquer l'équation (2) dans le processus de test de formation: 1. L'approche-entrée algorithme sémantique, les résultats de l'application de l'équation (2) au culation CAL- des corrélations Pearson dans la formation étape. 2. L'algorithme sémantique approche de sortie résulte de l'application de l'équation (2) aux résultats de prédiction: dans la phase de prédiction, nous transformons chaque note prédit pu, i dans sa notation sémantique pu, i = pu, i + Δu, i, que nous utilisons comme le résultat de prédiction finale. 3 CONCLUSION Nous effectuons des évaluations sur les MovieLens-GroupLens ensemble de données (Can, 2011), qui se compose de 2.113 utilisateurs, 10,197 films et 855,598 évaluations. Il contient également six attributs: genres, réalisateurs, acteurs, pays, les lieux et les étiquettes, avec 112,881 valeurs d'attributs distincts. Nous mesurons la précision, le rappel et f-mesure pour un algorithme de base (algorithme de non pearson sémantique) et l'algorithme sémantique. Nos expériences démontrent l'efficacité de l'utilisation des notations sémantiques avec un algorithme de filtrage en collaboration par l'utilisateur de l'utilisateur en fonction de Pearson tions corrélations. Lorsque des informations sur les attributs des objets est présente, en utilisant ces informations peut souvent conduire à de meilleures performances de prédiction par certains paramètres, même si nous ne recourons pas à un algorithme hybride complet. La force la plus évidente de la transformation sémantique est qu'il permet de proposer des algorithmes plus des éléments qui sont pertinents prouvable: plus sémantiquement influencé par un algorithme est, la plus grande précision et le rappel qu'il procure. Cette constatation est le plus directement pris en charge par le fait que l'algorithme sémantique pure approche de sortie, qui utilise simplement les amende- ments sémantiques que ses prédictions, donne une précision nettement plus élevé, le rappel et f-mesure. Références (2011). HetRec '11: Actes du 2e Atelier international sur l'information hétéro- généité et la fusion, systèmes de recommandation, New York, NY, USA. ACM. Pozo, M., R. Chiky et E. Métais (2016). Améliorer le filtrage collaboratif utilisant les relations implicites dans les données, p. 125-146. Berlin, Heidelberg: Springer Berlin Heidelberg. - 444 -"
108,Revue des Nouvelles Technologies de l'Information,EGC,2017,Machine Learning Based Classification of Android Apps through Text Features,,"Mohamed Guendouz, Abdelmalek Amine, Reda Mohamed Hamou",http://editions-rnti.fr/render_pdf.php?p1&p=1002312,http://editions-rnti.fr/render_pdf.php?p=1002312,en,"53-RNTI-E33 Machine Learning basée sur la classification des applications Android à travers le texte Caractéristiques Mohamed Guendouz *, Amine Abdelmalek * Reda Mohamed Hamou * * GeCode Laboratoire, Université Moulay Tahar de Saïda. Saida, Algérie @ email adresse, http: //www.une-page.html 1 Introduction Ce document traite avec le problème des applications mobiles Android en utilisant la classification et l'apprentissage des méthodes d'extraction de texte machine. Notre approche consiste à appliquer des méthodes d'apprentissage de la machine sur les caractéristiques de texte qui sont extraits de la description de l'application sur Google Play Store. Notre approche proposée consiste en deux phases principales. D'abord, nous recueillons des informations sur les applications de la boutique Google Play en utilisant un robot web. Ensuite, nous extrayons quelques informations texte à partir de ces données. Dans notre cas, pour chaque application, nous avons extrait sa description et sa catégorie. En second lieu, nous formons différents classificateurs, cette étape consiste à pré-traitement du texte qui inclut la suppression des URL et des chiffres, des tokens, et le calcul TF * IDF au texte de transfert à un vecteur numérique qui peut être utilisé comme entrée pour les classificateurs. Enfin, pour évaluer les performances de notre sys- tème, nous avons mené diverses expériences sur trois ensembles de données réelles en utilisant différents paramètres d'évaluation. La figure 1 illustre l'architecture de notre cadre de classification des applications. Enfin, nous évaluons notre approche sur trois ensembles de données réelles, a obtenu des résultats montre que l'utilisation des fonctionnalités de texte dans la classification des applications Android peuvent se comporte bien. FIGUE. 1 - Architecture du cadre proposé - 429 - Machine Learning basée sur la classification des applications Android à travers le texte Caractéristiques 2 résultats expérimentaux Comme il n'y a pas de jeu de données standard ou de référence pour ce type d'études, nous avons dû créer notre propre ensemble de données. Nous avons recueilli des métadonnées des applications gratuites Android du magasin Google Play, cette tâche a été fait automatiquement en utilisant un robot web développé spécialement pour cela. Un total de 7893 applications Android ont été collectées classées dans 4534 Apps et 3305 Jeux. Nous avons conçu notre jeu de données dans trois autres ensemble de données qui sont: ensemble de données générales qui contient toutes les applications classées en deux catégories: App et du jeu. Apps ensemble de données qui ne contient que des applications classées dans neuf catégories: COMMUNICATION, EDUCATION, ENTERTAINMENT, MÉDIAS ET VIDÉO, Médical, PHOTOGRAPHIE, SOCIAUX, OUTILS, TEMPS. Jeux datast qui ne contient que des jeux classés en six catégories: ARCADE, Éducatif, PUZZLE, RACING, SIMULATION, SPORTS. 2.1 Evaluation Metrics Afin d'évaluer la performance et la précision de notre approche, nous choisissons un bon ber nom- de mesures d'évaluation bien acceptées pour les classificateurs, y compris de précision, rappel, F-mesure, TPR (taux réel positif), TFP (taux de faux positifs ). Dans les expériences que nous utilisons la validation croisée dix fois pour évaluer chaque classificateur. 2.2 Résultats et analyse dans le présent paragraphe, nous analysons les résultats obtenus à partir d'expériences, nous avons d'abord évaluer Fiers classi- sur le premier ensemble de données qui contient toutes les applications catégorisées que dans deux catégories: applications et jeux, cela signifie une classification de classe binaire, nous évaluons la performance de différentes sifiers clas- comme: Naive Bayse, SVM, forêt d'arbres décisionnels. Le résultat montre que l'algorithme Random Forest meilleurs résultats que tous les autres, le rithmes rithmes meilleur résultat de F-mesure est 0,971 obtenus à partir de l'algorithme aléatoire Forêt avec 150 arbres, cela signifie qu'un grand nombre d'échantillons (97,1% environ) ont été correctement classés par notre système. Nous notons également que l'algorithme de SVM effectue des bons résultats avec une valeur de F-mesure égale à 0,963 et une valeur faible du taux FP égale à 0,042, cela est dû à la précision de la classification de l'algorithme SVM dans la classification de classe binaire. Nous avons réalisé une seconde expérience sur deux autres ensembles de données réelles, le premier contient uniquement les applications Android de type App classées dans neuf catégories qui signifie neuf classes, le deuxième jeu de données ne contient que les applications Android de type jeu classées en six catégories qui signifie six cl ânes donc le problème n'est plus une classification de classe binaire. Ces résultats montrent qu'il ya une diminution de la précision des classificateurs en particulier l'algorithme Naive Bayes et cela est dû au nombre de classes dans chaque jeu de données et aussi parce que les termes (texte) utilisés pour décrire ces applications ne sont pas différents, par exemple le terme « jeu ""est utilisé pour décrit les jeux qui appartiennent à différentes catégories de jeu comme puzzle, simulation et plus, cette distribution des termes plus plusieurs classes rapport résultats de la classification incorrecte. Bien que, l'algorithme Random Forest se comporte bien et mieux que tous les autres algorithmes et donne une bonne précision de la classification. - 430 -"
109,Revue des Nouvelles Technologies de l'Information,EGC,2017,Machine Learning for the Semantic Web: filling the gaps in Ontology Mining,"In the Semantic Web view, ontologies play a key role. They act as shared vocabulariesto be used for semantically annotating Web resources and they allow to perform deductivereasoning for making explicit knowledge that is implicitly contained within them. However,noisy/inconsistent ontological knowledge bases may occur, being the Web a shared and dis-tributed environment, thus making deductive reasoning no more straightforwardly applicable.Machine learning techniques, and specifically inductive learning methods, could be fruitfullyexploited in this case. Additionally, machine learning methods, jointly with standard reason-ing procedure, could be usefully employed for discovering new knowledge from an ontologicalknowledge base, that is not logically derivable. The focus of the talk will be on various ontol-ogy mining problems and on how machine learning methods could be exploited for coping withthem. For ontology mining are meant all those activities that allow to discover hidden knowl-edge from ontological knowledge bases, by possibly using only a sample of data. Specifically,by exploiting the volume of the information within an ontology, machine learning methodscould be of great help for (semi-)automatically enriching and refining existing ontologies, fordetecting concept drift and novelties within ontologies and for discovering hidden knowledgepatterns (also possibly exploiting other sources of information). If on one hand this means toabandon sound and complete reasoning procedures for the advantage of uncertain conclusions,on the other hand this could allow to reason on large scale and to to dial with the intrinsic uncer-tainty characterizing the Web, that, for its nature, could have incomplete and/or contradictoryinformation.",Claudia d’Amato,http://editions-rnti.fr/render_pdf.php?p1&p=1002262,http://editions-rnti.fr/render_pdf.php?p=1002262,en,"02-RNTI-E33 Machine Learning pour le Web sémantique: combler les lacunes dans les mines Ontologie Claudia d'Amato * * Université de Bari claudia.damato@uniba.it, http://www.di.uniba.it/~cdamato/ Résumé Dans l'affichage Web sémantique, ontologies jouent un rôle clé. Ils agissent comme des vocabulaires communs à utiliser pour les ressources Web sémantiquement annoter et ils permettent d'effectuer un raisonnement déductif pour faire la connaissance explicite qui est implicitement contenue en leur sein. Cependant, bruyant / incompatibles ontologique des bases de connaissances peuvent se produire, étant le Web un environnement partagé et bué dis-, rendant ainsi le raisonnement déductif plus applicable carrément. techniques d'apprentissage machine, et des méthodes d'apprentissage spécifiquement inductives, pourraient être également exploitées dans ce cas. De plus, les méthodes d'apprentissage de la machine, en collaboration avec la procédure de raisonne- ment standard, pourraient être utilement employés pour découvrir de nouvelles connaissances à partir d'une base de connaissances ontologique, qui ne sont pas logiquement dérivable. L'objectif de la conférence sera sur divers problèmes miniers de logie ontol- et sur la façon dont les méthodes d'apprentissage de la machine pourraient être exploitées pour y faire face. Pour ontologie minière, on entend toutes les activités qui permettent de découvrir les connaissances cachées à partir de bases de connaissances ontologique, en utilisant éventuellement seulement un échantillon de données. Plus précisément, en exploitant le volume de l'information au sein d'une ontologie, les méthodes d'apprentissage de la machine pourraient être d'une grande aide pour les (semi-) enrichir automatiquement et affiner ontologies, pour détecter la dérive du concept et des nouveautés dans les ontologies et de découvrir les modèles de connaissances cachées (peut-être aussi exploitant d'autres sources d'information). Si d'une part, ce moyen d'abandonner son et des procédures complètes de raisonnement pour l'avantage des conclusions incertaines, d'autre part, cela pourrait permettre de raisonner à grande échelle et de composer avec l'incertitude intrinsèque qui caractérise le Web, qui, pour sa la nature, pourrait avoir incomplète et / ou des informations contradictoires. Biographie Claudia d'Amato est un assistant de recherche à l'Université de Bari - Computer Science et Départe- ment elle a obtenu le Habilitation pour la fonction de professeur agrégé du secteur « 01 / B1 - Informatique » en Janvier 2014 (application: ronde 2012) . Elle a obtenu son doctorat en 2007 de l'Université de Bari, en Italie, en défendant la thèse intitulée « similarité Méthodes basées sur l'apprentissage pour le Web sémantique » pour lequel elle a obtenu la nomination comme auteur de l'un des - 3 - Meilleure thèse de doctorat italienne artificielle intelligence de la Com- mission intelligence artificielle italienne pour le prix AI * IA 2007. Elle a lancé la recherche sur les méthodes d'apprentissage pour la machine ontologie minière qui représente son principal intérêt de la recherche. Au cours de son activité de recherche, elle a remporté plusieurs prix du meilleur papier. Elle est membre du comité de rédaction du Journal Web sémantique et le Journal of Web Sémantique. Elle a / sert de président du programme à ISWC 2017, 2014 ESWC, vice-président à ISWC'09, Apprentissage piste Coprésidente à ESWC'12-'13- '16, président du Symposium de doctorat à ESWC'15 et atelier tutoriel coprésidente à ISWC'12, EKAW'12, ICSC'12. Elle a / sert en tant que membre du comité de programme d'un certain nombre de conférences internationales dans le domaine de l'intelligence artificielle, l'apprentissage automatique et sémantique Web tels que AAAI, IJCAI, OEEC, ECML, ISWC, WWW, ESWC. Elle a également été co-organisateurs de l'atelier international sur Inductive Raisonnement et Apprentissage sur le Web sémantique à ESCW'09-'11, l'incertitude internationale Atelier Raisonnement à ISCW'07-'11, l'Atelier international sur les données liées à l'extraction d'information à ISWC'13-15, l'Atelier international sur l'exploration de données sur les données liées à ECML / PKDD'13, l'Atelier international sur des données liées à la découverte de connaissances ECML / PKDD'15 et l'Atelier international sur la Croix-fertiliser diversifiée les domaines concernés et dans le Web sémantique à ISWC'15. Claudia d'Amato activité de recherche a été très apprécié au sein de la commu- nauté, comme documenté par son vaste réseau de coopérations académiques internationales, y compris, par exemple, l'INRIA, Sophia-Antipolis, France (Dr Fabien Gandon), Université de Landau Coblence, Allemagne (Prof. Dr. Steffen Staab), Université d'Oxford, Royaume-Uni (Prof. Dr. Thomas Lukasiewicz), Université de Poznan, Pologne (Dr Agnieszka Lawrynowicz), FBK, Trento, Italie (Dr. Luciano Serafini). Claudia d'Amato activité de recherche a été diffusée à travers 17 articles de revues, 12 chapitres de livres, 53 documents dans les collections internationales, 27 documents dans les procédures de l'atelier international et 13 articles dans les procédures de la conférence nationale et atelier. Elle a édité 27 livres et procédures et 3 revue les questions spéciales. - 4 -"
115,Revue des Nouvelles Technologies de l'Information,EGC,2017,PORGY : a Visual Analytics Platform for System Modelling and Analysis Based on Graph Rewriting,"PORGY est un environnement interactif utilisé pour la modélisationde systèmes obtenus àpartir de règles de réécriture, pilotés à l'aide de stratégies et basées sur des graphes utilisantdes noeuds à ports. Cette démonstration présente quelques uns des aspects de visualisation ana-lytique proposés par PORGY. Cette dernière facilite la modélisation du système, sa simulationainsi que l'analyse des résultats à différentes échelles.","Bruno Pinaud, Oana Andrei, Maribel Fernández, Hélène Kirchner, Guy Melançon, Jason Vallet",http://editions-rnti.fr/render_pdf.php?p1&p=1002327,http://editions-rnti.fr/render_pdf.php?p=1002327,en,"69-RNTI-E33 PORGY: une plate-forme visuelle Analytics basé sur le système de modélisation et d'analyse sur le graphique Réécriture Bruno Pinaud *, Oana Andrei **, Maribel Fernández *** Hélène Kirchner ****, Guy Melançon *, Jason Vallet * * Université de Bordeaux, LaBRI, France, Ecole {prénom.nom}@u-bordeaux.fr ** de l'informatique, Université de Glasgow, Royaume-Uni *** king College de Londres, Royaume-Uni **** Inria, France, helene.kircher @ inria.fr Résumé. PORGY est un environnement visuel pour la modélisation basée sur des règles basée sur les graphes des ports et des règles de réécriture de graphe de port dont l'application est pilotée par des stratégies de réécriture. L'objectif de cette démonstration est les caractéristiques visuelles et interactives offertes par PORGY, qui facilitent une approche exploratoire pour modéliser, simuler fin et analyser les différentes façons d'appliquer les règles lors de l'enregistrement de l'évolution du modèle, ainsi que le suivi et de traçage des paramètres du système. 1 Introduction Nous proposons PORGY 1 (Fernández, Kirchner et Pinaud, 2016) un cadre de modélisation visuelle générale (Fig. 1) sur la base de réécriture de graphes (réécriture ou graphique) pour les systèmes complexes. PORGY est basé sur l'utilisation des graphiques de port avec des attributs pour représenter les états du système. Dans un graphe de port, les bords se connecter à des noeuds à des points spécifiques, appelés ports. Des noeuds, des ports et des arêtes décrivent les composants du système et de leurs relations, tandis que les attributs encapsulent les valeurs de données associées à chaque entité. Nous utilisons des transformations graphiques basées sur des règles de réécriture de graphe de port pour décrire l'évolution du système. transformations graphiques sont généralement spécifiées au moyen de règles (Ehrig, Engels, Kreowski et Rozenberg, 1997) et ont été mises en œuvre dans une variété d'outils de modélisation, par exemple, BioNET Gen (Faeder, Blinov et Hlavacek, 2009) ou (RuleBender Smith, Xu, Sun, Faeder et Marai, 2012). Ces outils intègrent la visualisation à la modélisation et à la simulation de modèles chimiques à base de règles bio- mettant l'accent sur l'exploration visuelle modèle et l'exécution intégrée des simulations. Les Etats sont représentés par des graphiques décrivant les composants du système; leurs teractions in- sont définies par des règles régies par les constantes de vitesse associées, qui déterminent la fréquence à laquelle les règles sont applicables. BioNetGen utilise explicitement la structure des graphes de port, tandis que les autres outils utilisent des structures à base de graphes avec des étiquettes. D'une manière générale, les règles de réécriture de graphe de port sont des représentations graphiques des transformations du système, ils offrent ainsi un mécanisme direct, visuel pour observer portement BE- du système. En plus des graphiques de port et les règles de réécriture de notre approche de modélisation comprend des expressions de straté- gie pour diriger des applications de règles. Les stratégies permettent d'utiliser les opérateurs de combiner le graphique 1. http://porgy.labri.fr - 473 - PORGY: Visual Graph Réécriture Modélisation et analyse de la plate-forme figure. 1 - Vue d'ensemble PORGY: (1) la modification d'un graphique; (2) la modification d'une règle; (3) toutes les règles disponibles; (4) l'arbre de dérivation, une trace complète de l'histoire de calcul; (5) la modification d'une stratégie. règles de réécriture, ainsi que les opérateurs de définir l'endroit où les règles devraient ou ne devraient pas appliquer. Souvent, plus d'une transformation est possible à un état donné, auquel cas au lieu d'une seule étape de transformation, on peut avoir plusieurs alternatives à choisir, et à son tour, générer plusieurs séquences différentes de transformations. Les différentes séquences de transformation sont organisées en une structure arborescente, que nous appelons Derivation Tree (DT). Afin de soutenir les tâches liées à l'étude d'un système de réécriture de graphe, PORGY fournit des installations pour voir chaque composant en même temps (règles, stratégie, tout état du graphique réécrite, DT), pour effectuer la réécriture à la demande (stratégie ou à base fondé sur des règles) avec des mécanismes de glisser-déposer, pour synchroniser les différents points de vue de suivre l'évolution des propriétés du sys- tème, d'explorer un DT avec toutes les dérivations possibles à différentes échelles, pour suivre le processus de réécriture dans l'ensemble DT ou de tracer l'évolution d'un paramètre choisi. 2 Th e PORGY Modèle La conception ou PORGY est inspiré à l'origine de l'étude des systèmes biochimiques, c'est la raison pour laquelle nous détaillons l'utilisation de PORGY pour modéliser un réseau biochimique. Cependant, nous ré-cemment PORGY utilisé comme outil d'analyse visuelle pour comparer les modèles de propagation dans les travaux Net- sociaux (Fernández, Kirchner, Pinaud et Vallet, 2016). Les éléments graphiques comme port. Dans un graphe de port, les noeuds ont des points de connexion explicites appelées ports, les bords sont fixés aux ports; les noeuds, les ports et les bords sont marqués par des ensembles d'attributs. Nous représentons chacun des éléments de modèle (protéines) comme un noeud dont les ports représentent des sites de liaison qui peuvent être liés à d'autres sites de liaison. Interactions (réactions) que les règles de réécriture. Une règle de réécriture L ⇒ R est constitué de deux sous-graphes, L et R, reliés entre eux à un noeud particulier (⇒) qui code pour la correspondance entre les - 474 - B. Pinaud et al. les éléments de L et R (figure 1, tableau 2). Soit G un graphe de port tel qu'il y est un graphique de port morphisme g de L à G. En remplaçant le sous-graphe g (L) de G par g (R) et sa connexion avec le reste du graphe, on obtient un diagramme de port G 'représentant un résultat d'une étape de réécriture de G en utilisant L ⇒ R. la réécriture est intrinsèquement non déterministe depuis plusieurs sous-graphes d'un graphe de port peuvent être réécrites sous un ensemble de règles. Stratégie pour la règle d'application. Une stratégie consiste soit en une règle ou d'une composition de ateurs opéra- plus d'un ensemble de règles. applications de règles peuvent être régies par des probabilités. Au-delà des différents choix pour l'application de la règle, de nombreux autres choix doivent être faits pour contrôler la réécriture: choisir où appliquer une règle dans un graphique, définir une séquence de règles qui sont corrélées, itérer une règle ou une séquence de règles, etc. langue pleine de stratégie est présentée dans Fernández et al. (2016). Et Dérivations Derivation Tree. Une dérivation est une séquence d'étapes de réécriture. Chaque étape implique l'application d'une règle à une position spécifique dans le graphique. La navigation le long permet de comprendre tions comment instruments dérivés à un état spécifique a été atteint. En général, plusieurs dérivations sont possibles à partir de tout état, donnant lieu à la notion d'arbre de dérivation DT (Fig. 1, Tableau 4). 3 Expérimentation et l'analyse PORGY a été conçu avec le mantra de la recherche d'information visuelle de Shneiderman (1996) à l'esprit: Vue d'ensemble d'abord, le zoom et le filtre, détaille ensuite la demande. PORGY est construit sur le dessus du cadre de visualisation open-source TULIP 2 comme un ensemble de 11 modules externes TULIP C +. Vue d'ensemble d'abord. Pour comprendre le comportement des systèmes non-déterministe, il est souvent utile d'exécuter plusieurs fois le même programme de réécriture sur la même entrée pour voir les variations potentielles. Ceux-ci peuvent être considérées comme des branches du DT. Bien qu'il soit souvent une grande structure de données, il fournit une représentation indexée de l'évolution du système dans lequel chaque noeud représente un état du système, et un bord est l'application d'une règle ou d'une stratégie. PORGY nous permet d'analyser l'arbre de dérivation et de travailler avec elle à différents niveaux. Par exemple, les petits multiples (SMs) permettent de voir les états de graphique consécutifs comme une bande dessinée. Zoom et filtre. On peut être intéressé à tracer l'évolution d'un paramètre calculé à partir de chaque état intermédiaire. Un diagramme de dispersion interactif peut être construit comme sur la Fig. 2. En outre, toutes les vues graphiques sont synchronisés. Par exemple, si certains points intéressants sont sélectionnés à l'intérieur du nuage de points, ils sont également sélectionnés immédiatement à l'intérieur de la branche correspondante de l'arbre de dérivation. La synchronisation est également valable pour les éléments graphiques. Détails sur demande. Nous pouvons étudier plus les noeuds sélectionnés en effectuant un zoom et de voir distinctement les graphiques. Faisant passer le curseur de la souris sur un bord permet de voir quels éléments ont été modifiés par l'application de la règle. Les éléments modifiés sont mis en évidence dans l'image, pour afficher clairement ceux qui ont évolué. 4 Conclusion Nous avons illustré quelques caractéristiques principales de PORGY, un open-source général but Elling et analyse mo- ENVIRONNEM t. versions spécifiques au domaine de PORGY peuvent être facilement mented en œuvre par l'extension ou affiner les caractéristiques présentées ici. 2. http://tulip.labri.fr - 475 - PORGY: Visual Graph Réécriture Modélisation et analyse de la plate-forme figure. 2 - Un diagramme de dispersion montrant l'évolution de la concentration en protéines SA et l'arbre de dérivation corres- pondant. Les points sélectionnés dans le diagramme de dispersion (en bleu) sont automatiquement sélectionnés dans l'arbre de dérivation. Références Ehrig, H., G. Engels, H.-J. Kreowski et G. Rozenberg (1997). Handbook of Graph Gram- mars et en informatique par Graph Transformations, Volume 1-3. Scientifique mondiale. Faeder, J., M. Blinov, et W. Hlavacek (2009). Basée sur des règles de modélisation des systèmes avec Biochemical BioNetGen. Dans I. V. Maly (Ed.), La biologie des systèmes, Volume 500 des méthodes de biologie moléculaire, pp. 113-167. Humana Press. Fernández, M., H. Kirchner, et B. Pinaud (2016). Port stratégique Graphique Rewriting: Un inter-actif et modélisation Cadre d'analyse. Rapport de recherche, Inria. Fernandez, M., H. Kirchner, B. Pinaud et J. Vallet (2016). Labellisé Graphique Réécriture rencontre les réseaux sociaux. Dans RTA'16, Volume 9942 de LNCS, pp. 1-25. Springer. Shneiderman, B. (1996). Les yeux l'ont: Une tâche par la taxonomie de type de données pour la visualisation d'information. Dans Proc. de la Symp IEEE. sur les langues visuelles, pp. 336-343. Smith, A. M., W. Xu, Y. Sun, J. R. Faeder et G. Marai (2012). RuleBender: eling intégré mo-, la simulation et la visualisation pour la biochimie intracellulaire fondé sur des règles. BMC Bioinfor- Matics 13 (8). Résumé PORGY un environnement is used for the interactif modélisationde à partir Systèmes de obtenus rules de Réécriture, PILOTES à l'aide-ports de Stratégies et Graphes des Sur basées des Nœuds Ë Utilisant. This démonstration des UNS Présente aspects Quelques de visualisation par ana- lytique PORGY propose. This la Dernière Facilite du Système modélisation, sa simulation l'analyse Que AINSI des à Résultats Différentes échelles. - 476 -"
142,Revue des Nouvelles Technologies de l'Information,EGC,2017,"“Engage moi”: From retrieval effectiveness, user satisfaction to user engagement","The effective prediction of a click remains a primary challenge in the areas of search, digitalmedia and online advertising. In the context of search, satisfying a userâ&#728;A ´ Zs information needby returning results that they will click on is an important objective in any information retrievalsystem. Consequently, information retrieval systems have had a long and varied history of howto evaluate their effectiveness of responding to a given query. However, building such a systemthat not only only returns relevant results to a user query but also encourages a long-termrelationship between the user and the system is far more challenging. In this talk, we reviewthe current state-of-the-art evaluation approaches for search before exploring other ways ofquantifying more long-term engagement measures. Finally, the talk ends with a proposal ofhow the two approaches can be considered together to create a service that optimises for thequery and the longer term engagement aspects.",Mounia Lalmas,http://editions-rnti.fr/render_pdf.php?p1&p=1002261,http://editions-rnti.fr/render_pdf.php?p=1002261,en,"01-RNTI-E33 « Engagez moi »: De efficacité de la recherche, la satisfaction des utilisateurs à la participation des utilisateurs Mounia Lalmas * * Yahoo Labs, Londres nia@acm.org, http://www.dcs.gla.ac.uk/~mounia/ Résumé La prédiction efficace d'un clic reste un défi majeur dans les domaines de la recherche, des médias numériques et de la publicité en ligne. Dans le contexte de la recherche, la satisfaction d'un besoin d'information sur les résultats userâĂŹs par retour qu'ils cliqueront est un objectif important dans tout système de récupération de l'information. Par conséquent, les systèmes de récupération d'information ont eu une histoire longue et variée de la façon d'évaluer leur efficacité de répondre à une requête donnée. Cependant, la construction d'un tel système qui renvoie non seulement que des résultats pertinents à une requête de l'utilisateur, mais encourage également une relation à long terme entre l'utilisateur et le système est beaucoup plus difficile. Dans cet exposé, nous passons en revue l'état actuel de l'art aux méthodes d'évaluation de la recherche avant d'explorer d'autres moyens de quantification plus des mesures d'engagement à long terme. Enfin, les extrémités de parler avec une proposition de la façon dont les deux approches peuvent être envisagées pour créer un service qui optimise pour la requête et les aspects de l'engagement à long terme. Biographie Mounia Lalmas est un directeur de recherche à Yahoo Londres où elle dirige une équipe de scientifiques travaillant sur les sciences de la publicité. Elle détient également un professeur honoraire au Uni- versité College de Londres. Ses travaux portent principalement sur l'étude la participation des utilisateurs dans des domaines tels que la publicité native, les médias numériques, les médias sociaux et la recherche. Elle poursuit également la recherche dans les médias sociaux et la recherche. Avant cela, elle a occupé une chaire de recherche Microsoft Research / Raeng à l'École des sciences informatiques, Université de Glasgow. Avant cela, elle était professeur de recherche d'information au Département des sciences informatiques à l'Université Queen Mary de Londres. Elle a co-dirigé l'Initiative d'évaluation pour XML de recherche (INEX), un projet de grande envergure avec plus de 80 organisations participantes dans le monde entier, qui était chargé de définir la nature de la récupération XML, et comment il doit être évalué. - 1 -"
143,Revue des Nouvelles Technologies de l'Information,EGC,2016,A Relevant Passage Retrieval and Re-ranking Approach for Open-Domain Question Answering,"Les systèmes de questions-réponses (SQR)s visent à retourner directement des réponsesprécises à des questions posées en langage naturel. L'extraction et le reclassement des passagessont considérés comme les tâches les plus difficiles dans un SQR typique et exigent encore uneffort non trivial. Dans cet article, nous proposons une nouvelle approche pour L'extraction etle reclassement des passages en utilisant les n-grammes et SVM. Notre système d'extractionde passages basé sur la technique des n-grammes repose sur une nouvelle mesure de similaritéentre un passage et une question. Les passages extraits sont ensuite réordonnés en utilisant unmodèle basé sur RankSVM combinant différentes mesures de similarité afin de retourner lepassage le plus pertinent pour une question donnée. Nos expériences et nos résultats étaientprometteurs et ont démontré que notre approche est concurrentielle.","Nouha Othman, Rim Faiz",http://editions-rnti.fr/render_pdf.php?p1&p=1002161,http://editions-rnti.fr/render_pdf.php?p=1002161,en,"Un passage pertinent et récupération Re rang approche pour la question ouverte de domaine Répondre Nouha Othman *, Rim Faiz ** * Université de Tunis, l'Institut Supérieur de Gestion de Tunis, LARODEC, Tunisie othmannouha@gmail.com ** Université de Carthage Carthage IHEC , LARODEC, Tunisie rim.faiz@ihec.rnu.tn~~V~~singular~~3rd Résumé. Question systèmes répondant (QAS) s visent à revenir directement des réponses précises aux questions en langage naturel. Récupération et passages de re-classement sont considérés comme les plus difficiles tâches dans un QAS typique et requièrent un effort non négligeable. Dans cet article, nous proposons une nouvelle approche pour la récupération et RE- passages classement à l'aide de n-grammes et SVM. Notre moteur n-gramme à base de récupération passage repose sur une nouvelle mesure de similarité entre un passage et une ques- tion. Les passages récupérés sont encore re-classé en utilisant un modèle de classement SVM combinant différentes mesures de similarité de texte afin de retourner le passage le plus pertinent à une question donnée. Nos expériences et les résultats ont montré des résultats prometteurs et démontré que notre approche est compétitive. 1 Introduction Au cours des dernières années, avec le développement continu de la technologie de l'information, la quantité de données a augmenté massivement tous les jours. De ce fait, ces dernières années ont vu un intérêt croissant pour la réponse aux questions (QA) qui est l'un des principaux domaines de recherche en information de recherche (IR) avec des applications principalement à partir des informations d'extraction (IE) et le traitement du langage naturel (NLP) (Faiz, 2006) , dont l'objectif principal est de fournir directement une réponse précise et concise à une question posée par l'utilisateur en langage naturel à partir d'une importante collection de documents ou de base de données (Voorhees, 2001). En effet, le domaine QA se divise en deux catégories: QA fermé domaine qui traite des questions sur un domaine spécifique comme la biologie et la médecine, et QA domaine ouvert qui traite des questions d'ordre général dans divers domaines sans aucune limitation. Dans notre travail, nous nous concentrons sur la deuxième catégorie que les techniques utilisées ne sont pas adaptées vers un domaine spécifique. Fondamentalement, un QAS typique peut être largement considéré comme un pipeline qui se compose de quatre modules principaux (Tellex et al., 2003): analyse de la question, la recherche documentaire, la récupération de passage et l'extraction de réponse, où chaque module doit faire face à des défis spécifiques. En particulier, la récupération de passage (PR) est toujours considéré comme la tâche principale d'un QAS typique car elle permet de réduire l'espace de recherche d'une grande collection de documents à un nombre fixe de passages. De toute évidence, SQR ne peut pas trouver la bonne réponse à une question donnée, à moins que la réponse existe dans l'un des passages récupérés. Par conséquent, il a été largement étudié au cours des dernières années. passages Re rang est aussi une tâche cruciale à la fin du gazoduc QAS qui aspire à commander les passages récupérés tels que les plus pertinents - 111 - Un passage pertinent et récupération approche resitue classement pour la question les Answering apparaissent en premier. Dans le but d'améliorer la performance des existants SQR, nous nous concentrons sur ces deux tâches pour tenter d'augmenter le nombre de réponses correctes et retournées assurer leur pertinence. Dans cet article, nous proposons une nouvelle approche pour récupérer et re-classement des passages pour un domaine ouvert QAS. Notre passage module de recherche est basé sur un modèle n-gramme où nous proposons une nouvelle mesure de similarité entre un passage et une question en fonction du degré de proximité ou d'une dispersion des mots n-grammes de la question dans le passage. Les passages récupérés sont encore re-classé en utilisant un modèle SVM Classement (Joachims, 2002) combinant différentes mesures de similarité de texte qui constituent les caractéristiques. Ceux-ci comprennent notre nouvelle mesure à base de n-gramme, ainsi que d'autres caractéristiques lexicales et sémantiques qui ont déjà été éprouvées avec succès dans la tâche sémantique Justifications similarité (STS) à * SEM 2013 (Buscaldi et al., 2013). Nous l'intention de retourner le passage le plus pertinent, le premier rang un, comme une réponse pertinente à une question donnée. Le reste de cet article est organisé comme suit: Dans la section (2), nous donnons un aperçu des travaux connexes sur les tâches PR et le classement. Ensuite, nous présentons dans la section (3) notre approche proposée pour la récupération et re-classement des passages pour l'AQ. Section (4) est consacré à l'évaluation de notre méthode afin de prouver sa capacité. Dans la section (5) nous concluons notre article et présentons quelques perspectives. 2 Travaux connexes sur Passage récupération et tâches de classement Dans cette section, nous passons en revue les approches les plus importantes de la littérature liée à PR classées par type de correspondance appliquée entre les passages candidats et la question, et les principaux travaux proposés sur le classement pour la sélection des réponses classées selon le modèle appliqué. 2.1 Types de contrepartie pour récupération Passage En effet, PR est généralement considéré comme le noyau d'un QAS typique. Ainsi, de nombreuses approches ont été développées dans le but de relations publiques pour améliorer les performances de SQR. Tellex et al. (2003) ont réalisé une évaluation quantitative des différents algorithmes de relations publiques pour AQ basées sur la correspondance lexicale où il a déduit que les algorithmes les plus proposés traitent chaque terme de la question comme un symbole indépendant et ne considèrent pas l'ordre des termes et leurs relations de dépendance. Afin de remédier à cette lacune, il y a eu plusieurs tentatives pour prendre en compte les dépendances à long terme. En outre, plusieurs ouvrages reposent sur la comparaison syntaxique comme (Cui et al., 2005) découlant des relations syntaxiques pour correspondre à des questions avec des passages. Néanmoins, la principale limite de filtrage syntaxique est la nécessité d'un analyseur syntaxique qui n'est pas disponible pour tou- jours toutes les langues. Ajouté à cela, il faut l'adaptation et la performance du SQR dépend en grande partie sur la performance de l'analyseur. D'autres travaux ont été basées sur la correspondance sémantique tels que (Ofoghi et Yearwood, 2009). Bien qu'ils permettent de dé- tect de vraies réponses, ils ont besoin de ressources sémantiques qui ne sont généralement pas disponibles dans toutes les langues et ne couvrent pas tous les domaines, ni ni tous les termes. En outre, il y a eu d'autres plusieurs travaux combinant à la fois des techniques sémantiques et syntaxiques dans le contexte de relations publiques. Sinon, certains travaux ont eu recours à un modèle différent pour récupérer des passages allant au-delà de ce qui précède simple, correspondant lexical. Ces derniers comptaient sur n-grammes qui se réfère à des séquences de mots extraits consécutifs d'un texte donné, comme un outil puissant et rapide qui ne traite pas avec des termes comme symboles indépendants, mais il prend en compte la dépendance entre les simples. Dans ce - 112 - Contexte N. Othman et R. Faiz, Radev et al. (2005) ont proposé une méthode probabiliste pour le langage naturel basé sur le Web QA, où les documents web sont segmentés dans des passages qui seront classés en utilisant un score de n-gramme basé sur la pondération tf-idf. En outre, Correa et al. (2010) ont développé un système PR pour l'AQ basé sur une mesure de similarité n-grammes favorisant les passages contenant plus requête n-grammes et les plus de comparer les passages candidats extraits par la technique des mots-clés. En outre, Buscaldi et al. (2010) ont suivi la même approche précédente, mais la seule différence est sur le modèle n-gramme appliqué qui considère le passage n-grammes existant dans la question et leur proximité. Notre méthode pour récupérer des passages est différent de ceux à base de n-gramme ci-dessus, nous avons mis davantage l'accent sur n-grammes communs entre la question et le passage et nous procédons différemment pour extraire les n-grammes. En effet, nous introduisons une nouvelle similitude me- sûr calculée à partir des poids des n-grammes question. Nous construisons un poids de passage au total en naviguant sur la question n-grammes, en gardant le poids d'un n-gramme si elle est complètement trouvée dans le passage et réduire ce poids si elle est divisée en petits n-grammes dans le passage. 2.2 Approches de classement pour réponse de sélection L'une des étapes clés à la fin de la canalisation QAS est la sélection de réponse qui commence par le classement des passages candidats fournis par le module PR. Étant donné une requête qui exprime la question d'un utilisateur, la fonction de classement des ordres les passages récupérés tels que le mo st ceux qui sont pertinents à la requête donnée apparaissent en premier. Ainsi, plusieurs modèles de classement ont été proposées pour résoudre ce problème dans l'assurance qualité. En fait, certains travaux invoqués modèles basés sur la connaissance de rang tels que les sages passions (Bilotti et al, 2010;. Araki et Callan, 2014) où les connaissances en question et des réponses telles que l'entité nommée (NE) présente, est représentée comme un graphe d'annotation et les inférences sont tirées de la base de connaissances. Néanmoins, la principale limitation de la plupart des approches fondées sur le savoir est qu'ils comptent sur un grand nombre de déductions et manuel concep- tion de fonctions. En outre, la cartographie et prédicats correspondants et arguments aux relations du modèle reste une tâche très complexe. D'autres travaux mis en place des approches basées sur des modèles aussi appelé modèles qui ne sont plus d'un ensemble de règles d'évaluation de contexte prédéfinis conçus et extraits de la question et les passages candidats et ils sont souvent appliqués en fonction du type de question. Par exemple, Severyn et al. (2013) appliquée à des modèles d'apprentissage de rang pour apprendre automatiquement les modèles complexes, par exemple, l'apprentissage des structures sémantiques relationnelles qui apparaissent dans les questions ainsi que leurs passages. Cependant, la plupart des approches à base de motifs sont très complexes et nécessitent beaucoup de données de formation. Dans le cas contraire, certains travaux ont utilisé le contexte des mots comme simple intuition pour les passages de classement tels que (Toba et al, 2010;.. Yen et al, 2013). Dans ce dernier ouvrage, les auteurs ont proposé un modèle nommé (CRM) qui re-classe les passages récupérés à l'aide contextuelle classement de l'information contextuelle des noms propres, des motifs syntaxiques, traits sémantiques pour chaque mot dans la fenêtre contextuelle ainsi que la position de mot pour prédire si un passage candidat donné est pertinent pour le type de question. Cependant, la performance du CRM dépend fortement de la classification des questions. Il est intéressant de noter que notre modèle re-classement passage est largement inspiré de cette dernière approche basée sur SVM qui est un modèle d'apprentissage VISED super qui était dans la plupart des cas appliquées avec succès pour le classement passage comme dans (Moschitti et Quarteroni, 2011). Néanmoins, nous avons recours à modèle Classement SVM au lieu de SVM, qui intègre un ensemble de caractéristiques différentes de celles utilisées dans (Yen et al., 2013). Contrairement à ce dernier, nous sommes contraints ni par mots NE ni par une fenêtre de taille fixe. Nous sommes allés au-delà d'une simple détection de la classe NE, la forme et la partie étiquette partie du discours du mot à calculer lexical, des mesures de similarité sémantique et n-gramme. - 113 - Un passage pertinent de récupération et approche Re-classement pour la question Réponse 3 Approche proposée L'idée de base de notre approche est d'abord réduire l'espace de recherche en utilisant n-grammes pour récupérer les 10 premiers passages qui sont les plus susceptibles de répondre à la question de l'utilisateur . Le nombre de passages retournés est fixé à 10 comme la plupart de l'état des systèmes de relations publiques d'art prennent des valeurs proches de 10. Toutefois, étant donné que la structure n-grammes repose uniquement sur une simple dépendance entre les termes, il ne suffit pas de garantir une grande pertinence. Par conséquent, nous avons tendance à re-rang les passages récupérés à l'aide d'un modèle SVM classement qui combine des mesures supplémentaires de similarité lexicale et sémantique pour revenir le passage le plus pertinent, le premier rang un, pour répondre à la question d'entrée. Dans cette section, nous présentons notre approche pour récupérer et passages de re rang nommé PRR qui est essentiellement composé de trois modules principaux: analyse de la question, relations publiques et re rang passage. La figure 1 représente l'architecture globale de l'approche proposée PRR. Dans ce qui suit, nous détaillons ses différents constituants. FIGUE. 1 - Architecture globale de notre approche 3.1 PRR Question Module d'analyse L'objectif de l'analyse de la question est de générer une requête formelle par prétraiter la question saisie par l'utilisateur et l'extraction des termes utiles. Cette requête est générée en appliquant le nettoyage texte, suppression des mots interrogatifs, tokens, arrêt des mots retrait et à endiguer. La requête sera formellement définie comme suit: Q = {t1, t2, ..., tq} où t désigne un qu séparé terme ERY et q représente le nombre de termes dans la requête. 3.2 Passage Retrieval Module Fondamentalement, notre module PR est composé de deux éléments principaux: extraction de passage des candidats et d'extraction de passage pertinent. Ce paragraphe est consacré à les détailler. - 114 - N. Othman et R. Faiz 3.2.1 Passage du candidat Extraction d'abord, nous indexer la collection en coupant les documents dans les paragraphes nommés pas- sages. Pour chacun, nous enregistrons les informations associées, comme son identifiant, le nom du document, le numéro et le texte. Ensuite, on extrait les termes et éliminons les mots vides. Découlant est également appliquée pour faciliter la recherche et l'indexation des mots. Un passage est formellement défini comme suit: P = {t1, t2, ..., tp} où t représente une durée distincte du passage P et p désigne le nombre de termes de passage. Par la suite, nous donnons une fréquence à chaque terme de passage dans le passage afin de calculer la fréquence maximale et les poids à long terme. Une fois que les passages sont indexés, leurs termes seront stockés dans l'index inversé. Afin de calculer le poids des termes de la requête, nous avons re triés à la formule employée dans (Correa et al., 2010) qui ne tient pas compte de la fréquence des mots, mais il ne prend en compte leur pouvoir discriminant entre les passages. Que pour identifier les passages qui contiennent au moins un des termes de la requête, nous avons juste besoin de chercher les termes de la requête dans l'index inversé, où pour chaque terme, la liste des passages connexes est enregistrée et prendre l'intersection de ces passages. Les passages candidats nommés Pc sont définis comme bas vantes: Pc = {P1, P2, ..., Pn} où Pi est un passage candidat et n est le nombre de passages candidats. Notez que le poids d'un passage de termes candidats est calculé par la même manière que les termes de la requête. Pour filtrer les passages candidats, on calcule la similitude entre chaque passage des candidats et la question en utilisant la mesure de similarité suivante que les mots de Siders ne con- en commun entre la requête et le passage: s (p, q) = Σ ti∈P ∩Q w (ti, q) Σ ti∈Q w (ti, q). Les passages candidats sont ensuite classés en fonction de leurs valeurs de similarité. Ainsi, le nombre de passages candidats (n) est réduit à (nb). L'ensemble des passages candidats renvoyés sera réglé sur: Pc = {P1, P2, ..., Pnb} Nous devons réduire la complexité globale du système en termes de temps et de l'espace et permettre une analyse plus approfondie qui n'a pas été possible à l'avance en raison de la taille énorme de collec- tion. Par conséquent, le nb de nombre devrait être en essayant de trouver un juste milieu entre un grand et un petit nombre afin de réduire la complexité du système, mais sans exclure des passages qui peuvent contenir la réponse et sont mal classés. Par exemple, nous pouvons définir le nb de nombres à 100 Ensuite, nous allons appliquer un filtre aux passages candidats nb en utilisant des structures n-gramme afin d'en extraire ceux qui sont pertinents. 3.2.2 Passage pertinent Extraction Jusqu'à présent, les passages candidats renvoyés par l'étape précédente ont seulement un petit nombre de mots en commun avec la question. Néanmoins, ce critère n'est pas assez bon pour juger de la pertinence d'un passage. Il est donc nécessaire d'aller au-delà d'une simple vérification des occurrences de mots. Dans ce contexte, nos objectifs de méthodologie proposée pour exploiter d'autres critères de sélection tels que la présence de séquences de mots, leur longueur, leur dépendance, etc. A cet effet, nous utilisons la technique n-gramme qui assure la dépendance simple entre les termes, la réduction de l'ambiguïté, la langue l'indépendance et il peut non seulement faire face à une énorme quantité de données, mais aussi avec son hétérogénéité. Notre méthodologie pour extraire les passages pertinents se compose des éléments suivants: N-gramme Génération Nous avons d'abord identifier les termes communs entre une question et un passage donné et ensuite nous construisons le vecteur - → Tc des termes communs entre la question et le passage. Nous avons juste besoin de parcourir les termes de la question et de contrôle pour chacun d'eux - 115 - Un passage pertinent et récupération approche resitue classement pour la question Répondre si elle est un terme du passage pour l'ajouter dans le vecteur. Cette dernière est définie par: - → Tc   t1 P1Q [p11, .., P1M] t2 P2Q [p21, .., P2M] .. .. tn PNQ [PN1, .., PNM]    où ti est le terme ième en commun entre la question et le passage et i = {1..n}. n désigne le nombre des termes de question, PIQ représente la position du terme i dans la question, JIP est la position j du terme ième dans le passage et j = {} 1..m. m est le nombre de termes dans le passage.Thereafter, nous construisons les vecteurs n-gramme du vecteur question --- → QNG et le passage --- → NGP en naviguant sur le vecteur - → Tc et le regroupement des termes ayant des positions successives dans la question et dans le passage. N-gramme Pondération Le poids de chaque n-gramme de la question est calculée sur la base de sa longueur et la somme de ses pondérations à long terme conformément à: w (QNG) = l × Σti∈terms (QNG) w (ti, q) où l est le nombre de termes contenus dans la question n-gramme (QNG). En effet, la multiplication de la somme des poids de la longueur n-gram peut favoriser des mots adjacents plutôt que des indépen- dants. De toute évidence, les mots sont regroupés plus significatifs et moins ambigu que les mots séparés. Donc, il est raisonnable de donner un mot indépendant un poids plus grand quand il appartient à un n-gramme. En ce qui concerne le passage, les n-grammes sont pondérés en fonction de leur degré de similitude avec les n-grammes de la question. Nous attribuons un poids cumulé au passage en parcourant les n-grammes de la question et à chaque n-gramme, soit est son poids tout ou une plus faible est ajouté à la masse du passage. Plus précisément, si un n-gramme de la question se trouve dans le passage, son poids sera ajouté au poids cumulatif, mais si le n-gramme est divisé en petits n-grammes dans le passage d'un poids inférieur sera ajouté au poids cumulé. Ce poids inférieure est fixée en fonction du nombre des petits n-grammes. Il est à noter qu'il y a trois cas possibles: - Cas 1: la requête n-gramme est l'une des n-grammes passage: ngqì = ngPj, ngPj ∈NGP - Cas 2: La requête n-gramme est faite en combinant un certain nombre n du passage n-grammes: ngqì = ∪ PNG, PNG ∈ NGP - Cas 3: la requête n-gramme est inclus dans l'un des passages n-grammes: ngqì ∈ ngPj, ngPj ∈ NGP soit w le poids d'add au passage lorsque nous parcourons à travers la question n-grammes QNG. Dans les cas 1 et 3, NGQ existe dans le passage, de sorte que le poids supplémentaire w est calculé selon la formule suivante: w (PNG) = w (QNG) = l × Σti∈terms (QNG) w (ti, q) où l est la longueur du n-gramme QNG et w (ti, q) est le poids de sa durée ti. Dans le cas 2, QNG est divisé en sous-n-grammes dans le passage, que SNG être le nombre de ceux-ci. Dans ce cas, le poids supplémentaire w a la valeur: w (PNG) = w (QNG) GNS = l SNG × Σ ti∈terms (QNG) w (ti, q) Psim de similarité mesurer notre mesure de similarité de passage appelée Psim entre un passage et une question ne dépasse pas le rapport entre le poids du passage et celui de la question. Nous commandons les passages en fonction des valeurs de cette mesure afin de ramener ceux qui ont les valeurs les plus élevées. Le poids d'un passage est cumulatif comme mentionné ci-dessus. Elle est la somme des poids partiels calculés à chaque étape à ajouter à la masse totale de passage. A - 116 - N. et R. Othman Faiz correspond pas à pas de calcul à la vérification de l'apparition d'un n-gramme de la question dans le passage sous-jacent et le poids est donnée par la formule 1 suivante: w (P) = qΣ i = 1 × Σ li SNGI t∈terms (ngqì) w (t, q) (1) dans laquelle q désigne le numéro de la question n-grammes. En ce qui concerne le poids de la question, il est calculé selon la formule 2: w (Q) = l (Q) × Σ ti∈ (Q) w (ti, q) (2) où L (Q) est le nombre de les termes d'interrogation et w (ti, q) est le poids d'un terme d'interrogation. w (Q) est le même que le poids d'un n-gramme calculé en utilisant la formule 3.2.2 lorsque la QNG n-gramme est la question. l = l (Q) est le nombre de termes d'interrogation et SNG = 1 étant donné que toutes les conditions sont regroupées et forment un uni-gramme. Une fois eu la poids de la question et le poids de chaque passage, on calcule la mesure de similarité qui est fixée à la formule 3: Psim (p, q) = w (P) w (Q) = Σqi = 1 × li SNGI Σt∈terms (ngqì) w (t, q) l (Q) × Σti∈ (Q) w (ti, q). (3) De toute évidence, cette similitude est maximale quand elles sont toutes regroupées la question des termes dans le passage. Par exemple, nous considérons les termes suivants d'une question Q et ceux d'un passage P: Q (termes) = commerce, ammonium, nitrate, engrais, entravées, européen, économique, communautaire. P (termes) = ammonium, nitrate, essentiels, ingrédient, variété, produits, certains, destinés, utilisation, izers fertilisés, d'autres, explosifs, raison, DIVERGENCES, les dispositions nationales, classification, contenu, européen, économique, communautaire, règlements , le contrôle, la commercialisation. Le vecteur correspondant - → Tc entre la question donnée et le passage est réglé sur: ------ → Tc (Q, P) = ammonium, nitrate, engrais, européen, économique, communautaire. De ce dernier on peut en déduire: ------ → QNG (Q) = [engrais de nitrate d'ammonium] [Communauté économique européenne] et ------ → NGP (P) = [nitrate d'ammonium] [engrais] [ Communauté Économique Européenne]. Dans cet exemple, --- → NGQ est composé de deux n-grammes nous avons donc deux poids de passage partiel pour le calcul. La première question n-gramme est divisé en deux SUBn-grammes dans le passage de sorte, SNG est égal à 2 tandis que le second est exactement égale à un passage du n-gramme ainsi, SNG est égal à 1. Ainsi, compte tenu de 1.766 et 1.524 les pondérations à long terme de ngQ1 et ngQ2 respectivement, w1 (P) = L (ngQ1) sng1 × Σ t∈terms (ngQ1) w (t, q) = (3/2) × 1,766 whilew2 (P) = L (ngQ2) sng2 × Σt ∈terms (ngQ2) w (t, q) = (3/1) × 1,524. Donc, le poids total de passage est égale à la somme de w1 (P) et w2 (P). D'autre part, le poids de la question sera réglée à: w (Q) = l (Q) × Σti∈ (Q) w (ti, q) = 8 × 4,924 4,924 où est le résultat de la somme de termes de la requête poids. Pour résumer, notre méthode est différente de celles à base de n-grammes précédemment cité. Tout d'abord, nous procédons différemment pour extraire le n-grammes dans la mesure où au lieu d'extraire tous les n-grammes pour tous les n valeurs possibles de la question et le passage, comme dans (Correa et al., 2010) et (Buscaldi et al., 2010 ), ou tous les n-grammes de taille n, comme dans (Radev et al., 2005), on extrait seulement n-grammes communs entre la question et les passages de tailles différentes. Donc, on n'a pas besoin d'inclure - 117 - Un passage pertinent et récupération Re rang approche pour la question Répondre à une étape supplémentaire pour sélectionner n-grammes communs de tous les n-grammes extraits. En second lieu, le poids de n-grammes, nous prenons en compte à la fois la somme des termes de poids et de leur longueur comme dans (Radev et al., 2005), tandis que Correa et al. (2010) et Buscaldi et al. (2010) considèrent que la somme du poids des termes. Troisièmement, notre mesure Psim est calculée à partir du poids de la question n-grammes et dépend de la n-gramme est entièrement existe ou il est divisé dans le passage. 3.3 Passage Re rang module donné un ensemble de passages récupérés retournés par notre moteur n-gramme à base de PR, nous avons recours à la Ranking SVM appelé modèle RankSVM qui combine différents textes similitude me- sures qui constituent les caractéristiques de re-rang les en fonction de leurs degrés de similitude avec la question. RankSVM est une version classement du modèle SVM introduit pour résoudre le problème du classement de manière supervisée. L'idée derrière la méthode soulignage de cette machine ing populaire est de transformer le problème du classement dans la classification par paires et apprendre une fonction de classement de prédiction en utilisant l'intuition de SVM. Nous rappelons que RankSVM a été appliquée avec succès dans le cadre de l'IR, notamment pour la recherche de documents (Cao et al., 2006). Il est à noter que notre modèle de re-classement passage se compose de deux phases: la formation et les tests. Dans les deux phases, les différentes mesures de similarité sont calculées pour chaque passage et ces derniers sont entrés dans le classificateur RankSVM qui re rang les passages compte tenu de leurs valeurs caractéris- tique. Seul le passage au premier rang par notre modèle sera retourné par le sy la tige comme la plus pertinente réponse à la question d'un utilisateur donné. Au cours de la première phase, un ensemble de passages annotés entrés dans le modèle re rang passage à chaque passage classé où est marqué soit +1 (à droite) ou -1 (mauvais), alors que dans la phase de test, les passages ne sont pas étiquetés comme ils sont celles extraites par notre module de PR. Bien que la technique n-gramme assure seulement une dépendance simple entre les termes, il ne semble pas assez satisfaisante pour assurer la pertinence des sages extraits passions. Ainsi, nous avons recours à d'autres caractéristiques importantes, en particulier les sémantiques. En fait, les caractéristiques utilisées dans notre modèle de classement ont déjà fait leurs preuves avec succès dans la tâche Justifications sémantique similarité (Buscaldi et al., 2013) (STS) à * SEM 2013 qui nécessite des systèmes ING partici- pour déterminer le degré de similitude entre les paires des phrases de texte. Parmi les fonctionnalités proposées, nous ne considérerons WordNet basé conceptuelle similarité, entité nommée Overlap, Edit distance et au lieu de la N-gramme basé similarité appliquée dans cette tâche, nous re sorte à celle proposée par nous-mêmes dans ce travail. En effet, nous avons adapté ces caractéristiques au contexte de l'AQ. C'est-à-dire les paires de phrases deviennent paires de passage-question. Notez que plus nous ajoutons des fonctionnalités, plus la complexité du programme est. De plus, à ce stade, les mesures de similarité de texte basé sur les fréquences terme ne sont pas suffisantes pour récupérer les sages pertinents passions (Keikha et al., 2014). D'où, dans notre cas, nous avons principalement eu recours à des caractéristiques sémantiques pour assurer la pertinence des réponses. 4 L'évaluation expérimentale 4.1 datasets et outils Fondamentalement, deux principales ressources sont nécessaires pour le développement et l'évaluation d'un QAS: une collection de documents et un groupe de questions. Pour l'évaluation de notre module de relations publiques, nous - 118 - N. Othman et R. Faiz avons utilisé l'ensemble de données fournies dans l'exercice ResPubliQA 2009 de CLEF qui cherche à récupérer les paragraphes de la collection de test à réponse (Peñas et al., 2010) une question donnée choisi à partir d'une série de questions différentes. Dans nos expériences, nous utiliserons la collection française que seuls quelques systèmes ont été testés dans cette langue en raison de son ambiguïté. Les expériences sont réalisées sur 338 questions, 1388818 passages tirés de la collection donnée, 100 passages utilisés de ceux qui sont renvoyés par le modèle de recherche et 10 passages renvoyés par le modèle n-gramme. D'autre part, d'évaluer l'applicabilité de l'approche globale, nous avons utilisé les ressources fournies dans l'exercice de ResPubliQA2010 (Peñas et al., 2010) qui vise à renvoyer soit des paragraphes ou des réponses exactes que la sortie du système à un ensemble de 200 plus questions complexes sur deux collections d'essais. Notez que cette fois-ci, nous allons utiliser toute la collection anglaise pour la formation et les tests. En fait, nous travaillons sur les passages, les ensembles de données donnés devraient être les plus appropriés pour évaluer notre approche. Nous insistons sur le fait que nous avons testé l'approche globale à l'aide du corpus anglais que nous avons eu recours aux versions anglaises des principaux outils utilisés pour atteindre des performances jugées plus telles que la version anglaise de WordNet Lexical Database et le NE pour l'anglais reconnaisseur. , Nous pouvons évidemment aussi évaluer notre approche dans d'autres langues simplement en intégrant des outils multilingues. Afin de valider notre approche, nous avons mis au point un système nommé PexRank (extraction de passage et système de classement) et nous avons utilisé le système open source JIRS 1 (information JAVA Système de récupération) décrits dans Gómez et al. (2007) pour les processus d'indexation et de recherche et adapté à nos besoins. Pour le passage classement, nous avons utilisé la SVM open source lumineuse 2. Notez que les passages des flux de formation étiquetés dérivés des fichiers de jugement de paires question / réponse ResPubliQA2009 anglais. 4.2 Mesures d'évaluation Pour évaluer la performance de notre moteur PR, nous sommes basés sur les mesures suivantes: - La précision: qui est défini comme le pourcentage de réponses correctes par rapport à tous posé des questions (dans notre cas pour les 10 premières positions). - Le nombre de les questions ayant passage correct au premier rang. - Le Rang réciproque moyen (de MRR) qui indique l'inverse multiplicatif de la position de rang de la première réponse correcte et a été largement été utilisée dans QA pour répondre le classement. Pour évaluer l'approche globale, nous sommes basés sur les mesures CLEF suivantes: - Le c @ 1 mesure qui a été introduite par CLEF comme la principale mesure d'évaluation pour les tâches de passage et de sélection de réponse. La formule de c @ 1 est réglé sur: c @ 1 = 1n (+ nR nU NRN) où nR désigne le nombre de questions correctement répondu, est nU le nombre de questions sans réponse et n est le nombre total de questions. - Précision globale: la précision calculée sur toutes les réponses évaluées. - Nous avons également utilisé le nombre de questions sans réponse (#NoA), le nombre de réponses correctes (#R), le nombre de questions a répondu à tort (#W), le nombre de questions sans réponse où une réponse candidat est mis au rebut (# NoA R): Dans ce cas, le système choisit de laisser la question sans réponse (comportement pessimiste) et le nombre de questions sans réponse avec une mauvaise réponse du candidat (#NoA W). Notez que nous avons fixé une valeur de seuil pour le score final à 0,15 comme il a été choisi par de nombreux auteurs pour le résultat du classement final. Donc, nous répondons à la question que si le score le plus élevé 1. http://sourceforge.net/projects/jirs/ 2. http://svmlight.joachims.org/ - 119 - Un passage pertinent et récupération Re rang approche pour question valeur est supérieure à 0,15 Answering. Dans le cas contraire, nous ne revenons pas de réponse à la question donnée. Nous croyons que le retour sans réponse à une question posée est mieux qu'offrir une mauvaise réponse. 4.3 Résultats et discussion Nous avons comparé les résultats obtenus par notre moteur PR à ceux du système NLEL (Correa et al., 2010) qui a utilisé un modèle n-gramme à base de PR et il a été classé au premier rang dans la CLEF 2009 piste QA pour les Français la langue. LANGUETTE. 1 - Les résultats de la comparaison entre PexRank et NLEL PexRank NLEL Nombre de questions ayant correctes 272 260 passages dans les 10 premières positions Précision 0,804 0,670 Nombre de questions dont correct 159 142 passage est en première position MRR 0,409 0,365 Le tableau 1 montre que notre système a donné de meilleurs résultats que NLEL dans tous les critères. En effet, PexRank a répondu à un grand nombre de questions, avec une différence égale à 12 questions plus que NLEL. Nous avons obtenu plus de réponses dans la première position avec une différence égale à 17 questions. De plus, nous avons déduit que la valeur de MRR de PexRank est supérieure à celle de NLEL parce que le nombre de réponses obtenues par notre système est plus grande dans les premières positions et tions plus faibles dans les derniers. Maintenant, nous passons à évaluer l'approche globale. Les résultats obtenus dans notre système de fonctionner sont présentés dans le tableau 2, où l'on compare nos résultats à ceux rapportés par d'autres systèmes participant à l'exercice de ResPubliQA2010 effectuer la même tâche de sélectionner le passage le plus pertinent pour répondre à une question donnée, décrite dans (Peñas et al ., 2010). LANGUETTE. 2 - Comparaison entre PexRank et systèmes similaires Précision du système c @ 1 #R #W #NoA #NoA R #NoA W PexRank 0,74 0,83 149 26 25 0 0 uiir101PSenen 0,72 0,73 143 54 3 0 3 bpac102PSenen 0,68 0,68 136 64 0 0 0 dict102PSenen 0,67 0,68 117 52 31 17 14 bpac101PSenen 0,65 0,65 129 71 0 0 0 elix101PSenen 0,65 0,65 130 70 0 0 0 nlel101PSenen 0,64 0,65 128 68 4 2 2 uned102PSenen 0,65 0,65 129 71 0 0 0 Les résultats rapportés dans le tableau 2 montrent que PexRank surpasse toute les autres systèmes EFFECTUEREZ dES la même tâche en termes de précision et c @ 1 mesure avec une précision score égal à 0,74 et ac @ 1 score égal à 0,83, qui sont nettement bons résultats. En outre, le fait que notre c @ 1 valeur est supérieure à la cote de précision prouve que l'utilisation de notre aucun critère de réponse est justifiée et a permis d'obtenir un haut c @ 1 mesure. Nous avons remarqué que, sur 99 questions complexes, notre système a répondu avec succès 48 questions. La plupart des questions sans réponse où les questions d'opinion. De même, l'en correctement répondu aux questions étaient pour la plupart ceux d'opinion et les causes. Néanmoins, il est possible d'autres expériences sur - 120 - N. Othman et R. Faiz grands ensembles de données pour décider de la valeur de seuil pour le classement score final. Notez que, nous avons choisi de ne pas fournir une réponse candidate pour les questions sans réponse, ni correctes, ni in- correct. Bien que la tâche de sélection de paragraphe est juste une PR, la principale différence de IRs pur est d'ajouter la possibilité de laisser la question sans réponse à l'étape de validation. Il est à noter que cette tâche permet de poster des questions complexes et de les évaluer de manière simple. 5 Conclusion Poser une question en langage naturel et d'avoir une réponse précise devient aujourd'hui un atout majeur pour l'utilisateur et constitue un défi considérable dans de nombreux domaines d'application tels que le commerce électronique, l'enseignement à distance et la recherche mobile. Dans cet article, nous avons abordé deux tâches cruciales dans l'assurance qualité et propose une nouvelle approche pour la récupération et des passages re rang prenant ad- vue de n-grammes et modèles RankSVM. Bien que nos résultats expérimentaux ont montré des résultats prometteurs, nous croyons que notre approche pourrait être améliorée en incorporant d'autres fonctionnalités telles que celle syntaxique dans le modèle de re-classement sans augmenter de manière significative la complexité du programme. À l'avenir, nous attendons avec impatience d'évaluer notre approche sur corpus écrits dans d'autres langues et de permettre à notre système de retourner une réponse précise au lieu d'un passage. Références Araki, J. et J. Callan (2014). Un modèle de similitude d'annotation dans le passage de classement pour histor- validation ical d'information. Dans Proc. de l'ACM SIGIR internationale 37e sur la recherche et conf de développement dans IR, p. 1111-1114. ACM. Bilotti, M. W., J. Elsas, J. Carbonell et E. Nyberg (2010). l'apprentissage de Rang pour répondre à la question factoid des contraintes linguistiques et sémantiques. Dans Proc. du 19 international ACM conf de l'information et KM, p. 459-468. ACM. Buscaldi, D., J. Le Roux, J. J. G. Flores, et A. Popescu (2013). LIPN-core: similarité de texte sémantique en utilisant n-grammes, wordnets, analyse syntaxique, esa et recherche d'information fonctionnalités basées. Dans Proc. du 2ème joint Sémantique conf lexicales et computationnelle, p. 63. Buscaldi, D., P. Rosso, J. M. Gómez-Soriano, et E. Sanchis (2010). Répondre aux questions avec un moteur de recherche de passages à base de n-gramme. JIIS 34 (2), 113-134. Cao, Y., J. Xu, T.-Y. Liu, H. Li, Y. Huang et H.-W. L'honorable (2006). Adaptation du classement svm à la recherche de documents. Dans Proc. du SIGIR international annuel ACM 29 sur la recherche et conf de développement dans IR, p. 186-193. ACM. Correa, S., D. Buscaldi et P. Rosso (2010). Nlel-Maat à respubliqa. Dans multilingue Informa- tion accès I. évaluation texte expériences de récupération, pp. 223-228. Springer. Cui, H., R. Sun, K. Li, M.-Y. Kan, et T.-S. Chua (2005). Question réponse la recherche de passages en utilisant les relations de dépendance. Dans Proc. du SIGIR international annuel 28 ACM pour la recherche et conf de développement dans IR, p. 400-407. ACM. Faiz, R. (2006). Identification des phrases pertinentes dans les articles de presse pour l'extraction d'informations sur les événements. IJCPOL 19 (01), 1-19. - 121 - Un passage pertinent la recherche et l'approche Re rang pour la question Réponse Gómez, J. M., D. Buscaldi, P. Rosso et E. Sanchis (2007). JIRS ​​système de récupération de passage indépendant de la langue: Une étude comparative. Dans Proc. du 5ème international sur la PNL conf (icône- 2007), pp. 4-6. Keikha, M., J. H. Park, W. B. Croft, et M. Sanderson (2014). Récupération des passages et trouver des réponses. Dans Proc. du Symposium Computing document Australasie 2014, p. 81. ACM. Moschitti, A. et S. Quarteroni (2011). noyaux linguistiques pour réponse re rang dans les systèmes répondant à des questions. IPM 47 (6), 825-842. Ofoghi, B. et J. Yearwood (2009). Peut-information de classe sémantique peu profonde aide réponse la recherche de passages? En 2009 AI: Les progrès de l'intelligence artificielle, pp 587-596.. Springer. Peñas, A., P. Forner, Á. Rodrigo, R. F. E. Sutcliffe, C. Forascu et C. Mota (2010). Vue d'ensemble de respubliqa 2 010: Question de répondre à l'évaluation sur la législation européenne. En 2010 CLEF et ateliers, LABs documents portables, 22-23 Septembre 2010, Padoue, Italie. Peñas, A., P. Forner, R. Sutcliffe, a. Rodrigo, C. Forăscu, I. Alegria, D. Giampiccolo, N. Moreau et P. Osenova (2010). Vue d'ensemble respubliqa 2009: Question d'évaluation de réponse sur la législation européenne. Dans l'information multilingue Accès I. évaluation texte expériences de récupération, pp. 174-196. Springer. Radev, D., W. Fan, H. Qi, H. Wu, et A. Grewal (2005). question probabilistes répondre sur le web. JASIST 56 (6), 571-583. Severyn, A., M. Nicosie, et A. Moschitti (2013). structures de construction classificateurs pour le passage de reclassement. Dans Proc. du 22 international ACM conf CIKM, pp. 969-978. ACM. Tellex, S., B. Katz, J. Lin, A. Fernandes, G. et Marton (2003). L'évaluation quantitative des algorithmes de récupération Pas- sage pour répondre à la question. Dans Proc. de l'international annuel 26 conf ACM SIGIR sur la recherche et le développement en IR, p. 41-47. ACM. Toba, H., S. Sari, M. Adriani, et R. Manurung (2010). Approche contextuelle pour la sélection du paragraphe dans la tâche de réponse aux questions. Dans CLEF (Documents portables / Labos / ateliers). Voorhees, E. M. (2001). La piste TREC répondeur question. JNLE 7 (04), 361-378. Yen, S.-J., Y.-C. Wu, J.-C. Yang, Y.-S. Lee, C.-J. Lee, et J.-J. Liu (2013). Un modèle de contexte classement basé machine vecteur de support pour répondre à la question. JIS 224, 77-87. Les CV de questions-Systèmes Réponses (SQR) s Visent à RETOURNER des reponses precises Directement Ë des questions posees en langage naturel. L'extraction et le des passages Sont reclassement considérés les Comme les plus de Difficiles Tâches de un SQR et typique encore l'effort exigeant de un non trivial. Dans this article, nous proposons Une nouvelle approche Pour L'extraction et le reclassement des passages en Utilisant les n-grammes et SVM. Notre Système d'extraction de passages basons sur la technique des n-grammes repos Sur une nouvelle de mesure Entre passage un similarité et Une question. Les passages ensuite réordonnés extraits en Sont un modèle basons Utilisant sur RankSVM Combinant de Measures Différentes de RETOURNER similarité le AFIN passage Le plus pertinent verser question juin Donnée. Nos expériences et nos ÉTAIENT Résultats et prometteurs Que notre have démontré approach is concurrentielle. - 122 -"
146,Revue des Nouvelles Technologies de l'Information,EGC,2016,Analyse exploratoire par k-Coclustering avec Khiops CoViz,"En analyse exploratoire, l'identification et la visualisation des interactionsentre variables dans les grandes bases de données est un défi (Dhillon et al.,2003; Kolda et Sun, 2008). Nous présentons Khiops CoViz, un outil qui permetd'explorer par visualisation les relations importantes entre deux (ou plusieurs)variables, qu'elles soient catégorielles et/ou numériques. La visualisation d'unrésultat de coclustering de variables prend la forme d'une grille (ou matrice) dontles dimensions sont partitionnées: les variables catégorielles sont partitionnéesen clusters et les variables numériques en intervalles. L'outil permet plusieurs variantesde visualisations à différentes échelles de la grille au moyen de plusieurscritères d'intérêt révélant diverses facettes des relations entre les variables.","Bruno Guerraz, Marc Boullé, Dominique Gay, Vincent Lemaire, Fabrice Clérot",http://editions-rnti.fr/render_pdf.php?p1&p=1002206,http://editions-rnti.fr/render_pdf.php?p=1002206,en,"Analyser par k-exploratoire Coclustering Avec Khiops CoViz Bruno Guerraz *, Marc Boullé *, Dominique Gay *, **, Vincent Lemaire *, Fabrice Clérot * * Orange Labs ** Laboratoire d'Informatique et de Mathématiques de l'Université de La Réunion CV. En analyse exploratoire, l'identification et la visualisation des interac- tions Entre les variables Dans les grandes Bases de données is a DÉFI (Dhillon et al., 2003; et Kolda Sun, 2008). Nous Présentons Khiops CoViz, un outil PERMET D'explorateur Sie visualisation par les relations Importantes Entre deux (ou several) variables catégorielles qu'elles Soient and / or Numériques. La visualisation d'un de coclustering de Résultat des variables La Forme d'Prend Une grille (ou matrice) les dimensions Dont: les partitionnées Sont les variables catégorielles partitionnées en grappes Sont les ET des variables en Intervalles Numériques. L'Outil PERMET Riantes de PLUSIEURS va- Ë DIFFERENTES visualisations de la Grille Échelles au Moyen de Criteres D'INTERET PLUSIEURS REVELANT Facettes des relations Diverses les variables. Entre les 1 Khiops CoViz: Visualisation des modèles en grille Khiops CoViz, developpee en Flex, la is Logicielle de visualisation brique de Khiops cluster Co (KHC) 1. ETANT Données, deux (ou plus) les variables catégorielles de les Numériques, KHC Réalise un partitionnement simultané les variables des: les facts de les variables catégorielles en grappes Sont groupées et les Variables en partitionnées Numériques Intervalles are - Ce Qui re à un Vient de coclustering Problème. Le produit des partitions uni-Variees partition forme Une multivariée de l'espace de représentation, à savoir Une grille OU matrice de et il cellules also des Nations Unies des variables Représente Estimateur de density jointé des. De la auswählen AFIN « better » grille M * (les Connaissant Données) de l'espace de M Modèles, nous exploitons Une approche Bayé- Maximum A Sienne dite Posteriori (MAP). KHC explorer l'espace de modèles en un critere bayésien minimisant, coût Appelé, Qui Réalise un Compromis between et la précision du modèle robustesse: coût (M) = - log (p (M | D)} {{} postérieur) α - log (p (M)} {{} avant × p (D | M)} {{} vraisemblance) (1) les parties KHC also Une hiérarchie Construit des de dimension each (c.-à-clusters Intervalles les adjacents) en un Utilisant Stratégie agglomératif ascendante, en M * de Partant, la grille opti- male de la résultante d'optimisation procédure, JUSQU'A M∅, le modèle NUL, à savoir, la grille (unicel- lulaire) where dimension Ne est partitionnée Aucune. Les en hiérarchies Sont construites 1. http://www.khiops.com fusionnant - Pour plus de détails sur de l'KHC de Implémentation, voir Boullé (2011) - 493 - Analyse par k-exploratoire Coclustering Avec Khiops CoViz les partis Sie minimisent l'indice de dissimilarité Δ (C1, C2) = coût (Mc1∪c2) - les parties coûts (M), where c1, c2 deux d'une partition Sont juin d'une dimension de la grille M et Mc1∪c2 la grille après fusion de c1 et c2. Of this Manière, la fusion de partis minimiser la dégradation du coût critère, la DonC minimiser d'information par perte rapport à la grille M avant fusion. L'lisateur may uti- la auswählen AINSI de la grille granularité fils à analyser Nécessaire tout en controlling le Nombre de Soit partis le Soit d'information Taux (par exemple, le pourcentage, d'information Gardé in the modèle: IR (M ') = (coût (M ') -Cost (M∅)) / (coût (M *) - coût (M∅)) La grille opti- M mâle * Et Les hIÉRARCHIES les constituants correspondantes structures de notre Principales de visualisation 2 outil.. Interface Utilisateur: Exploration & Interactivité la Figure 1 l'Interface Utilisateur Présente de l'outil sous-Pour un ensemble de la DBLP de Données de base Nous considérons des dimensions Données à trois (Auteur × Y oreille × Event) verser 16 000 auteurs AYANT. published at least Une Fois Dans Une des principales Conférences de bases de Données de Fouille de ous Données -. Y oreille is l'année de la publication Le directeur de visualisation panneau de l'outil de l'Est composé s de deux dimensions hiérarchies Selection- Nées, des partis Terminales des et de la hiérarchies composition d'Une partie Sélectionnée. La visualisation de la grille Correspondante is also available via le panneau principal (voir la figure 2 à gauche). L'outil de Përmet les partis parmi Naviguer D'une dimension TANDIS Que les deux dimensions Autres et Sont fixées à la visualisation dédiées (pour les variables Le à plus de de CAS deux). FIGUE. 1 - principale de Khiops Panneau CoViz: (de gauche à droite), les Hiérarchies des partis dimensions de deux (Auteur et événement), des partis La Liste des Terminales HIÉRARCHIES et La composition des partis selectionnées. - 494 - B. Guerraz et al. La auswählen verser Visualiseur granularité la voulue grille se fait au Moyen de la Fonctionnalité « hiérarchie déplier » (voir la figure 2 à droite). Un analyste may CONTROLER Le Nombre de partis dimension nominale ou Le Taux d'information (Comme decrit, plus haut) par fusion optimale par OU personnalisée de fusion (non optimale). Les d'SELON applications Données, la grille optimale de may Être composée de grappes beaucoup (may-être trop l'écoulement analyst) et des grappes AUGMENTE fusionner le Nombre de mécaniquement facts par groupe: pour l'analyse des faciliter les clusters, l'outil propose, l'Deux Measures d'un groupe Passions et la typicité d'Une valeur cluster Dans un. CÉS deux Dérivées du Measures Sont coût critère (Guigourès, 2013) et versez Sont Utiles les grappes Ordonner les ous facts de grappes et se pair Passions sur les focaliser Les plus interessantes Composantes (voir figure 1). De visualisation panneau. Pour Deux des variables partitionnées X et Y à Visualiseur, les visua- lisations classiques, Telles les « carte de chaleur » is available: il EST AINSI possible de Visualiseur la fréquence des cellules, la Probabilité JOINTE, la Probabilité conditionnelle, la density JOINTE OU conditionnelle. De plus, l'outil propose Deux critères (derivatives de l'Information MI Mutuelles (X, Y), voir Guigourès (2013) pour les Complètes définitions) Qui des infor- mations fournissent Sur les interactions supplementaires des variables Entre: - Contribution à l « informations Mutuelles: La CMI les commentaires Indique contribu- buent à cellules l'information MI Mutuelles (X, Y); Positivement (rouge), négativement (bleu), nullement (blanc) respectively Indique l'ONU interactions D'excès, un déficit OU interaction Particulière Aucune à Ce Qui comparée is en Attendu d'indépendance des CAS riables partitionnées va-. - Contraste: Le contraste aux grilles is 3D Dédié (ou plus). Etant Données deux des variables partitionnées fixées X, Y à visualisateur et Une partie Pi D'une third variables K, ap- pelée Contexte, le contraste rencontré en lumière les cellules Qui caractérisent Pi par rapport à toute les Données. Comme précédemment, un contraste positif, négatif OU NUL EST rencié par les diffé- couleurs rouge, bleue et blanche. FIGUE. 2 - (Gauche): Visualisation de la contribution à l'information l'MUTUELLE Y verser l'oreille × événement. (Droite): Panneau de la sélection de la grille de granularité. - 495 - Analyse par k-exploratoire Coclustering Avec Khiops CoViz 3 Utilisation multiple des modèles en grille L'analyse des variables des interactions Entre exploratoire Par le Biais de la visualisation des modèles en grilles is Adaptée de types A applications PLUSIEURS et Données Domaines d'( Bondu et al., 2013). Nous Présentons Une liste non exhaustive des applications d'TYPIQUES Données Qui Etudie have Déjà par Khiops Été CoViz: - Pour le à deux dimensions CAS: - Marketing: Les clients Avec la liste des Achetes Produits (Client ×) - Exploitation minière produit Web: analyse de journaux identifiant web des répandrai de navigation Comportements (Cookie × Pageweb) - Télécom: Dimensionnement de par mobile l'réseau analyse des comptes rendus d'ap- pels (CDRs) (AntenneSource × AntenneCible), par exemple, analyser des CDRs à exploratoire l'échelle d'un paie (Guigourès et al., 2015). - Fouille de textes: (co) regroupement de textes (Texte de MOT) - Pour le Cas A 3 dimensions: - Fouille de graphes: (. Guigourès et al, 2012) Données multigraphes temporels (NoeudSource × × NoeudCible Temps), par exemple, d'analyser des emplacements de vélos à Londres. - Clustering de Données Fonctionnelles (série Temporelles Numériques ous catégorielles (Boullé, 2012, Gay et al, 2015).): (TimeSeriesId × Temps × Valeur) OU (TimeSeriesId × Temps × événement), par exemple, le groupement de courbes (Boullé, 2012), l'analyse de consommation électrique (Boullé et al., 2012) ou Encore l'analyse de Données bibliographiques (Gay et al., 2015). Une sélection de bureaux d'applications Données à la démonstration serviront de Khiops CoViz. Par ailleurs, de l'Une démonstration sur les tool du Données « EGC 2016 2 Défi » sera proposed also. 4 Conclusion & Travaux Futurs Présentons NOUS Khiops CoViz, Nations Unies sur les outil de carte modèles en grilles, verser Visualisateur identificateur et les interactions des variables interessantes Entre catégorielles and / or Numériques. Cepen- Dant, versez applications de les Nombreuses, l'un analyst de plus de Besoin Souvent sentants tion Qu'une des Résultats matricielle, par exemple, de les verser Représentations Graphiques de Graphes Données, des projections de Cartes pour les Géographiques Données: des extensions de l'outil des pair plug-ins dédiés aux applications ACTUELLEMENT Sont étudiées. Bondu Références, A., M. Boullé, et D. Gay (2013). Les grilles modèles en. , L'évaluation, Principes applications rithmes rithmes et. Tutoriel donné à EGC. 2. http://www.egc.asso.fr/Manifestations_dEGC/71-FR-Defi_EGC_2016_ Communaute_EGC_quelle_histoire_et_quel_avenir - 496 - B. Guerraz et al. Boullé, M. (2011). modèles de grille de données pour la préparation et la modélisation dans l'apprentissage supervisé. En I. Guyon, G. Cawley, G. Dror, et A. Saffari (Eds.), Travaux Pratiques Reconnaissance: défis dans l'apprentissage machine, volume 1, pp 99-130.. Microtome Publishing. Boullé, M. (2012). Les données fonctionnelles de clustering via constante par morceaux estimation de densité non paramétrique. Pattern Recognition 45 (12), 4389-4401. Boullé, M., R. Guigourès, et F. Rossi (2012). regroupement hiérarchique non paramétrique des données fonc- tionnels. Dans EGC (meilleur volume), pp. 15-35. Dhillon, I. S., S. Mallela, et D. S. Modha (2003). co-regroupement des informations-théorétique. Dans Actes de la neuvième ACM SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données, Washington, DC, États-Unis, le 24 Août - 27, 2003, pp 89-98.. Gay, D., R. Guigourès, M. Boullé, et F. Clérot (2015). TESS: séquence temporelle événement compression. IEEE Conférence internationale sur la science et analyses (Advanced DSAA'15), Paris, France, Octobre. 19-21 Guigourès, R. (2013). Utilisation des Modèles de co-classification l'analyse verser des Données exploratoire. Ph. D. de thèse, Université Paris 1 Panthéon-Sorbonne. Guigourès, R., M. Boullé, et F. Rossi (2012). Une approche triclustering pour les graphes de temps en évolution. 12e Conférence internationale IEEE sur les ateliers d'exploration de données, ateliers ICDM, Bruxelles, Belgique, le 10 Décembre 2012, p. 115-122. Guigourès, R., D. Gay, M. Boullé, F. Clérot, et F. Rossi (2015). analyse exploratoire à l'échelle du pays des enregistrements détaillés des appels à travers la lentille de modèles de grille de données. Dans Apprentissage et découverte des connaissances dans les bases de données - Conférence européenne, ECML PKDD 2015, Porto, Portugal, 7-11 Septembre, 2015, Compte rendu, Partie III, pp 37-52.. Kolda, T. G. et J. Sun (2008). décompositions tensorielles évolutive pour l'exploration de données multi-aspect. Dans Actes de la 8e Conférence internationale IEEE sur les mines de données (ICDM 2008), 15-19 Décembre 2008, Pise, Italie, pp. 363-372. Résumé L'identification et l'analyse visuelle des interactions intéressantes entre les variables dans des ensembles de données à grande échelle par k-coclustering est d'une grande importance. Nous présentons Khiops CoViz, un outil d'analyse visuelle des relations intéressantes entre deux ou plusieurs variables (catégoriques et / ou numérique). La visualisation des variables k coclustering prend la forme d'une grille / matrice dont les dimensions sont séparés: les variables catégoriques sont regroupées int o clusters et ables Vari numériques sont discrétisées. L'outil permet plusieurs types de visualisation à différentes échelles de représentation de la grille des résultats coclustering au moyen de plusieurs critères dont chacun fournir différentes perspectives sur les données. - 497 -"
156,Revue des Nouvelles Technologies de l'Information,EGC,2016,Clustering visuel semi-interactif,"Nous proposons dans cet article une approche de clustering visuelsemi-interactif. L'approche proposée utilise la perception visuelle pour guiderl'utilisateur dans le processus interactif. Les clusters sont extraits de manièresuccessive et itérative, puis évalués selon leur ordre d'extraction. Pour l'utilisateur,l'approche semi-interactive permet non seulement d'évaluer les classes enfonction d'un critère déterminé mais aussi d'évaluer l'influence de l'extractiond'un cluster sur ceux précédemment extraits. Un protocole de test est présentéafin de comparer cette approche avec les approches purement automatiques etpurement interactives. Cet article est un résumé d'un papier accepté 1 pour unjournal international.","Lydia Boudjeloud, Philippe Pinheiro, Alexandre Blansché, Thomas Tamisier, Benoît Otjacques",http://editions-rnti.fr/render_pdf.php?p1&p=1002183,http://editions-rnti.fr/render_pdf.php?p=1002183,en,"Clustering semi-visuel Lydia Boudjeloud-interactif Assala *, Philippe Pinheiro **, Alexandre Blansché *, Thomas Tamisier ** et Benoît Otjacques ** * Laboratoire d'Informatique Théorique et Appliquée, Lita-EA 3097, Université de Lorraine, Metz, F -57045, France prenom.nom@univ-Lorraine.fr ** LIST- Luxembourg Institut des sciences et de la technologie, Esch-sur-Alzette, Luxembourg prenom.nom@list.lu CV. L'article de Nous proposons this Une approche de regroupement semi-visuel interactif. L'utiliser la approach proposed perception visuelle verser Guider l'Utilisateur in the Processus interactif. Les grappes Sont extraits de Manière successive et itérative, Puis évalués SELON Leur ordre d'extraction. Pour l'utilisa- teur, l'approche semi-interactive Përmet non d'EVALUER les only en cours d'un critere fonction specified d'EVALUER Mais aussi l'influence de l'extraction d'un groupe sur Ceux précédemment extraits. Un essai de protocole is presented de comparateur this AFIN approach with the approaches Purement et Purement Interactives automatiques. This article is un résumé d'un papier journal 1 pour accepté un international. 1 Introduction Dans le Processus d'extraction de connaissances heuristiques à partir de Données, il y a deux au means de Moins faire les methods collaborer Avec des methods automatiques interac- tives visuelles. Il Est possible d'methods de les UTILISER la visualisation de l'en prétraitement Algorithme en post automatique ous-treatment of this same Algorithme. En de prétraitement Données, sur s'aperçoit Que, bien Souvent, l'intuition Une des concepts cachés Initiale can be de Façon Visuelle acquise Dans les très grandes d'information Quantités. This may also Guider étape l'Utilisateur in the des choix de Algorithmes les plus de fouille de their ous pertinents paramêtres. En post-treatment des connaissances heuristiques, les methods de visualisation Sont utilisées répandrai Plutôt EVALUER et des interprète en se Basant Résultats sur des AC-, plus Représentations Graphiques cessibles Que des colonnes de un ensemble ous Chiffres de rules. CÉS interactions Différentes l'illustrent de faire interest des methods Coopérer et des methods automatiques in- teractives visuelles. La des Compréhension is Résultats et la accumulera AINSI des algorithmes UA- précision tomatiques can be Améliorée Facilement. Une des increase la verser Possibilités partie de la visualisation Dans les algorithmes de fouille de faire de l'Est Données l'Coopérer tomatique de l'UA- algorithme de fouille un Données Avec algorithme visuel interactif. On parle de fouille 1. Alors interactif et clustering visuel itérative, l'information de visualisation Journal, pp 1-17, 2015 (DOI: 10.1177 / 1473871615571951) - 321 - Clustering semi-visuel de Visuelle interactif Qui se Données de la visualisation distingué d'information et Consiste Fait, en l'uti- de la visualisation lisation Comme outil la répandrai assisteur fouille. L'approach proposed Dans cet article s'inscrit Dans la direction this, nous nous sur les projections des bassins des dimensions en petites- Données verser selectionner des Groupes Homogènes (D'INDIVIDUS les classes, les clusters). CÉS grappes par PEUVENT Être le Processus Extraits Comme ils PEUVENT automatique Être SELECTOR tionnés interactivement par l'Utilisateur. Notre approche d'EVALUER les Përmet grappes Différents au fur et extraits à mesure, l'Utilisateur may AINSI, DANS LE Être guidé d'extraction Processus. Nous proposons un also essai de protocole de comparateur l'AFIN approach semi-interactif Avec l'approche Purement et automatique Avec l'approche Purement sans interactive aide-automatique Aucune. Nous VERRONS Que l'approach MONTREE de semi-interactif se est ainsi en Annoter de Efficace de regroupement Résultats. 2 Présentation de l'approach Nous proposons Une approche itérative Qui d'Përmet les cours les Extraire après les Unes Autres. Le REPETE HNE Processus itératif à la Demande de l'Utilisateur. A each itération, Une nouvelle classe is Extraite. Il mio methods verser les PLUSIEURS Une classe homogène Extraire. Dans this article, nous utilisons Une méthode d'extraction de cours sur la détection basée de limite de classe (Blansché et Boudjeloud-Assala, 2013). Une classe à partir Extraite HNE d'un centre. Nous calculons la distance la Entre each et le centre objet de la classe et Cherchons Alors la première augmentation Dans SCÉ facts abrupte Qui la limite de indiquera la classe Extraite. Nous OPTE Avons Pour la méthode de détection de photos présentéisme Dans (Palshikar, 2009), sur le differential appliquée des distances. Une Fois la limite de classe évaluée, tous les objets Qui Ont juin à la distance Inférieure this à la limite appartiennent Extraite classe. Nant l'évaluation nécessaires sur son des cours, nous proposons d'deux critères d'UTILISER répandrai EVALUER les évaluation des classes independently les Unes des Autres, et Blasnché développés PAR Boudjeloud Dans Blansché et Boudjeloud-Assala (2013). Aux critères d'Contrairement en évaluation classifica- tion non supervisee Qui l'ensemble de évaluent Données fils Dans et ne donnent intégralité pas Une évaluation des cours independently les Unes des Autres. Le premier rapport le critère d'is IR inertie (rapport Entre l'intra-Klasse inertie et l'inertie des Données totale, par le normalisé d'objets Nombre Dans la classe et l'ensemble de Dans oùCk Données k la Représente -ième Extraite Classe: IR (Ck) = Card (D) Σ o∈Ck d (o, ck) 2 Carte (Ck) Σ o∈D d (o, g) 2 Le deuxième rapport le critère de Représente de la limite classe CLR, le rapport representative Entre la distance de du dernier objet de la classe sur la distance de du premier objet hors de la classe: CLR (Ck) = max o∈Ck (d (o, ck)) min o / ∈Ck (d (o, ck)) Comme each is Extraite classe individuellement, nous nous Devons also I'assureur-ci Que Celles les Unes Sont des Différentes nous autres, de proposons, d'ajouter Fait juin péna- -. 322 - L. Boudjeloud-Assala et al .. La figure 1 -.. Interface de l'outil semi-interactif des cours sation chevauchement SELON Avec Les Leur catégories Le critère précédemment de découvertes OP calc pénalité ule juin l'intersection pénalité et SELON l'union de la classe Extraite Avec les catégories Précédentes (λ ≥ 0 le poids de Représente la l'importance pénalité Que SELON l'sur aux chevauchements Donné). OP (Ck) = λ max i = 1 ... k1 carte (Ck ∩ Ci) carte (Ck ∪ Ci) Interface Une visuelle permettant d'Interagir Avec le Processus d'extraction itératif de clas- se is also proposed (figure 1 ). L'ensemble de l'Est Projeté Données Avec T-SNE (van der Maaten et Hinton, 2008). La projection de l'ensemble de l'à Përmet Données d'in- teragir Utilisateur et de selectionner des same lui-centres de clusters, d'EVALUER les grappes Sélectionnés, de fixeur de la Manière interactive limite (rayonne) (figure 1 f) et AINSI voir l'effet sur la projection Avec Les critères d'évaluation décrits précédemment, Qui sont mis à jour (figure 1-e) et Qui s'af- fichent au fur et à mesure Que les grappes Sont construits. Les objets de l'ensemble de n'ont pas Qui Données Été Dans les différents affectés grappes Par l'approach en gris et apparaissent les objets à deux Qui appartiennent les classes, plus òû en rouge apparaissent (figure 1). Les objets les plus de centres des proches, en de distances Annoter, au pôle Sont affectés Représenté par centre fils et rayonne fils. La distance de l'Est calculee sur l'espace de l'd'origine ensemble de Données. L'Utilisateur may les Donnees AINSI Manipuler JUSQU'à Ce qu'il SATISFAIT des SDIS obte- Résultats Nus ET Le regroupement de bains Valide en enregistrant les Obtenu sous Résultats classes. Formes de Il may AINSI Interagir Avec le Processus d'extraction et d'exploration ou Le laisser indé- pendamment Travailler. Si l'Utilisateur Ne EST PAS SATISFAIT il may selectionner d'Autres centres sur il may Reproduire Lesquels le same Processus. 3 d'utilisation Nous Scénarios decrire Allons, Dans Ce Qui costume, les presentes à Différents scénarios l'Utilisateur. nous ut ilisons l'outil l'exploration répandrai Développé et le regroupement des Données. La variable de classe - 323 - Clustering semi-visuel interactif et nous is supprimée les comparons les Résultats obtenus Avec des classes vraies several SELON Critères d'évaluation du regroupement (Jaccard, Rand, Rogers-Tanimoto (R-T) et de similarité). SCÉNARIO 1: Purement interactive premier Le Scenario Consiste à tester les sibilités d'pos- Différentes interactions Avec la projection de l'ensemble de les Taches Données du SELON clustering. Nous Voulons obtain des groupes en cours de les sélectionnant Façon pu- Rement sans intervention interactive de l'approach Aucune automatique (partie droite de l'interface: la figure 1-d, e, g). SCÉNARIO 2: semi-interactif Le Deuxieme Consiste à tester scénario les possibi- lités d'Différentes interactions Visuelle COMBINEES à l'approche automatique. En de la same Partant projection des Données-t réalisée Avec SNE, l'Utilisateur may de auswählen l'ap- proche lancier en fixatif un automatique a priori de Nombre clusters, or in de les très demandé au fur et Extraire à par le Processus mesure itératif (figure 1-d). En les examinant Critères d'évaluation IR + OP et / CLR + OP OU (figure 1-e), l'Utilisateur may also, DE- cidre de fixeur lui-Même les centres PROCHAINS et Rayons, bien OU, sur each centre Fixé manuellement, faire Varier le rayon (figure 1-f). Les Résultats des critères d'évaluation mis à jour Sont à la suite each modification de l'Utilisateur. SCÉNARIO 3: Purement Le dernier automatique Consiste à appliquer scénario l'approach pu- Rement sur l'ensemble automatique de Données, en executant le d'ex itératif Processus de traction cours Avec les deux critères d'évaluation et en le fixatif Nombre de au cours de cours Nombre bobine. 4 Évaluation Utilisateurs Choix de l'ensemble de Données essai Nous Avons Décidé de tester l'approche Avec l'ensemble de Données OpticalDigits (5 620 objets, 64 ET 10 attributs de classes REELLES) de l'UCI Machine de dépôt d'apprentissage (Blake et Merz (1998)). Il s'agit d'un ensemble de Don- Nées la reconnaissance optique décrivant de Chiffres manuscrits. L'ensemble de contains Une représentation Données de l'image statique à la suite générée du mouvement de la pointe du stylet. La projection de this ensemble de Données Avec l'approche t-END is donataire Dans la Figure 1. This is projection obtenue Avec les 64 attributs de l'ensemble de Données, la distance de is also calculee à partir de l'espace d d'Origine (64 attributs). Notre EVALUER d'EST Objectif les Résultats du regroupement (automatique, interactif, semi-interactif) avec les cours REELLES de l'ensemble de Données (les characters riques nom-). Participants Nous participants de AVONS Onze de 30 l'âge de 40 ans Ë, tous de notre laboratoire Membres. Tous les participants des Chercheurs Dans Sont le domaine de l'informatique. Deux participants Parmi eux des thématiques de Ontario recherche Dans la visualisation de l'Information et de l'interaction en général, d'Entre trois des thématiques Eux de have recherche in the de fouille et les Données Avec des Autres thématiques Différentes. Nous tout protocole d'ABORD Avons, l'ensemble de Explique et l'Données de notre outil Objectif. Nous fait essai un Avons interactif d'AFIN commentaire L'outil tentatives de viol et ensuite nous Fonctionne Avons le Explique premier scénario. This minutes Cinq- Prend étape, nous l'Laissé Avons faire le Premier Utilisateur Exercice Qui a Duré also minutes. Cinq - 324 - L. Boudjeloud-Assala et al. When l'Utilisateur fils Termine Processus de regroupement, il les Enregistre Résultats. En- Suite, nous Présentons RAPIDEMENT le second cas de figure with the approach automatisée, sur Puis l'Utilisateur laisse Travailler. This étape de Cinq- Prend minutes Moins l'expli- cation with. L'Utilisateur devait TROUVER les grappes Les plus significatifs tout en réduisant le chevauchement des points (en rouge) et le Nombre d'objets non classés (points en gris). When l'util isateur les deux Termine exercices, nous les cours lui REELLES montrons de l'ensemble de Données. 5 Résultats Scénario Critère U1 U2 U3 U4 U5 U6 U7 U8 U9 U10 U11 Moyenne Sc 1 Similarité 0,65 0,64 0,54 0,69 0,71 0,59 0,60 0,68 0,66 0,65 0,63 0,64 Sc 2 Similarité 0,66 0,72 0,67 0,64 0,73 0,68 0,71 0,74 0,63 0,69 0,64 0,68 Sc 3 Similarité 0,63 0,63 0,63 0,63 0,63 0,63 0,63 0,63 0,63 0,63 0,63 0,63 Sc 1 Rand 0,89 0,86 0,79 0,89 0,90 0,87 0,81 0,90 0,85 0,85 0,85 0,86 Sc 2 Rand 0,86 0,91 0,88 0,85 0,91 0,89 0,85 0,92 0,84 0,89 0,83 0,88 Sc 3 Rand 0,79 0,79 0,79 0,79 0,79 0,79 0,79 0,79 0,79 0,79 0,79 0,79 Sc 1 Jaccard 0,33 0,30 0,22 0,36 0,39 0,31 0,22 0,38 0,28 0,30 0,28 0,31 Sc 2 Jaccard 0,30 0,38 0,32 0,27 0,42 0,36 0,29 0,43 0,27 0,36 0,25 0,33 Sc 3 Jaccard 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 0,20 Sc 1 RT 0,83 0,85 0,82 0,86 0,87 0,84 0,82 0,85 0,84 0,85 0,83 0,84 Sc 2 RT 0,84 0,85 0,84 0,82 0,87 0,85 0,85 0,86 0,84 0,86 0,82 0,84 Sc 3 RT 0,81 0,81 0,81 0,81 0,81 0,81 0,81 0,81 0,81 0,81 0,81 0,81 Sc 1 Nbr Cluster 14 11 9 9 10 11 10 10 11 9 15 - Sc 2 Nb r Cluster 11 15 10 13 9 10 10 12 9 9 12 - Sc 3 Nbr Cluster 10 10 10 10 10 10 10 10 10 10 10 - Tab. 1 -. Résultats Numériques Après Avoir Obtenu les deux Résultats (scénario 1 et 2, tableau 1) des participants, Avons NOUS réalisé essai un de Shapiro-Wilk verser Les Quatre critères d'évaluation (Jaccard, Rand, Rogers-Tanimoto (R-T) et de similarité). Le tableau 2 les Résultats obtenus main leur CURRICULUM VITAE. Les p-diffé- rentes valeur à la Sont supérieures de α = Valeur 0,05, l'hypothèse nulle Que les Résultats du regroupement Une loi suivent ne normalien can be rejetée. Nous pouvons le test de Fait de UTILISER Student si les verser DETERMINER means Résultats Sont, deux à deux, de Différents significativement. Nous Avons CMPARER: l'approche interactive par rapport automatique, semi-interactif par rapport automatique, et enfin, semi-interactive par rapport interactif. Nous pouvons voir tableau 3 Dans le Qué les approaches semi-interactive et interactive Sont Que l'globalement Meilleures approach et Qué versez automatique le critère de l'approche semi-similarité interactive is better Que l'approche interactive. Approches Similarité Rand Jaccard R-T p-value valeur p valeur p valeur p Interactive 0,8134 0,2453 0,5768 0,8752 semi-interactif 0,4556 0,4711 0,7453 0,5987 TAB. 2 - Test de Shapiro-Wilk. - 325 - Clustering semi-visuel interactif Approches Similarité Rand Jaccard RT valeur p valeur p valeur p valeur p Interactive / Automatique 0,4178 05.10 04.10 04.10 semi-interactif / Automatique 0,0011 06.10 05.10 10-5 semi-interactive / interactive 0,0368 0,2029 0,1968 0,5804 TAB. 3 - Test de l'étudiant. 6 Conclusion Nous presented Dans Avons cet article de juin approach regroupement semi-visuel interactif Avec Une évaluation quantitative des Résultats Utilisateurs. Nous Sommes partie de l'hypothèse la perception Que Visuelle les Pouvait secouriste à obtain Algorithmes de automatiques Meilleurs délais d'exécution résul-. Pour l'Utilisateur, l'approche semi-interactive proposed non Përmet d'EVALUER les only groupes Sélectionnés en fonction d'visuellement un critere specified Mais aussi de commenter la sélection Mesurer d'un nouveau groupe Influe sur Ceux précédemment extraits. Les tout Résultats statistiques montrent d'ABORD Que l'approche semi-interactif obtient de la classification en Meilleurs Résultats non supervisee. Les premiers ministres très encourageants Résultats et are nous à proposeur l'poussent d'évaluation Une classification sur Faites sous-espaces Différents (SELECTOR tion de variables) d'exploiteur permettant la AINSI perception visuelle et l'interactivité répandrai les approaches de biclustering et de multi-vues. Références Blake, C. et C. Merz (1998). dépôt UCI des bases de données d'apprentissage de la machine. Rapport technique, Université de Californie, Irvine, Département de l'information et des sciences informatiques. http: // Ar- chive.ics.uci.edu/ml/data sets.html. Adhérer en janvier 2014. Blansché, A. et L. Boudjeloud-Assala (2013). i Processus tératif d'extraction de cours en non supervisee. Dans Extraction et gestion des connaissances heuristiques (EGC'2013), Actes, 29 janvier - 01 février 2013, Toulouse, France, pp 9-14.. Palshikar, G. (2009). algorithmes simples pour la détection de pointe dans le temps de la série. Dans Actes de la 1ère Conférence internationale sur l'analyse des données Advanced Business Analytics et Intelligence, p. 2-13. van der Maaten, L. et G. Hinton (2008). Visualisant des données de grande dimension à l'aide de t-sne. Journal of Research Machine Learning 9, 2579-2605. Résumé Ce document propose un système semi-interactif pour l'exploration de données visuelles à l'aide d'un regroupement qui combine une approche automatique avec un interactif. L'utilisateur peut manuellement effectuer le regroupement, il peut aussi choisir de laisser l'approche automatisée de trouver des solutions optimales et interagir avec le processus afin d'améliorer les résultats de regroupement en fonction de sa perception visuelle et la connaissance de domaine. Les expériences montrent que l'approche semi-interactive obtient de meilleurs résultats que d'autres. - 326 - Fouille interactive, la visualisation, l'image Clustering semi-Boudjeloud Lydia Interactif Visuel, Philippe Pinheiro, Alexandre Blansché, Thomas Tamisier, Benoit Otjacques"
176,Revue des Nouvelles Technologies de l'Information,EGC,2016,Fairness-Aware Data Mining,"In data mining we often have to learn from biased data, because, for instance, data comesfrom different batches or there was a gender or racial bias in the collection of social data. Insome applications it may be necessary to explicitly control this bias in the models we learn fromthe data. Recently this topic received considerable interest both in the research community aswell as more general, as witnessed by several recent articles in popular news media such asthe New York Times. In this talk I will introduce and motivate research in fairness-aware datamining. Different techniques in unsupervised and supervised data mining will be discussed,dividing these techniques into three categories: algorithms of the first category adapt the inputdata in such a way to remove harmful biases while the second adapts the learning algorithmsand the third category modifies the output models in such a way that its predictions becomeunbiased. Furthermore different ways to quantify unfairness, and indirect and conditionaldiscrimination will be discussed, each with their own pros and cons. With this talk I hope toconvincingly argument the validity and necessity of this often contested research area.",Toon Calders,http://editions-rnti.fr/render_pdf.php?p1&p=1002147,http://editions-rnti.fr/render_pdf.php?p=1002147,en,"L'équité-Aware Data Mining Toon Calders * * Université Libre de Bruxelles (ULB) - CP 165/15 Avenue F.D. Roosevelt 50, B-1050 Bruxelles toon.calders@ulb.ac.be http://cs.ulb.ac.be/members/tcalders Biographie Toon Calders a obtenu son doctorat à l'Université d'Anvers en Belgique en 2003. Il re- cemment rejoint en 2012 l'ULB où il occupe la chaire en Business Intelligence, après avoir travaillé pendant 6 ans à l'Université de technologie d'Eindhoven aux Pays-Bas en tant que profes- seur adjoint au département des mathématiques et de l'informatique. Ses principaux domaines de recherche sont la Business Intelligence, la découverte des connaissances dans les bases de données, l'apprentissage automatique et data mining. Toon Calders est rédacteur en chef de la zone de la revue Data Mining Springer, il a été l'un des président du programme de la conférence de ECMLPKDD en Septembre 2014, et sera l'un des présidents du programme de la conférence Discovery Science en 2016. Toon Calders publié plus de 60 articles en la zone d'extraction de données comprenant des 17 articles de revue dans les lieux d'extraction de données à haut niveau (DMKD, KDD, ICDM, SDM, ECML / PKDD). Ses recherches portent notamment sur l'exploitation minière de modèle, la résolution de l'entité, la discrimination et l'équité exploration de données au courant, et le traitement des flux de données. Résumé des données que nous avons souvent minières apprendre des données biaisées, parce que, par exemple, les données proviennent de différents lots ou il y avait un sexe ou préjugés raciaux dans la collecte des données sociales. Dans certaines applications, il peut être nécessaire de contrôler explicitement ce biais dans les modèles que nous apprenons à partir des données. Récemment, ce sujet a suscité un intérêt considérable tant dans la communauté de la recherche, ainsi que plus générale, comme en témoignent plusieurs articles récents dans les médias d'information populaires tels que le New York Times. Dans cet exposé, je présenterai et de motiver la recherche dans le secteur minier de données équité Connaisseur. Différentes techniques dans l'exploration de données non supervisée et supervisée seront discutées, divisant ces techniques en trois catégories: les algorithmes de la première catégorie adapter les données d'entrée de manière à éliminer les préjugés néfastes tandis que le second adapte les algorithmes d'apprentissage et la troisième catégorie modifie la sortie modèles de telle sorte que ses prédictions deviennent non biaisée. De plus façons différentes pour quantifier l'injustice et la discrimination indirecte et conditionnelle seront discutés, chacun avec leurs propres avantages et inconvénients. Avec cet exposé, je l'espère à l'argument de façon convaincante la validité et la nécessité de ce domaine de recherche souvent contestée. - 3 -"
189,Revue des Nouvelles Technologies de l'Information,EGC,2016,"Learning from Massive, Incompletely annotated & Structured Data","The MAESTRA project (http://maestra-project.eu/) addresses the ambitious taskof predicting different types of structured outputs in several challenging settings, suchas semi-supervised learning, mining data streams and mining network data. It developsmachine learning methods that work in each of these settings, as well as combinationsthereof. The techniques developed are applied to problems from the area of biology andbioinformatics, sensor data analysis, multimedia annotation and retrieval, and socialnetwork analysis. The talk will give an introduction to the project and the topicsit addresses, an overview of the results of the project, and a detailed description ofselected techniques and applications: Semi-supervised learning for structured-outputprediction (SOP) and SOP on data streams will be discussed for the task of multitargetregression (MTR), as well as applications of MTR for the annotation/retrievalof images.",Saso Dzeroski,http://editions-rnti.fr/render_pdf.php?p1&p=1002149,http://editions-rnti.fr/render_pdf.php?p=1002149,en,"Apprendre de Massive, incomplètement annotée et données structurées Saso Dz̆eroski * * Département des technologies du savoir, Józef Stefan Institut Jamova cesta 39, Ljubljana, Slovénie. http://www-ai.ijs.si/SasoDzeroski/ Biographie Saso Dz̆eroski est conseiller scientifique à l'Institut Jozef Stefan et le Centre d'excellence pour les approches intégrées en chimie et biologie des protéines, à la fois à Ljubljana, en Slovénie. Il est également professeur titulaire au Jozef Stefan international Postgra- duate école. Ses recherches portent principalement dans le domaine de l'apprentissage machine et l'exploration de données (y compris la prévision de sortie structurée et la modélisation automatisée des systèmes dynamiques) et leurs applications (principalement en sciences de l'environnement, y compris. Écologie et sciences de la vie, les systèmes y compris. Biologie). Il est co-auteur / co-éditeur de plus de dix livres / volumes, ding inclu- « de programmation logique inductive », « Data Mining Relational », « l'apprentissage des langues dans la logique », « informatique Découverte des connaissances scientifiques » et « inductives - bases et contraintes fondé sur l'exploration de données ». Il a participé à de nombreux projets de recherche internationaux (principalement financés par l'UE) et coordonné deux d'entre eux dans le passé. Il est actuellement le coordinateur du projet FET xtrack Maestra (apprentissage de Massive, incomplètement annotée et données structurées) et l'un des principaux vestigators in- dans le FET Flagship Human Brain Project. Résumé Les adresses projet Maestra (http://maestra-project.eu/) la tâche ambitieuse de prédire différents types de sorties structurés dans plusieurs contextes difficiles, tels que l'apprentissage semi-supervisé, extraction des flux de données et les données du réseau minier. Il développe des méthodes d'apprentissage de la machine que le travail dans chacun de ces paramètres, ainsi que des combinaisons de ceux-ci. Les techniques développées sont appliquées à des problèmes de la région de la biologie et de la bio-informatique, l'analyse des données du capteur, l'annotation multimédia et la récupération et l'analyse des réseaux sociaux. La conférence donnera une introduction au projet et les sujets qu'elle traite, un aperçu des résultats du projet, et une description détaillée des techniques et des applications sélectionnées: apprentissage semi-supervisé pour la prédiction sortie structurée (SOP) et SOP sur les données cours d'eau seront discutés pour la tâche de régression cible multi (MTR), ainsi que des applications de MTR pour l'annotation / récupération d'images. - 7 -"
210,Revue des Nouvelles Technologies de l'Information,EGC,2016,TOM: A library for topic modeling and browsing,"In this paper, we present TOM (TOpic Modeling), a Python libraryfor topic modeling and browsing. Its objective is to allow for an efficient analysisof a text corpus from start to finish, via the discovery of latent topics. To thisend, TOM features advanced functions for preparing and vectorizing a text corpus.It also offers a unified interface for two topic models (namely LDA usingeither variational inference or Gibbs sampling, and NMF using alternating leastsquarewith a projected gradient method), and implements three state-of-the-artmethods for estimating the optimal number of topics to model a corpus. What ismore, TOM constructs an interactive Web-based browser that makes exploringa topic model and the related corpus easy.","Adrien Guille, Edmundo-Pavel Soriano-Morales",http://editions-rnti.fr/render_pdf.php?p1&p=1002199,http://editions-rnti.fr/render_pdf.php?p=1002199,en,"TOM: Bibliothèque A pour la modélisation de sujet et navigation Adrien Guille *, Edmundo-Pavel Soriano-Morales * * Laboratoire ERIC, Université Lumière Lyon 2 adrien.guille@univ-lyon2.fr, edmundo.soriano-morales@univ-lyon2.fr Résumé . Dans cet article, nous présentons TOM (Modélisation Topič), une bibliothèque Python pour la modélisation et la navigation sujet. Son objectif est de permettre une ana- lyse efficace d'un corpus de textes du début à la fin, par la découverte de sujets cachés. A cet effet, TOM propose des fonctions avancées pour la préparation et vectorisation un texte cor- pus. Il offre également une interface unifiée pour deux modèles de sujet (à savoir LDA à l'aide de l'inférence soit variationnelle ou échantillonnage de Gibbs, et NMF à l'aide de carrés moins avancés en alternance avec une projection de méthode gradient), et met en œuvre trois méthodes état de l'art pour estimer la valeur optimale nombre de sujets au modèle un corpus. Qui plus est, TOM construit un navigateur basé sur le Web interactif qui permet d'explorer un modèle de sujet et le corpus connexe facile. 1 modèles Présentation Sujet sont des outils utiles pour dévoiler la structure d'actualité latente des corpus de textes. Ils peuvent faire la recherche, la navigation et la synthèse de ces corpus plus facile. Plusieurs modèles et des algorithmes pour les rapproche ont été proposées ces dernières années. La qualité des sujets découverts dépend du modèle, l'algorithme d'approximation, la nature du corpus étudié, ainsi que le nombre de sujets (Stevens et al., 2012). Par conséquent, afin d'effectuer une analyse comparative thématique efficace d'un corpus de texte, il est important de comparer plusieurs approches pour identifier les sujets les plus pertinents. Toutefois, cela est une tâche difficile, car les implémentations existantes des algorithmes d'approximation sont indépendants, ce qui signifie que l'on doit apprendre comment les données sont structurées dans chaque mise en œuvre; ce que les fonctions de manipuler chaque modèle de sujet sont; comment adapter ces modèles de sujet sur l'ensemble exactement les mêmes caractéristiques, etc. D'autre part, plusieurs méthodes ont été proposées pour estimer le nombre optimal de sujets pour modéliser un corpus, mais - au meilleur de nos connaissances - leurs mises en œuvre sont pas accessible au public. Dans ce court article, nous présentons TOM (Modélisation Topič), une bibliothèque open source écrit en Python pour l'analyse d'un corpus de textes du début à la fin, par la découverte de sujets cachés. Outre les fonctions de préparation de corpus avancées, TOM offre une interface unifiée pour les implémentations existantes robustes d'algorithmes d'approximation, qui rend la manipulation et montage des modèles de sujet facile. Il met également en œuvre plusieurs fonctions pour estimer le nombre optimal de sujets pour modéliser un corpus. Qui plus est, TOM peut construire automatiquement une interface Web pour explorer un modèle de sujet et un corpus de manière interactive. - 451 - TOM: Bibliothèque A pour la modélisation de sujet et navigation 2 bibliothèque proposée Dans cette section, nous avons d'abord décrire les capacités de la bibliothèque proposée, TOM, nous illustrons comment l'utiliser avec l'aide de courts extraits de code. Les sources et la documentation sont disponibles en ligne à https://github.com/AdrienGuille/TOM. 2.1 Caractéristiques TOM fonctionne sur un corpus de texte, éventuellement complété par des méta-données telles que les auteurs ou les dates d'écriture / publication. préparation Corpus Préparation du corpus est fondamental, dans le sens où la pertinence de tout traitement ultérieur dépend de cette étape. fonctions de préparation avancées sont disponibles pour les français et en anglais. TOM peut lemmatiser français en utilisant Melt, un système de marquage partiel de synthèse vocale basée sur un modèle de Markov entropie maximale spécialement conçu pour le français (Denis et Sagot, 2012), et Lefff, un lexique morphologique et syntaxique pour le français (Sagot, 2010), à apparier avec lemmes {mot, une partie du discours}. Il peut également lemmatiser anglais d'une manière similaire, en utilisant un autre modèle d'entropie maximale formé pour l'anglais (Bird et al., 2009) et le lexique WordNet (Miller, 1995). Finalement, TOM construit la représentation d'espace vectoriel avec unigrammes ou n-grammes comme caractéristiques, en utilisant soit tf i · df ou simplement tf. L'espace vectoriel est une matrice n × m, avec n le nombre de textes et m le nombre de fonctions. modèles sujet donné la représentation d'espace de vecteur d'un corpus et un petit nombre de feuilles supérieure ics k (k m), un modèle de sujet se compose de deux matrices: W et H. W est une matrice n × k qui décrit les textes en fonction de thèmes, et H est une matrice k × m qui décrit les sujets en termes de caractéristiques (à savoir des mots ou des n-grammes de mots). Plus précisément, le wi coefficient, j définit l'importance du sujet j dans le texte i, et le coefficient salut, j définit l'importance de la fonction j dans le sujet i. Deux modèles sont disponibles dans le sujet TOM: (i) Latent Dirichlet Allocation (LDA), un modèle de sujet générative probable- abilistic proposé par Blei et al. (2003), et (ii) Méthode non négatif factorisation de la matrice (NMF), un espace vectoriel factorisation qui a récemment devenu populaire pour la modélisation sujet (Berry et Browne, 2005). En ce qui concerne LDA, la bibliothèque propose deux algorithmes: approximation de l'algorithme d'inférence variationnelle originale et la variante de Gibbs (Griffiths et Steyvers, 2004). En ce qui concerne NMF, elle repose sur un algorithme basé sur les moindres carrés alternant avec descente de gradient projeté (Lin, 2007). Estimation des paramètres Choix d'un nombre approprié de sujets est essentiel pour assurer une modélisation d'un per- tinent corpus de texte. TOM met en œuvre trois méthodes pour guider ce choix: (i) la méthode basée sur la stabilité proposée par Greene et al. (2014), (ii) la méthode basée sur le consensus pro- posé par Brunet et al. (2004), et (iii) la méthode de la divergence proposé par Arun et al. (2010). Chacune de ces méthodes est basée sur une hypothèse particulière et conduit à la computa- tion d'une mesure spécifique, dont la valeur est liée à la qualité d'un modèle de sujet pour un corpus et un certain nombre de sujets. TOM offre également des fonctions pour tracer ces mesures afin de faciliter leur inspection visuelle. - 452 - A. Guille et E.P. Soriano-Morales Sujet navigateur modèle Les offres du navigateur modèle sujet 3 aperçus: l'index des auteurs, le vocabulaire complet et le nuage de sujet, où chaque sujet est représenté par une bulle marquée avec les mots les plus importants et dont le diamètre est proportionnel à sa fréquence globale. Il offre également des vues détaillées interactives pour: chaque thème, chaque document, chaque auteur et chaque fonction (à savoir des mots ou n-gramme de mots) de l'espace vectoriel. Par exemple, la vue détaillée sur un sujet présente les traits les plus pertinents, l'évolution de la fréquence de sujet à travers le temps, la liste des textes connexes et le réseau de collaboration que les auteurs de liens. La présentation détaillée de texte présente un des plus importants des caractéristiques, la distribution de sujet et les textes plus similaires. Notez que certains éléments peuvent être absents, selon les méta-données disponibles avec le corpus d'entrée. 2.2 Utilisation Charger et préparer un corpus de texte L'entrefilet code suivant montre comment charger un corpus de documents en français, les lemmatiser et vectoriser les utiliser unigrammes et Tf · idf. corpus = Corpus (SOURCE_FILE_PATH = 'entrée / raw_corpus.csv', language = 'français', # langue stop-mots vectorisation = 'TFIDF', n_gram = 1, max_relative_frequency = 0,8, min_absolute_frequency = 4, préprocesseur = FrenchLemmatizer ()) print 'taille du corpus:', print corpus.size 'la taille du vocabulaire:', len (corpus.vocabulary) print 'vecteur pour le document 0: \ n', corpus.vector_for_document (0) instancier un modèle de sujet et d'estimer le nombre optimal de sujets ici, nous instancier un modèle thématique basé NMF et générer des tracés pour les trois paramètres pour estimer le nombre optimal de sujets pour modéliser le corpus chargé. topic_model = NonNegativeMatrixFactorization (corpus) à savoir = visualisation (topic_model) viz.plot_greene_metric (min_num_topics = 5, max_num_topics = 50, tao = 10, étape = 1, top_n_words = 10) viz.plot_arun_metric (min_num_topics = 5, max_num_topics = 50, = itérations 10) viz.plot_consens_metric (min_num_topics = 5, max_num_topics = 50, = 10 itérations) pour utiliser avec LDA Gibbs échantillonnage au lieu de NMF, il suffit de remplacer la première ligne avec les éléments suivants: topic_model = Allocation de Dirichlet latente (corpus, méthode = ») gibbs Déduire un modèle de sujet et enregistrer / charger Pour permettre la réutilisation des modèles de sujets déjà appris, TOM peut les enregistrer sur le disque, comme indiqué ci-dessous. topic_model.infer_topics (num_topics = 15) utils.save_topic_model (topic_model, 'sortie / NMF_15topics.tom') topic_model = utils.load_topic_model ( 'sortie / NMF_15topics.tom') - 453 - TOM: Une bibliothèque pour la modélisation de sujet et les informations de navigation Imprimer à propos d'un modèle sujet Cet extrait de code illustre comment on peut manipuler un modèle de sujet, par exemple obtenir la distribution de sujet pour un texte (un vecteur de longueur k) ou la distribution de mot pour un sujet (un vecteur de longueur m). imprimer « \ la distribution nTopic pour le document 0: », \ topic_model.topic_distribution_for_document (0) print « \ la distribution nword pour sujet 0: », \ topic_model.word_distribution_for_topic (0) Pour faciliter les interactions avec les modèles sujet, TOM offre des fonctions de niveau supérieur, par exemple d'identifier le sujet avec le poids le plus élevé pour un document donné, pour obtenir le plus de mots pertinents pour un sujet donné ou imprimer rapidement tous les sujets. imprimer '\ Nmost sujet probablement pour le document 0:', \ topic_model.most_likely_topic_for_document (0) imprimer '\ n10 mots les plus pertinents pour le sujet: 0', \ topic_model.top_words (0, 10) print '\ nTopics:' topic_model.print_topics (NUM_WORDS = 10) 3 démonstration Dans cette démo, le public sera invité à explorer l'anthologie EGC (817 articles publiés de 2004 à 2015) par le biais d'un navigateur modèle de sujet construit automatiquement par TOM. Voir Guille et al. (2016) pour plus de détails sur ce modèle de sujet. Fig. 1 et Fig. 2 (page 5) donnent un aperçu de ce que les partici- pants auront accès. Ils montrent respectivement la vue d'ensemble des nuages ​​de sujet de l'anthologie, et les queues dé- sur l'un des sujets. Ce navigateur modèle sujet peut être consulté en ligne à l'adresse http: //mediamining.univ- lyon2.fr/people/guille/egc2016. Les participants auront également l'occasion d'explorer un autre navigateur modèle de sujet basé sur les transcriptions des 900+ discours prononcés par François Hollande comme président de la France, entre mai 2012 et Janvier 2016. 4 Les travaux futurs travaux futurs comprend l'ajout de modèles plus sujet et approximation algorithmes. Plus particulièrement, il serait intéressant de mettre en œuvre un algorithme d'approximation qui optimise la Kullback-Leibler diver- fonction objectif gence, puisque Ding et al. (2008) ont montré que NMF est alors équivalente à probabilistes analyse sémantique latente (PLSA), un modèle de sujet séminal proposé par Hofmann (1999). Références Arun, R., V. Suresh, C. V. Madhavan, et M. N. Murthy (2010). En trouvant le nombre naturel de sujets avec l'allocation Dirichlet latente: Quelques observations. En PAKDD, pp. 391-402. Berry, M. W. et M. Browne (2005). surveillance de courriel en utilisant la matrice non négative factorisation. Journal of Computational et organisation mathématique Théorie 11 (3), 249-264. Oiseau, S., E. Klein et E. Loper (2009). Traitement du langage naturel avec Python. O'Reilly Media. Blei, D. M., A. Y. Ng, et M. I. Jordan (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research 3, 993-1022. - 454 - A. Guille et E.P. Soriano-Morales figure. 1 - Vue d'ensemble du corpus avec le nuage de sujet. FIGUE. 2 - Vue détaillée sur l'un des sujets découverts. - 455 - TOM: Bibliothèque A pour la modélisation de sujet et navigation Brunet, J., P. Tamayo, et J. Golub, T.R.and Mesirov (2004). Metagenes découverte de motif moléculaire à l'aide de factorisation de la matrice. Actes de l'Académie nationale des sciences 101 (12), 4164 à -4169. Denis, P. et B. Sagot (2012). Le couplage d'un corpus annoté et un lexique pour l'état de l'art marquage pos. Ressources Linguistiques et évaluation 46 (4), 721-736. Ding, C., T. Lib et W. Peng (2008). Statistiques de calcul et d'analyse des données (52), 3913 à -3927. Greene, D., D. O'Callaghan et P. Cunningham (2014). Combien de sujets? analyse de la stabilité pour les modèles sujet. Dans ECML PKDD, pp. 498-513. Griffiths, T. L. et M. Steyvers (2004). Trouver des sujets scientifiques. Compte rendu de la Nati Académie des sciences onal 101, 5228-5235. Guille, A., E. P. Soriano Morales et C. O. Truica (2016). Modélisation du sujet et l'exploration de hypergraphe pour analyser l'histoire de la conférence EGC. En EGC. Hofmann, T. (1999). l'indexation sémantique latente probabilistes. En SIGIR, pp. 50-57. Lin, C. J. (2007). les méthodes de gradient projeté pour matrice non négative factorisation. tion neurale Computa- 19, 2756-2779. Miller, G. A. (1995). Wordnet: Une base de données lexicale pour l'anglais. Communications de l'ACM 38 (11), 39-41. Sagot, B. (2010). Le Lefff, un libre accès et grande couverture lexique morphologique et syntaxique pour le français. En LRGC. Stevens, K., P. Kegelmeyer, D. Andrzejewski et D. Buttler (2012). Explorer la cohérence des sujets sur de nombreux modèles et de nombreux sujets. Dans EMNLP-CoNLL, pp. 952-961. Article résumé This TOM present, Une bibliothèque Python Pour la modélisation et l'exploration de l'Objectif Dont thématiques is de de permettre d'analyser juin Mener Efficace, de bout en bout, d'un corpus Textuel par la découverte de Latentes thématiques. TOM des offres la préparation répandrai fonctions et la vectorisation de corpus interface Une Unifiée Pour Deux de Modèles thématiques (LDA et NMF), et methods verser trois implémente le Estimer optimal de Nombre thématiques. Par ailleurs, TOM CONSTRUIT ONU EXPLORATEUR automatiquement FACILEMENT D'Interactif etudier permettant Modele de l'ONU les documents THÉMATIQUES ET Liés. - 456 -"
211,Revue des Nouvelles Technologies de l'Information,EGC,2016,Topic modeling and hypergraph mining to analyze the EGC conference history,"Dans le cadre du défi proposé à l'édition 2016 de la conférence EGC, nous exploitons lesarticles qui y ont été publiés de 2004 à 2015, avec pour but d'expliquer sa structure et sonévolution. A partir des thématiques latentes découvertes et d'autres propriétés des articles (e.g.auteurs, affiliations), nous mettons en lumière des caractéristiques intéressantes des structuresthématique et collaborative d'EGC. A l'aide d'une méthode d'extraction d'itemsets dans leshyper-graphes nous mettons aussi en avant des liens latents entre auteurs ou entre thématiques.De plus, nous proposons des recommandations d'auteurs ou de thématiques. Enfin, nous décrivonsune interface Web pour explorer les connaissances découvertes.","Adrien Guille, Edmundo-Pavel Soriano-Morales, Ciprian-Octavian Truica",http://editions-rnti.fr/render_pdf.php?p1&p=1002191,http://editions-rnti.fr/render_pdf.php?p=1002191,en,"Modélisation du sujet et l'exploration de hypergraphe pour analyser l'histoire de la conférence de EGC Adrien Guille *, Edmundo-Pavel Soriano-Morales *, Ciprian-Octavian Truica ** * Laboratoire ERIC, Université Lumière Lyon 2 {adrien.guille; edmundo.soriano-} @univ Morales -lyon2.fr ** département informatique., Université Politehnica de Bucarest ciprian.truica@cs.pub.ro Résumé. Chaque année, la conférence EGC rassemble des chercheurs et des praticiens du domaine de la découverte des connaissances et la gestion de présenter leurs dernières avancées. L'édition propose un défi ouvert qui encourage les partici- pants à tirer parti de la riche anthologie EGC de cette année qui se étend de 2004 à 2015. L'objectif ultime est de mettre en évidence la dynamique de l'histoire de la conférence et d'essayer d'obtenir un aperçu des années à venir. Dans ce contexte, nous décrivons d'abord notre méthodolo- gie pour déduisant sujets latents qui imprègnent ce corpus en utilisant ma- non négatif trix factorisation. Sur la base des sujets découverts et d'autres propriétés des articles (par exemple, les auteurs, les affiliations) nous a mis en lumière des faits intéressants sur les structures d'actualité et de collaboration de la société EGC. En second lieu, nous utilisons un procédé d'extraction de itemset hypergraphe pour découvrir les relations existantes mais latentes entre auteurs ou entre les sujets. Nous vous proposons également des recommandations sujet-auteur et auteur avec une approche Auteur- basée sur le contenu. Enfin, nous décrivons une interface Web pour la navigation de cette collection d'articles complétés par la connaissance découverte. 1 Introduction Dans cet article, nous décrivons le travail fait dans le cadre de la première édition du Lenge chal- EGC, dont le but ultime est de mettre en évidence la dynamique de l'histoire de la conférence et d'essayer d'obtenir un aperçu des années à venir. Les participants du jeux de données défi sont fournis avec les descriptions de 1935 articles Publica- tion RNTI, dont 1041 sont des articles présentés à la conférence EGC. Chaque article est décrit par plusieurs champs: année, titre, résumé (potentiellement manquantes), la liste des auteurs, et une URL pointant vers la première page de l'article (potentiellement manquant ou injoignable). Quand il est possible, nous enrichissons les descriptions des articles avec (i) la langue détectée de leurs résumés et (ii) les affiliations auteurs. Détection de la langue repose sur un Bayes naïf classi fier, formé sur les articles de Wikipedia couvrant français et en anglais, en utilisant n-grammes de caractères comme caractéristiques (Cavnar et Trenkle, 1994). L'identification des affiliations des auteurs repose sur des expressions régulières pour correspondre aux adresses e-mail dans le contenu de la première page des articles, en supposant - 383 - Modélisation du sujet et des mines de hypergraphe pour analyser l'histoire de la conférence EGC TAB. 1: statistiques DataSet. Type d'articles Count (environ. Proportion) articles pour lesquels le résumé est disponible 896 (86%) articles pour lesquels la première page est disponible 936 (89,9%) des articles pour lesquels la langue détectée est le français 817 (78,5%) Articles pour lesquels affiliation sont connus 893 (86%) que les domaines de ces adresses permettent d'identifier les institutions. Languette. 1 donne quelques statistiques sur ce jeu de données. Principaux résultats et organisation du papier Dans Sec. 2, nous présentons les résultats basés sur des sujets latents découverts à partir du contenu des résumés des articles. Nous constatons que la portée de la conférence EGC évolue au fil du temps, notamment pour incorporer de nouvelles questions intéressantes. Nous constatons aussi que, même si la plupart des travaux présentés à la conférence émane du milieu universitaire, l'industrie est très actif sur certaines questions ciblées. De plus, on observe que les auteurs ont différentes façons de collaborer à l'intérieur d'un sujet ou sur des sujets, et que certains auteurs sont très spécialisés sur un sujet alors que d'autres sont un peu généralistes. Dans Sec. 3, nous détaillons les résultats pour deux recommandations approches: l'exploitation minière hypergraphe et recommandation basée Content-. Nous constatons que les recommandations proposées ont un sens en fonction des thèmes de chaque auteur travaille avec. Nous avons aussi observer qu'il ya certains domaines de recherche qui semblent être intéressant de se développer. De plus, nous montrons des recommandations intéressantes pour très publiées et de nouveaux-auteurs à venir à EGC. Dans Sec. 4, nous décrivons une interface Web pour explorer l'anthologie complétée par les connaissances extraites. 2 Structure topique de la conférence EGC Le but de cette section est de mettre en lumière quelques faits intéressants au sujet de la conférence EGC, en fonction de sa structure d'actualité. Nous commençons par décrire notre méthodologie pour déduisant sujets latents des résumés des articles, avant d'analyser ces sujets dans les détails. 2.1 Méthodologie Préparation des articles à éviter des sujets spécifiques de langue, et parce que EGC est principalement une conférence de langue française, nous choisissons de ne considérer que des articles qui font abstraction est écrit en français. Afin d'améliorer la qualité des sujets découverts, nous lemmatiser les résumés. Pour cela, nous utilisons Melt, un sys- tème de marquage maximum d'entropie de Markov modèle à base de partie vocale spécialement conçu pour le français (Denis et Sagot, 2012), et Lefff, un lexique morphologique et syntaxique pour le français (Sagot, 2010) , pour correspondre à des paires {mot, une partie du discours} avec mas lem-. De plus, nous pruneau lemmes dont la fréquence absolue dans le corpus est inférieur à 4, ainsi que lemmes dont la fréquence relative est supérieure à 80%, dans le but de ne garder que les plus importantes. Finalement, nous construisons la représentation de l'espace vectoriel de ces articles avec tf · pondération idf. Il est une matrice n × m désigné par A, où chaque ligne représente un article, avec n = 817 (à savoir le nombre d'articles) et m = 1739 (à savoir le nombre de lemmes). - 384 - A. Guille et al. 20 40 0,55 0,6 0,65 0,7 Nombre de sujets St ab ty ili figure. 1: mesure pondérée de la stabilité moyenne Jaccard pour un certain nombre de sujets variant tween BE- 5 et 50 (plus est mieux). 20 40 0,1 0,2 Nombre de sujets D iv er ge nc e FIG. 2: Symmetric Kullback-Liebler de gence de pour un certain nombre de sujets variant entre 5 et 50 (inférieur est mieux). Le choix d'un modèle de sujet donné la matrice A et un petit nombre de sujets k (k m), un modèle de sujet consiste en deux matrices: W etH. W est une matrice n × k qui décrit les articles en termes de sujets, et H est une matrice k × m qui décrit les sujets en termes de mots. Plus précisément, le wi coefficient, j définit l'importance du sujet j à l'article i, et le coefficient salut, j définit l'importance du mot j dans le sujet i. Nous considérons deux méthodes de candidats pour la modélisation sujet: (i) Latent Dirichlet Allocation (LDA), un modèle probabiliste sujet générative proposé par Blei et al. (2003), et (ii) Méthode non négatif factorisation de la matrice (NMF), un espace vectoriel factorisation qui a récemment devenu populaire pour la modélisation sujet (Berry et Browne, 2005). Il convient de noter que nous utilisons pour l'inférence variationnelle LDA et une méthode de gradient prévu pour NMF. L'estimation du nombre optimal de sujets statuent sur une valeur appropriée de k est essentielle pour assurer une analyse pertinente de l'anthologie EGC. Si k est trop petit, les sujets seront découverts trop vagues; si k est trop grand, alors les sujets découverts seront trop étroits et peuvent être redondants. Pour nous aider dans cette tâche, nous calculons deux mesures. Tout d'abord, on calcule la métrique proposée par Greene et al. (2014), qui est basé sur l'hypothèse selon laquelle un modèle avec un nombre approprié de sujets est plus robuste aux données manquantes. Compte tenu d'une valeur de k, il évalue la stabilité des sujets découverts pour plusieurs modèles équipés de sous-échantillons des données d'origine. La stabilité est mesurée en fonction de la distance de Jaccard moyenne pondérée entre les ensembles ordonnés de mots décrivant les sujets. Ainsi, plus la stabilité, la k plus proche est à une valeur appropriée. On calcule également la mesure proposée par Arun et al. (2010), qui est basé sur l'hypothèse que la distribution des valeurs singulières de H est proche de la répartition de la norme L2 de la ligne-W lorsque k est proche d'un nombre optimal de sujets. Cette mesure est définie comme t il symétrique divergence de Kullback-Liebler entre ces deux distributions, ce qui signifie que plus la divergence est le plus proche k est un nombre approprié de sujets. 2.2 Résultats Nous constatons que LDA et NMF identifier dans ce corpus sujets très similaires, avec un léger avantage pour NMF en termes de sujet séparabilité. Guidé par les deux mesures décrites précédemment, nous évaluons manuellement la qualité des sujets identifiés avec k variant entre 15 et 20. - 385 - Modélisation du sujet et l'exploitation minière hypergraphe pour analyser l'histoire de la conférence EGC TAB. 2: La description des sujets découverts. id Sujet le plus sujet mots pertinents 0 réseau, social, communauté, détection, méthode, analyse, interaction, lien sujet 1 ontologie, alignement, sémantique, annotation, concept, domaine, hibou, Entre sujet 2 règle, association, extraction, mesure, la base , Extraire, confiance, sujet indice 3 séquence, temporel, Événement, série, modèle, évènement, vidéo, spatio- sujet 4 motif, séquentiel, extraction, contrainte, fréquente, Extraire, découverte, sujet Donnée 5 document xml, annotation, recherche, l'information, la structure, requête, MOTs sujet 6 Utilisateur, web, information, site, Système, page, sémantique, sujet comportement 7 Connaissance, gestion, expert, l'agent, système, compétence, sujet de modélisation 8 variables, classification, surveillant, méthode, classe , apprentissage, sujet de l'image 9 sélection, afc, segmentation, recherche, région, objet, classification, sujet par satellite 10 graphè, voisinage, représentation, interrogation, fouille, la visualisation, le sujet de la structure 11 Donnée, flux, la base, requête, cube, fouille, visualisation, sujet entrepôt 12 Algorithme, arbre, svm, ensemble, décision, nouveau, grand, sujet Résultat 13 carte, topologique, auto, organisatrice, som, cognitif, probabiliste, sujet de contrainte 14 texte, corpus, automatique, textuel, partir, méthode, opinion, clr à titre d'illustration, la figure 1 et la figure 2 respectivement tracer les mesures basées sur la stabilité et sur la base de divergence pour k ∈ [5..; 50] à l'aide NMF. Finalement, on juge que les meilleurs résultats sont obtenus avec NMF pour k = 15. Le tableau 2 donne la liste des mots pertinents pour chacun des 15 sujets découverts à partir des articles avec NMF. Ils révèlent que les personnes qui forment la société de l'EGC sont inté- ressent dans une grande variété de ces deux questions théoriques et appliquées. sujets 8 et 12, par exemple, sont liés à des questions théoriques: 8 sujet couvre des documents sur le modèle et la sélection des variables, et le sujet 12 couvre documents qui proposent des algorithmes d'apprentissage nouveaux ou améliorés. D'autre part, les sujets 0 et 6 sont liés à des problèmes: le sujet 0 couvre documents sur l'analyse des réseaux sociaux, et le sujet 6 couvre des papiers sur l'exploitation minière de l'utilisation du Web. Dans ce qui suit, nous misons sur les thèmes découverts pour mettre en évidence des particularités intéressantes au sujet de la société EGC. Pour être en mesure d'analyser les sujets, complétées par des informations sur les documents connexes, nous divisons les papiers en 15 groupes de non-chevauchement, à savoir un groupe par sujet. Chaque article i ∈ [0; 1- n] est affecté à la classe j qui correspond à la question avec la plus haute wij de poids, comme dans l'équation formalisé. 1. clusteri = arg j (wi, j) (1) déviant l'attention, l'évolution des intérêts Fig. 3 montre la fréquence des sujets 0 (analyse des réseaux sociaux et des mines) et 2 (Association minière de la règle) par an, de 2004 à 2015. la fréquence d'un sujet pour une année donnée est défini comme étant la proportion d'articles, parmi ceux publiés cette année, qui appartiennent au cluster correspondant. Ce chiffre révèle deux tendances opposées: 0 sujet est en train d'émerger et le sujet 2 se fane au fil du temps. Bien qu'il n'y ait apparemment pas d'article sur l'analyse des réseaux sociaux en 2004, en 2013, 12% des articles présentés à la conférence étaient liés à ce sujet. En revanche, les documents relatifs à l'extraction de règles d'association sont les plus fréquentes en 2006 (12%), mais leur fréquence a chuté à aussi bas que 0,2% - 386 - A. Guille et al. 0 0,05 0,1 200420062008201020122014 An Fr éq ue nc e sujet 2 sujet 0 FIGUE. 3: Fréquence du sujet 0 (analyse de réseau asocial) et le sujet 2 (extraction de règle asso- tion) par an. 100 200 0 0 1 2 3 4 5 6 7 8 9 1011121314 id Sujet O rd er réseau entier la plus grande reliée figure composant. 4: Ordre du réseau de collaboration et or- der de la plus grande composante connexe pour chaque sujet. 2014. Ceci illustre comment l'attention des membres de la société EGC se déplace entre les sujets au fil du temps. Cela va à montrer que la société EGC évolue et élargit son champ d'incorporer des œuvres sur des questions nouvelles. Collaborations entre les sujets, avec quelques collaborateurs par sujet Fig. 4 montre l'ordre (par exemple nombre de noeuds) des 15 réseaux de collaboration thématiques sage et l'ordre de la plus grande composante connectée dans chacun de ces réseaux. Compte tenu d'un sujet, les nœuds du réseau de collaboration représentent les auteurs des documents attribués au cluster connexes. Edges paires de connexion des auteurs qui a co-écrit un ou plusieurs documents attribués à ce groupe. Nous observons que tous les réseaux de collaboration sont faites de beaucoup de petits composants connectés (la plupart des cliques), à l'exception du réseau de collaboration construit à partir des documents affectés à la section 4 (modèle minier), qui consiste principalement en une grande composante connexe. La proportion moyenne des auteurs dans la plus grande composante connexe dans les réseaux de collaboration thématiques sage est de 0,16 (0,74 pour le thème 4), alors que la proportion des auteurs dans la plus grande composante connecté au réseau mondial (c.-à-fait de toutes les collaborations sans considération des sujets) est d'environ 0,47. Cela indique que les auteurs ont tendance à collaborer avec différents ensemble des auteurs à travers différents sujets. La plupart du temps la recherche universitaire, la recherche industrielle intense sur les questions ciblées Fig. 5 montre le nombre d'articles, par institution, thème 8 (modèle et sélection de variables). Whe- REAS tous les autres sujets est dominé par les établissements d'enseignement - en termes de nombre de publi- cations, ce sujet est dominé par l'industrie. Plus précisément, il y a 12 documents distincts impliquant des auteurs affiliés à Orange (à savoir en utilisant une adresse e-mail quel domaine est soit orange-ftgroup.com, orange-ft.com ou orange.com). auteurs spécialisés, auteurs généralistes figure. 7 et Fig. 8 montrent respectivement la répartition du poids sur les sujets de l'auteur le plus spécialisé (qui a publié 9 articles), et l'auteur le plus généraliste (10 articles) parmi les 88 auteurs qui ont au moins 5 publications. Nous quantifions le degré de spécialisation des auteurs en termes de coefficient de moment de dissymétrie de leur distribution de poids respectifs sur les sujets de Pearson. Plus le dissymétrie, plus - 387 - modélisation du sujet et l'exploitation minière hypergraphe pour analyser l'histoire de la conférence EGC 2 4 6 8 @ orange-ftgroup.com @ univ-paris13.fr @ univ-orleans.fr @ orange.com @inria. fr @ u-cergy.fr @ @ univ-lyon2.fr ceremade.dauphine.fr @ orange-ft.com @ univ-nantes.fr @ isg.rnu.tn Nombre d'articles figure. 5: Nombre d'articles attribués aux affiliations des auteurs sujet 8 vs. affiliations seulement avec au moins 2 publications sont présentés. 0 1 2 3 0 10 20 Skewness N um soit ro fa ut ho rs Fig. 6: Histogramme de par-auteur ness skwe- de la répartition du poids sur ics Top-. 0 5 10 15 0 0,1 0,2 0,3 Sujet W ei gh t FIG. 7: La répartition du poids sur les sujets de l'auteur le plus spécialisé. 0 5 10 15 0 0,02 0,04 0,06 Discussion W ei gh t FIG. 8: La répartition du poids sur les sujets de l'auteur le plus généraliste. spécialisé sur un sujet particulier est un auteur. Pour l'obtention d'un auteur donné, cette distribution en faisant la moyenne des distributions de poids de tous les papiers qu'il a publié. Fig. 6 montre l'histogramme du coefficient de skweness mesuré pour les 88 auteurs mentionnés ci-dessus. Il montre qu'il ya peu d'auteurs de généralistes, avec la plupart des auteurs ayant une dissymétrie entre 1 et 2, ce qui correspond à peu près à une distribution avec deux grands thèmes. 3 Recommandations La section suivante décrit la méthodologie que nous utilisons pour proposer auteur-auteur, le sujet et les recommandations auteur sujet-sujet-. Nous employons deux approches différentes: d'une part, nous considérons comme les interactions entre les auteurs et les transactions fréquentes latentes extrait itemsets en utilisant une méthode d'extraction à base hypergraphe. Les découvertes itemsets peuvent en effet être considérés comme des recommandations. D'autre part, nous faisons directement des recommandations basées sur - 388 - A. Guille et al. un graphique bipartite et en utilisant des mesures de similarité d'article classique. Les deux approches sont décrites ci-dessous. 3.1 Contexte Hypergraphe fondé sur l'exploration Notre objectif est de trouver des relations intéressantes entre les auteurs. À cette fin, nous suivons la méthode de découverte de l'élément d'hypergraphe proposé par Liu et al. (2011). L'objectif est de trouver (entre itemsets fréquents auteurs ou sujets) ne reposent pas sur leur support (fréquence d'apparition), mais sur leur ressemblance telle que mesurée par la distance Commute temps moyen (ACT) Fouss et al. (2007). Il est notre intuition que les auteurs proches (liés par leurs sujets et des années de publication) sont de bons candidats pour travailler ensemble à l'avenir. La distance de l'ACT, une distance de sommet à base de marche aléatoire, a la propriété avantageuse de diminution lorsque le nombre de trajets reliant deux noeuds augmente et lorsque la longueur des trajets diminue. Dans cette expérience, nous travaillons avec le corpus complet EGC français afin de trouver éventuellement une recommandation pour chaque auteur dans l'ensemble de données. recommandation basée sur le contenu Ce type d'utilisations de recommandation attributs pour décrire les éléments qui sont ensuite proposés aux utilisateurs. Dans notre cas, les utilisateurs sont les auteurs et les articles sont les sujets qui sont en effet découverts précédemment trouvés en mettant à profit les termes utilisés dans les articles de l'auteur. Nous utilisons un système de recommandation point-similitude Ricci et al. (2011) pour proposer des auteurs et des sujets pour chaque auteur. En bref, étant donné auteur, notre modèle suggère des sujets classés en fonction de leur ressemblance avec les sujets observés pour l'auteur en question. Grâce à la similitude entre les sujets dits, nous pouvons également trouver les auteurs les plus similaires pour chaque auteur. 3.2 Méthodologie 3.2.1 auteurs minière Hypergraphe Filtrage La première étape que nous suivons est de construire une liste des auteurs de l'anthologie EGC française. A partir des 817 articles, nous avons établi une liste de 1307 auteurs. Nous aurions pu continuer seuls les premiers auteurs apparaissant de chaque article, en supposant qu'ils ont contribué la majeure partie de l'écriture. Néanmoins, le placement de nom de l'auteur est une politique particulière à chaque recherche labo- ratoire qui peuvent ou peuvent ne pas représenter le montant de la contribution au manuscrit soumis. , Nous ne disposons toujours filtrer les auteurs en fonction de leur nombre de publications. Création de l'incidence des matrices hypergraphe Nous voulons influencer la similitude entre deux auteurs en fonction de leurs thèmes communs et de leurs années de co-occurrents de publication. Notre intuition est que deux auteurs peuvent travailler ensemble si elles partagent des sujets similaires et si les deux sont auteurs actifs au cours d'une même période de temps. Afin de tenir compte de ces deux caractéristiques, nous construisons deux matrices d'incidence hypergraphe. Une matrice d'incidence auteur-sujet qui liens auteurs avec des sujets et une matrice d'incidence auteur ans qui décrit la relation entre les auteurs et les années de publication de leurs articles respectifs. La matrice d'incidence sujet de l'auteur est créé en définissant les auteurs comme sommets (lignes) et des sujets comme hyperarêtes (colonnes). Chaque cellule contient 1 si l'auteur correspondant est représenté par l'un des 15 sujets, sinon il contient 0. Dans ce cadre, les auteurs sont d'abord représentés par leurs articles et ceux-ci sont représentés par la liste des sujets trouvés dans la section 2. Ainsi, chaque auteur - 389 - modélisation du sujet et l'exploitation minière hypergraphe pour analyser l'histoire de la conférence EGC représentée indirectement par un ensemble de sujets. Il est clair que l'auteur pourrait ne pas être représenté par tous les sujets trouvés, il est donc important de déterminer le degré d'influence de chaque sujet en fonction du poids attribué par le modèle de sujet. Ainsi, pour un g Iven auteur et pour chacun de ses papiers, nous avons d'abord donner plus d'importance à ces sujets avec un poids plus élevé (ou la probabilité), puis nous sélectionnons les sujets qui apparaissent à plusieurs reprises à travers l'ensemble des documents de l'auteur. Les sujets choisis sont utilisés pour représenter chaque auteur dans la matrice d'incidence auteur-sujet Comme dit précédemment, nous voulons influencer la similitude entre les auteurs en considérant les années qu'ils ont publié. La matrice auteur ans a des auteurs comme sommets (lignes) et les années d'anthologie EGC français (de 2004 à 2015). hyperarêtes Chaque cellule contient 1 si l'auteur correspondant publié dans l'une des années mentionnées. Il contient par ailleurs zéro. Construire la matrice de similarité de SCT Afin de prendre en compte les informations des deux matrices d'incidence décrites précédemment, nous informatise ACT carré similitude 1 matrice (avec des auteurs comme les lignes et les colonnes) pour chaque matrice d'incidence. Enfin, les deux matrices sont réunies en une seule matrice de similarité SCT en faisant la moyenne des matrices en une seule. Trouver les 2-itemsets fréquents Une fois que la matrice de similarité SCT est calculée, nous pouvons dé- couples d'auteurs liés Termine en fixant un seuil sur la valeur de similitude entre chacun d'eux. Si le seuil est dépassé, qui est, deux auteurs ont une similitude supérieure au seuil fixé, on considère les deux comme auteurs fréquents itemset et donc une éventuelle recommandation de coworking. Trouver les meilleurs paramètres de mesure de la performance des systèmes de recommandation est complexe et peut être subjective. En tant que mesure de la performance pour la relation auteur-auteur minier, nous avons mis à maximiser le rapport α défini comme le nombre d'articles écrits réels par les paires intéressantes minées d'auteurs divisé par le nombre total de paires trouvées. Notre système a trois paramètres réglables: (i) θ, la valeur de similarité minimale dans la matrice SCT à considérer quelques auteurs comme liés, (ii) γ, le nombre minimum de publications d'un auteur doit avoir, et (iii) X, le nombre de sujets qui décrivent chaque auteur. Pour trouver le meilleur rapport α possible, on commence une recherche de la grille en utilisant la plage de valeurs suivantes pour chaque paramètre: θ = {0,5, 0,55, 0,6, ..., 0,95}, γ = {3, 4, 5, .. ., 9} et λ = {2, 3, 4, ..., 15}. 3.2.2 recommandations basées sur le contenu Dans cette approche, nous voulons étudier deux groupes spécifiques d'auteurs: l'ancien combattant et le groupe des nouveaux arrivants. auteurs anciens combattants sont les meilleurs chercheurs de dix avec le plus grand nombre de publica- tions. Les nouveaux arrivants auteurs sont les chercheurs qui ont deux ou plusieurs publications et ont commencé à participer à la conférence EGC depuis 2012. Notre objectif est d'offrir à chaque chercheur dans ces groupes une série de recommandations avec lesquelles il serait intéressant de collaborer. Pour les auteurs vétérans, il peut leur permettre de diversifier et d'élargir leur domaine de recherche. Pour les auteurs de nouveaux arrivants, une carte claire qui travaille sur des sujets similaires, à un niveau relativement proche, peut contribuer à améliorer leur carrière universitaire. À tout le moins, ces recommandations peuvent contribuer à faciliter le travail en réseau au cours de la conférence elle-même. Nous montrons les recommandations pour ces auteurs sous forme de graphiques de recommandation ci-dessous. 1. Étant donné que l'ACT est une distance, nous le transformer en une mesure de similarité en normalisant et en soustrayant de celui-ci. - 390 - A. Guille et al. auteurs de filtrage et de représentation Nous retrouvons les auteurs appartenant aux groupes vétérans et nouveaux arrivants en appliquant les filtres décrits ci-dessus. Chaque auteurs est représenté par les cinq sujets (classés par leur poids, tel que déterminé par l'approche NMF ou LDA) qui correspond le mieux les décrire. À ce stade, nous créons un graphe biparti entre auteurs et sujets reliés par des bords pondérés (les poids étant les mêmes décrit précédemment). Auteurs et sujets recommander à recommander des sujets aux auteurs, notre modèle construit d'abord une matrice de similarité entre les sujets utilisant les observations des auteurs décrits par leurs sujets connexes. Ensuite, elle marque un sujet t pour auteur une aide d'une moyenne pondérée sur les observations passées d'une, qui est, les sujets déjà utilisés par un. De la même façon, le modèle recommande auteurs aux auteurs en les classant en fonction de leur similitude avec les sujets partagés entre eux. Le choix des paramètres optimaux et la meilleure mesure de similarité Les paramètres de cette tech- nique sont deux: γ, qui, comme ci-dessus, est le nombre minimum de publications; et λ, le nombre de sujets qui décrivent chaque auteur. Une mesure de similarité est nécessaire pour relier les auteurs avec d'autres auteurs ou avec des sujets. Nous testons trois types de mesures de similarité que: Jaccard, Pearson et similitudes cosinus. Expérimentalement, nous avons constaté que la similitude cosinus est le plus performant tout en maximisant le ratio de la collaboration réelle décrite dans l'approche précédente. 3.3 Résultats auteur-auteur Hypergraphe relations minées Après l'exécution de la recherche de la grille pour trouver les paramètres mal opti- pour la recommandation auteur-auteur, nous avons trouvé la valeur maximale α = 0,33, qui est, un tiers des prévisions trouvées existe déjà dans le EGC anthologie. Les valeurs des paramètres sont: θ = 0,65, γ = 8 et λ = 2. Dans ce contexte, 37 les auteurs sont considérées et 9 prédictions dépassent le seuil de θ. Les relations minées entre les auteurs sont présentés dans le tableau 3. Les trois relations déjà existantes dans l'anthologie EGC sont en caractères gras. D'après nos expériences, nous constatons que ces noms sont en effet proches en fonction des mots partagés par leurs sujets représentatifs. sujet-sujet Hypergraphe relations minées Afin de proposer des recommandations thématiques sujet, qui est, qui deux domaines thématiques pourraient travailler ensemble selon les auteurs impliqués dans les deux, nous transposons simplement la matrice auteur-sujet (décrit ci-dessus au paragraphe 3.2. 1) et de calculer directement la matrice de similarité SCT. Nous utilisons les mêmes paramètres dans la section précédente, à l'exception du nombre de sujets décrivant chaque auteur, λ, qui a été fixé à 5, pour éviter une matrice singulière pendant les calculs. Les résultats sont présentés dans le tableau 4. En général, les recommandations suggèrent l'intersection des technologies du Web sémantique avec des données non structurées, telles que des images et des documents texte. Recommandations pour les auteurs des nouveaux arrivants et d'anciens combattants figure. 9 on peut voir les trois suggestions pour les 10 auteurs anciens. Nous gardons les recommandations seulement entre les auteurs eux-mêmes eran d'EFP. Fig. 10 montre le graphique trois recommandations haut pour les auteurs de nouveaux arrivants. Nous avons décidé de garder tous les écrivains de nouveaux arrivants qui ont satisfait nos seuils pour qu'il soit utile aux chercheurs de nouveaux arrivants. Dans les deux figures les plus épaisses liens décrivent une colla- boration qui a déjà eu lieu. Les liens gris définissent les associations proposées. Nous utilisons - 391 - Modélisation du sujet et des mines de hypergraphe pour analyser l'histoire de la conférence EGC TAB. 3: En couple d'auteurs ont découvert l'approche minière hypergraphe, triées par leur valeur correspondante SCT. En gras, les paires qui ont déjà collaboré à EGC. relations latentes minées SCT Djamel Abdelkader Zighed, Marc Plantevit 1.000 Djamel Abdelkader Zighed, Gilbert Ritschard 0,953 Frédéric FLOUVAT, Marc Plantevit 0,950 Jean-François Boulicaut, Marc Plantevit 0,950 Gilbert Ritschard, Frédéric FLOUVAT 0,920 Jean-François Boulicaut, Gilbert Ritschard 0,915 Djamel Abdelkader Zighed, Sandra Bringay 0,865 Sandra Bringay, Frédéric Flouvat 0,785 Jean-François Boulicaut, Sandra Bringay 0,760 TAB. 4: Top 5 couples de sujets, représentés par leurs trois concepts les plus importants classés par leur valeur correspondante SCT. relations thématiques sujet minées SCT T1: ontologie, alignement, sémantique T9: l'image, afc, segmentation 0,990 T3: Sequence, temporel, T5 Événement: document, xml, annotation 0,844 T5: document, xml, annotation T13: carte, topologique, auto , organisatrice 0,783 T5: document, XML, annotation T11: Donnée, flux, la base 0,770 T7: Connaissance, gestion, T9 expert: l'image, afc, segmentation 0,768 γ = 5 et λ = 7 pour cette expérience. Dans les deux graphiques de collaboration, sous-structures claires peuvent être appréciées. ils suis un y fournissent de nouvelles informations avec plus d'expérimentation. Les recommandations de l'auteur-sujet sont omis en raison des contraintes d'espace. Néanmoins, ils seront inclus dans l'interface Web de notre système, que nous décrivons ci-dessous. 4 Interface Web pour explorer l'EGC anthologie L'interface (disponible à http://mediamining.univ-lyon2.fr/people/guille/egc2016/) offre 3 façons d'avoir une vue d'ensemble des articles avec: l'indice de l'auteur, la complète vocabulaire Pascal PonceletPascal Henri Poncelet Mathieu Briand BriandHenri RocheMathieu Roche Marc Boullé BoulléMarc Yves Lechevallier LechevallierYves Maguelonne TeisseireMaguelonne Anne LaurentAnne Laurent Teisseire Younès Bennani BennaniYounès Christine LargeronChristine Largeron Mustapha LebbahMustapha Lebbah figure. 9: graphique Recommandation pour les auteurs vétérans. - 392 - A. Guille et al. N. LabrocheN. LABROCHE A. CornuéjolsA. Cornuéjols C. CruzC. Cruz O. ParisotO. A. Parisot BOYERA. Boyer L. KhouasL. Khouas W. CorreaW. Correa P. MarteauP. A. Bruna Marteau. Brun N. Messain. Messai C. SalperwyckC. Salperwyck D. BuscaldiD. Buscaldi D. IencoD. Ienco J. SanhesJ. Sanhes M. CollardM. Collard G. BoscG. Bosc A. Simac-LejeuneA. Simac-Lejeune M. DermoucheM. Dermouche N. GrabarN. Grabar L. FahedL. Fahed D. WernerD. Werner P. PaponP. F. Papon BoualiF. L. bouali ThiryL. E. Thiry ViennetE. Viennet D. peignés. Combe N. DuguéN. Dugué P. BruneauP. Bruneau E. StattnerE. Stattner M. NidhalM. Nidhal A. pereza. Perez E. Egyed-ZsigmondE. Egyed-Zsigmond P. PinheiroP. Pinheiro C. KurtzC. Kurtz A. GuilleA. F. Guille BarigouF. Barigou A. Chaïbia. Chaibi D. HaoyuanD. Haoyuan M. MahfoudhM. M. Mahfoudh HassenforderM. Hassenforder C. PasquierC. S. Pasquier LoudniS. Loudni figure. 10: graphique Recommandation pour les auteurs de nouveaux arrivants. et le nuage de sujet, où chaque sujet est représenté par une bulle marquée avec le plus perti- mots Vant et dont le diamètre est proportionnelle à sa fréquence globale. Il a également des vues détaillées des offres pour: chaque sujet (voir la figure 11.), Chaque article, chaque auteur (voir la figure 12.), Et chaque mot du vocabulaire. La vue détaillée sur un sujet présente la distribution de poids sur les mots les plus importants, l'évolution de sa fréquence au fil des ans, la liste des articles connexes et le réseau de collaboration. La vue détaillée pour un article présente le plus de mots significatifs, les claviers plupart des documents similaires, la répartition du poids sur les sujets et un lien pour accéder à la version électronique de l'article sur le site Web de l'éditeur. La vue détaillée sur un auteur affiche son / sa distribution de sujet, son / sa liste de publications, ainsi que la collaboration personnalisée et suggestions sujet. Références Arun, R., V. Suresh, C. V. Madhavan, et M. N. Murthy (2010). En trouvant le nombre naturel de sujets avec l'allocation Dirichlet latente: Quelques observations. En PAKDD, pp. 391-402. Berry, M. W. et M. Browne (2005). surveillance électroniques à l'aide matrice non négative factoriza- tion. Journal of Computational et organisation mathématique Théorie 11 (3), 249-264. Blei, D., A. Ng, et M. Jordan (2003). Latent Dirichlet Allocation. Journal of Machine Learning 3, 993-1022. Cavnar, W. et J. Trenkle (1994). N-gramme basé catégorisation texte. En SDAIR, pp. 161-175. - 393 - Modélisation du sujet et de l'exploitation minière hypergraphe pour analyser l'histoire de la conférence de EGC figure. 11: Détails sur un sujet: (1) les mots les plus représentatifs, (2) la fréquence à travers le temps, (3) des articles connexes. FIGUE. 12: Détails sur un auteur: (1) la répartition du poids sur les sujets, (2) des suggestions de collaboration et de sujet, (3) articles connexes. Denis, P. et B. Sagot (2012). Le couplage d'un corpus annoté et un lexique pour l'état de l'art marquage pos. Ressources Linguistiques et évaluation 46 (4), 721-736. Fouss, F., A. Pirotte, J.-M. Renders, et M. Saerens (2007). Marche aléatoire calcul des similitudes entre les noeuds d'un graphe avec l'application de la recommandation de collaboration. IEEE Transactions sur les connaissances et l'ingénierie des données. 19 (3), 355-369. Greene, D., D. O'Callaghan et P. Cunningham (2014). Combien de sujets? analyse de la stabilité pour les modèles sujet. Dans ECML PKDD, pp. 498-513. Liu, H., P. LEPENDU, R. Jin, et D. Dou (2011). Procédé d'hypergraphe pour la découverte de jeux d'éléments sémantiquement associées. En ICDM, pp. 398-406. Ricci, F., L. Rokach, B. Shapira et P. B. Kantor (Eds.) (2011). Manuel recommender Systems. Springer. Sagot, B. (2010). Le Lefff, un libre accès et grande couverture lexique morphologique et syntaxique pour le français. En LRGC. Dans le cadre résumé du défi à l'édition Proposé 2016 de la conférence EGC, nous exploitons les articles Qui y were de 2004 à publiés 2015, mais d'écoulement Avec sa structure de fils tentatives de viol et de évolution. A partir des découvertes thématiques Latentes et d'Autres Propriétés des articles (par exemple auteurs, affiliations), nous Mettons en lumière des Caractéristiques des structures interessantes thématique et collaborative d'EGC. A l'aide d'Une méthode d'extraction d'itemsets Dans les hyper-Graphes nous en avant also Mettons des liens Entre empreintes latentes Entre auteurs OU thématiques. De plus, nous proposons des recommandations d'auteurs de OU thématiques. Enfin, l'explorateur Pour nous crivons DE- interface web Une les connaissances heuristiques découvertes. - 394 -"
212,Revue des Nouvelles Technologies de l'Information,EGC,2016,"Towards generic and efficient constraint-based mining, a constraint programming approach","In today's data-rich world, pattern mining techniques allow us to extract knowledge fromdata. However, such knowledge can take many forms and often depends on the application athand. This calls for generic techniques that can be used in a wide range of settings. In recentyears, constraint programming has been shown to offer a generic methodology that fits manypattern mining settings, including novel ones. Existing constraint programming solvers do notscale very well though. In this talk, I will review different ways in which this limitation hasbeen overcome. Often, this is through principled integration of techniques and data structuresfrom pattern mining into the constraint solvers.",Tias Guns,http://editions-rnti.fr/render_pdf.php?p1&p=1002151,http://editions-rnti.fr/render_pdf.php?p=1002151,en,"Vers le développement minier à base de contrainte générique et efficace, une approche de programmation par contraintes Tias Guns * * KU Leuven, Celestijnenlaan 200A, Louvain, Belgique tias.guns@cs.kuleuven.be http://people.cs.kuleuven.be/ tias.guns / 1 Présentation les champs de l'exploration de données et la programmation par contraintes sont parmi les sous-domaines les plus réussis de l'intelligence artificielle. Pourtant, leurs méthodes sont très différentes. Constraint pro- grammation préconise une approche de modélisation et de résolution déclarative de satisfaction de contraintes et problèmes d'optimisation. L'exploration de données d'autre part a mis l'accent sur le traitement des grands ensembles de données complexes qui se posent dans des applications particulières. l'extraction de motifs vise plus particulièrement à extraire des modèles intéressants d'un ensemble de données, où interestingness est souvent définie par l'applica- tion à portée de main. méthodes ad hoc actuelles se concentrent souvent sur des algorithmes à usage spécial à des problèmes spécifiques et des critères Interestingness. Ce code donne généralement complexe qui est très efficace, mais difficile à modifier ou réutiliser dans d'autres applications. Par conséquent, moins d'attention a été consacrée à la question des stratégies de solutions générales et génériques. Néanmoins, il est nécessaire de techniques génériques qui peuvent gérer des variations de tâches connues, ainsi que les contraintes par les applications (Dzeroski et al, 2010;. De Raedt et al., 2011). La nature typique itérative du cycle des connaissances découverte (Han et Kamber, 2000), dans lequel sont définies itérativement les données et la définition des problèmes basés sur le prototypage et les petites évaluations à grande échelle. Dans ce cas, la spécification du problème change généralement entre les itérations, ce qui peut à son tour nécessiter la modification des algorithmes. Cela est reconnu dans le domaine de l'exploitation minière à base de contraintes, qui adopte la dologie métho- de formuler un problème en termes de contraintes (Nijssen, 2010; Boulicaut et Jeudy, 2005). Par exemple, pour itemset l'exploitation minière, une grande variété d'autres traints Cons- et une gamme d'algorithmes pour la résolution de ces problèmes à base de contraintes d'exploitation itemset (Mannila et Toivonen, 1997 (Agrawal et al., 1993). Jr. et al, 2000, Pei et Han, 2000, Pei et al, 2001;. Bucila et al., 2003; Han et al, 2007;. Soulet et Crémilleux, 2005; bonchi et Lucchese, 2007) a permis l'application de itemset minière à de nombreux d'autres problèmes, allant de l'exploration du Web à la bio-informatique (Han et al., 2007). cadres génériques dans la littéra- minière de contrainte-ture portaient essentiellement sur l'monotonicité (anti-) des contraintes (Mannila et Toivonen, 1997, Pei et Han, 2000;. Bucila et al, 2003) conduisant à des systèmes tels que (CONQUÊTE bonchi et Lucchese, 2007), MusicDFS (Soulet et Crémilleux, 2005) et Molfea (De Raedt et Kramer, 2001). Alors que de nombreuses tâches d'exploration de données typiques consistent en (anti) contraintes monotones, de nombreuses autres contraintes ne rentrent pas dans ce cadre, comme la recherche de motifs fermés dans les données denses (Pasquier et al, 1999; -. 13 - Vers par contraintes génériques et efficace minière Pei et al, 2000), ou l'extraction de modèles corrélés dans les données surveillées (Morishita et Sese, 2000; Cheng et al., 2007).. Les cadres qui sont plus génériques que (anti-) et dans lequel monotonicity combinaisons arbitraires de contraintes sont autorisées ont été portées disparues. 2 La programmation par contraintes et itemset minière Le cadre de CP4IM (De Raedt et al, 2008;.. Armes à feu et al, 2011a) a été le premier à proposer un cadre à base de CP générique pour l'exploitation minière itemset fondé sur la contrainte. Le cadre a passé englober itemset fréquentes et contraintes allant des contraintes monotones typiques (anti) tels que la taille et le coût du modèle, ainsi que les contraintes de représentation condensés tels que fermée et maximale. Depuis lors, de nombreux travaux ont étendu cette approche, y compris: - L'utilisation de différentes techniques de résolution déclarative. D'autres techniques explorées comprennent la compilation des connaissances et BDD, Set de réponse Programmation (Järvisalo, 2011) et la résolution de SAT (Cambazard et al., 2010) (Jabbour et al, 2015;.. Coquery et al, 2012); - l'exploitation minière de jeu de modèle, également kn propre comme modèles de n-aire. Ici, le but est de ne pas trouver tous les modèles indivi- double, mais plutôt de trouver un ensemble concis des motifs de n (armes à feu et al, 2011b;. Khiari et al., 2010); - Optimisation et exploitation minière top-k. Ici aussi le but est de ne pas énumérer tous les satisfaire sternes PAT- mais plutôt de trouver le modèle optimal, par exemple selon une mesure de corrélation ou la discrimination (Nijssen et al., 2009), ou pour trouver le top-k modèles les plus optimaux (Jabbour et al., 2013). - l'optimisation multi-objectifs, également connu sous le nom skypatterns minières. Dans ce cas, plusieurs mesures sont données et les solutions optimales sont Pareto recherchées (Kemmar et al, 2014;.. Rojas et al, 2014). Une généralisation de plusieurs objectifs aux relations de dominance englobe également des représentations condensées telles que motif fermé / maximal des mines et trouver des sous-groupes pertinents (Negrevergne et al., 2013). Fait intéressant, la plupart de ces approches utilisent solveurs non modifiés et sont encore en mesure d'obtenir une efficacité raisonnable, en particulier dans le cas où de nombreuses contraintes sont présentes (armes à feu et al., 2011a). La clé contrainte de bas niveau que ces formulations utilisent est une contrainte de somme pondérée réifiée sur des variables booléennes, où les moyens réifiées que la valeur de vérité de la contrainte se traduit par une variable d'indicateur booléen. A noter à cet égard est que l'on peut mettre en œuvre un résolveur de CP sur des variables booléennes en utilisant les structures de données utilisées dans les algorithmes extraction jeu d'éléments, et de réaliser la même extensibilité que l'extraction du jeu d'éléments en profondeur d'abord, tout en ayant le même caractère général que d'autres CP solveurs ont pour itemset l'exploitation minière (Nijssen, 2010 ET armes à feu). 3 La programmation par contraintes et la séquence minière Un motif séquentiel est une liste ordonnée d'événements. Cela diffère de commande séquentielle de l'interprétation traditionnelle (non ordonnée) d'un modèle. Itemset De plus, le même événement peut se reproduire plusieurs fois dans un modèle séquentiel. Une propriété essentielle de toute méthode d'extraction de motif est la capacité à calculer la fréquence d'un motif; elle consiste à vérifier pour chaque entrée dans la base de données si le modèle se produit dans cette entrée (par exemple, une entrée est souvent appelé une transaction pour jeux d'éléments, ce qui correspond à la vérification. - 14 - T. Guns ce que le motif est un sous-ensemble de la transaction, et pour les séquences qu'il est une suite de la transaction Works que l'utilisation des programmes de contraintes pour l'exploitation minière à base de contrainte peut être divisée en deux camps, en fonction de la représentation d'une séquence: -.. les séquences avec des caractères génériques explicites exemple An est <a, * , B> où * est le joker. Cela correspondra une transaction qui contient ana, suivi d'un seul événement arbitraire, suivie immédiatement par B. événement Il ne correspondrait pas à une transaction telle que <a, C, C, B>, mais il correspondrait avec <C, a, C, B>. Ce problème peut être formulé d'une manière qui est très similaire à l'extraction de motifs fréquents (Coquery et al., 2012) et, par conséquent un grand nombre des mêmes contraintes et variations peuvent être exprimé, y compris fréquents, fermé et maxi mal (Coquery et al., 2012) et le haut-k et les sous-groupes pertinents (Kemmar et al., 2014). - Les séquences avec des jokers implicites. Ceci est le motif de séquence plus traditionnelle consi- Dered, où un motif <A, B> est une sous-séquence de l'ensemble de <C, A, B>, <A, C, C, B> et <A, C,. . . , C, B> comme il y a des jokers implicites entre tous les symboles. Cela est beaucoup plus difficile à exprimer dans une résolution de contraintes (et al Métivier., 2013) comme dans le cas général, tester la relation pour une transaction séquence individuelle nécessite la recherche sur tous les appariements possibles, ce qui est exponentielle pire cas. Deux façons de surmonter ce sont tout d'abord, d'ajouter une contrainte globale qui fait cela de manière transparente pour le solveur de CP, et le second pour décomposer la contrainte de séquence et de la traiter pour chaque transaction comme un sous-problème indépendant qui exige la recherche (Negrevergne ET Armes à feu, 2015). La première approche est plus efficace que la même technique de projection préfixe utilisé dans PrefixSpan (Han et al., 2001) peuvent être utilisés, y compris la taille des extensions peu fréquentes. Mieux encore sc alability peut être obtenue en ayant une contrainte globale qui fait cela pour toutes les transactions à la fois, au lieu d'avoir une contrainte pour chaque tran- saction (Kemmar et al., 2015). Le travail sur les séquences nous montre que itemsets sont tout à fait exceptionnel que toutes les contraintes, y compris les représentations condensées, peuvent être exprimées à l'aide des contraintes standard disponibles dans CP. Seulement top-k, les relations d'optimisation et de dominance multi-objectifs exigent des changements à la procédure de résolution. D'autre part, pour modéliser la séquence tout en obtenant des performances de résolution raisonnable contraintes ou la recherche spécialisée procédures doivent être écrites. En outre, ding le contrôle de hi-séquence dans une contrainte globale est la plus efficace, mais ne permet pas de changer la relation de séquence, par exemple faire respecter un écart maximal entre les éléments qui correspond, sans modifier le code d'application de la contrainte. Il y a donc encore de la place pour les techniques Neric vraiment gé- pour l'extraction de la séquence, ainsi que pour d'autres tâches d'extraction de motifs structurés tels que l'exploitation minière de graphique. Voir (armes à feu et al., 2016) pour une discussion plus détaillée des défis et des solutions possibles. 4 Une langue pour l'exploitation minière de modèle générique? Le développement des langues génériques pour le modèle minier est une quête de longue date (bonchi et Luc- chese 2007, Soulet et Crémilleux, 2005. Blockeel et al, 2012;. Métivier et al, 2012;. Armes à feu et al, 2013). De nombreux efforts ont leurs racines dans l'idée de bases de données inductives (Mannila, 1997); ce sont des bases de données dans lesquelles les données et les modèles sont des citoyens de première classe et peuvent être interrogés. (. Meo et al, 1996; Imielinski et Virmani, 1999) La plupart des langages de requêtes inductives, par exemple, étendre - 15 - Vers le développement minier à base de contrainte générique et efficace SQL avec des primitives pour extraction de motifs. Ils ont seulement une langue restreinte pour exprimer les problèmes miniers, et sont généralement liés à un algorithme d'exploration. Un déve- loppement plus avancé est celui des vues minières (Blockeel et al., 2012), qui donne accès à des modèles paresseux à travers une table virtuelle. SQL standard peut être utilisé pour effectuer des requêtes, et la mise en œuvre ne se concrétisera ces modèles dans la table qui sont pertinentes pour la requête. Ceci est réalisé en utilisant un algorithme d'exploitation traditionnel. Des travaux plus récents ont étudié les langages de haut niveau qui ont une traduc- tion directe dans une spécification déclarative du problème (armes à feu et al, 2013;.. Métivier et al, 2012). (. Frisch et al, 2008;;. Nethercote et al, 2007; Van Hentenryck et Michel, 2005 Van Hentenryck, 1999) En même temps, de nombreux langages de modélisation de niveau élevé existent dans la contrainte de programmation ture littéra-. Le système MiningZinc (armes à feu et al., 2013) unifie les deux approches en adoptant le langage de programmation de contrainte Mi- niZinc (Nethercote et al., 2007), tout en offrant en même temps des abstractions supplémentaires qui se produisent souvent dans les problèmes d'extraction de motifs. La langue est indépendante de toute technologie de résolution qui donne au système MiningZinc la capacité de ve- rify si un algorithme existant existe qui correspond à la formulation du problème, ou si un solveur de contrainte générique doit être utilisée. Dans le premier cas, un algorithme spécialisé très efficace et évolutive peut être utilisée. De plus, en utilisant les règles de réécriture, le système peut détecter qu'un spécialiste peut être utilisé pour résoudre une partie du problème, et que les contraintes restantes peuvent être traitées après. Le résultat est une hybredisation de techniques de résolution, tout ce qui se cache derrière un langage générique de haut niveau. 5 Conclusions Dans cet exposé et document d'accompagnement, je mis en évidence les progrès récents ont à combler le fossé méthodologique entre les domaines de l'exploration de données et la programmation par contraintes. Le but sur endoloris est de faire l'exploration de données approches plus souples et déclarative, afin de le rendre facile de changer le modèle sans travaux de remise en marche sur le solveur. En effet, la plupart des approches sont référencées plus génériques que les systèmes existants. D'autre part, il y a souvent un compromis entre la généralité et effic iency et mettre au point des méthodes qui sont à la fois générique et évolutive est le premier défi. De nombreux succès récents ont d'une certaine manière hybridée données structures ou des algorithmes à partir de méthodes spécialisées dans solveurs de contraintes génériques. Ceci est une approche très prometteuse qui apporte l'exploration de données et les champs de programmation par contraintes plus non seulement au niveau de l'application, mais aussi au niveau algorithmique. Remerciements Je voudrais remercier Luc De Raedt et Siegfried Nijssen pour les nombreuses et fructueuses des collaborations, des discussions et collègues Anton Dries, Benjamin Negrevergne et Behrouz Ba- Baki qui ont contribué à façonner et à développer ces idées. Ce travail est le résultat de nombreux documents intéressants et des discussions avec d'autres chercheurs lors d'ateliers et de conférences. - 16 - T. Guns Biographie Tias Guns est un camarade post-doctoral au laboratoire DTAI de la KU Leuven. Ses recherches se trouve à la frontière entre l'exploration de données et la programmation par contraintes, et son principal intérêt réside dans les méthodes de Bining com- des deux champs. Dans le cadre de son doctorat, il a mis au point le cadre de CP4IM qui a montré pour la première fois la possibilité d'utiliser la programmation par contraintes pour ning de modèle. Sa thèse a reçu à la fois le prix de thèse de programmation de contrainte et le prix de thèse de l'intelligence artificielle ECCAI. Il est un membre actif de la communauté et a organisé un certain nombre d'ateliers et d'un numéro spécial sur le thème de combiner la programmation par contraintes avec l'apprentissage de la machine et l'exploration de données. Agrawal Références, R., T. Imielinski, A. et N. Swami (1993). Exploitation minière règles d'association entre ensembles d'éléments dans les grandes bases de données. Dans Actes de la SIGMOD rence internationale Confé- sur la gestion des données 1993, pp. 207-216. ACM Press. Blockeel, H., T. Calders, É. Fromont, B. Goethals, A. Prado, et C. Robardet (2012). Un système de base de données basée sur in- productive vues minières virtuelles. Données Min. Knowl. Décou. 24 (1), 247-287. Bonchi, F. et C. Lucchese (2007). L'extension de l'état de l'art de la découverte de modèle basé sur des contraintes. Les données Knowl. Eng. 60 (2), 377-399. Anglais Boulicaut, J.-F. et B. Jeudy (2005). l'exploration de données à base de contraintes. Dans O. Maimon et L. Rokach (Eds.), Data Mining et connaissances Manuel Découverte, pp. 399-416. Springer US. Bucila, C., J. Gehrke, D. Kifer, et W. M. White (2003). Dualminer: Un algorithme double pour la taille itemsets avec des contraintes. Données Min. Knowl. Décou. 7 (3), 241-272. Cambazard, H., T. Hadzic, et B. O'Sullivan (2010). compilation des connaissances pour ning itemset mi-. Au 19ème Conférence européenne sur l'intelligence artificielle, volume 215 des frontières dans l'intelligence artificielle et applications, p. 1109-1110. IOS Press. Cheng, H., X. Yan, J. Han, et C.-W. Hsu (2007). analyse de modèles fréquents discriminante pour la classification efficace. Dans Actes de la 23e Conférence internationale sur l'ingénierie des données, p. 716-725. IEEE. Coquery, E., S. Jabbour, L. Saïs, et Y. Salhi (2012). approche basée sat-A pour découvrir les modèles fréquents, fermés et maximale dans une séquence. En ECAI 2012 - 20 rence européenne Confé- sur l'intelligence artificielle. Y compris les applications prestigieux de l'intelligence artificielle (PAIS-2012) Démonstrations système piste, Montpellier, France, 27-31 Août 2012, pp. 258-263. De Raedt, L., T. Armes à feu, et S. Nijssen (2008). La programmation par contraintes pour l'exploitation minière itemset. Dans Actes de la 14e SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données (KDD-08), pp. 204-212. ACM. De Raedt, L. et S. Kramer (2001). L'algorithme de l'espace version et son par niveaux à la recherche applica- tion de fragments moléculaires. Dans Actes de la dix-septième Conférence internationale conjointe sur l'intelligence artificielle, pp. 853-862. Morgan Kaufmann. - 17 - Vers le développement minier à base de contrainte générique et efficace De Raedt, L., S. Nijssen, B. O'Sullivan, et P. Van Hentenryck (2011). La programmation par contraintes rencontre l'apprentissage automatique et l'exploration de données (Dagstuhl séminaire 11201). Rapports Dagstuhl 1 (5), 61-83. Dzeroski, S., B. Goethals, et P. Panov (2010). Bases de données inductive et Data Mining Constraint Based (1re éd.). Springer-Verlag New York, Inc. Frisch, A., W. Harvey, C. Jefferson, B. M. Hernández, et I. Miguel (2008). Essence: Une langue contrainte pour spécifier des problèmes combinatoires. Contraintes 13 (3), 268-306. Armes à feu, T., Dries A., G. Tack, S. Nijssen, et L. De Raedt (2013). MiningZinc: Un langage de modélisation pour l'exploitation minière à base de contraintes. Dans Actes de la Conférence mixte internationale Vingt-troisième sur l'intelligence artificielle, Conférence internationale conjointe sur l'intelligence artificielle, Beijing, Chine 3-9 Août 2013, p. 1365-1372. AAAI Press. Armes à feu, T., S. Nijssen, et L. De Raedt (2011a). Itemset minière: Une perspective de programmation par contraintes. Artif. Intell. 175 (13/12), 1951-1983. Armes à feu, T., S. Nijssen, et L. De Raedt (2011b). ensemble k-modèle minier sous contraintes. IEEE Transactions sur les connaissances et l'ingénierie des données à apparaître. Aussi comme rapport technique CW596, oct 2010. Guns, T., S. Nijssen, et L. De Raedt (2013). k-directivité ensemble minier sous contraintes. IEEE Transactions sur les connaissances et l'ingénierie des données 25 (2), 402-418. Guns, T., S. Paramonov, et B. Negrevergne (2016). Sur la modélisation déclarative de l'exploitation minière de modèle structuré. Dans AAAI Atelier sur la base déclarative apprentissage Programmation, Phoenix, Arizona USA, 12-13 Février 2016. acceptées. Han, J., H. Cheng, D. Xin, et Yan X. (2007). l'exploitation minière fréquente modèle: les directions d'état actuels et futurs. Données Min. Knowl. Décou. 15 (1), 55-86. Han, J. et M. Kamber (2000). Data Mining: Concepts et techniques. Morgan Kaufmann. Han, J., J. Pei, B. Mortazavi-Asl, H. Pinto, Q. Chen, U. Dayal, et M. Hsu (2001). Prefixspan: extraction de motifs séquentiels efficacement par la croissance du modèle préfixe projeté. ICDE'2001, Avril, 215-24. Imielinski, T. et A. Virmani (1999). MSQL: Un langage de requête pour l'extraction de la base de données. Data Mining et Knowledge Discovery 3, 373-408. Jabbour, S., L. Saïs, et Y. Salhi (2013). Le haut-k fréquentes minière itemset fermé en utilisant problème SAT haut k. Dans Apprentissage et découverte des connaissances dans les bases de données - Conférence européenne, ECML PKDD 2013, Prague, République tchèque, 23-27 Septembre 2013, ding Procee-, Partie III, pp 403-418.. Jabbour, S., L. Saïs, et Y. Salhi (2015). Décomposition encodages base SAT pour les problèmes miniers. Itemset En avancées de la recherche du savoir et Data Mining - Conférence 19 Asie-Pacifique, PAKDD 2015, Ho Chi Minh-Ville, Vietnam, mai 19 à 22 2015, Compte rendu, Partie II, pp 662-674.. Järvisalo, M. (2011). Itemset l'exploitation minière comme une application de défi pour tion ensemble de réponses énumérative. Dans la programmation logique et non monotone Raisonnement - 11e Conférence internationale, LPNMR 2011, Vancouver, Canada, 16-19 mai 2011. Compte rendu, pp 304-310.. Jr., R. J. B., R. Agrawal, et D. Gunopulos (2000). l'extraction de règles à base de contraintes dans les grandes bases de données denses. Données Min. Knowl. Décou. 4 (2/3), 217-240. - 18 - T. Guns Kemmar, A., S. Loudni, Y. Lebbah, P. Boizumault, et T. Charnois (2015). PROJECTION contrainte globale par préfixe pour l'exploitation minière de modèle séquentiel. Dans Principes et pratique de la programmation par contraintes - 21e Conférence internationale, CP 2015, Cork, Irlande 31 Août - 4 Septembre, 2015, Proceedings, pp 226-243.. Kemmar, A., W. Ugarte, S. Loudni, T. Charnois, Y. Lebbah, P. Boizumault, et B. Crémilleux (2014). Exploitation minière modèles de séquence pertinents avec le cadre en fonction cp. Dans la Conférence nationale 26 IEEE Inter sur les outils d'intelligence artificielle, ICTAI 2014, Limassol, Chypre, 10-12 Novembre, 2014, p. 552-559. Khiari, M., P. Boizumault, et B. Crémilleux (2010). La programmation par contraintes pour l'exploitation des modèles n-aire. Dans Principes et pratique de la programmation par contraintes, volume 6308 de Lecture Notes in Computer Science, pp. 552-567. Springer. Mannila, H. (1997). bases de données inductives et représentations condensées pour l'exploration de données. En ILPS, pp. 21-30. Mannila, H. et H. Toivonen (1997). recherche et frontières par niveaux de théories dans la découverte de connaissances. Données Min. Knowl. Décou. 1 (3), 241-258. Meo, R., G. Psaila, et S. CERI (1 996). Un nouvel opérateur comme SQL pour l'extraction des règles d'association. En VLDB, pp. 122-133. Métivier, J., P. Boizumault, B. Crémilleux, M. Khiari, et S. Loudni (2012). Une contrainte lan- jauge pour la découverte de modèle déclaratif. Dans Actes du Symposium ACM sur Applied Computing, SAC 2012, Riva, Trento, Italie, 26-30 Mars, 2012, pp. 119-125. Métivier, J.-P., S. Loudni, et T. Charnois (2013). Une approche de programmation par contraintes pour l'extraction de motifs séquentiels dans une base de données de séquence. Dans ECML / PKDD 2013 Atelier sur les langues pour les mines et l'apprentissage automatique des données. également disponible en arXiv: 1311,6907. Morishita, S. et J. Sese (2000). Treillis avec itemset traversant la taille métrique statistique. Dans Actes du XIXe SIGMOD-SIGACT-SIGART Symposium sur les principes de base de données des systèmes, p. 226-236. ACM. Negrevergne, B., A. Dries, T. Guns, et S. Nijssen (2013). programmation Dominance pour l'exploitation minière de jeu item-. 13e IEEE Conférence internationale sur l'exploration de données, IEEE Conférence internationale sur l'exploration de données, Dallas, Texas, USA, 7-10 Décembre 2013, p. 557-566. IEEE Computer Society. Negrevergne, B. et T. Guns (2015). séquence basée sur les contraintes minières en utilisant la contrainte pro- grammation. Dans l'intégration de l'IA et OR Techniques en programmation par contraintes, Barcelone, Espagne, 18-22 mai 2015. Springer International Publishing. Accepté. Nethercote, N., P. J. Stuckey, R. Becket, S. Marque, Canard J. G., et G. Tack (2007). MiniZinc: Vers un langage standard de modélisation CP. En CP, Volume 4741 de LNCS, pp. 529-543. Springer. Anglais Nijssen, S. (2010). l'exploitation minière à base de contraintes. Dans C. Sammut et Webb G. (éd.), Encyclopedia of Machine Learning, pp. 221-225. Springer US. Nijssen, S. et T. Guns (2010). L'intégration de la programmation par contraintes et l'exploitation minière itemset. Dans Apprentissage et découverte des connaissances dans les bases de données, Conférence européenne (ECML / PKDD-10), volume 6322 de Lecture Notes in Computer Science, pp. 467-482. - 19 - Vers le développement minier à base de contrainte générique et efficace Springer. Nijssen, S., T. Armes à feu, et L. De Raedt (2009). Corrélé minière itemset dans l'espace ROC: Une approche de programmation par contraintes. En KDD, pp. 647-656. ACM. Pasquier, N., Y. Bastide, R. Taouil, et L. Lakhal (1999). Découverte de itemsets fermés fréquents pour les règles d'association. Dans la théorie de base de données, volume 1540 de Lecture Notes in Computer Science, pp. 398-416. Springer. Pei, J. et J. Han (2000). Peut-on pousser plus de contraintes dans les mines de fréquentes modèle? Dans Actes de la sixième conférence internationale SIGKDD sur la découverte des connaissances et l'exploration de données, p. 350-354. ACM. Pei, J., J. Han, et L. V. S. Lakshmanan (2001). Exploitation minière ensembles d'objets fréquents avec des contraintes convertibles. Dans Actes de la Conférence internationale IEEE sur l'ingénierie des données, p. 433-442. IEEE. Pei, J., J. Han, R. et Mao (2000). Closet: Un algorithme efficace pour l'exploitation minière itemsets fermés fréquents. Dans SIGMOD Atelier sur les questions de recherche dans l'exploration de données et de découverte du savoir, pp. 21-30. ACM. Rojas, W. U., P. Boizumault, S. Loudni, B. Crémilleux, et A. Lepailleur (2014). Mines (Soft-) skypatterns utilisant CSP dynamique. Dans l'intégration de l'IA et OR techniques en programmation par contraintes -. 11e Conférence internationale, CPAIOR 2014, Cork, Irlande, 19-23 mai 2014. Proceedings, pp 71-87. Soulet, A. et B. Crémilleux (2005). Un cadre efficace pour l'exploitation des contraintes flexibles. Progrès de la connaissance Découverte et exploration de données, volume 3518 de Lecture Notes in Computer Science, pp. 43-64. Springer. Van Hentenryck, P. (1999). Le langage de programmation d'optimisation de l'OPL. MIT Press. Van Hentenryck, P. et L. Michel (2005). Constraint-Based Local Search. MIT Press. Résumé Dans le monde des données riches d'aujourd'hui, les techniques d'extraction de motifs nous permettent d'extraire des connaissances à partir de données. Cependant, ces connaissances peuvent prendre plusieurs formes et dépend souvent de l'application à portée de main. Cela exige des techniques génériques qui peuvent être utilisés dans une large gamme de paramètres. Ces dernières années, la programmation par contraintes a été démontré que d'offrir une méthodologie générique fi ts de nombreux paramètres de l'extraction de motifs, y compris les nouvelles. solveurs de programmation de contraintes existantes n'échelle pas très bien cependant. Dans cet exposé, je vais passer en revue les différentes façons dont cette limitation a été surmontée. Souvent, cela est grâce à l'intégration des principes des techniques et des structures de données de l'extraction de motif dans les solveurs de contraintes. - 20 -"
227,Revue des Nouvelles Technologies de l'Information,EGC,2015,A Clustering Based Approach for Type Discovery in RDF Data Sources,"RDF(S)/OWL data sources are not organized according to a predefined schema, as they are structureless by nature. This lack of schema limits their use to express queries or to understand their content. Our work is a contribution towards the inference of the structure of RDF(S)/OWL data sources. We present an approach relying on density-based clustering to discover the types describing the entities of possibly incomplete and noisy data sets.","Kenza Kellou-Menouer, Zoubida Kedad",http://editions-rnti.fr/render_pdf.php?p1&p=1002113,http://editions-rnti.fr/render_pdf.php?p=1002113,en,"Une approche fondée sur Clustering pour le type de découverte en RDF Sources de données Kellou-Menouer Kenza *, Zoubida Kedad * * PRISM - Université de Versailles Saint-Quentin-en-Yvelines, 45 avenue des Etats-Unis, Versailles, France kenza.menouer@prism. uvsq.fr, zoubida.kedad@prism.uvsq.fr Interrogation et l'exploitation RDF (S) / sources de données OWL nécessite des informations sur les sources et les propriétés re- qu'ils contiennent. Sans une description de l'ensemble de données, il est difficile de cibler les propriétés pertinentes et des ressources, et des ensembles de données de navigation afin de comprendre leur tente con- peut être un processus fastidieux. Une caractéristique de RDF (S) / sources de données OWL important est qu'ils ne sont pas organisés selon l'une quelconque schéma prédéfini. Ils sont, par nature, et anhiste les langues utilisées pour décrire les données sur le Web n'imposent pas de contraintes ou restrictions sur les propriétés décrivant les ressources. Notre objectif est de découvrir manquant définitions de type dans un RDF (S) / OWL ensemble de données. Nous proposons une approche basée sur le regroupement des entités où sont regroupées en fonction de leur similitude. La similitude entre les deux entités données est évaluées en tenant compte de leurs ensembles respectifs de propriétés en utilisant la similarité Jaccard. Nos exigences pour la découverte de type sont les suivants: d'une part, le nombre de types ne sont pas connus à l'avance, et d'autre part, les ensembles de données sont en train d'évoluer, de grandes et peuvent contenir du bruit. Approche la plus appropriée est de regroupement en cluster basé sur la densité, introduite par Ester et al. (1996), car il est robuste au bruit, déterministe et il trouve des classes de forme arbitraire, ce qui est utile pour les ensembles de données où les ressources sont décrites avec des ensembles de propriétés hétérogènes. De plus, à la différence des algorithmes basés sur k-means et k-médoïde, le nombre de classes n'est pas nécessaire. Afin d'accélérer le processus de regroupement, et surtout pour effectuer des exécutions successives avec des valeurs de paramètres différents (le rayon maximum de ε de voisinage et le nombre minimum de voisins pour une MinPts entité), nous effectuons une fois pour toutes le calcul des voisins les plus proches de chaque entité. A cet effet, l'indice de nous les données et nous commander les entités en fonction de leur similitude. Nous enregistrons une matrice de voisinage contenant pour chaque entité la liste ordonnée de ses voisins, ainsi que la distance entre cette liste des entités et le numéro de la ligne qui représente l'indice d'une entité. Merci à l'indexation des données et l'ordre des voisins selon la distance, il n'est pas nécessaire de passer par l'ensemble de la matrice pour trouver les voisins d'une entité donnée, et la complexité de la recherche des voisins devient o linéaire (n). Nous avons effectué des expériences sur des ensembles de données existantes pour évaluer la qualité des types féré in-. Nous avons extrait les définitions de types existants de nos ensembles de données et les considérer comme un étalon-or. Ensuite, nous avons exécuté notre algorithme sur les ensembles de données sans les initions du type et le évalué pour chacune des classes inférée précision et de rappel. Nous avons annoté chaque classe inférée avec la définition de type le plus fréquent de ses entités. Pour chaque étiquette de type Tr - 471 - Type Découverte en RDF des sources de données de taille nr dans l'ensemble de données et chaque classe Ci de taille ni inférée par notre approche, telle que Tr est l'étiquette de Ci, les mesures de qualité sont évalués pour chaque Ci par rapport à Tr comme suit: NRI étant le nombre d'instances dans la classe Ci qui appartiennent à Tr, la précision P (Tr, Ci) est défini comme nri / ni, le rappel R (Tr, Ci) est défini comme nri / nr. En plus d'obtenir un bon Cision avant et le rappel même lorsque les ensembles de propriétés décrivant les entités sont très hétérogènes, l'approche a permis de déduire les définitions de types qui ne sont pas spécifiées dans l'ensemble de données. Quelques travaux dans la littérature ont abordé le problème de déduisant type d'un ensemble de données semi-structuré. Wang et al. (2000) proposent une DataGuide approximative basée sur COBWEB, mais il n'est pas déterministe, coûteux et pas très approprié pour les grands ensembles de données. Le algor proposé Ithm dans Nestorov et al. (1998), utilise le regroupement ascendante. Cependant, la méthode nécessite un seuil de similitude, et le nombre de classes. Christodoulou et al. (2013) l'utilisation ascendante hiérarchique pour en déduire les résumés de structure de données liées. Contrairement à notre approche, seules les propriétés sortantes sont considérées et la classification hiérarchique est très coûteux. Type de SD- (Paulheim et Bizer (2013)) enrichit une entité par plusieurs types en utilisant des règles d'inférence RDFS, et calcule la confiance d'un type pour une entité. La contribution est plus à évaluer la pertinence des types inférées pour une entité plutôt que de trouver son type, que les règles d'inférence RDFS sont utilisés pour cela. Dans nos travaux futurs, nous aborderons la génération des classes floues, pour permettre à plusieurs types d'une entité. Nous allons également aborder l'annotation des classes extraites, ainsi que la découverte de liens possibles entre eux afin de produire une description de schéma complet. Références Christodoulou, K., N. W. Paton, et A. Fernandes A. (2013). inférence de structure pour les sources de données liées à l'aide de clusters. Dans Actes des conjointes EDBT / CIDC 2013 Ateliers, pp. 60-67. ACM. Ester, M., H.-P. Kriegel, J. Sander, et X. Xu (1996). Un algorithme basé sur la densité pour les clusters découvrir- ING dans les grandes bases de données spatiales avec le bruit. Dans Kdd, Volume 96, pp. 226-231. Nestorov, S., S. Abiteboul et R. Motwani (1998). Schéma à partir de données Extraction semistructurée. Dans SIGMOD Record, Volume 27, pp. 295-306. ACM. Paulheim, H. et C. Bizer (2013). Inférence de type sur les données de rdf bruyants. Dans la sémantique Web- ISWC 2013, p. 510-525. Springer. Wang, Q. Y., J. X. Yu et K.-F. Wong (2000). l'extraction approximative du schéma graphique pour les données semi-structurées. Dans Les progrès de la technologie de base de données-EDBT 2000, pp. 302-316. Springer. Résumé RDF (S) / sources de données OWL ne sont pas organisés selon un schéma prédéfini, comme ils sont, par nature, sans structure. Cette absence de schéma limite leur utilisation pour exprimer des requêtes ou à comprendre leur contenu. Notre travail est une contribution à la conclusion de la structure RDF (S) / sources de données OWL. Nous présentons une approche reposant sur le regroupement basé sur la densité pour découvrir les types décrivant les entités éventuellement incomplètes et des ensembles de données bruyants. - 472 - Affiches Une approche fondée sur Clustering pour le type de découverte en RDF Sources de données Kellou-Menouer Kenza, Zoubida Kedad"
228,Revue des Nouvelles Technologies de l'Information,EGC,2015,A Framework for Mesh Segmentation and Annotation using Ontologies,"La segmentation et annotation de maillages utilisant la sémantique a été l'objet d'un intérêt grandissant avec la démocratisation des techniques de reconstruction 3D. Une approche classique consiste à réaliser cette tâche en deux étapes, tout d'abord en segmentant le maillage, puis en l'annotant. Cependant, cette approche ne permet pas à chaque étape de profiter de l'autre. En traitement d'images, quelques méthodes combinent la segmentation et l'annotation, mais ces approches ne sont pas génériques, et nécessitent des ajustements d'implémentation ou des réécritures pour chaque modification des connaissances expertes. Dans ce travail, nous décrivons un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre d'étapes de segmentation, et nous présentons des résultats préliminaires qui montrent la faisabilité de l'approche.Notre système fournit une ontologie générique qui décrit sous forme de concepts les propriétés d'un objet (géométrie, topologie, etc.), ainsi que des algorithmes permettant de détecter ces concepts. Cette ontologie peut être étendue par un expert pour décrire formellement un domaine spécifique. La description formelle du domaine est alors utilisée pour réaliser automatiquement l'assemblage de la segmentation et de l'annotation d'objets et de leurs propriétés, en sélectionnant à chaque étape l'algorithme le plus pertinent, étant données les information sémantiques déjà détectées. Cette approche originale comporte plusieurs avantages. Tout d'abord, elle permet de segmenter et d'annoter des objets sans aucune connaissance en traitement d'images ou de maillages, en décrivant uniquement les propriétés de l'objet en terme de concepts ontologiques. De plus, ce cadre de fontionnement peut facilement être réutilisé et appliqué à différents contextes, dès lors qu'une ontologie de domaine a été définie. Finalement, la réalisation conjointe de la segmentation et de l'annotation permet d'utiliser d'une manière efficace la connaissance experte, en réduisant les erreurs de segmentation et le temps de calcul, en lançant toujours l'algorithme le plus pertinent.","Thomas Dietenbeck, Ahlem Othmani, Marco Attene, Jean-Marie Favreau",http://editions-rnti.fr/render_pdf.php?p1&p=1002088,http://editions-rnti.fr/render_pdf.php?p=1002088,en,"Un cadre pour la segmentation Mesh et annotation à l'aide ontologies Thomas Dietenbeck a, b, c, d, e, Ahlem Othmania, b, Marco Attenef, Jean-Marie Favreaua, b aClermont Université, Université d'Auvergne, ISIT, BP10448, F-63000 Clermont-Ferrand bCNRS, UMR6284, BP10448, F-63000 Clermont-Ferrand cSorbonne Universités, UPMC Univ Paris 06, UMR 7371, UMR_S 1146, LIB, F-75013, Paris, France dCNRS, UMR 7371, LIB, F-75013, Paris , France eINSERM, UMR_S 1146, LIB, F-75013, Paris, France FCNR-IMATI, Italie Résumé. Mesh segmentation et d'annotation en utilisant la sémantique a reçu un intérêt accru avec la démocratisation récente des méthodes de reconstruction 3D. L'approche commune est d'effectuer cette tâche en deux étapes, d'abord segmenter le maillage et annoter il. Cependant, cette approche ne permet pas une partie de tirer profit de l'autre. Dans le traitement d'images, certaines méthodes combinent la segmentation et l'annotation, mais ils ne sont pas génériques et nécessitent des ajustements de mise en œuvre ou réécritures pour chaque modification des connaissances d'experts. Dans ce travail, nous décrivons un cadre original de mise de mélanges et d'annotation, tout en minimisant l'analyse géométrique nécessaire et nous donnons des résultats préliminaires montrant sa faisabilité. Notre cadre fournit une ontologie générique décrivant les concepts de caractéristiques de l'objet (géométrie, topologie, etc.) et des algorithmes permettant de détecter ces concepts. Cette ontologie peut être agrandie par un expert pour décrire un do- principal spécifique formellement. La description dans le domaine formel est ensuite utilisé pour effectuer automatiquement la segmentation joint et l'annotation d'objets et leurs caractéristiques, en sélectionnant à chaque étape la plus pertinente algorithme donné le la sémantique précédemment détectée. Cette méthode présente plusieurs avantages. Firsly il permet à des objets de segment et annoter sans aucune connaissance dans le traitement de maille ou de l'image par sim- plement décrire l'objet comporte en termes de concepts ontologique. D'autre part ce cadre peut être facilement réutilisé et appliqué à différents contextes par la construction sim- plis sur notre ontologie générique. Enfin effectuer la segmentation conjointe et annotation permet d'utiliser de manière efficace les connaissances d'experts, ce qui réduit les erreurs possibles de segmentation et le temps de calcul en lançant toujours l'algorithme le plus efficace. 1 Introduction Au cours des deux dernières décennies, un travail important a été fait dans l'exploration de données et les communautés de traitement maillage pour intégrer une dimension sémantique à leur travail. L'un des principaux objectifs - 275 - Un cadre pour la segmentation Mesh et annotation à l'aide ontologies est d'être en mesure d'extraire une description abstraite des données manipulées, en utilisant des descripteurs sémantiques. Combler l'écart entre les données brutes et les concepts sémantiques est une tâche très compliquée, ce qui implique généralement une bonne connaissance du domaine spécifique les applicatif systèmes travaillent. Ce lien entre les connaissances d'experts (à savoir la sémantique) et les données brutes peuvent être obtenues en utilisant des techniques d'apprentissage ou en concevant un système déterministe, exprimant la connaissance de l'expert dans une langue de la science informatique. Les techniques d'extraction sémantique ont été étudiées plus précisément pour la segmentation mesh, où les objets et leurs sous-parties peuvent être décrites de manière très précise en utilisant des termes sémantiques pour décrire la forme, la structure ou la fonctionnalité. Un problème classique est d'être en mesure d'identifier un objet donné sa forme, et de reconnaître chacun de ses sous-parties d'abord en les segmentant, puis en étiquetant ces sous-parties en utilisant les concepts disponibles sur le domaine sémantique de cette classe d'objets. Certaines de ces approches (Hudelot et al, 2008;.. Hassan et al, 2010;. Fouquier et al, 2012) sont en mesure de remettre en question la description sémantique partielle de la scène et d'adapter leur comportement au contexte. Cependant, nous avons remarqué que toutes ces approches contiennent dans leur mise en œuvre, des algorithmes ou des procédures très spécifiques au domaine applicatif. Dans cet article, nous décrivons un ori cadre gine pour la segmentation maille qui poussent l'approche sémantique, ce qui crée un pont bewteen une description des connaissances d'experts et les algorithmes de segmentation. Ce cadre permet à un expert dans un domaine spécifique pour décrire formellement son propre domaine en termes d'une ontologie fondamentale, sans aucune compétence dans les algorithmes géométriques. Cette description du domaine formel est ensuite utilisé par le système pour reconnaître automatiquement les objets et leurs fonctions dans ce domaine. La généricité de notre approche est assurée par une multi-couche ontologie de modélisation des connaissances d'experts. La première couche correspond aux propriétés de base de tout objet, tels que les formes et structures. Les couches suivantes sont spécifiques à chaque application, décrivant les fonctionnalités et les configurations possibles des objets dans ce domaine. La segmentation et identification mécanisme est caché derrière les concepts de la première couche, qui sont associées aux algorithmes de segmentation. Nous tenons à souligner ici que ce travail d'exploration et d'études que la faisabilité de l'approche proposée sur un contexte simple. Après un aperçu des méthodes existantes dans les systèmes experts sémantiques axée sur et pour la segmentation maillage en utilisant la sémantique, nous présentons notre cadre pour la segmentation du maillage sémantique conduit et l'annotation, en commençant par la modélisation des connaissances d'experts, suivi d'un catalogue synthétique de la segmentation algorithmes, et enfin avec la description du système expert proposé. La quatrième section présente des résultats expérimentaux pour illustrer la pertinence et la faisabilité de notre approche. Enfin, nous présentons des extensions possibles de ces travaux et les améliorations futures. 2 Travaux connexes La conception des systèmes de vision améliorée a tirer parti des connaissances sémantiques tels que la stratégie axée sur l'ontologie. Le paradigme de l'ontologie en sciences de l'information constitue l'un des outils les plus diffusés pour que les gens de divers horizons travaillent ensemble (Seifert et al., 2011). En outre, des interfaces ontologie sont un élément clé d'une ergonomique (Seifert et al., 2011), le système informatique adaptatif (Seifert et al., 2011), en particulier dans les domaines de la biologie / clinique (Othmani et al., 2010) pour les concepts et les normes changent constamment. Further- plus, les capacités de raisonnement intégrés dans le cadre logique sur lequel les logiciels de l'ontologie - 276 - T. Dietenbeck et al. édifiez devrait être un pont définitif entre les scientifiques de vision informatique et les ingénieurs de la connaissance. Même si encore fragile et limité, des déductions raisonnement sur des données visuelles peuvent améliorer l'expérience du système de vision (et Othmani al., 2010) ainsi. infographie est l'un des différents domaines qui bénéficie de la connaissance sémantique grâce à une stratégie d'ontologie et pour cela, différentes applications sont conçues pour assurer l'articulation entre les ontologies et de traitement de maillage et d'assurer une représentation sémantique dans différents domaines, y compris l'anatomie (Hassan et al. , 2010), la conception des produits en fabrication électronique (Attene et al, 2009), robotique (Albrecht et al, 2011;.. Gurau et Nüchter, 2013). Dans (Camossi et al., 2007), un système pour soutenir un utilisateur dans la recherche et l'annotation sémantique des modèles 3D d'objets dans différents contextes d'application est présentée. L'ontologie fournit une représentation des connaissances nécessaires à la forme de l'objet déduisent, la fonctionnalité et le comportement. Ensuite, l'annotation et la récupération est effectuée en fonction des caractéristiques fonctionnelles et le comportement du modèle 3D. In (Attene et al., 2009), l'ontologie a été utilisée pour caractériser et à annoter segmenté parties d'une maille à l'aide d'un système appelé « ShapeAnnotator ». Dans ce but, l'ontologie est chargée selon le type de l'entrée de maille et l'utilisateur peut relier des segments à des concepts pertinents ex pressés par l'ontologie. Bien que l'annotation des parties de maillage se fait par un simple lien avec le « ShapeAnnotator », Gurau et Nüchter (2013) et Shi et al. (2012) a proposé d'alimenter une ontologie avec un ensemble de règles définies par l'utilisateur (par exemple propertie géométrique s des objets, des navires RELATION spatiales) et l'annotation finale est construite d'après eux. In (Hassan et al., 2010), une ontologie y compris une approximation de la forme géométrique de certains organes anatomiques ont été utilisés pour guider la segmentation de maillage. Les paramètres nécessaires pour segmenter le maillage d'entrée ont été fournies par l'ontologie. Pour le cas de la classification sémantique, une ontologie a été utilisé dans (Albrecht et al., 2011) dans un SLAM généré carte du nuage de points 3D. Après la reconstruction des plans de surface dans le nuage de points, l'ontologie est utilisée pour générer des hypothèses d'emplacements d'objets possibles et initiale pose l'estimation et le résultat final est une carte sémantique hybride, dans lequel tous les objets identifiés ont été remplacés par leurs modèles CAO correspondants . Récemment, Feng et Pan (2013) ont proposé un cadre unifié qui relie la sémantique et de traitement mesh. Le maillage est divisé en un nombre fixe de parties correspondant au nombre de concepts dans l'ontologie. Les pièces sont ensuite annotés sur la base des règles définies dans l'ontologie (par exemple, la tête est très différente avec des membres). 3 Méthode proposée La segmentation d'un objet suivant sa géométrie est un problème non trivial, ainsi que des concepts sémantiques associant à chaque objet et de ses sous-parties. Ces deux questions ont besoin d'un très spécifique Process- ment. Des travaux antérieurs (Hudelot et al, 2008;. Attene et al, 2009;. Hassan et al, 2010;.. Fouquier et al, 2012) sur ce sujet sont de plus en plus vont dans le sens de mélanger les deux problèmes, afin à l'aide de la segmentation en utilisant la sémantique déjà extraites, et par extraction de la sémantique des segmentations partielles. Le cadre que nous présentons dans cette section porte le même but, avec un fort accent sur la séparation entre les algorithmes de segmentation et les raisonnements sémantiques. Les avantages de cette approche seront expérimentées dans la section suivante, et les extensions possibles ont discuté dans le dernier, mais nous pouvons déjà souligner un avantage direct de cette approche: en utilisant ce cadre pour répondre à un nouveau domaine applicatif ne nécessitent l'utilisateur la conception pondant - 277 - Un cadre pour Mesh Segmentation et annotation à l'aide de l'ontologie ontologies, sans modification de code, en supposant que les concepts de base dont il aura besoin font partie du noyau d'origine du cadre. Le seul pont entre les algorithmes et la sémantique reste ici sur la classification et la description de chaque algorithme. Dans une première partie, nous décrivons notre paradigme de l'ontologie multi-couches et la façon dont les connaissances d'experts sur un domaine spécifique est mis en œuvre sur des concepts élémentaires, nous donnons les spécifications des algorithmes de segmentation élémentaires. Enfin, nous allons décrire la façon dont notre cadre utilise ces deux modules pour répondre à la question de la segmentation du maillage sémantique axée. 3.1 Description des connaissances d'experts Nous vous proposons dans notre cadre de modéliser les connaissances d'experts en 2 étapes: 1) les concepts sémantiques associés au domaine sont regroupés en applicatif une ontologie multi-couches; 2) les combinaisons possibles des concepts d'une couche donnée pour former des concepts d'un niveau supérieur. Ces combinaisons correspondent à des concepts équivalents dans l'ontologie et seront classés au moment de l'exécution par le raisonneur de l'ontologie. Étant donné que le cœur de nos besoins cadres à des algorithmes de segmentation de connexion avec l'ontologie de domaine applicatif, nous avons conçu une ontologie de base, appelée S0 qui contient tous les concepts élémentaires nécessaires à un processus de segmentation d'objets. 3.1.1 concepts sémantiques élémentaires A de segmentation et de processus d'étiquetage sémantique implique que la partie algorithmique est capable d'identifier et de régions de l'étiquette avec des propriétés spécifiques, telles que les caractéristiques géométriques (par exemple, bâton, planche, cube, région verticale), la couleur ou les propriétés de texture (par exemple, uniformité de la couleur, la réflexion, motifs de texture), mais également des propriétés liées à la position et la configuration des sous-parties en ce qui concerne d'autres (par exemple, les régions parallèles, A est jusqu'à wrt B, A est comprise entre B et C). Nous désignerons ces propriétés comme unaire (reliant une région de maillage à un concept, par exemple, la géométrie, la couleur, la texture) ou n-aire (reliant plusieurs régions entre elles par une conception, par exemple la topologie, distance). Dans la description des connaissances d'expert dans notre cadre, ces propriétés sont con- TITUANT la première couche S0 de notre ontologie. Dans cette modélisation, nous avons choisi de regrouper séparément les propriétés géométriques et unaires et les propriétés chromatiques n-aire topologiques. Plus précisément, une propriété d'objet et un concept de distance est associée à chaque propriété unaire dans l'ontologie (le domaine étant les parties d'objet). Le concept de gamme (par exemple la forme, l'orientation, la couleur) est alors spécialisée dans les concepts sémantiques élémentaires (par exemple forme → cube, cylindre, sphère,...) Qui correspondra à la valeur réelle de la propriété de l'objet. Ontologies CAN- modèle ne sont pas directement des propriétés n-aire et nous les représentons donc en utilisant deux propriétés d'objet et un concept de domaine. Le concept de domaine (par exemple la position, la distance) est également spécialisée dans les concepts sémantiques élémentaires (par exemple la distance → reliée, à proximité, jusqu'à présent,...), Qui sont alors reliés à des parties d'objets à travers 2 propriétés de l'objet (isReferenceRegion et isTargetRegion). Fig. 1a montre un unaire et une propriété de n-aire tandis que la Fig. 1b représente une sous-partie d'une ontologie de base possi- ble avec les concepts domaine / intervalle et leurs concepts sémantiques élémentaires correspondants. - 278 - T. Dietenbeck et al. hasShape Cube Cube isReferenceRegion isTargetRegion UpFrom (a) (b) FIG. 1: (a) Des exemples de biens unaire et n-aire. correspondent A et B à la région de l'objet. (B) Un sous-ensemble des concepts sémantiques élémentaires de l'ontologie de base. Le nombre à côté de certains concepts indiquent qu'ils relient plusieurs régions en même temps (par exemple 2 UpFrom concerne les régions A et B). S 0 S 1 S 2 Po lo gy géométrie en couleur Meubles Seat Back Support S 0 S 1 S 2 S 3 meubles Siège arrière du dossier Appui-tête Appui-pieds jambe pour gy po lo géométrie de couleur (a) Exemple d'ontologies multi-couches pour meubles S 0 S 1 S 2 S 3 Façade porte WindowWallRoof tation Vege Road Street ColorGeometry Po lo gy S 0 S 1 S 2 S 3 S 4 ColorGeometry Po lo g yFacade porte Windowsill WindowShutter fenêtre WallRoof tation Vege Road Street (b) Exemple de plusieurs -CALQUE ontologies pour les rues figure. 2: Exemples d'ontologies multi-couches pour meubles (a) et la segmentation de rue (b). Notez comment un en ajoutant simplement une couche de description détaillée des connaissances d'experts plus du même domaine peuvent être atteints (par exemple pour les concepts de dos ou de soutien en cas de Mobiliers) 3.1.2 multicouche ontologie Cette première couche (S0, blocs bleus la Fig. 2) fait partie du noyau de notre cadre. Il est enrichi pour chaque contexte avec des concepts sémantiques applicatif spécifiques. Dans la section 3.3, nous allons décrire le système expert efficace qui fait le pont entre la sémantique et des algorithmes. Elle exige que les concepts sémantiques spécifiques sont regroupés en deux (ou plus) couches: une couche appelée S1 (blocs jaunes sur la figure 2.), En utilisant uniquement les références aux concepts de la couche S0, et qui décrivent toutes les configurations d'objets qui peuvent être combinatoirement établi, à savoir le résultat d'un produit cartésien entre les concepts sémantiques élémentaires. Les couches supplémentaires (S2,..., Sn, vert, rouge et violet blocs de la Fig. 2) décrivent les règles de combinaison pour remplir une scène complète. - 279 - Un cadre pour la segmentation Mesh et annotation à l'aide ontologies 3.1.3 Relier deux couches: concepts équivalents Outre les concepts sémantiques du domaine d'application, une autre connaissance importante d'experts consiste à la façon dont les concepts sont liés les uns aux autres. Cela peut être exprimé en un ensemble de concepts équivalents de la couche Sn décrivant les combinaisons possibles ou impossibles des concepts de la couche Sn-1. Le travail de l'expert est donc fortement simplifiée puisqu'il ne peut décrire non seulement des règles positives, mais aussi négatifs. Par exemple, dans le ontol meubles logie, un pied de chaise peut être décrit comme une forme de bâton et une orientation verticale; d'autre part, la combinaison d'un appui-tête sans dossier est une configuration incompatible. Dans la pratique, les configurations incompatibles sont spécialisées dans des concepts spécifiques, un pour chaque type d'incompatibilité. Cette spécialisation permet d'effectuer un raisonnement et à la classification des individus partiellement annotées. Une fois que ces concepts équivalents sont donnés par l'expert, ils sont utilisés de deux façons: soit de construire un arbre de décision (qui sera détaillée dans la section 3.3) ou de suggérer une correction de segmentation. En effet, au cours du processus de segmentation / annotation, configuration incompatible peut apparaître en raison de soit une erreur de segmentation ou un concept équivalent manquant. Dans ce cas, le raisonneur peut demander la raison de l'incompatibilité qui est ensuite présenté à l'utilisateur pour la correction. Le principal avantage de cette approche est qu'elle nous permet de demander à l'utilisateur d'corriger les erreurs que dans les régions qui ont causé la classification comme incompatible au lieu d'avoir à explorer l'ensemble maillage / étiquetage. 3.2 signatures de type sémantique d'algorithmes Dans ce travail, l'idée de base de la partie algorithmique est de diviser la méthode en algorithmes élémentaires qui sont dédiés à l'un des concepts sémantiques élémentaires. Mais cette ration consi- ne peut être le seul guide pour produire un catalogue synthétique des algorithmes: la signature de type de ces algorithmes ne peut être définie que par un seul concept. Chacun de ces algorithmes seront impliqués dans le processus de segmentation et en divisant la labellisation région donnée et en ajoutant des descripteurs sémantiques aux sous-parties. Étant donné qu'une région peut être décrite par plus d'un concept (par exemple, une partie peut être un bâton et vertical), et puisque notre objectif est d'avoir des algorithmes plus élémentaires que possible, on en déduit qu'il existe également des algorithmes qui ne sont pas le fractionnement la région donnée, mais de plus en plus que les connaissances sur cette région. Enfin, et parce que nous voulons traiter des concepts qui ne sont pas une région impliquant necessarly seule, il faut distinguer les fonctions associées aux propriétés unaire et fonctions associées à celles de n-aire. Le catalogue de synthèse suivant est une proposition visant à identifier chaque algorithme de segmentation dans le contexte de l'étiquetage sémantique: les questions sémantiques commencent à trouver tous les (SF), avec des concepts unaire (par exemple, trouver tous les rectangles dans la région A), les questions sémantiques commençant par est un ( SI), avec des concepts unaires (par exemple, est B une région plate), des questions topologiques (T), l'identification des rela- tions n-aires entre les régions (par exemple, sont B et C reliées), des questions topologiques à partir de trouver tous les (TF), en utilisant une relation n-aire et de 1 à n-1 des régions, pour trouver des régions vérifiant la relation wrt les régions d'entrée (par exemple, trouver toutes les régions connectées à la région B, trouver toutes les régions entre les régions B et C). Chaque algorithme reçoit en entrée une ou plusieurs régions de l'objet d'origine et renvoie un ensemble de régions, chacune d'entre elles enrichie par une description sémantique générée par la fonction, en plus d'une partition dans [0; 1] pour illustrer la correspondance entre cette région et le concept associé. Ce - 280 - T. Dietenbeck et al. (A) Vue d'ensemble de la méthode proposée appliquée sur le domaine du meuble. (B) génération d'arbre de décision hors connexion dans le logie ontol- du domaine d'application. FIGUE. 3: Vue d'ensemble de la méthode proposée. premier catalogue synthétique de possibles signatures de type sémantique couvre tous les algorithmes pour la segmentation maillage en utilisant la description sémantique, comme illustré dans les sections suivantes. 3.3 Système expert Dans cette section, nous décrivons comment les connaissances d'experts est utilisé dans notre algorithme pour segmenter efficacement et annoter un objet. Fig. 3a donne un aperçu de ce cadre. L'un des avantages de notre approche est qu'il donne la possibilité de calculer facilement un arbre contenant l'ordre des questions à poser pour arriver à la solution la plus efficace façon. Pour construire cet arbre de décision, nous avons d'abord nous e les concepts équivalents de chaque couche pour construire l'ensemble des configurations possibles. Pour chaque couche et à partir de la couche S0, le produit cartésien entre toutes les propriétés de la couche est effectuée. Le logicien est ensuite utilisé pour classer les cas dans des concepts équivalents et les candidats incompatibles sont supprimés. Les restants sont ensuite utilisés dans la couche ci-dessus en tant que concepts sémantiques dans le produit cartésien pour calculer la nouvelle liste des configurations possibles. Une fois que l'ensemble de toutes les configurations possibles Ω est créé, l'idée est de le diviser selon le concept maximisation d'un critère C. Le choix de ce concept nous donne la question à poser et donc un nœud de l'arbre. Pour chaque réponse possible, nous obtenons alors le sous-ensemble correspondant et recherchez le prochain concept maximisant C. Cette opération est réitérée jusqu'à ce qu'une seule possibilité est laissée dans chaque sous-ensemble. Dans l'arborescence résultante, la racine correspond donc à Ω et stocke la première question à poser, chaque feuille est une configuration possible et les noeuds intermédiaires sont des sous-ensembles de Ω et de stocker la prochaine question à poser. Notez que cette étape peut être effectuée qu'une seule fois et hors ligne afin d'accélérer le processus. Cette procédure est illustrée sur la Fig. 3b. La première étape en ligne de notre système expert est de parcourir l'arbre de décision: à partir de la racine, le système est la question à poser et exécuter l'algorithme élémentaire associé à l'objet d'entrée. Le système sélectionne alors le noeud enfant correspondant au résultat de l'algorithme - 281 - Un cadre pour Mesh segmentation et d'annotation en utilisant des ontologies (a)-Layer multi ontologie utilisée pour segmenter les meubles. (B) le résultat de la segmentation et de l'identification de 5 objets à partir du domaine du meuble. Résultat de l'identification: repos back- orange⇒, siège blue⇒, d'autres pieds colors⇒. FIGUE. 4: Ontologie et mailles utilisées pour nos expériences sur la segmentation des meubles et annota- tion. et le processus jusqu'à une feuille itère de l'arbre est atteinte signifie que la sémantique associée à l'objet est connu comme un seul reste de configuration possibles. Dans certains cas, le système d'experts pourrait atteindre une feuille avant chaque partie du maillage est segmenté ou annotés (parce que certains concepts peuvent être déduites de la présence / absence d'autres). Certains algorithmes supplémentaires sont ensuite exécutés sur le maillage pour confirmer les tics globaux seman- et annoter les parties manquantes. Cela peut encore être fait de façon très efficace par le raisonneur questionnant, ce qui donnera au système expert les concepts manquants et donc l'algorithme élémentaire à exécuter. 4 Les expériences sur les expériences et Mobiliers Segmentation Annotation Les ont été faites sur la segmentation et l'annotation des meubles, en utilisant des formes de base pertinentes à ce domaine (voir Fig. 4b). Dans ce travail, nous nous concentrons notre intérêt sur le système expert afin d'étudier la faisabilité du cadre présenté. la sélection Finement et le réglage des algorithmes de segmentation élémentaires sera l'un des futurs travaux que nous mentionnons dans la section 5. Nous avons mis le système expert décrit dans la section 3.3 en utilisant Java et l'API OWL, et conçu notre prototype de telle sorte que le maillage purement manipulations sont écrites en C ++ , en utilisant CGAL. La liaison entre ces deux parties est effectuée en utilisant un paradigme client / serveur par l'intermédiaire des douilles. Pour tenir compte du contexte spécifique des meubles, nous avons conçu une ontologie dédiée (voir la figure. 4a) en utilisant 1 sur Protégé d'une version simplifiée des concepts élémentaires introduits dans sec- tion 3.1.1. L'arbre de décision associé à cette ontologie est générée selon la procédure décrite dans la section 3.3 où le critère C a été choisie pour être la dichotomie, on cherche à savoir les concepts permettant de diviser l'ensemble en deux sous-ensemble de même taille. 1. http://protege.stanford.edu/ - 282 - T. Dietenbeck et al. (A) R0 (b) R1 (c) R2 (d) R3 (e) R4 1. SF oneseat dans R0: false 2. SF VerticalOrientation dans R0 - R1: true - R2: true - R3: true 3. SI DownPosition R1: false 4. SI DownPosition R2: true 5. SI DownPosition R3: true 6. SI StickShape R2: false 7. SI StickShape R3: false 8. SI DownPosition R1: false 9. SI BoardShape R1: true 10. SI BoardShape R2: true 11. SI BoardShape R3: true 12. SI BoardShape R0 - {R1, R2, R3 } - R4: true figure. 5: Liste des algorithmes élémentaires et les réponses correspondantes générées par notre système d'experts mailler segment et annoter un banc. SF: sémantique trouve tout, SI: sémantique est un. Objet #subparts S. puis I. Naive S & I Notre pré-traitement de la méthode - - # SF # SI # SF # SI banc # SF # SI 1 4 0 21 9 0 2 10 banc 2 6 0 31 9 0 3 13 couch 6 0 31 9 0 3 14 chaise 1 6 0 0 31 9 2 15 chaise 2 8 0 41 9 0 2 19 TAB. 1: Nombre d'étapes de segmentation pour une identification complète et la segmentation. Une fois l'arbre de décision est généré, nous pouvons exécuter notre système de segmentation d'experts sur les mailles. Fig. 5 donne la liste des questions qui sont calculées pour segmenter et reconnaître le premier banc à la Fig. 4b. Les autres images de la Fig. 4b sont illustrant le processus de segmentation tion et identifica- en utilisant la même ontologie d'experts avec différentes mailles. Nous avons comparé notre segmentation et la méthode d'identification avec d'autres approches équivalentes, et nous résumer les résultats dans l'onglet. 1. La première colonne intitulée S. I. puis correspond à une approche où un premier pré-traitement de segmentation se fait à des régions de division, chaque ré- gion est marqué en utilisant les concepts sémantiques. La deuxième colonne intitulée Naive S & I correspond à une approche où les algorithmes de segmentation dédiés à la nature spécifique des formes sont gérées indépendamment pour identifier les régions. La dernière colonne correspond à notre approche. Pour chaque procédé, on a détaillé le nombre de découverte sémantique tout (SF) et sémantique est-ce un des algorithmes (SI) nécessaires pour segmenter et annoter les 5 objets représentés sur la Fig. 4b. Nous avons choisi de distinguer les 2 types d'algorithmes, car la complexité de chaque famille d'algorithme est significativement différent: un algorithme SF, il faudra parcourir toutes les régions données (peut-être le maillés ensemble), et il faudra extraire les sous-parties de celui-ci. En comparaison, un algorithme SI ne doit valider ou non une caractéristique sur une région donnée. Minimiser le nombre de pistes SF est donc l'objectif principal d'un processus de segmentation et d'identification. Pour chaque région d'un objet, l'ontologie expert utilise 3 concepts de gamme (forme, position, orientation) qui implique 8 concepts sémantiques élémentaires. Chaque objet est également caractérisée par un concept de gamme supplémentaire (nombre de sièges) qui implique 2 concepts sémantiques élémentaires par objet. Le nombre d'algorithmes SI dans le S. puis I. a été estimé comptage pour chaque sous-région du maillage par un SI notion sémantique élémentaire (moins 1 par concept de gamme qui - 283 - Un cadre pour la segmentation Mesh et annotation à l'aide ontologies peuvent être déduites ), ainsi que les 1 concepts sémantiques élémentaires de l'objet complet. Le Naive S & I consiste à courir tous les algorithmes SF disponibles. Le nombre de SF et SI de notre méthode provient de la trace des essais expérimentaux (voir, par exemple, à la Fig. 5). Nous comparons tout d'abord notre travail à S. puis I. approche, où un pré-traitement de segmentation est appliquée avant l'identification. Nous ne pouvons pas comparer quantitativement cette approche avec la nôtre, mais puisque nous utilisons les connaissances d'experts pour réduire le nombre d'algorithmes SF dans notre approche, nous pouvons en déduire que nos calculs SF sont presque équivalente cher que le stade preprocess- ING du S. je . approche 2. Le nombre d'algorithmes SI est fortement réduit par notre approche. La deuxième approche envisagée pour la comparaison est un Naive S & I approche, où tous les algorithmes SF sont exécutés. Le nombre d'algorithmes SF est fortement réduit par notre approche, et la comparaison entre l'approche Naive S & I peut être résumée par une comparaison entre la complexité de SF et de la complexité des algorithmes SI. Ces premiers résultats illustrons la pertinence de notre méthode par rapport aux approches existantes: mélanger les étapes de segmentation et d'identification est une bonne approche pour réduire la com plexité de l'algorithme global. 5 Conclusion et travaux futurs Dans cet article, nous avons présenté un nouveau cadre pour la segmentation efficace et annotation de mailles. Il est composé de deux blocs: un multi-couche ontologie sémantique regroupant les sur le domaine d'application et une partie de traitement permettant de détecter géométriques élémentaires, chromatiques et concepts topologiques. Le principal avantage de notre procédé est qu'il sépare les connaissances sur le domaine du traitement permettant de segmenter un expert et annoter un objet sans connaissance dans l'image ou la transformation mesh. Un autre avantage est que l'utilisation de l'expertise, nous sommes en mesure de construire un arbre de décision d'effectuer une recherche efficace parmi l'ensemble des objets possibles tout en étant en mesure de proposer des corrections de segmentation et d'annotation à l'utilisateur si une configuration impossible est atteinte. L'ontologie que nous avons conçu pour cette expérience est très basique, et nous prévoyons d'expérimenter au cours des prochains mois une ontologie plus complète avec plus de concepts. En particulier, les propriétés n-aire seront intégrées afin d'exprimer des contraintes plus réalistes entre les sous-parties des objets. La simulation nous avons présenté à la section 4 utilise segmentations manuelles au résultat mimétique des algorithmes. Notre prochaine étape sera de sélectionner et d'ajuster les algorithmes élémentaires de la litterature, comme les extracteurs géométriques (Mortara et al., 2004;. Li et al, 2011) ou la segmentation approches fondées par des caractéristiques de fonctionnalité (Laga et al, 2013.). Présentation de segmentations automatiques ouvriront de nombreuses questions connexes que nous prévoyons de poignée dans un avenir proche. L'un des prochains défis consistera à introduire de meilleures approches pour choisir entre les algorithmes que celui dichotomique. Un premier critère à considérer pourrait être un système d'ING qui favorisent des algorithmes en poids avec un petit temps de calcul, ou d'inclure l'exactitude des algorithmes. Ces poids seront introduits dans le calcul de l'arbre de décision afin de concevoir un système d'experts qui gèrent la question de l'efficacité. 2. La meilleure stratégie pour un pré-traitement peut être de choisir le concept de plage le plus petit, puis exécutez pour chaque concept élémentaire un algorithme SF. - 284 - T. Dietenbeck et al. L'application de notre approche sur des maillages fournies par les appareils à basse résolution va compliquer la tâche des algorithmes de segmentation. Il va probablement générer des sous-régions incohérentes, avec ou parties superpositions non marquées. Une approche possible consiste à utiliser des cartes floues pour décrire les régions, mais une autre question devra être traitée: comment ajuster une partielle existante la segmentation? Notre cadre est un bon candidat pour fournir une réponse spécifique à ce problème, étant donné que les connaissances d'experts contient des informations sur les configurations attendues. Une extension possible de ce travail pourrait consister à introduire des algorithmes d'ajustement pour chaque élémentaires, concepts qui seront en mesure d'ajuster une première segmentation en utilisant la connaissance globale d'un domaine spécifique. Enfin, une extension à long terme de ce travail sera de l'introduire dans une démarche d'apprentissage de la machine, où l'ontologie sera déduite ou prolongée d'un existant, en utilisant un ensemble de formes du domaine. Ce cadre élargi sera un challenger possible de la forme 3D de récupération du concours (SHREC) organisé chaque année dans la communauté de segmentation du maillage. Remerciements Marco Attene remercie le projet européen FP7 VISIONAIR pour avoir ses contributions SUP- dans portés cette recherche. Références Albrecht, S., T. Wiemann, M. Günther et J. Hertzberg (2011). Correspondant à els mo- objet de CAO en correspondance sémantique. Dans l'atelier Perception sémantique, la cartographie et l'exploration (ICRA'11), p. 1. Attene, M., F. Robbiano, M. Spagnuolo, et B. Falcidieno (2009). Caractérisation des pièces de forme 3D pour l'annotation sémantique. Conception Assistée par Ordinateur 41 (10), 756-763. Camossi, E., F. Giannini, et M. Monti (2007). Dérivation fonctionnalité de formes 3D: On- annotation axée sur logie et la récupération. Conception assistée par ordinateur et des applications 4 (6), 773- 782. Feng, X. et X. Pan (2013). Un cadre unifié pour la segmentation mesh et annotation de la pièce. J. de Computational des systèmes d'information 9 (8), 3117-3128. Fouquier, G., J. Atif et I. Bloch (2012). segmentation basée sur un modèle séquentiel et de recon- naissance des structures d'image entraînées par les caractéristiques visuelles et les relations spatiales. Vision et image compréhension 116, 146-165. Gurau, C. et A. Nüchter (2013). Les défis à l'utilisation des connaissances sémantiques pour la classification des objets 3D. Dans KI 2013 Atelier sur visuelles et spatiales Cognition, p. 29. Hassan, S., F. Hétroy et O. Palombi (2010). segmentation maille guidée ontologie. Dans FOCUS K3D Conf. Sémantique 3D et de contenus médiatiques. Hudelot, C., J. Atif et I. Bloch (2008). ontologie de la relation spatiale floue pour l'image inter- prétation. Ensembles flous et systèmes 159, 1929-1951. Laga, H., M. Mortara, et M. Spagnuolo (2013). Géométrie et contexte sémantique respondance cor- et la reconnaissance des fonctionnalités dans des formes d'origine humaine 3D. ACM Trans. Graphics (TOG) 32 (5). - 285 - Un cadre pour la segmentation Mesh et annotation à l'aide ontologies Li, Y., X. Wu, Y. Chrysathou, A. Sharf, D. Cohen-Or et N. Mitra (2011). Globfit: consistances montage tently primitives en découvrant les relations internationales. ACM Trans. Graphics (TOG) 30 (4), 52. Mortara, M., G. Patané, M. Spagnuolo, B. Falcidieno et J. Rossignac (2004). Plombier: une méthode pour une décomposition multi-échelle de formes 3D en primitives tubulaires et corps. Dans ACM Symposium sur la modélisation solide et applications, pp. 339-344. Othmani, A., C. Meziat et N. Loménie (2010). analyse d'image axée sur les ontologies pour les images histopathologiques. Dans Int. Symposium sur l'informatique visuelle (ISVC'10). Seifert, S., M. Thoma, F. Stegmaier, M. Hammon, M. Kramer, M. Huber, H. Kriegel, A. Caval- Laro, et D. Comaniciu (2011). Combiné recherche sémantique et la similitude dans les bases de données d'images médicales. Dans SPIE Medical Imaging. Shi, M., H. Cai et L. Jiang (2012). Une approche de l'annotation sémantique semi-automatique sur les scènes Web3D basé sur un cadre de l'ontologie. Dans les systèmes intelligents de conception et d'application (ISDA'12), pp. 574-579. Résumé La segmentation de Maillages ET annotation La Sémantique a Utilisant Ete L'objet D'un intêret Grandissant AVEC La des techniques de démocratisation reconstruction 3D. Une clas- sique approach Consiste à en Réaliser deux this Tâche ÉTAPES, tout d'ABORD en segmentant le MAILLAGE, en l'annotant Puis. Cependant, ne this approach pas à Përmet each Étape de de l'Autre Profiter. En d'treatment images, methods Quelques la segmentation et combinent l'annotation, ne approaches bureaux Mais pas Génériques are, et des nalisées d'nécessitent OU des réécritures Implémentation each modification des répandrai Connaissances expertes. Dans CE travail, nous décri- vons un cadre de functioning Qui segmentation et annotation chiné de Réduire le AFIN d'Nombre de segmentation ÉTAPES, et nous Présentons des Préliminaires Qui Résultats la montrent de l'approach faisabilité. Notre Système fournit Une decrit Qui Générique.Nous ontologie sous forme de concepts les pro- priétés d'un objet (géométrie, topologie, etc.), Que des Algorithmes AINSI de detecter concepts permettant SCÉ. This can be ontologie par expert Étendue un DÉCRIRE verser un domaine Spécifique formellement. La Description du domaine formelle is verser Alors Utilisée matiquement l'auto Réaliser l'assemblage de la segmentation et de l'annotation d'objets et de their proprié- Tés, en each à sélectionnant l'étape Le plus pertinent algorithme, being les Informa Données - tion sémantiques de déjà détectées. This includes several approach originale Avantages. Tout d'Abord, de segmenteur Elle Përmet et d'annoter des objets sans en Aucune Connaissance d'images Tement trai- OU de Maillages, en les décrivant Uniquement Propriétés de l'Objet en ontologiques Terme de concepts. De plus, cadre de fontionnement CE may Réutiliser et Facilement Être à Appliqué Contextes Différents, LORs Dès de domaine Qu'une ontologie was definie. FINALEMENT, la conjoindre réalisation te de la segmentation et de l'annotation d'Përmet D'une Manière UTILISER la Connaissance Experte Efficace, en les réduisant de segmentation et Erreurs le temps de calcul, en l'lançant Toujours le plus de pertinence Algorithme. - 286 - D - Sémantique et ontologies Un cadre pour la segmentation Mesh et annotation à l'aide ontologies Thomas Dietenbeck, Ahlem Othmani, Marco Attene, Jean-Marie Favreau"
235,Revue des Nouvelles Technologies de l'Information,EGC,2015,Big Data and the Dawn of Algorithms in Everything,"The mainstream adoption of the internet as a source for knowledge and interaction for the past decades has given rise to new data sources that are characterized by large sizes and rapid creation. In addition, sensory data from mobile devices and machinery are on the rise with similar characteristics. All these sources have the commonality that they will tell us something new or something more detailed than before. From a business standpoint these data sources holds the opportunity to create more customized services and improved products in practically anything, however, they also present a challenge since they are big and typically residing outside the traditional server structure of organizations. This talk will explore the challenges of integrating these new, so-called Big Data, in decision processes. Specifically, we will explore the paradigm shifts when external data become equally or more important than internal data. We will also explore the emerging shift in decision making becoming algorithmic as opposed to human discovery driven.",Morten Middelfart,http://editions-rnti.fr/render_pdf.php?p1&p=1002057,http://editions-rnti.fr/render_pdf.php?p=1002057,en,"Big Data et l'aube des algorithmes dans tout ce que Morten Middelfart social Quant, Inc. 449 S 12th St # 2603 Tampa, FL 33602 USA morton@targit.com http://targit.com/research Résumé L'adoption généralisée de l'Internet en tant que source de connaissances et de l'interaction au cours des dernières décennies a donné naissance à de nouvelles sources de données qui sont caractérisées par de grandes tailles et création rapide. De plus, les données sensorielles provenant d'appareils mobiles et les machines sont à la hausse avec des caractéristiques similaires. Toutes ces sources ont les points communs qu'ils nous diront quelque chose de nouveau ou quelque chose de plus détaillé qu'auparavant. D'un point de vue commercial de ces sources de données détient la possibilité de créer plus de services personnalisés et des produits améliorés dans pratiquement rien, mais ils présentent aussi un défi car ils sont grandes et résidant habituellement en dehors de la structure traditionnelle du serveur d'organisations. Cette conférence explorera les défis de l'intégration de ces nouvelles, que l'on appelle Big Data, dans les processus de décision. Plus précisément, nous allons explorer les changements de paradigme lorsque les données externes deviennent tout aussi ou plus important que les données internes. Nous allons également explorer le changement émergent dans la prise de décision de devenir algorithmiques, par opposition à la découverte humaine conduit. - 7 - Big Data et INVITÉS l'aube Conférences des algorithmes dans tout ce que Morten Middelfart"
236,Revue des Nouvelles Technologies de l'Information,EGC,2015,Big Data is all about data that we don't have,"Big Data is now becoming a buzz word in information technology industry and research. Is Big Data only about large volume of data?, and if it is yes, why is it suddenly becoming a trend. Hasn't the growth of data volume been gigantic in the last decade? From a research point of view, it is not surprising to see researchers from all walks of computer science are trying to align their research to Big Data for the sake of being trendy. The question remains whether it tackles the real Big Data problems. In this talk, I will describe the misconceptions of Big Data, present motivating cases, and discuss the unavoidable challenges faced by industry and research.",David Taniar,http://editions-rnti.fr/render_pdf.php?p1&p=1002055,http://editions-rnti.fr/render_pdf.php?p=1002055,en,"Big Data est tout au sujet des données que nous ne disposons pas David Taniar Université Clayton École de technologie de l'information Clayton Monash, Victoria 3800 Australie david.taniar@monash.edu http://users.monash.edu/~dtaniar Résumé Big Data est maintenant devenir un mot à la mode dans l'industrie des technologies de l'information et de la recherche. Big Data est seulement grand volume de données ?, et si oui, pourquoi est-il soudainement devenir une tendance. N'a pas la croissance du volume de données gigantesque été dans la dernière décennie? D'un point de vue de la recherche, il est surprenant de voir les chercheurs de tous les horizons de la science informatique tentent d'aligner leurs recherches à Big Data pour le plaisir d'être à la mode. La question reste de savoir si elle aborde les vrais problèmes Big Data. Dans cet exposé, je vais décrire les idées fausses de Big Data, les cas actuels de motivation, et de discuter des défis inévitables auxquels sont confrontés l'industrie et de la recherche. - 1 - Big Data de Conferences tout au sujet des données que nous n'avons pas David Taniar"
237,Revue des Nouvelles Technologies de l'Information,EGC,2015,"Challenges and Opportunities in HCI, Visual Analytics and Knowledge Management for the development of Sustainable Cities","While overtly exposed in the media, the challenges faced by our societies to transition towards sustainable energy use are quite formidable. A simple visual refresher of the cold hard facts should amply reveal the importance of visualization to assess the situation. Private companies, such as IBM, and public research centers are joining forces and investing to design and evaluate novel approaches to build and manage Cities, defined as the rational organisation of dense human habitat. Information and Communication technologies are certainly part of the answers, in particular in areas related to knowledge management, data mining, HCI and social computing. Illustrated with telltaling examples of research work carried at IBM, the CSTB and the Efficacity Institute, I will argue that Interactive Information Technologies can help managing the energy transition of cities in 3 key aspects:   — to support the city design process, notably computer supported tooling and information infrastructure that help taming the complexity of the intertwinning actors and interests at play,   — to help understand better the city's dynamics, identifiy inefficiencies and reveal optimization opportunities, where knowledge management and extraction is crucial,   — and foremost, to ease the necessary changes that will have to happen in our mobility and housing habits with novel tools and services that alleviate our energy needs.",Thomas Baudel,http://editions-rnti.fr/render_pdf.php?p1&p=1002058,http://editions-rnti.fr/render_pdf.php?p=1002058,en,"Défis et opportunités dans HCI, l'analyse visuelle et la gestion des connaissances pour le développement des villes durables Thomas Baudel BP ILOG 83-9, rue de Verdun 94253 Gentilly Cedex France baudelth@fr.ibm.com http://thomas.baudel.name/Personnel / Résumé Bien exposé ouvertement dans les médias, les défis auxquels sont confrontés nos sociétés à la transition To- quartiers consommation d'énergie durable sont tout à fait formidable. Un simple recyclage visuel des faits durs froids devrait révéler amplement l'importance de la visualisation pour évaluer la situation. les Panies com- privées, comme IBM, et les centres de recherche publics unissent leurs forces et investir pour concevoir et évaluer de nouvelles approches pour construire et gérer les villes, définies comme l'organisation rationnelle de l'habitat humain dense. technologies de l'information et de la communication font certainement partie des réponses, en particulier dans les domaines liés à la gestion des connaissances, l'exploration de données, HCI et l'informatique sociale. Illustré avec telltaling exemples de travaux de recherche menés au sein d'IBM, le CSTB et l'Institut effi- cacity, je soutiendrai que l'information Interactive Technologies peut aider à gérer la transition énergétique des villes en 3 aspects clés: - pour soutenir le processus de conception de la ville, notamment l'ordinateur outillage pris en charge et de l'infrastructure d'information qui aident dompter la complexité des acteurs et des intérêts intertwinning en jeu, - pour aider à mieux comprendre la dynamique de la ville, l'inefficacité identitée et révèlent les possibilités de plan optimisation, où la gestion des connaissances et l'extraction est cruciale, - et avant tout, faciliter les changements nécessaires qui devront se produire dans nos habitudes de mobilité et de logement avec de nouveaux outils et services qui soulagent nos besoins énergétiques. - 9 - CONFÉRENCES Défis et opportunités INVITÉS dans HCI, l'analyse visuelle et la gestion des connaissances pour le développement durable des villes Thomas Baudel"
243,Revue des Nouvelles Technologies de l'Information,EGC,2015,"Comparison of linear modularization criteria using the relational formalism, an approach to easily identify resolution limit","La modularisation de grands graphes ou recherche de communautés est abordée comme l'optimisation d'un critère de qualité, l'un des plus utilisés étant la modularité de Newman-Girvan. D'autres critères, ayant d'autres propriétés, aboutissent à des solutions différentes. Dans cet article, nous présentons une réécriture relationnelle de six critères linéaires: Zahn-Condorcet, Owsi´nski- Zadro&#729;zny, l'Ecart à l'Uniformité, l'Ecart à l'Indétermination et la Modularité Equilibrée. Nous utilisons une version générique de l'algorithme d'optimisation de Louvain pour approcher la partition optimale pour chaque critère sur des réseaux réels de différentes tailles. Les partitions obtenues présentent des caractéristiques différentes, concernant notamment le nombre de classes. Le formalisme relationnel nous permet de justifier ces différences d'un point de vue théorique. En outre, cette notation permet d'identifier facilement les critères ayant une limite de résolution (phénomène qui empêche en pratique la détection de petites communautés sur de grands graphes). Une étude de la qualité des partitions trouvées dans les graphes synthétiques LFR permet de confirmer ces résultats.","Patricia Conde-Céspedes, Jean-François Marcotorchino, Emmanuel Viennet",http://editions-rnti.fr/render_pdf.php?p1&p=1002080,http://editions-rnti.fr/render_pdf.php?p=1002080,en,"Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel, une approche d'identifier facilement la limite de résolution Patricia Conde-Céspedes *, Jean-François Marcotorchino **, Emmanuel Viennet *, 1 * L2TI - Institut Galilée - Université Paris 13 99, av. Jean-Baptiste Clément; 93430 Villetaneuse - France prenom.nom@univ-paris13.fr ** Thales Communications et Sécurité 4 av. des Louvresses; 92230 - France jeanfrancois.marcotorchino@thalesgroup.com Gennevilliers CV. La modularisation de grands Graphes recherche de OU EST COMMUNAUTES l'optimisation Comme abordée d'un critere de qualité, l'un des plus de la being utilisés de Newman-modularité Girvan. D'Autres critères, d'Autres pro- Ayant priétés, à des aboutissent solutions Différentes. Dans this article, nous Présentons Une de six réécriture relationnelle Criteres Linéaires: Zahn-Condorcet, Owsiński- Zadrozny, l'à l'Ecart uniformité, l'à l'indétermination Ecart et la Equilibrée modularité. Nous utilisons la version de l'Une Générique.Nous d'optimisation de Algorithme Louvain verser Approcher partition la Optimale verser each sur des RE- critère Seaux Réels de tailles Différentes. Les cloisons presentent des caracté- obtenues ristiques différentes, vous concerning including le cours. Nb de Le nous Përmet formalisme de relationnel justificateur bureaux point d'différences non de vue théorique. En outre, la notation this d'identifiant Përmet les critères Facilement Une Li- mite Ayant de résolution (en empèche Qui phénomène pratique Detection of sur de petites COMMUNAUTES Grands Graphes). Une étude de la qualité des partitions vés Dans les trou- Graphes de Përmet LFR synthétiques confirmateur bureaux Résultats. 1 Introduction Les réseaux sont étudiés dans de nombreux contextes tels que la biologie, la sociologie, les réseaux sociaux en ligne, le marketing, etc. Les graphiques sont des représentations mathématiques des réseaux, où les entités sont appelées noeuds et les connexions sont appelés bords. très grands graphiques sont difficiles à analyser, et il est souvent utile de les diviser en plus petits composants homogènes plus facile à manipuler. Le processus de décomposition d'un réseau a reçu différents noms: regroupement graphique (dans l'analyse des données), la modularisation, l'identification de la structure communautaire. Les groupes peuvent être appelés ou modules commu- nautés; dans cet article, nous utilisons ces mots comme synonymes. 1. Ce travail est soutenu par le projet REQUEST entre Thales et l'Université Paris 13. - 203 - Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel évaluation de la qualité d'une partition graphique nécessite un critère de modularisation. Cette fonction sera optimisée pour trouver la meilleure partition. Divers critères de modularisation ont été formule- dans le passé ted pour répondre différentes applications pratiques. Ces critères diffèrent dans la définition donnée à la notion de communauté ou d'un cluster. Pour comprendre les différences entre les partitions optimales obtenues par chaque critère, nous montrons comment les représenter en utilisant le même formalisme de base. Dans cet article, nous utilisons pour exprimer six critères de modularisation linéaires de l'analyse relationnelle mathé- matique (MRA). critères linéaires sont faciles à manipuler, par exemple, la méthode Louvain peut être adaptée à des fonctions de qualité linéaire (voir Campigotto et al. (2014)). Les six critères sont étudiés: la modularité Newman-Girvan, le critère Zahn-Condorcet, le critère Owsinski-Zadrozny, l'écart à l'uniformité, l'écart d'indice indétermination et la modularité équilibré (détails dans la section 3). La représentation relationnelle permet de comprendre les propriétés de ces critères de modularisation. Il permet d'identifier facilement les critères souffrant d'une limite de résolution, d'abord discuté par LUCRATIF tunato et Barthélemy (2006). Nous terminerons cette étude théorique par des expériences sur des réseaux réels et synthétiques, ce qui démontre l'efficacité de notre classement. Le présent document est organisé comme suit: La section 2 présente l'app de la mathématique Relational Ana- gardon, nous introduisons la propriété de l'équilibre des critères linéaires et sa relation avec la propriété de limite de résolution. Dans la section 3, nous présentons les six critères de modularisation linéaires dans le formalisme relationnel. Ensuite, la section 4 présente quelques expériences sur des graphiques réels et artificiels pour confirmer les propriétés théoriques trouvées précédemment. 2 Relational approche Analyse Il existe un lien étroit entre l'analyse mathématique Relational 2 et la théorie des graphes: un graphe est une structure mathématique qui représente les relations binaires entre les objets appartenant au même ensemble. Par conséquent, un non-orienté et non-graphe pondéré G = (V, E), avec N = | V | noeuds et M = | E | bords, est une relation binaire symétrique sur son ensemble de noeuds V représenté par sa matrice d'adjacence A comme suit: aii '= {1 s'il existe une arête entre i et i' ∀ (i, i ') ∈ V × V 0 sinon (1) On note le degré de di noeud i le nombre d'arêtes incident i. Il peut être calculé en additionnant les termes de la ligne (ou colonne) i de la matrice de contiguïté: di = Σ i 'aii' = Σ i 'ai'i = ai. = A.i. On note δ = 2M N2 la densité des arêtes du graphe entier. Le partitionnement d'un graphe implique la définition d'une relation d'équivalence dans l'ensemble des noeuds V, ce qui signifie une relation symétrique, réflexif et transitif. Mathématiquement, la relation d'équivalence est représentée par une matrice carrée X d'ordre N = | V |, dont les entrées sont définies comme suit: 2. Pour plus de détails sur la théorie relationnelle analyse et voir Marcotorchino Michaud (1979) et Marcotorchino (1984). - 204 - P. Conde-Céspedes, JF Marcotorchino et E. Viennet xii '= {1 si i et i' sont dans le même cluster ∀ (i, i ') ∈ V × V 0 sinon (2) modularisation implique un graphique pour trouver X aussi près que possible de A. modularisation cri- tère F (X) est une fonction qui mesure soit une similitude ou une distance entre a et X. par conséquent, le problème de la modularisation peut être écrit en fonction d'optimiser F (X) où X inconnu est soumis aux contraintes d'une relation d'équivalence 3. Nous définissons ainsi ax et que la relation inverse de X et A respectivement. Leurs entrées sont définies comme xII '= 1 - xii' et Aii '= 1 - aii' respectivement. Dans la suite on note κ le nombre optimal de clusters, cela signifie que le nombre de grappes de la partition X qui maximise le critère F (X). 2.1 Critères équilibré linéaire Chaque critère linéaire est une fonction affine de X, donc dans la notation relationnelle peut être écrit sous la forme: F (X) = NΣ i = 1 NΣ i '= 1 φ (aii) xii' + K, (3) où la fonction φ (aii) ne dépend que des données d'origine (par exemple, la matrice d'adjacence). Dans la suite, nous utiliserons K pour désigner une constante ne dépendant que les données d'origine. Définition 1 (propriété d'équilibre linéaire) un critère linéaire est équilibré si l'on peut écrire sous la forme générale suivante: F (X) = NΣ i = 1 NΣ i '= 1 φ (aii) xii' + NΣ i = 1 NΣ i '= 1 & phiv (aii) xii' + K. (4) où φ (.) Et ɸ (.) Sont des fonctions non-négatives ne dépendant que des données originales et la vérification ΣN i = 1 i ΣN '= 1 φii> 0 et ΣN i = 1 i ΣN' = 1 φ̄ii> 0. 3. en fait, le problème de la modularisation peut être écrit sous la forme générale: Max X (F (X)) sous réserve des contraintes d'une relation d'équivalence: xii '∈ {0, 1} = Binary xii 1 ∀i réflexivité xii '- xi'i = 0 ∀ (i, i') de symétrie xii '+ xi'i' '- xii' '≤ 1 ∀ (i, i', i '') Transitivité La résolution exacte de ce 0 - 1. programme linéaire en raison de la taille des contraintes est peu pratique pour les grands réseaux. Ainsi, les approches heuristiques sont la seule façon raisonnable de procéder. - 205 - Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel En remplaçant x par sa définition 1- xii ', l'équation peut être réécrite (4) comme suit: F (X) = NΣ i = 1 NΣ i' = 1 ( φii '- φ̄ii') xii '+ K. (5) De cette expression, nous pouvons en déduire l'importance de la propriété de l'équilibre des critères linéaires. Si le critère est fonction de maximiser, la présence et / ou l'absence des termes φii 'et φ̄ii' a l'impact suivant sur la solution optimale: - Si φ̄ii '= 0∀i, i' la solution qui maximise F (X) est la partition dans laquelle tous les noeuds sont regroupés dans un seul groupe, de sorte que κ = 1 et xii '= 1 ∀ (i , i ') et F (X) = ΣN i = 1 i ΣN' = 1 φii. - Si φii '= 0∀i, i', puis la solution optimale qui maximise F (X) est la partition dans laquelle tous les noeuds sont séparés, de sorte que κ = N et xii '= 0 ∀ i = 6 i' et XII = 1 ∀ i donc F (X) = ΣN i = 1 i ΣN '= 1 φ̄ii. En d'autres termes, l'optimisation d'un critère linéaire qui ne vérifie pas la propriété du solde soit regrouper tous les nœuds d'un cluster unique ou d'isoler chaque noeud dans son propre cluster, ce qui oblige donc l'utilisateur de fixer le nombre de grappes à l'avance. On peut en déduire les paragraphes précédents que les valeurs prises par les fonctions et & phiv & phiv créer une sorte d'équilibre entre le fait de générer autant de groupes que possible, κ = N, et le fait de générer un seul cluster, κ = 1. ce qui suit, nous appellerons la quantité ΣN i = 1 ΣN i '= 1 φ (IAI) xii ""le terme des accords positifs et la ΣN quantité i = 1 ΣN i' = 1 & phiv (IAI) xII «la durée des accords négatifs. 2.2 Différents niveaux d'équilibre Nous définissons deux niveaux d'équilibre pour tous critère équilibré linéaire: Définition 2 (propriété de l'équilibre local) Un critère linéaire équilibré dont les fonctions φii 'et φ̄ii' satisfaire φii '+ φ̄ii' = KL ∀ (i, i ' ) où KL est une constante qui ne dépend que de la paire (i, i ') (donc pas en fonction des propriétés globales du graphe) a la propriété de l'équilibre local. Quelques remarques sur la définition 2: - Puisque KL ne dépend que des propriétés de la paire (i, i '), qui est des propriétés locales, nous appelons cet établissement équilibre local. - Quand on parle de propriétés globales on se réfère au nombre total de noeuds, le nombre total d'arêtes ou d'autres propriétés décrivant la structure globale du graphique. - Dans le cas particulier de l'équilibre local où KL est constante ∀ (i, i '), qui est φii' 'somme à une constante, nous avons la situation suivante: alors que φii' et φ̄ii augmente φ̄ii '- 206 - P. Conde-Céspedes, JF Marcotorchino et E. Viennet diminue et vice versa. Considérons le cas particulier où φ (IAI) = aii ', le terme général de la matrice de contiguïté. Un modèle nul est un graphique ayant le même nombre total d'arêtes et de noeuds et où les bords sont distribuées au hasard. Appelons le terme général de la matrice de contiguïté de ce graphe aléatoire & phiv (IAI). Un critère basé sur un modèle nul considère qu'un graphe aléatoire ne possède pas la structure communautaire. Le but d'un tel critère est de maximiser l'écart entre la courbe réelle, représentée par φ (aii) et la version du modèle nul de ce graphe, représenté par & phiv (aii) comme représenté dans l'équation (5). Cela implique ΣN i = 1 ΣN i '= 1 φii' = ΣN i = 1 ΣN i '= 1 φ̄ii' = 2M. Cette contrainte implique que φ̄ii 'dépend du nombre total d'arêtes M. Par conséquent, la décision de regroupement toge- utres deux sous-graphes dépend d'une caractéristique de l'ensemble du réseau et le critère n'est pas invariant échelle, car elle dépend d'une propriété globale du graphique. La définition du modèle nul pour des critères linéaires peut être généralisée comme suit: Définition 3 (critère basé sur un modèle nul) Un critère linéaire équilibré dont les fonctions φii 'et φ̄ii' satisfaire aux conditions suivantes: NΣ i = 1 NΣ i '= 1 φii '= NΣ i = 1 NΣ i' = 1 φ̄ii 'φii' + φ̄ii '= g (KG) ∀ (i, i') où g (KG) est une fonction en fonction des propriétés globales du graphe KG est une rion cri- basé sur un modèle nul. KG peut être par exemple le nombre total d'arêtes ou de noeuds. On peut en déduire les définitions 2 et 3 qu'un critère linéaire ne peut pas être équilibré local et basé sur un modèle nul en même temps. Dans le cas particulier où diminue si la & phiv taille du réseau augmente, il devient négligeable pour les grands graphiques. Comme expliqué précédemment, si ce terme tend vers zéro, l'optimisation du critère aura tendance à mettre en place plus facilement les nœuds. Par exemple, un bord unique entre deux sous-graphes serait interprété par le critère que le signe d'une forte corrélation entre les deux groupes, et en optimisant le critère conduirait à la fusion des deux groupes. Un tel critère est dit d'avoir une limite de résolution. La limite de résolution a été introduite par Fortunato et Barthélemy (2006), où les auteurs ont étudié la limite de résolution de la modularité de Newman-Girvan. Ils ont démontré que l'optimisation des dularity mo- peut échouer pour identifier les modules plus petits qu'une échelle qui dépend des caractéristiques globales du graphe même des graphiques complets faiblement reliés entre eux, qui représentent les meilleures communautés identifiables, seraient fusionnés par ce genre de critères d'optimisation si le Net- le travail est suffisamment grande. Selon Kumpula et al. (2007) la limite de résolution est présent dans tout critère de modularisation basé sur l'optimisation globale des bords intra-cluster et extra- liens communautaires et sur une comparaison à un modèle nul. - 207 - Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel Dans la section 4, nous montrerons comment les critères ayant une limite de résolution ne parviennent pas à identifier certains groupes de noeuds connectés à forte densité. 3 critères modularisation en notation relationnelle Graphique des critères de classification dépendent fortement du sens donné à la notion de commu- nauté. Dans cette section, nous décrivons six critères de modularisation linéaires et leur codage dans le tableau 1. relationnelle Nous supposons que les graphiques que nous voulons modularisation sont sans échelle, ce qui signifie que leur répartition degré suit une loi de puissance. 1. Le critère Zahn-Condorcet (1785, 1964): C.T. Zahn (voir Zahn (1964)) a été le premier auteur qui a étudié le problème de trouver une relation d'équivalence X, qui représente le mieux une relation symétrique donnée A au sens de minimiser la distance de la différence symétrique. Toutefois, le critère défini par Zahn correspond au critère de la double Condorcet (voir Condorcet (1785)) introduite dans relationnelle Consensus et dont le codage relationnelle est donnée dans Marcotorchino et Michaud (1979). Ce cri- tère exige que chaque noeud dans chaque groupe est connecté à au moins la moitié des noeuds totaux à l'intérieur de la grappe. Par conséquent, pour chaque cluster de la fraction à l'intérieur des bords du cluster est au moins 50% (voir Conde-Céspedes (2013) pour la démonstration). 2. Le critère Owsinski-Zadrozny (1986) (voir Owsinski et Zadrozny (1986)), il est une généralisation de la fonction de Condorcet. Il a un paramètre α, qui permet, selon le contexte, de définir le pourcentage minimal d'arêtes requis intragrappe: α. Pour α = 0,5 ce critère est équivalent au critère de Condorcet. amendes dé- Le équilibre entre les accords positifs terme et les accords négatifs terme du paramètre. Pour chaque groupe de la densité des arêtes est au moins α% (voir Conde-Céspedes (2013)). 3. Le critère Newman-Girvan (2004) (voir Newman et Girvan (2004)): Il est le plus connu critère de modularisation, parfois appelé simplement modularité. Elle repose sur un modèle nul. Sa définition implique une comparaison du nombre de intragrappe arêtes dans le réseau réel et le nombre attendu de ces arêtes dans un graphe aléatoire où les bords sont distribués selon la structure de l'indépendance (un réseau sans tenir compte de la structure communautaire). En fait, la modularité mesure l'écart à l'indépendance. Comme mentionné dans la section précédente, ce critère, basé sur un modèle nul et il a une limite de résolution (voir Fortunato et Barthelemy (2006)). En fait, le réseau devient plus M - → ∞, le terme φ̄ii '= ai.a.i'2M tend vers zéro pour puisque la distribution de degré suit une loi de puissance. 4. L'écart à Uniformité (2013) Ce critère maximise l'écart à la structure d'uniformité, il a été proposé à Condé-Céspedes (2013). Il compare le nombre d'intra-grappe bords dans le réel graphique et le nombre attendu de ces arêtes dans un graphe aléatoire (le modèle nul) où les bords sont répartis uniformément, donc tous les noeuds ont le même degré égal au degré moyen de le graphique. Ce critère est - 208 - P. Conde-Céspedes, J. F. Marcotorchino et E. Viennet basé sur un modèle nul et il a une limite de résolution. En effet, δ - → 0 comme N - → ∞. 5. L'écart à l'indétermination (2013) à la fonction Newman analogue-Girvan, ce critère compare le nombre d'arêtes intra-cluster dans le réseau réel et le nombre attendu de ces arêtes dans un graphe aléatoire où les bords sont distribués sui- vantes l'indétermination structure 4 (un graphique sans tenir compte de la structure communautaire), introduite en Marcotorchino (2013) et Marcotorchino et Conde-Céspedes (2013). La déviation à l'indétermination est basé sur un modèle nul, donc il a une limite de résolu- tion. 6. La modularité équilibré (2013) Ce critère, introduit dans Conde-Céspedes et Marcotorchino (2013), a été construite en ajoutant à la modularité Newman-Girvan un terme prenant en compte l'absence de bords a. Alors que la Newman-rité de Girvan compare la valeur réelle de aii 'à son équivalent dans le cas d'un ai.ai de graphe aléatoire ""2M, le nouveau terme compare la valeur de Aii' à sa version en cas d'un graphe aléatoire (N -ai.) (N-ai ') N2-2M. Il est basé sur un modèle nul et il a une limite de résolution. Critère relationnelle notation Zahn-Condorcet (1785, 1964) FZC (X) = NΣ i = 1 NΣ i '= 1 (aii'xii' + Aii 'XII') Owsinski - Zadrozny (1986) FZOZ (X) = N Σ i = 1 NΣ i '= 1 ((1- α) aii'xii' + αāii 'XII') avec 0 <α <1 Newman-Girvan (2004) FNG (X) = 1 2M NΣ i = 1 NΣ i '= 1 (aii' - ai.ai '2M) xii' Déviation à unifor- mité (2013) FUNIF (X) = 1 2M NΣ i = 1 NΣ i '= 1 (aii' - 2M N2 ) xii 'Déviation à indétermination (2013) FDI (X) = 1 2M NΣ i = 1 NΣ i' = 1 (aii '-. AI S - ai' N + 2M N2) xii 'Le Modu équilibré larité (2013) FBM (X) = NΣ i = 1 NΣ i '= 1 ((aii' - Pii ') xii' + (Aii '- PII) XII') où Pii '= ai.ai' 2M et PII '= (Aii' - (N-aI) (N-aI ') N2-2M.) TABLEAU 1 - notation relationnelle des fonctions de modularité linéaires. 4. Il existe une dualité entre la structure de l'indépendance et la structure indétermination (voir Marcotorchino (1984), Marcotorchino (1985) et Ah-Pine et Marcotorchino (2007)). - 209 - Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel Les six critères linéaires du tableau 1 vérifient la propriété de l'équilibre, il est donc pas nécessaire de fixer à l'avance le nombre de clusters, plus précisément: l'équilibre général Critère modèle nul équilibre local Commentaire Zahn-Condorcet X φii '+ φ̄ii' = aii '+ Aii' = 1. Owsinski-Zadrozny X φii '+ φ̄ii' = (1- α) aii '+ αāii. Newman-Girvan X NΣ i = 1 NΣ i '= 1 φ̄ii' = NΣ i = 1 NΣ i '= 1 ai.a.i' = 2M 2M. Déviation à unifor- mité XNΣ i = 1 NΣ i '= 1 φ̄ii' = NΣ i = 1 NΣ i '= 1 2M 2M N2 = Déviation à Indeter- mination XNΣ i = 1 NΣ i' = 1 (ai + ai N 'N - 2M N2 = 2M modularité équilibré XNΣ i, i.)' = 1 NΣ i '= 1 PII' = NΣ i = 1 NΣ i '= 1 Aii' = N 2 - 2M TABLEAU 2 - Solde biens critères linéaires. D'après les tableaux 1 et 2, on peut facilement en déduire que pour les critères ayant une limite de résolution de la quantité φ̄ii 'diminue lorsque la taille du graphe devient plus grande. 4 essais avec des réseaux réels et artificiels Nous modularisés six véritables réseaux de tailles différentes:. Jazz (Gleiser et Danon (2003)), Inter- net (et Hoerdt Magoni (2003)), Web nd.edu (Albert et al (1999) ), Amazon (Yang et Leskovec (2012) 5) et Youtube (Mislove et al. (2007)). Nous avons couru une version générique de Louvain algorithme (voir Campigotto et al. (2014) et Blondel et al. (2008)) jusqu'à obtention d'une valeur stable de chaque critère. Le nombre de clusters obtenus pour chaque réseau est indiquée dans le tableau 3. Le tableau 3 montre que les critères Zahn-Condorcet et Owsiński- Zadrozny génèrent beaucoup plus de groupes que les autres critères ayant une limite de résolution, pour laquelle le nombre de grappes est assez comparable. De plus, cette différence augmente avec la taille du réseau. Notez que le nombre de grappes pour le critère Owsiński- Zadrozny diminue avec l'α, qui est la fraction requise minimale des bords intra-grappe, de sorte que le critère devient plus flexible. rez-de-vérité Seules les communautés qui se chevauchent sont définies sur ces previuos réseaux réels. Ce fait rend difficile de juger de la qualité des partitions obtenues. Ce si pourquoi nous Generators cinq ted graphiques de LFR de référence (voir Lancichinetti et al. (2008)) de différentes tailles 1000, 5000, 10000, 100000 et 500000. Les paramètres d'entrée sont les mêmes que ceux qui sont considérés dans Lancichinetti et Fortunato ( 2009). Le degré moyen est de 20, le degré maximum 50, l'exposant 5. les données ont été prises à partir de http://snap.stanford.edu/data/com-Amazon.html. - 210 - P. Conde-Céspedes, JF Marcotorchino et E. Viennet Réseau Jazz Internet Web nd.edu Amazon Youtube N ~ 198 70k 325k 334k 1M M ~ 3k 351k 1M 925k 3M · 1,44 ô 0.14 10 au 4 février , 77 · 10-05 1,65 10-05 4,64 · · 10-06 Critère κ κ κ κ κ ZC 40 123 201 38 647 161 439 878 849 α = OZ 30 897 34 0,4 ​​220 967 121 370 744 680 OZ α = 0,2 23 24 470 184 087 77 700 601 800 CIONU 20 173 711 265 51 584 NG 4 46 511 250 5 567 DI 6 39 324 246 13 985 BM 5 41 333 230 6 410 TABLEAU 3 - Ref: Zahn-Condorcet (ZC ), Déviation à Homogénéité (UNIF), Newman-Girvan (NG), écart à l'indétermination (AI) et Modularity équilibré (BM). de la distribution du degré est de -2 et que la distribution de la taille de la communauté est -1. Afin de tester l'existence de la limite de résolution que nous avons choisi de petites tailles de collectivités, allant de 10 à 50 noeuds, et un paramètre faible mélange, 0,10. Ainsi, les communautés sont clairement définies. La figure 1 montre le nombre moyen de grappes pour 100 courses de l'algorithme générique Louvain. ● ● ● ● ● 100 500 5000 50000 500000 50 10 0 50 0 50 00 50 00 0 Nombre de pôles Taille du réseau: NN um soit r c lu er er s ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● réel ZC OZ_0.40 OZ_0.30 OZ_0.20 NG DU DI BM FIGURE 1 - nombre moyen de grappes pour les graphes de LFR artificiels (échelle logarithmique). La figure 1 montre clairement la différence entre le comportement de ces critères ayant une limite de solution re- (NG, DU, DI et BM) et le comportement des critères définis localement (ZC et - 211 - Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel OZ ). Comme la taille du réseau augmente les quatre critères qui souffrent de la résolution limite dé- Tect moins que ceux des groupes prédéfinis. Le nombre de grappes est assez comparable pour ces quatre fonctions, l'une des raisons peut être le fait que le terme d'accords négatifs tend vers zéro lorsque le réseau devient plus grand. A l'inverse, les critères définis localement identifiés plus grappes que ceux prédéfinis, spécialement ZC. Le critère qui approche au mieux le nombre réel de pôles est OZ avec α = 0,2. La figure 2 montre la moyenne Normalisée information mutuelle pour les partitions Figure 1. ● ● ● ● ● 50 100 500 5000 50000 500000 0. 80 0. 85 0. 90 0. 95 1. 00 Taille Normalisée Réseau d'information mutuelle: NNMi ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ZC OZ_0.40 OZ_0.30 OZ_0. 20 NG dU DI BM FIGURE 2 - l'information mutuelle moyenne normalisée (INM) sur les graphiques à 1. la figure 2 montre que la NMI moyenne diminue avec la taille du réseau des critères ayant une limite de résolution. Le critère de la plus haute NMI est OZ avec α = 0,2 qui garantit une densité au sein du cluster de 20%. 5 Conclusions Nous avons présenté six critères de modularisation linéaire en notation relationnelle, Zahn-Condorcet, Owsiński- Zadrozny, la modularité Newman-Girvan, l'écart à l'index Uniformité, l'écart à l'index indétermination et le Modularité équilibré. Cette notation nous a permis d'identifier facilement les critères souffrant d'une limite de résolution. Nous avons constaté que les deux premiers critères ont une définition locale alors que les autres, sur la base d'un modèle nul, avait une limite de résolution. Ces résultats ont été confirmés par modularisation graphiques réels et artificiels en utilisant une version générique de l'algorithme Louvain. Nous avons comparé le nombre de grappes trouvés par les six critères et les informations Normalisée mutuelle pour les graphes artificiels. Les résultats ont montré que ces critères ba- - 212 - P. Conde-Céspedes, J. F. Marcotorchino et E. Viennet sed sur une définition locale avait une meilleure performance que celles basées sur un modèle nul lorsque la taille de l'augmentation du graphique. Références Ah-Pin, J. et F. Marcotorchino (2007). Statistique, indépendances géométriques et logiques entre les variables catégoriques. Proc. du Symposium ASMDA2007, La Canée, Grèce. Albert, R., H. Jeong, et A. Barabási (1999). Internet: Diamètre du web mondial. ture Na- 401 (6749), 130-131. Blondel, V., J.-L. Guillaume, R. Lambiotte, et E. Lefebvre (2008). déploiement rapide des commu- nautés dans les grands réseaux. Journal de la mécanique statistique: théorie et expérience P10008. Campigotto, R., P. Condé-Céspedes, et J. Guillaume (2014). Une généralisée et moi- adaptative ThOD pour la détection de la communauté. CoRR abs / 1406,2518. Conde-Céspedes, P. (2013). Modélisations et extensions du de l'Analyse formalisme Nelle RELATION à la modularisation Mathématique des Grands Graphes. Thèse de doctorat, Université Pierre et Marie Curie. Condé-Céspedes, P. et F. Marcotorchino (2013). Comparaison différents modularisation cri- ria à l'aide de métrique relationnelle. Dans F. Nielsen et F. Barbaresco (Eds.), Proc. Première Conférence Internatio- nale, la science géométrique de l'information, numéro 1, Paris, France, pp. 180-187. Springer-Verlag. Condorcet, C. A. M. d. (1785). Essai sur l'demande de l'analyse à la des décisions rendues Probabilité à la Pluralité des voix. Journal de mathématiques Sociologie 1 (1), 113-120. Fortunato, S. et M. Barthélemy (2006). limite de résolution dans la détection de la communauté. Dans tintements Procee- de l'Académie nationale des sciences des États-Unis d'Amérique. Gleiser, P. et L. Danon (2003). Structure communautaire dans le jazz. Les progrès dans les systèmes complexes (ACS) 06 (04), 565-573. Hoerdt, M. et D. Magoni (2003). Compte rendu de la 11ème Conférence internationale sur les articles Soft-, télécommunications et réseaux informatiques 257. Kumpula, J., J. Saramäki, K. Kaski, et J. Kertesz (2007). résolution limitée dans la détection de la communauté de travail Net- complexe avec approche modèle potts. Le European Physical Journal B 56 (1), 41-45. Lancichinetti, A. et S. Fortunato (2009). algorithmes de détection communautaire: une analyse comparative. Phys. Rev. E 80, 056117. Lancichinetti, A., S. Fortunato, et F. Radicchi (2008). graphiques de référence pour tester les algorithmes de détection de commu- nauté. Phys. Rev. E 78 (4). Marcotorchino, F. (1984). Utilisation des Comparaisons en pair statistique des paires gences contingences (partie i). Publication du Centre Scientifique IBM de Paris, F057, et les Cahiers du Séminaire Analyse des Données et Stochastiques Université Libre Processus de Bruxelles, 1-57. Marcotorchino, F. (1985). Utilisation des en paires nominale Comparaisons des contingences gences statistique (partie III). F-081 Etude du Centre IBM de Paris Scientifique, 1-39. - 213 - Comparaison des critères de modularisation linéaires en utilisant le formalisme relationnel Marcotorchino, F. (2013). transport optimal, les modèles d'interaction spatiale et les problèmes connexes, les impacts sur les paramètres relationnels, l'adaptation aux grands graphiques et la modularité des réseaux. Publication interne de Thales. Marcotorchino, F. et P. Conde-Céspedes (2013). Transport optimal et problème commercial minimal, impact sur les paramètres relationnels et des applications aux grands graphiques et la modularité des réseaux. Dans F. Nielsen et F. Barbaresco (Eds.), Proc. Première Conférence internationale, la science géométrique de l'information, numéro 1, Paris, France, pp. 169-179. Springer-Verlag. Marcotorchino, F. et P. Michaud (1979). Optimisation en Analyse ordinale des Données. Paris: Masson. Mislove, A., M. Marcon, K. Gummadi, P. Druschel, et B. Bhattacharjee (2007). La mesure et l'analyse des réseaux sociaux en ligne. Dans Actes du 5ème ACM / Usenix Internet Me Conférence asurement (de IMC'07), San Diego, CA. Newman, M. et M. Girvan (2004). La recherche et l'évaluation de la structure des communautés dans les réseaux. Physical Review E. 69 (2). Owsinski, J. et S. Zadrozny (1986). Clustering pour les données ordinales: une programmation linéaire formula- tion. Contrôle et Cybernetics 15 (2), 183-193. Yang, J. et J. Leskovec (2012). Définir et évaluer les communautés de réseau basées sur la vérité chaussée. Dans la Conférence internationale sur l'exploration de données, abs volume / 1205.6233, p. 745-754. IEEE Computer Society. Zahn, C. (1964). Approximation relations symétriques par des relations d'équivalence. SIAM Journal sur les mathématiques appliquées 12, 840-847. Résumé La modularisation des grands graphiques ou la détection de communautés dans les réseaux est généralement proached AP- comme un problème d'optimisation d'une fonction de qualité ou d'un critère, par exemple, la ularity de Newman-mo- Girvan. Il existe d'autres critères de regroupement, avec leurs propres propriétés conduisant à des solutions différentes. Dans cet article, nous présentons six critères de modularisation linéaires rela- tionnel des notations comme la modularité Newman-Girvan, Zahn-Condorcet, Owsiński- Zadrozny, l'écart à l'index Uniformité, l'écart à l'index indétermination et la Balanced- Modularité. Nous utilisons une version générique de l'algorithme Louvain pour aborder la partition optimale des critères avec les réseaux réels de différentes tailles. Nous avons constaté que ces différences présentent d'importantes partitions concernant le nombre de grappes. Le formalisme relationnel nous permet de justifier ces différences d'un point de vue théorique. De plus, cette notation permet d'identifier facilement les critères ayant une limite de résolution (un phénomène qui provoque le critère à ne pas identifier les modules plus petits qu'une échelle donnée). Cette constatation est confirmée dans les graphiques de LFR de référence artificiels. - 214 - C - Visualisation et complexes Comparaison des Données critères de modularisation linéaires en utilisant le formalisme relationnel, une approche pour identifier les limites de la résolution Patricia Conde-Céspedes, Jean-François Marcotorchino, Emmanuel Viennet"
255,Revue des Nouvelles Technologies de l'Information,EGC,2015,Feedback - Study and Improvement of the Random Forest of the Mahout library in the context of marketing data of Orange,"L'apprentissage automatique a fait son apparition dans l'écosystème Hadoop créant, de par la puissance promise, une opportunité sans précédent pour ce domaine. Dans cet écosystème, Apache Mahout est une réponse à la question du temps de calcul et/ou de la volumétrie: il consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous portés afin de s'exécuter sur Map/Reduce. Ce rapport se concentre sur le portage et l'utilisation de l'algorithme des Random Forest dans Mahout. Il montre à travers notre retour d'expérience les difficultés qui peuvent être rencontrées tant pratiques que théoriques et suggère une piste d'amélioration.","C. Thao, Nicolas Voisine, Vincent Lemaire, R. Trinquart",http://editions-rnti.fr/render_pdf.php?p1&p=1002104,http://editions-rnti.fr/render_pdf.php?p=1002104,en,"Commentaires - Etude et amélioration de la forêt aléatoire de la bibliothèque Mahout dans le contexte des données de marketing d'Orange C. Thao *, **, N. * Voisine, V. Lemaire *, R. Trinquart * * Orange Labs, 2 avenue Pierre Marzin, 22300 Lannion, France ** Predicsis, 5 rue de Broglie, 22300 Lannion, France Résumé. Dans le domaine des systèmes Big Data, Hadoop a émergé comme l'un des systèmes les plus populaires et un écosystème très diversifié a grandi autour d'elle, toutes sortes, satisfait des besoins fonctionnels et techniques. Un créneau qui aurait dû être une place de choix dans cet écosystème sont des données d'analyse: d'abord parce que l'obtention de valeur de grands ensembles de données nécessite des algorithmes d'apprentissage (ML) machine efficace, OND sec- parce que les grandes grappes avec des ressources CPU abondantes semblent comme playfields appropriées pour ML algorithmes qui sont souvent des tâches informatiques très exigeant beaucoup de ressources. Malheureusement, parmi la myriade de projets open source, il y a très peu d'outils d'analyse de données qui ont été portés au cadre Hadoop. Apache Mahout se distingue parmi les rares initiatives: ce projet est principalement connu pour son application de recommandation, mais il offre également un entrepôt d'algorithmes ML, annoncés pour lancer sur la carte / Réduire. Nous avons enquêtons les vingt algorithmes pro- posés dans les Mahout et dans le présent rapport, nous nous concentrons sur les plus prometteurs d'un: la mise en œuvre forêt aléatoire. En se fondant sur des tests approfondis, y compris les données de marketing spécifiques d'Orange, nous fournissons une rétroaction approfondie sur l'utilisation de cet outil, aussi bien du point de vue théorique et pratique, et nous proposons plusieurs améliorations. 1 Introduction La baisse du coût du stockage des données a conduit à l'accumulation de grands ensembles de données complexes, qui sont largement considérées comme de nouvelles opportunités pour les entreprises. Orange - une société de télécommunications multinationale - doit analyser les données de son réseau pour améliorer la rentabilité et de créer de nouveaux services. Pour donner une idée de l'échelle, afin d'accroître la satisfaction des clients sur les services, Orange doit analyser la qualité des services (QoS) et la qualité d'expérience (QoE) des indicateurs pour ses 150 millions de clients mobiles. Ces indicateurs de QoS et QoE résultent de la combinaison de différentes sources de données (sonde de réseau, SI). L'objectif principal consiste à la détection ou la prévision en temps réel de la qualité de service ou QoE. Cela permettrait d'Orange soit d'améliorer la qualité du réseau ou de fournir de nouveaux services basés sur QoE. Par conséquent, l'application des techniques de ing ces grandes quantités de données de données est cruciale. Cela soulève de nombreuses questions telles que l'évolutivité des algorithmes d'exploration de données, l'automatisation du processus d'exploration de données et le contrôle de surajustement. - 413 - Commentaires - Forêt aléatoire des données de marketing de Mahout Le problème d'évolutivité est généralement le premier que les gens ont à l'esprit avec de grandes données. La disponibilité des environnements informatiques efficaces tels que Hadoop (HAD) clusters avec le carte- réduire cadre est souvent considéré comme la solution à la question de l'évolutivité. Un projet open source, nommé Mahout (Mah), prétend en fait de fournir la mise en œuvre de plusieurs algorithme d'apprentissage qui obtiennent non seulement des données de dépôt Big Data, mais en réalité exécuté sur un cluster Hadoop, tirant ainsi profit de la parallélisation. Ce projet a attiré l'attention de plus en plus après certaines entreprises ont déclaré avoir utilisé avec de bons résultats. Pour être plus précis, Mahout fronces bibliothèques pour les deux encadrés et apprentissage non supervisé. Les histoires de succès sur Mahout concernent toutes les bibliothèques d'apprentissage non supervisées. Au contraire, il y a très peu dit au sujet des bibliothèques surveillées. Pourtant, ces environnements informatiques ont été conçus à l'origine pour les tâches de moteur de recherche, basé sur des milliards d'indexation des documents; ils sont efficaces pour certaines familles de tâches, mais ne peuvent pas être considérés comme des modèles universels pour le calcul parallèle. Parmi les tâches d'exploration de données, la phase de déploiement est intensif de données et est susceptible de tenir bien dans le cadre de carte-reduce. Au contraire, la modélisation phase est d'UC, et l'exploitation efficace des ressources d'un cluster Hadoop est un problème ouvert. Dans une étude précédente (Dream, 2013), nous avons observé que aléatoire Forrest (RF) est (un) la meilleure méthode de Mahout, tant en termes de qualité des modèles et en termes d'évolutivité puisque cet algorithme est nativement un processus parallèle . Le but de notre travail est d'étudier et d'améliorer si nécessaire l'algorithme RF de Mahout pour une utilisation sur le cluster Hadoop. Dans la première partie de ce rapport, nous présentons une étude préliminaire de la forêt aléatoire de la bibliothèque Mahout. Dans la deuxième partie, nous vous proposons plusieurs améliorations de la bibliothèque initiale. Dans une troisième partie, nous proposons un nouvel algorithme d'arbre de décision pour améliorer les performances et réduire surajustement. Nous fournissons des résultats sur les deux ensembles de données académiques et les données d'Orange avant d'arriver à une conclusion. 2 Etude préliminaire Dans la première partie de notre étude, nous avons étudié le comportement des forêts aléatoires (__gVirt_NP_NN_NNPS<__ RF) de la bibliothèque Mahout comme il est emballé dans sa dernière version disponible (0,9). Cette section commence par un rappel de l'algorithme RF. Ensuite, la deuxième sous-section fournit les conditions expérimentales que nous avons utilisées pour toutes les expériences de ces rapports. Dans le troisième paragraphe, nous présentons les résultats obtenus, ce qui conduit à une discussion sur les problèmes potentiels et les solutions. 2.1 Forêt aléatoire Les « forêts aléatoires » est un classificateur supervisé présenté par Leo Breiman (Breiman, 2001). Elle est liée à l'approche de l'arbre de décision, mais le modèle prédictif ne consiste plus en un seul arbre: au lieu, il rassemble une multitude d'arbres. forêts aléatoires sont une combinaison de facteurs prédictifs de l'arbre de telle sorte que chaque arbre dépend des valeurs d'un ensemble aléatoire de vecteur échantillonné de façon indépendante et avec la même distribution de tous les arbres dans la forêt. L'erreur de généralisation des forêts converge vers une limite le nombre d'arbres dans la forêt devient grande. L'erreur de généralisation d'une forêt de classificateurs d'arbres dépend de la force des arbres individuels dans la forêt et la corrélation entre les deux. En utilisant une sélection aléatoire de caractéristiques de diviser le rendement de chaque noeud taux d'erreur qui se comparent favorablement à Adaboost (Freund et Schapire, 1996), mais sont plus robustes par rapport au bruit. erreur de contrôle interne des estimations, la force, - 414 - Thao C., N. Voisine, Lemaire V., Trinquart R. et corrélation et ceux-ci sont utilisés pour montrer la réponse à l'augmentation du nombre de fonctionnalités utilisées dans la division. Les estimations internes sont également utilisés pour mesurer l'importance variable. Dans les forêts au hasard, il est nécessaire d'utiliser un ensemble de données de validation. En effet, au cours de la mise en sac, de nombreux cas ne sont pas utilisés pour la construction d'un arbre. Chaque classificateur apprend qu'une partie des données. les données non utilisées sont appelées Out-Of-Bag. Ils fournissent une bonne façon d'estimer les performances de généralisation du classificateur. 2.2 Conditions expérimentales 2.2.1 Contexte industriel Dans ce rapport, nous nous concentrons sur le comportement des Mahout lorsqu'ils traitent des données qui ont les caractéristiques de « Orange Data », à savoir: - contraintes de données: (i) Hétérogène, (ii) des valeurs manquantes, (iii) Deux ou Plusieurs classes, (iv) des distributions très asymétriques, avec - de nombreuses échelles: (i) des dizaines de millions de cas, (ii) des dizaines à des dizaines de milliers de ables Vari; - de nombreux types de données: (i ) numérique, (ii) catégorielles, (iii) texte, (iv) l'image. Nous limitons l'étude à la tâche d'apprentissage supervisé, id problème de classification is tels que la détection de désabonnement, la prévision de appetency ... Et parmi les ensembles de données potentiels pour l'évaluation, nous avons un vif intérêt pour les plus proches des problèmes de commercialisation 2.2.2 les paramètres qui influencent le comportement de Mahout Lors de l'installation de la bibliothèque Mahout sur une plate-forme Hadoop il y a plusieurs paramètres qui vont influencer les performances obtenues à l'aide de RF du Mahout:. le nombre de machines (hochement es) du cluster et de leurs caractéristiques, la configuration des systèmes de fichiers HDFS de distribués, en particulier en ce qui concerne les blocs de manière sont créés (taille et réplication fait r), le critère de répartition utilisé dans l'algorithme RF, le nombre d'arbres dans les forêts, la façon de combiner les arbres. Les paragraphes qui suivent décrivent chacun de ces points. Nous indiquons également à la fin de cette section de la ligne de base nous permet de vérifier la validité des résultats obtenus avec le RF. Cluster: les tests Tous sont effectués sur une petite exploration Hadoop 1 cluster (HAD) de Orange Labs qui a les propriétés suivantes: (i) 6 noeuds avec: 2 Intel (R) Xeon (R) CPU E5-2407 0 @ 2,20 GHz, total des cœurs de processeur par noeud: 8, 32 Go de mémoire vive, la version Hadoop: Cloudera CDH4 (Hadoop 0,20), Mahout 0,7 (tel qu'il est emballé à l'intérieur de Cloudera CDH4.2). Données nœuds et taille de bloc: Les principaux composants d'une plate-forme Hadoop sont HDFS et MapReduce. HDFS est un système distribué conçu pour stocker des fichiers de très gros volumes de données sur un grand nombre de machines, alors que MapReduce est un cadre pour la distribution de Process- tion sur des fichiers volumineux. Lorsque ces deux éléments sont combinés sur le même ensemble de machines (nœuds de la grappe), la plate-forme agit comme un système unique résultant, fournissant une haute disponibilité, de la charge 1. Hadoop (haute disponibilité plateforme orientée objet distribué) est un système distribué qui adresses les questions de « grandes données ». Hadoop utilise un système de stockage distribué, qui est appelé HDFS (Hadoop Distributed File System) et incorpore des systèmes d'analyse comme MapReduce, Mahout ou Spark. Hadoop permet aux données de division et de l'exécution de l'analyse sur le traitement en parallèle. - 415 - Commentaires - Forêt aléatoire des cornac données de marketing d'équilibrage, et le traitement parallèle. Avec HDFS, tout grand fichier de données est décomposé en blocs et ceux-ci sont répartis entre les nœuds du cluster. Sur la base des blocs HDFS, la carte et réduire les fonctions peuvent être réparties sur le cluster et effectuer sur des sous-ensembles de grands ensembles de données, ce qui permet une meilleure évolutivité. Contrairement à un système de stockage classique, où les blocs sont une question de kilo-octets, la taille de bloc est réglé par défaut à 64 MB. Cette valeur par défaut peut être réglée par l'administrateur du cluster et a même changé à la volée par un utilisateur pour une mouche spécifique et / ou ING Process-. tailles de blocs typiques sont 128Mo, 256 Mo, 512 Mo ou 1 Go. Notre groupe a été configuré avec une taille par défaut de 128 Mo. Note: Le comportement des forêts aléatoires dans Mahout est quelque peu différente de la version originale de Breiman et donc sa théorie. Tout d'abord, les échantillons bootstrap ne sont pas effectués sur l'ensemble des données. Étant donné que les données sont réparties dans les noeuds de données (et donc les cartographes), chaque cartographe réalise son propre bootstrap. Les cartographes réalisent donc une formation de plusieurs arbres dans la forêt. L'algorithme des forêts aléatoires de Breiman est donc en partie « remplie ». Chaque mappeur effectue une partie de la forêt, mais sur des données qui ne sont pas un « sac » dans le sens d'une mise en sac. Ensuite, la forêt de chaque cartographe sont combinés à réaliser la forêt « globale ». critère de Split: Le RF Mahout utilise par défaut des arbres non binaires. Le gain d'information (Quinlan, 1986) est le critère de partage par défaut utilisé. Lorsqu'une variable est choisie pour effectuer la séparation: (i) pour les attributs numériques deux feuilles sont élaborés après un noeud, (ii) pour les feuilles catégorique Vari ables Q sont élaborés après un noeud où Q est le nombre de modalités de la variable choisie . Note: Il est important de noter que Mahout ne gère pas les valeurs manquantes. Ainsi, dans une première partie de notre expérience, nous avons remplacé toutes les valeurs manquantes par -9999. Pour une variable, en remplaçant-9999 valeurs manquantes implique que l'on considère la valeur manquante comme information. Pour une variable numérique, l'objectif est de mettre une valeur inférieure à toute autre valeur. Nombre d'arbres: Le nombre d'influences arbres, la valeur de l'ASC: augmenter le nombre d'arbres augmente les chances d'avoir des arbres de discrimination. Cette amélioration est particulièrement observable dans le début d'une courbe lors du tracé de l'AUC par rapport au nombre d'arbres dans la forêt. Pour cette base de données, KDD petite upselling, la bonne valeur asymptotique est proche de 4000 mais le gain en Performa nces après 1000 est faible. Le nombre d'arbres a aussi une grande influence sur le temps qui sera nécessaire pour déployer le modèle. Dans les expériences présentées ci-dessous dans ce document, nous fixons le nombre d'arbres à 1000 (donc 1000 / M pour chaque mappeur où M est le nombre de cartographes). En combinant les arbres: Après un grand nombre d'arbres est généré, ils votent pour la classe la plus populaire. Nos résultats de base Orange a développé un logiciel puissant nommé Khiops www.khiops. com) qui est capable de travaux sur ensemble de données très grand (et Guyon al., 2010) ou jeu de données multi-tables (Boullé, 2014)). Nous avons utilisé ce logiciel comme base de référence dans sa « version standard». Appliquée sur une base de données sin- gle (et non sur une base de données distribuée) afin d'évaluer les résultats obtenus avec le RF de Mahout Le classificateur est un calcul de la moyenne de sélective Naive Bayes décrit dans (Boullé, 2007) - 416 - Thao C., N. Voisine, Lemaire V., Trinquart R. 2.3 datasets Nous avons utilisé 3 types de données (8 jeux de données) qui sont: - Adulte: les données appartenant au référentiel d'apprentissage automatique UCI ( Bache et Lichman, 2013), - OCR du défi d'apprentissage à grande échelle Pascal (Sonnenburg et al, 2008); -. KDD (grandes et petites) la commercialisation des données fournies par orange pour le défi KDD 2009 (orange, 2009, Guyon et al ., 2010) Leurs caractéristiques sont données dans le tableau 1. les critères utilisés pour évaluer les résultats obtenus est l'aire sous la Comme son nom l'indique la courbe ROC (AUC (Fawcett, 2006))., il est aléatoire dans l'algorithme de Random les forêts. Nous avons donc effectué une formation de dix. Dans le reste de cet article Tra l'ASC et de test ASC présentés dans les tableaux seront l'AUC moyenne au cours des dix expériences. Dataset Ni Nn Nc CP PCTrain PCTest #Mappers Adulte 48 842 6 8 2 76,17% 70% 30% 8 OCR 3 500 000 1156-2 50% 60% 40% 71 KDD Grand Upselling 50 000 14740 260 2 7.4% 50% 50% 27 KDD Grand Churn 50 000 14740 260 2 7,3% 50% 50% 27 KDD Grande appétence 50 000 14740 260 2 1,8% 50% 50% 27 KDD Petit Upselling 50 000 190 40 2 7,4% 50% 50% 20 KDD Petit Churn 50 000 190 40 2 7,3% 50% 50% 20 KDD Petit 50 000 190 appétence 40 2 1,8% 50% 50% 20 TAB. 1 - Ensemble de données utilisé: Ni = nombre de cas, Nn = nombre de variables numériques, Nc = nombre de Categorical variabkes, C = nombre de classes P: Pourcentage de la classe cible, PCTrain: Pourcentage utilisé pour la formation, PCTest: Pourcentage utilisés pour le test. 2.4 Résultats avec la bibliothèque « standard » Nous présentons les résultats que nous avons obtenus avec la forêt aléatoire de Mahout lors de leur utilisation avec l'installation par défaut de la bibliothèque: (i) le critère de répartition est le gain d'information, (ii) la valeur manquante sont traités selon la description donnée dans la section précédente, (iii) le nombre d'arbres est de 1000, (iv) le nombre de mappeurs est indiqué dans la dernière colonne du tableau 1, (v) la classe prédite est la plus populaire prédit classe dans la forêt. Le tableau 2 montre (colonnes 1 et 2) Les performances obtenues lors du test AUC et le rapport de train AUC / Test AUC (colonne 3) pour donner un élément de robustesse (une valeur supérieure à 1 indique un surajustement) .Les points clés de ceux-ci les résultats sont (i) RF présentent des résultats médiocres sur le petit ensemble de données, mais ont de bonnes performances sur le plus grand ensemble de données OCR (le résultat semble être connu par la communauté d'apprentissage de la machine (voir (Debreuve) diapositive 17), (ii) les résultats ne sont pas mauvais, mais inférieur résultats de la littérature sur ces ensembles de données, (iii) les résultats sont plus faibles (sauf OCR) que ceux obtenus par Khiops, (iv) un « grand » overfitting existe sur un grand nombre de l'ensemble de données. Nous analysons en profondeur ces résultats pour comprendre où viennent les problèmes l'un d'eux vient du critère de répartition -. le gain d'information -. et l'autre est la façon de traiter les valeurs manquantes pour le premier est biaisé le gain d'information quand il y a des variables catégoriques qui ont beaucoup de modalités (Harris, 2002). Dans thi s cas, les arbres individuels sont moins profondes et plus larges. Pour la seconde la voie initiale pour remplacer la valeur manquante est pas - 417 - Fe edback - Forêt aléatoire des données appropriées de marketing de Mahout et doit être changé pour obtenu de meilleurs résultats. Cependant, le framework Hadoop permet de diminuer beaucoup le temps de formation comme l'a montré dans le tableau 3 Dataset Khiops Mahout Robustesse Mahout Adult 0,925 0,910 1,02 KDD Petit Upselling 0,872 0,648 1,45 KDD Petit Churn 0,731 0,612 1,49 KDD Petit appétence 0,814 0,682 1,44 OCR 0,830 0,953 1,00 KDD Grand Upselling 0,897 0,877 1,07 KDD Grande Churn 0,743 0,681 1,26 KDD Grande appétence 0,852 0,735 1,30 TAB. 2 - Premier résultat avec la « RF standard » dans Mahout OCR KDDSmall Upselling KDDLarge Upselling Khiops 4h55m 28s 45min 39s de les Mahout de TAB. 3 - Le temps de formation pour Khiops et Mahout 2.5 Premières idées pour contourner les problèmes observés Face aux résultats obtenus dans la section précédente, nous essayons d'appliquer plusieurs patchs à la RF de Mahout afin d'améliorer la performance de la bibliothèque: - Mahout « 2 »: où l'on change le gain information du ratio de gain; - Mahout « 3 »: où sont remplacés par la valeur médiane corres- pondant (Breiman) les valeurs manquantes des variables numériques. Dataset Khiops Mahout Par défaut Mahout 2 Mahout 3 adultes 0,925 0,910 0,919 0,917 KDD Petit Upselling 0,872 0,648 0,690 0,778 KDD Petit Churn 0,731 0,612 0,596 0,621 KDD Petit appétence 0,814 0,682 0,667 0,694 OCR 0,830 0,953 0,946 0,953 KDD Grand Upselling 0,897 0,877 0,705 0,873 KDD Grand Churn 0,743 0,681 0,503 0,681 KDD Grande appétence 0,852 0,735 0,597 0,763 TAB. 4 - Résultat avec le « RF modifié » dans Mahout. Mahout 2: Mahout avec un rapport de gain, cornac 3: cornac des informations de gain et les valeurs manquantes remplacées par la valeur médiane. Les résultats obtenus pour l'AUC Test avec ces deux version modifiée sont présentés dans le tableau 4. Seule la version 3 Mahout a de meilleurs résultats que la version standard. Mais cette amélioration pourrait être coûteuse puisque la valeur médiane de toutes les variables numériques est nécessaire et parce que la valeur médiane doit être calculée sur l'ensemble de données complet (qui est divisé en différents noeuds). Nous avons également tester la modification des autres (non présentés ici) pour la base de données déséquilibrée que l'utilisation de la racine carrée dans le calcul du gain d'information ((Flach, 2012), la section « Sensibilité aux distributions de classe faussés » page 143)) ... mais sans amélioration de - 418 - Thao C., N. Voisine, Lemaire V., R. Trinquart les résultats. Face à ces résultats et qui souhaitent encore plus robustes performances, nous nous tournons vers l'approche MODL qui est connu pour être robuste (Boullé, 2006, 2005). 3 régularisation Ajout dans la forêt aléatoire Dans cette section, nous montrons deux modifications de l'arbre de décision utilisés par Mahout Forêt aléatoire pour augmenter les performances de classification et de réduire surajustement. La première repose sur l'approche MODL développée pour arbre de décision avec succès (2010 et al Voisine.). Elle consiste à changer la méthode de fractionnement de la variable numérique et introduit une méthode de regroupement pour les variables catégoriques. La deuxième amélioration consiste à changer le mode de scrutin de Ma- Hout RF. Par défaut Mahout utilise la majorité méthode de vote pour estimer les probabilités de classe. Nous vous proposons d'estimer les probabilités de classe en faisant la moyenne des estimations de probabilité dans chaque feuille de tous les arbres de décision. 3.1 Modifications Ajout de régularisation: Nous décrivons une nouvelle division et les méthodes de regroupement pour deux classes tâche de classification d'apprentissage. La méthode utilisée fractionnement de critères MODL discrétisation basés sur des approches LDM. La méthode de discrétisation MODL pour la classification supervisée offre la plus discrétisation probable compte tenu des données. expériences comparatives rapport détaillé haute performance (Boullé, 2006). Le cas de la valeur de regroupement des variables est traitée dans (Boullé, 2005) en utilisant une approche similaire. Une approche bayésienne est appliquée pour choisir le meilleur modèle de discrétisation, qui se trouve en maximisant la probabilité p (Modèle | données) du modèle compte tenu des données. Utilisation de la règle de Bayes et puisque la probabilité p (Data) est constante sous différents du modèle, ce qui est équivalent t o maximiser p (Model) p (données | Model). Nous avons décidé dans cette étude pour élaborer RF avec un arbre de décision binaire pour le problème de classification des deux classes qui sont plus présents dans le problème Orange. Par conséquent, dans chaque nœud le critère de répartition sera consacrée à trouver le meilleur divisé en deux intervalles pour les variables numériques ou le meilleur groupe en deux groupes pour les variables qualitatives. Ce paramètre permet d'avoir algorithme rapide (pas détaillé dans ce rapport en raison de lieu compte). Soit N le nombre de cas d'une feuille. Ni représente le nombre d'instances dans l'intervalle i et Nij le nombre d'occurrences d'une valeur de sortie j à l'intervalle i. Dans notre contexte, le nombre de cas N et le nombre de classes J sont censées être connues. Le nombre d'intervalle est fixé par deux. Pour les variables numériques (Boullé, 2006), le critère d'évaluation lorsque le nombre d'tervals entrée et le nombre de classes sont tous deux égaux à 2 et le nombre de classes est: log 2 + log (N + 1) + log (N1 + 1 ) + log (N2 + 1) + Σ2 i = 1 log Ni! Ni1! Ni2! . Une fois que le critère d'évaluation est établie, le problème est de concevoir un algorithme de recherche afin de trouver un modèle de discrétisation qui minimise le critère. En Boullé (2006), une heuristique ascendante avide standard est utilisé pour trouver un bon discrétisation. Pour les variables catégoriques Boullé (2005), le critère d'évaluation lorsque les groupes NumberOf est égal à deux groupes est la suivante: V log 2 + log (1- 1 2V -1) + log (N + 1) + log (N1 + 1) + log (N2 + 1) + Σ2 i = 1 log Ni! Ni1! Ni2! , Où V est la valeur numérique d'une variable catagorical. - 419 - Commentaires - Forêt aléatoire des données de Mahout marketing vote: En général, la classe prédite est basée sur les votes des arbres pour la classe la plus populaire. Nous changeons cette règle pour effectuer des estimations de probabilité de la moyenne dans chaque feuille de tous les arbres de déci- sion, tels que: P (CJ | X) = argmaxJ 1T ΣT t = 1 P (C t J), où P (C t J) est le congé de fin de l'arbre t. Dans ce cas, la classe prédite sera celui avec la plus haute probabilité et la probabilité de confiance sera la probabilité de la moyenne correspondante. La confiance estimée permettra une meilleure estimation pour le calcul de la CUA. 3.2 Résultats Dataset Khiops Mahout Mahout 2: Mahout 3: Mahout Mahout Gain par défaut Ratio IG + Média MODL MODL + nouveau vote adulte 0,925 0,910 0,919 0,917 0,911 0,916 KDD Petit Upselling 0,872 0,648 0,690 0,778 0,800 0,820 KDD Petit Churn 0,731 0,612 0,596 0,621 0,619 0,675 KDD petit appétence 0,814 0,682 0,667 0,694 0,555 0,754 OCR 0,953 0,946 0,953 0,830 0,947 0,948 KDD Grand Upselling 0,877 0,705 0,873 0,897 0,866 0,875 KDD Grand Churn 0,681 0,503 0,681 0,743 0,628 0,667 KDD Grande appétence 0,735 0,597 0,763 0,852 0,686 0,725 TAB. 5 - Résultat de l'AUC de test avec le « RF modifié » dans Mahout. Mahout 2: Informations Gain remplacé par le ratio de gain, cornac 3: cornac des informations de gain et les valeurs manquantes remplacées par la valeur médiane, cornac MODL avec prédiction basée sur la classe la plus populaire et cornac MODL avec prédiction basée sur des estimations de probabilité de la moyenne. Dataset Nombre de nœuds Robustesse Mahout Par défaut Mahout MODL Mahout MODL Mahout MODL adultes 1763 60 1,02 1,00 KDD Petit Upselling 9895 5 1,45 1,06 KDD Petit Churn 9954 3 1,49 1,12 KDD Petit appétence 4013 2 1,44 1,15 OCR 3396 969 1,00 1,00 KDD Grande Upselling 1658 13 1,07 1,02 KDD grande baratte 1864 8 1,26 1,10 KDD grande appétence 688 5 1,30 0,94 TAB. 6 - Taille Robutness et modèle des différents RF (* nombre moyen par arbre) Nous comparons la nouvelle RF qui a utilisé l'approche MODL comme critère de répartition des résultats obtenus dans la section précédente. Les résultats du « Mahout MODL Random Forest » sont intéressants (i) les résultats sont meilleurs, voir le tableau 5 (ii) le Robustesse est mieux, voir le tableau 6 et les modèles sont « plus petits » voir le tableau 6. Ces résultats sont intéressants même . si elles sont pires que ceux obtenus par Khiops - 420 - Thao C., N. Voisine, Lemaire V., travaille Trinquart R. 4 Future: regarder de plus près l'échantillonnage bootstrap et les données dist ribution dans HDFS Dans notre étude, nous nous sommes concentrés jusqu'à présent sur la compréhension du réglage RF de Mahout a été l'impact du processus de construction des arbres et surtout nous avons fait l'accent sur le processus de fractionnement. Une chose que nous avons fait mention, mais ne pas creuser dans est la façon dont les ensembles de données sont répartis entre les nœuds et utilisés comme base pour l'échantillonnage bootstrap. C'est ce qui est examinée dans cette section. 4.1 Taille de bloc et les données d'un fichier aléatoire Lorsque l'ensemble de données est tombé dans HDFS, il est divisé en blocs de taille fixe et les blocs sont répartis sur les nœuds du cluster. Sur le cluster Hadoop que nous avons utilisé pour des expériences, la taille de bloc par défaut est de 128 Mo et nous n'a pas changé cela. Le processus d'apprentissage pour Mahout RF consiste à un emploi Plan-Reduce: cartographes construire des arbres qui sont collectés par un seul réducteur. Chaque Mapper procède un seul bloc HDFS: de sorte que les arbres construits par un mappeur sont toutes basées sur un sous-échantillonnage limité par le bloc cartographe travaille. Par conséquent dès le début, le principe de l'échantillonnage d'amorçage est différente de la version originale de Breiman: ce n'est pas un échantillonnage aléatoire avec le remplacement sur l'ensemble des données. Cette différence a deux conséquences majeures: - D'abord la distribution initiale de la cible dans le fichier de jeu de données peut avoir un impact profond sur les arbres générés par différents cartographes. Dans le pire paramètre possible, si les lignes de l'ensemble de données sont triées par valeur cible, on pourrait mappeur procède lignes qui sont toutes associées à la modalité de la même cible. - En second lieu, en fonction du nombre de colonnes, un bloc peut contenir des lignes relativement peu et les arbres peuvent être construits sur un petit échantillon. Cela dépend vraiment de la taille du bloc qui a été efficace lorsque l'ensemble de données a été mise en HDFS. Il peut être un problème pour les datamarts de marketing très grandes, qui facilement englobent plus de 5000 colonnes. Le problème de la taille des blocs peut être abordée en étudiant l'impact de la taille des blocs sur la performance de prédiction pour différents jeux de données. Espérons que nous pouvons trouver où placer le curseur afin que nous puissions configurer notre système HDFS correctement pour un datamart donné. Mais ce n'est pas une approche très agile dans un environnement où nous devons considérer différents datamarts et où il est plus facile d'ajouter des variables à un datamart. Et cela ne résoudrait pas le problème de la répartition cible. Une approche plus robuste pour aborder simultanément les deux problèmes mentionnés ci-dessus serait de concevoir un travail qui prend soin de diviser l'ensemble de données initial tout en prenant soin des contraintes suivantes: - le travail doit produire des sous-ensembles N, où N est le degré de parallélisme nous souhaitons, - le travail doit prendre soin de stocker chaque sous-ensemble de sortie avec une taille de bloc spécifique de sorte que chaque sous-ensemble se entièrement traitée exactement un mappeur, - le travail doit remplir chaque sous-ensemble par un processus d'échantillonnage aléatoire avec rem- placement sur l'ensemble jeu de données initial - le travail doit offrir une option pour contrôler la façon dont les modalités de cibles sont réparties dans les sous-ensembles (d'échantillonnage stratifié) - 421 - commentaires - Forêt aléatoire des cornac données marketing algorithme 1 Préparer l'entrée dataset algorithme: l'apprentissage échantillon Z stocké dans HDFS N: degré de parallélisme colTarget: indice de la colonne cible dans l'ensemble de données de sortie: Echantillon préparé pour l'apprentissage Mahout RF stockés dans HDFS Ceci est une séquence de deux-Carte réduire les emplois: TargetDistribution → DatasetSplit Le travail TargetDistribution prend pour l'ensemble de données d'entrée Z et la distribution des sorties des modalités cibles. Ce premier emploi n'a qu'un seul réducteur. Le travail DatsetSplit prend pour l'ensemble de données d'entrée Z et délivre en sortie l'apprentissage de l'échantillon Splitted. Les cartographes de DatasetSplit ont également accès à la distribution des modalités cibles. Ce second emploi a N Réducteurs. Mapper TargetDistribution 1: la valeur d'extrait de targetModality à la colonne colTarget de la ligne de courant 2: écrire à réducteur <targetModality, 1> Réducteur TargetDistribution 1: entrée est de la forme <targetModality, Liste des compteurs> 2: écriture <targetModality, somme des compteurs> Mapper d atasetSplit 1: Avant itération sur les lignes du bloc d'entrée, charger la distribution cible 2: Pour rangée à partir du bloc d'entrée 3: Pour i = 1 à n 4: mod_target la modalité de la cible d'extrait ← de ligne courante 5: dessiner si vous garder la ligne dans cette division basée sur la distribution des mod_target 6: si oui, écrire <i, filterRows (currentRow> to réducteur réducteur DatasetSplit 1:. d'entrée est de la forme <reducerindex, Liste des rangées> 2: pour aRow ∈ lignes 3: écriture aRow à la sortie Split fichier. un tel travail pourrait facilement être conçue comme une séquence de deux carte-reduce emplois. le premier estimerait les modalités cibles de distribution et le second serait en fait effectuer l'échantillonnage aléatoire stratifié. on notera que la subtilité principale dans ce une réside approche dans le bon déroulement de la taille des fentes par rapport à la taille moyenne d'une ligne et le nombre d'individus dans les ensembles de données d'origine. Si les lignes sont très grandes, il aura un impact sur le nombre de lignes que nous pouvons tenir dans un seul Split, et par conséquent e e taille de l'échantillon pour la construction d'un arbre. Ainsi, les grandes lignes appellent à la taille du plus grand bloc. Une autre stratégie pourrait être de décider que toutes les colonnes doivent être effectuées dans chaque fraction des ensembles de données: la première étape de l'échantillonnage consiste à choisir une sous-famille de toutes les colonnes pour une scission définitive de l'ensemble de données. Cela permettrait aux petites lignes et le nombre ainsi plus de lignes dans chaque fente. Le croquis de la carte et de réduire les fonctions d'un tel travail est présenté dans l'algorithme 2. 4.2 arbres Shifting entre les nœuds L'approche qui a été esquissée dans le paragraphe précédent tournait autour d'une idée principale: préparer les données d'une manière plus appropriée afin que nous puissions prendre le meilleur de Mahout RF sans changer la bibliothèque. Mais il a un coût en termes d'utilisation: on doit dupliquer le - 422 - Thao C., N. Voisine, Lemaire V., les données Trinquart R. de l'ensemble de données inital aux divisions préparées. Ce processus peut prendre le temps additionnel et le coût espace de stockage supplémentaire. Une autre approche pourrait être utilisée pour la même question: faire l'arbre que d'un est construit à partir de la plus large échantillon de l'ensemble de données initial. Notre idée est que chaque cartographe est lié au travail sur un seul bloc HDFS. Ainsi, au lieu d'avoir chaque cartographe responsable de la croissance des arbres complets, nous vous proposons de transformer le processus Mahout RF en une itération de carte-réduction des emplois, avec des arbres que de plus en plus d'une fraction à chaque course. Pensez que vous voulez construire une forêt d'arbres un T (disons un millier) et l'ensemble de données a été divisé en blocs B (par exemple 10). Le processus serait lancé en générant des fichiers B, contenant chacun de ces T / B (qui est une centaine ici) Arbres vides. Ensuite, nous affecter au hasard chacun de ces fichiers à chaque cartographe et exécuter une étape du processus d'apprentissage. Cette étape se développerait les arbres par deux degrés de profondeur et de stocker les arbres mis à jour dans les fichiers B à nouveau. L'étape suivante consiste à associer au hasard les fichiers mis à jour à nouveau cartographes et itérer jusqu'à ce que tous les arbres sont complètement développés. Une telle approche nécessiterait substantielle de codage sur la bibliothèque Mahout existante, plus particulièrement au niveau de stocker et ensemble Parse d'arbres incomplètes. De plus, il y a certainement quelques paramètres à régler (combien devrions-nous développer des arbres à chaque étape?). Mais l'avantage est qu'il est alors possible de construire des arbres à partir d'un ensemble plus large échantillon sans déplacer l'ensemble de données lui-même. 5 Conclusion Cette étude présente le comportement de la forêt aléatoire mis en œuvre dans le travail du cornac sur plusieurs ensembles de données d'intérêt pour Orange. La première déclaration est l'observation, pour la version standard, des performances bien au-dessous celle que nous pouvons observer dans l'état de l'art. La mise en œuvre pratique dans le cadre de Hadoop et Mahout ne respecte pas totalement le cadre théorique. Cela se traduit par des performances ci-dessous des attentes. Nous vous proposons quelques évolutions, le principal étant l'introduction de régularisation explicite lors de la formation des arbres. Nos tests sur divers ensembles de données montrent une amélioration par rapport à la bibliothèque standard. Il y a néanmoins une importation le travail de fourmi de réaliser, selon nous, de sorte que la forêt aléatoire mis en œuvre dans le cadre de Mahout atteint les performances d'un algorithme de traitement par lots. Il y a un compromis entre la précision connue et la vitesse / volumétrie, mais le prix à payer semble assez important pour le moment. Références Le projet ApachTM Hadoop® développe des logiciels open source pour fiable, évolutive, Sacrifiés informatique, http://hadoop.apache.org/ dis-. L'objectif du projet est de ApachTM Mahout® construire une bibliothèque d'apprentissage machine évolutive, http: //mahout.apache.org/. Bache, K. et M. Lichman (2013). référentiel apprentissage automatique UCI http: //archive.ics.uci. edu / ml. Boullé, M. (2005). Une approche optimale Bayes pour partitionner les valeurs des hommages at- catégorique. Journal of Research Machine Learning 6, 1431-1452. - 423 - Commentaires - Forêt aléatoire de données marketing de Mahout Boullé, M. (2006). MODL: une méthode de discrétisation optimale de Bayes pour les attributs continus. Machine Learning 65 (1), 131-165. Boullé, M. (2014). Vers la construction de fonction automatique pour classication supervisé. Dans ECML / PKDD 2014. accepté pour publication. Boullé, M. (2007). la moyenne des classificateurs bayésiens naïfs sélectifs à base de compression. Journal of Research Machine Learning 8, 1659-1685. Breiman, le remplacement L. Valeur manquante pour l'ensemble de la formation https://www.stat.berkeley.edu/ ~ Breiman / RandomForests / cc_home.htm # missing1. Breiman, L. (2001). forêts aléatoires. Mach. Apprendre. 45 (1), 5-32. Debreuve, E. Une introduction aux forêts aléatoires fichiers http://perso.math.univ-toulouse.fr/motimo/ / 2013/07 / forest.pdf-de hasard. Dream, A. (2013). EVALUER l'apport des Hadoop Pour la plateformes classification. Mémoire de maîtrise, Polytech Lille. Fawcett, T. (2006). Une introduction à l'analyse roc. Motif reconna. Lett. 27 (8), 861-874. Flach, P. (2012). Machine Learning: L'art et la science des algorithmes qui ont du sens des données. La presse de l'Universite de Cambridge. Freund, Y. et R. E. Schapire (1996). Des expériences avec un nouvel algorithme de rappel. Dans la Conférence nationale sur Inter Mchine Learning (ICML), pp. 148-156. Morgan Kaufmann. Guyon, I., V. Lemaire, M. Boullé, G. Dror et D. Vogel (2010). Conception et analyse de la coupe de KDD 2009: notation rapide sur une grande base de données clients orange. SIGKDD Explor. Newsl. 11 (2), 68-76. Harris, E. (2002). gain d'information en fonction du rapport de gain: Une étude des biais de la méthode de répartition. En AMAI. Orange (2009). Coupe kdd 2009: prévision de la relation client - données kddlarge et kddsmall http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction. Quinlan, J. R. (1986). Induction d'arbres de décision. Mach. Apprendre. 1 (1), 81-106. Sonnenburg, S., V. Franc, E. Yom-Tov, et M. Sebag (2008). défi d'apprentissage Pascal à grande échelle -.. ocr http://largescale.ml.tu-berlin.de/about/ » Voisine, N., M. Boullé, et C. Hue (2010) Un Bayes critère d'évaluation pour les arbres de décision. Les progrès dans la découverte et la gestion des connaissances (AKDM-1) 292, 21-38. Apprenticeship CV a fait son automatique apparition dans l'ecosystem Hadoop creant, de la promesse par la de puissance, sans d'uNE Opportunité pour ce domaine précédent. dans this éco - système, Apache Mahout is juin à la question Réponse du temps de calcul et / Ou de la vo- lumétrie: il Consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous de s'exécuter Portés sur AFIN Map / Reduce Ce. rapport se CONCENTRE sur le l'utilisation et portage de l'algorithme des Forêts aléatoires Dans Mahout Il montre à notre retour d'Travers les difficultés Qui expérience PEUVENT Être rencontrées Tant et Pratiques Que suggested Une théoriques piste d'improvement -.. 424 - G - session Feedbac Industrielle k - Etude et amélioration de la forêt aléatoire de la bibliothèque Mahout dans le contexte des données de marketing d'Orange Cedric Thao, Nicolas Voisine, Vincent Lemaire, Romain Trinquart"
262,Revue des Nouvelles Technologies de l'Information,EGC,2015,LeveragingWeb 2.0 for Informed Real-Estate Services,"The perception about real estate properties, both for individuals and agents, is not formed exclusively by their intrinsic characteristics, such as surface and age, but also from property externalities, such as pollution, traffic congestion, criminality rates, proximity to playgrounds, schools and stimulating social interactions that are equally important. In this paper, we present the Real-Estate 2.0 System that in contrary to existing Real-Estate e-services and applications, takes also into account important externalities. By leveraging Web 2.0 (content from Social Networks, POI listings) applications and Open Data enables the thorough analysis of the current physical and social context of the property, the context-based objective valuation of RE properties, along with an advanced property search and selection experience that unveils otherwise “hidden” property features and significantly reduces user effort and time spent in their RE quest. The system encompasses the above to provide services which assist individuals and agents in making more informed and sound RE decisions.","Papantoniou Katerina, Athanasiadis Marios - Lazaros, Fundulaki Irini, Georgis Christos, Stavrakas Yannis, Troullinos Michalis, Tsitsanis Anastasios",http://editions-rnti.fr/render_pdf.php?p1&p=1002105,http://editions-rnti.fr/render_pdf.php?p=1002105,en,"Tirer parti du Web 2.0 pour les services immobiliers en connaissance de Papantoniou Katerina *, Athanasiadis Marios - Lazaros *, Fundulaki Irini *, Georgis Christos * Stavrakas **, Yannis Troullinos Michalis **, *** * Tsitsanis Anastasios Institut des sciences informatiques, FORTH-ICS , Grèce {papanton, mathanas, Fundul, Georgis} @ ics.forth.gr, ** Institut pour la gestion des systèmes d'information, Centre de recherche - ATHENA {yannis, mtroullinos} @ imis.athena-innovation.gr *** TREK Conseil t.tsitsanis@trek.gr Résumé. La perception des biens immobiliers, tant pour les particuliers et les agents, ne se forme pas exclusivement par leurs caractéristiques intrinsèques, comme sur- face et de l'âge, mais aussi des externalités de propriété, comme la pollution, la congestion du trafic, taux de criminalité, la proximité des terrains de jeux, les écoles et les interactions sociales qui sont stimulante ing tout aussi important. Dans cet article, nous présentons le Immobilier 2.0 système qui contrairement aux éléments existants e-services immobiliers et des applications, prend également en compte les externalités importantes. En tirant parti de Web 2.0 (le contenu des réseaux sociaux, listes POI) des applications et des données ouvertes permet l'analyse approfondie du contexte physique et social actuel de la propriété, l'évaluation objective fondée sur le contexte des propriétés de RE, ainsi qu'une recherche et une sélection de propriété avancée expérience qui dévoile autrement « den hid- » caractéristiques de propriété et réduit considérablement l'effort de l'utilisateur et le temps passé dans leur quête RE. Le système comprend ci-dessus pour fournir des services qui aident les particuliers et les agents à prendre des décisions éclairées et plus de RE sonore. 1 Introduction Les contemporains économistes immobilier (Geltner et H., 2007), (Van Dijk et al., 2011) conviennent que la valeur d'un appartement, bâtiment ou un terrain dans une zone urbaine est non seulement représentée exclusivement par les caractéristiques intrinsèques du biens, tels que la qualité, l'emplacement et la taille de sa construction, mais aussi par les externalités positives comme la proximité des terrains de jeux, écoles, dynamiques relations culturelles locales, des circuits intellectuels d'échange, pacifiques et stimulant les interactions sociales; et externalités négatives telles que la pollution de l'air et le bruit, la congestion du trafic, des voisins bruyants, les taux élevés de criminalité. Immobilier 2.0, contrairement aux services et aux applications Web existantes Immobilier (xe 1, Foxtons 2) qui sont soumis à des restrictions que les caractéristiques intrinsèques des propriétés, vise à récolter et à intégrer diverses formes d'information Real Estate (RE) qui est en fait disponible aujourd'hui dans diverses sources Web 2.0, sous la forme d'un point d'intérêt (POI), le contenu et les opinions exprimées dans 1. http://www.xe.gr/ 2. http://www.foxtons.co.uk/ - 425 - Tirer parti du Web 2.0 pour Immobilier Web services sociaux et connaissance d'autres informations pertinentes publiées comme Open Data. L'objectif du système est triple, d'abord à fournir un accès très personnalisé à travers l'espace d'information de RE intégré au moyen de riches contraintes liées spatiales, temporelles et de contenu, d'autre part pour appuyer l'analyse multicritères et l'évaluation de tous les aspects de l'emplacement du site de manière à affecter les décisions finales d'investissement stratégique et troisième à fournir des analyses et des rapports immobiliers, en profitant du caractère des données disponibles. Le premier objectif dans cette direction est liée à l'acquisition de quatre types distincts de données RE: (a) la propriété dispose, (b) POIs dans la zone des propriétés environnantes, (c) de contenu social sous la forme de messages de discussion de Social en ligne médias, qui fournissent des preuves matérielles concernant le contexte physique et social des propriétés et (d) les données ouvertes principalement de sources gouvernementales qui fournissent des statistiques officielles, la démographie et des rapports sur divers aspects de la vie dans une région. Immobilier 2.0 repose sur les propriétés données par les agents immobiliers à travers leurs bases de données en interne. Ces données sont complétées par des données de propriétés supplémentaires en rampant principaux sites RE de publicité et portails. R l'acquisition de n ce qui concerne POIs, Immobilier 2.0 récoltes les données pertinentes d'une multitude de sites Web administratifs et touristiques, y compris les bases de données disponibles gratuitement des données GPS, ou contenu social lié à POIs. Le principal défi à cet égard est la capacité de diriger le processus d'acquisition de points d'intérêt ou à proximité d'une zone géographique d'intérêt, ainsi que la détection de nouveaux POI comme évolue dans le temps. Contenu social peut être acquis auprès de plusieurs sources, comme les blogs, les forums ou les plateformes sociales telles que Google Adresses et Foursquare. Alors que les blogs et les forums sont normalement accessibles sous forme de pages Web, l'accès à Google Adresses et Foursquare est différent car ces sources donnent un accès limité à leurs propres données par leurs API. Open Data est de savoir qui peut être utilisé, réutilisé et redistribué. Les dernières années ont vu un énorme effort en publiant des données provenant de sources ouvertes scientifiques et gouvernementales. Des initiatives telles que Data.gov.uk 3, NYC Open Data 4 et 5 de l'OCDE offrent une pléthore de jeux de données, généralement dans des formats différents, de sorte que l'acquisition et l'intégration dans notre système était une autre tâche difficile. L'intégration de l'information RE acquise est en fait le deuxième objectif et déterminera la qualité des données réellement exploitées par Immobilier 2.0 services. Tout d'abord, l'information de propriété doit être intégré à POIs. La question subtile à cet égard est liée à la précision de l'information géographique disponible dans les annonces (par exemple associées à des zones de voisinage ou plus larges au lieu des adresses spécifiques). Une telle situation est commune lorsque les annonces immobilières sont obtenues automatiquement à partir de sources au moyen d'exploration ciblée. Un autre défi consiste à associer POIs avec le contenu social des réseaux sociaux. Les utilisateurs ne font pas nécessairement référence à POIs de manière cohérente, mais peuvent utiliser des abréviations, langage familier, ce qui est plus difficile à analyser automatiquement. En outre, le contenu généré par les utilisateurs peut être court (par exemple, jusqu'à 140 caractères dans le cas de Twitter), ou peut créer un lien vers des pages Web contenant des informations supplémentaires ou des opinions anonymisées. Un autre objectif concerne la mise en œuvre des services d'utilisateur final vers la réalisation des objectifs mentionnés précédemment. Le système doit permettre aux utilisateurs d'explorer les informations intégrées et longitudinale en recherchant des propriétés répondant à un ensemble de données 3. http://data.gov.uk/ 4. https://nycopendata.socrata.com/ 5. http: / /www.oecd.org/ - 426 - K. Papantoniou et al. les contraintes spatio-temporelles et les préférences des utilisateurs individuels concernant la qualité de vie et les besoins socio-économiques. En résumé, le système Immobilier 2.0 fournit: - une infrastructure générique pour la récolte, l'extraction, l'agrégation et curating informations contextuelles liées aux propriétés de RE de sites de médias administratifs, touristiques ou sociaux (par exemple, des forums de discussion, des sites de réseautage social), ce qui nous permet de réduire le coût d'acquisition et le maintien des données de localisation de haute qualité au fil du temps (Batini et al., 2009); - une interface pour les agents et les individus à explorer les trajectoires spatio-temporelles des zones entourant les propriétés ou les points d'intérêt, afin d'évaluer les effets positifs et négatifs de biens; - les modèles d'affaires avancés pour les agences immobilières pour gérer leur portefeuille en fonction de l'évolution de la valeur de la propriété passée (Anselin, 1988) et les indices liés à la localisation (Holly et al, 2010a.), Dans le reste de (Holly et al, 2010b). cet article, nous exemplifient la vision du système par scanarios de cas d'utilisation alors que dans la section 3, nous décrivons l'architecture du système et nous analysons chaque couche. Ensuite, nous citons quelques travaux connexes et conclure par une discussion sur les contributions et les travaux futurs. 2 Études de cas Dans ce qui suit, nous illustrent la vision principale du système par deux scénarios d'utilisation de base mettant en évidence comment le système Immobilier 2.0 pourrait (a) aider les personnes à acheter des résidences à domicile et (b) gérer un portefeuille de commercial et résident propriétés ntial ou des terres dans le cadre de l'optimisation des investissements des entreprises RE. RE comparative analyse de marché: Imaginez Maria, un travailleur en col blanc femme qui a besoin de passer à une nouvelle ville en raison de son travail. Être dans une ville qui est complètement inconnu à quelqu'un et vouloir acheter (ou même louer) une maison de famille peut être une tâche extrêmement difficile. Maria a déjà des exigences qui se dégagent de la famille, le travail, ainsi que les préférences personnelles. Elle a besoin d'être proche (ou à distance de marche) d'une station de métro pour qu'elle puisse prendre le métro pour travailler tous les jours. D'autre part, son mari doit se rendre au travail alors il doit avoir soit un espace de parking dans l'immeuble ou de l'accès au stationnement dans la rue. De plus ses deux enfants, un garçon et une fille de 10 ans et 15 respectivement, doivent être près de l'école et d'avoir accès à des installations à proximité pour leurs activités de programmes supplémentaires comme la natation, cours de langues étrangères, ballet, etc. Le fait de devoir réellement ensemble tous ces les préférences personnelles ou de groupe est évidemment une tâche fastidieuse; afin de pouvoir répondre à toutes les exigences les informations requises ne doit pas être limitée à la disponibilité, mais devrait également inclure une discussion sur la qualité: il y a des écoles de ballet, mais comment ils sont bons, il y a des places de stationnement dans la rue, mais est-il sûr le quartier pour le stationnement de la rue, il y a une école à distance de marche, mais à quel point est-il, il y a un super-marché, mais quelle est la qualité des produits, il y a une aire de jeux dans la région, mais est-il assez sûr / grande? De plus on peut avoir besoin de trouver des informations sur les choses qui sont spécifiques dans la région: ayant un terrain de football à côté d'une maison de peut-être excellent pour les enfants pour l'exercice, mais pourrait aussi être un gros problème si elle est également utilisé par une équipe de football qui a fans qui parfois créer des problèmes de jeux de football autour. Ainsi, nous pouvons facilement reconnaître que Maria bénéficiera d'un service qui utilise une plus large - 427 - Tirer parti du Web 2.0 pour la piscine en connaissance de services immobiliers de données (POI, contenu social et Open Data) en dehors des simples listes de propriétés disponibles RE en une région. L'emplacement est pas la seule propriété des actifs de RE, l'environnement social et physique et son évolution au cours du temps jeu également un rôle important. L'information intégrée permet d'analyser l'évolution des valeurs immobilières. Gestion du portefeuille Intelligent RE et optimisation: Imagine House - Investments SA, une société d'investissement immobilier international, avec un portefeuille de plus de 1.000 propriétés commerciales, résidentielles et des éléments de terres en Grèce, situé sur des marchés diversifiés (régions et villes) du pays en afin de faciliter la gestion et l'atténuation des risques d'investissement potentiels. Le conseil d'administration de la société a décidé et fixer des objectifs spécifiques pour les 10 prochaines années, en ce qui concerne la performance des titres de propriété et de la terre et le rendement des investissements qui ont fait (et faire) à la construction du portefeuille immobilier de la société. Vers la réalisation de ces objectifs, la société a mis au point une stratégie équilibrée qui comprend les actions suivantes: - La vente de propriétés directement et objets terrestres dans les régions importantes projections foncières et la valeur des propriétés de réduction au cours des 10 prochaines années. - Re-investir dans de nouvelles propriétés et achats d'objets terrestres dans les régions avec une augmentation de la valeur prévue au cours des 10 prochaines années et de les revendre à la période de pointe de la valeur prévue pour la maximisation des profits. - Crédit bail et objets terrestres dans les régions avec une importante hausse attendue de la location. - En outre, la location sera appliquée dans toutes ces propriétés de la 1ère catégorie qui n'ont été vendus au cours des deux premières années de la mise en œuvre de la stratégie. Dans chacun des cas mentionnés ci-dessus, l'entreprise doit évaluer les projections relatives à la propriété, des terres et des valeurs de crédit-bail dans les différentes régions du territoire grec pour optimiser la performance du portefeuille et de maximiser l'upc enant les rendements des investissements. Ces projections seront basées sur l'analyse et le traitement des données géo-référencées historiques concer- nant les valeurs mentionnées ci-dessus (comment ils ont effectué au cours des périodes antérieures), et par leur corrélation avec les indicateurs cycliques tels que l'appréciation du capital et la croissance du PIB pour les emplacements et propriétés choisies et pondérations du portefeuille (les caractéristiques qui ont influé sur les changements de valeur), les estimations sonores seront effectués sur les valeurs des propriétés futures prévues pour différentes périodes de temps. 3 Système Immobilier 2.0 Architecture L'architecture globale du système est représenté sur la figure 1. Il y a trois couches, à savoir: services, l'intégration et l'exploration. couche de services RE comprend des modules qui mettent en œuvre de nouveaux e-services immobiliers à l'aide de l'état des technologies Web 2.0 d'art, ainsi que les données intégrées et les services d'accès pris en charge par le système Immobilier 2.0. La couche d'intégration est responsable de la consolidation, le géoréférencement et la gestion des données récoltées d'une manière qui permet un accès spatio-temporelle et uniforme au contenu actuel et archivé. Enfin, la couche rampants met à la disposition des données du système Bien que l'exploration systématique et ciblée du Web. La communication entre les couches est obtenue grâce à des API alors que l'ensemble du système suit une logique axée sur les services de manière à fonctionner en tant que système autonome et la fourniture d'une intégration avec les systèmes existants. - 428 - K. Papantoniou et al. FIGUE. 1: Immobilier 2.0 architecture. 3.1 Couche Crawling La couche rampants est responsable du lancement et de la coordination crawls ciblées afin d'extraire, de stocker et de faire qui constituent facilement accessibles toutes les informations les externalités des éléments immobiliers dans une zone spécifique. Le sous-système rampants vise trois types d'informations distinctes: POI, commentaires sociaux et Open Data. Pour chaque type d'information, un certain nombre de sources qui fournissent ces informations sont prises en charge. Par exemple, les POI sont collectées en interrogeant les API de services mondiaux tels que Foursquare et Google Places. Nos prototypes objectifs de mise en œuvre pour couvrir les deux régions métropolitaines, Londres et Athènes, donc les sites de POI locaux sont également interrogés au cours du processus de collecte des données. Le processus de récupération est limité aux seuls points d'intérêt qui se trouvent dans une zone géographique déterminée. commentaires sociaux proviennent de deux catégories de sources: sources de POI (mentionnées plus haut), et les réseaux sociaux comme Twitter. Encore une fois, le défi est de lier des commentaires pertinents à la région où ils se réfèrent. Dans le cas des sources POI cela est simple, puisque les commentaires des utilisateurs sont directement associés à un point d'intérêt spécifique. En revanche, dans le cas des tweets outils d'analyse linguistiques spéciaux doivent être utilisés afin d'établir une relation entre un tweet et une zone géographique ou un point d'intérêt. Enfin, Open Data récupéré par le sous-système rampants sont généralement données démographiques ou statistiques fournies par les autorités étatiques qui fournissent des informations sur des domaines tels que la pollution, la criminalité ou la congestion du trafic. La grande hétérogénéité des formats Open Data et les méthodes d'extraction nécessitent méthode d'accès spécialisé pour chaque type de données. Par conséquent, on ne peut pas mettre en œuvre une solution aussi générale que pour les points d'intérêt et des commentaires sociaux, mais aller pour une approche adaptée à une zone spécifique et des sources de données spécifiques. Le sous-système rampant se concentre sur les trois types d'information - 429 - Tirer parti du Web 2.0 pour Informed Immobilier Services décrits ci-dessus. Le concept clé est celui d'une campagne de collecte de données. Une campagne est définie en définissant un certain nombre de paramètres, ce qui peut être accompli soit par programme par l'intermédiaire d'une API, ou de manière interactive par l'intermédiaire d'une interface utilisateur (appelé le cockpit sur chenilles). Ces paramètres décrivent, en premier lieu, la zone géographique d'intérêt sous la forme d'un cercle (centre et rayon). Ce cercle est converti en quelles que soient les sources de données comprennent, afin de minimiser la récupération des informations non pertinentes. Othe paramètres de campagne r permettent: (a) La sélection des catégories de POI à inclure dans la campagne. POI sont classés dans un ensemble de la définition de l'intervalle de temps pour l'exploration de chaque catégorie de POI des catégories prédéfinies, comme par exemple les écoles, les parcs, les restaurants, etc. (b). Chaque catégorie de POI est associée à un intervalle de temps qui définit la fréquence à laquelle la campagne doit rafraîchir les points d'intérêt particuliers en relançant les robots d'exploration respectifs. (C) La sélection des types de supports pour chaque élément récupéré, y compris le texte, l'image et la vidéo. Notre solution fait face à un certain nombre de défis technologiques, y compris: (a) le soutien à des campagnes simultanées pouvant durer jusqu'à mois, (b) le respect des contraintes de source sur le nombre de requêtes autorisées par unité de temps, et (c) la coordination d'un grand nombre de crawler actif instances. les résultats de la campagne sont stockés dans un référentiel constitué d'une base de données relationnelle pour les données structurées et les métadonnées, et une structure de répertoire pour garder les articles des médias. L'API Layer Crawler offre un moyen flexible pour récupérer les données de la campagne, en définissant un certain nombre de critères que les résultats doivent satisfaire. Depuis une campagne peut fonctionner pendant de longues périodes de temps, une méthode supplémentaire pour récupérer uniquement les dernières informations sont également disponibles à la couche d'intégration. 3.2 Couche d'intégration La couche d'intégration offre une vue syndiquée de l'information (POI, Open Data, le contenu social et propriétés immobilières) stockées dans une base de données centralisée permettant la consolidation de toutes les informations fondées sur des prédicats spatiales et temporelles. Le schéma de la couche d'intégration Immobilier 2.0 est basé sur les points d'intérêt du W3C Data Model (Hill et M., 2012), la recommandation du W3C pour la modélisation des POI sur le World Wide Web. Dans le cadre de ce modèle, un point d'intérêt est défini comme un ensemble de termes faiblement couplés et géographiques liés entre eux, composé d'emplacements, POI et lieux. Sur la base de la terminologie ci-dessus, le modèle de données de POI est constitué d'une entité de POI qui a un certain nombre de propriétés pour capturer des informations descriptives (par exemple la catégorie, liens externes, la description, les gammes de prix, heures d'ouverture et de clôture) avec une entité de localisation décrivant son emplacement . Le schéma Real- Domaine 2.0 suit en termes généraux POI W3C Modèle de données et prend en compte les entités POI et emplacement que le centre d'intégration pour toutes les données disponibles. Plus précisément, un bien immobilier - une construction humaine avec un emplacement fixe, ouvert au marché à louer ou vendre - est conçu comme un type particulier d'un point d'intérêt avec des caractéristiques différentes. Le POI W3C Modèle de données suppose aucune distinction entre un lieu et un point d'intérêt parce qu'ils partagent les mêmes attributs bien souvent avec des interprétations différentes en fonction de l'échelle. Par analogie, nous considérons également REProperty comme un type spécial de POI avec de petites différences dans les interprétations des attributs (par exemple catégorisation différente). Cela permet l'intégration uniforme des externalités (par exemple le contenu social, Open Data) avec les entités de base du modèle (POI, REProperty). Pour la représentation de l'information géospatiale, nous avons adopté la norme WGS84 (World Geodetic System) qui est définie par une longitude, latitude et altitude triple. Ce système géodésie permet des requêtes géospatiales complexes (par exemple de la proximité des biens immobiliers à POIs, la valeur de la criminalité dans un polygone défini par l'utilisateur) et offre une flexibilité dans la représentation des entités au-delà des points uniques par - 430 - K. Papantoniou et al. les polygones et les lignes (par exemple un polygone pour un parc national, une ligne d'une artère) par rapport à une autre représentation par exemple des adresses postales. L'élévation a été éliminé dans notre modèle en raison de l'absence d'informations d'altitude dans les données disponibles. Pour la modélisation du contenu social et Open Data, nous avons adopté une approche uniforme avec l'introduction de l'entité externalité, malgré la forte hétérogénéité des sources et le format des données disponibles allant du texte libre (comme dans les commentaires, messages, etc.), au contenu multimédia , r Arrêt Défin (aime par exemple dans Facebook, Check-ins) et des mesures dans différents systèmes métriques. Une instance de externalité peut être liée soit à un POI ou entités de propriété Immobilier. Cela nécessite géoréférencement pour la mise en relation des externalités à une entrée de localisation et la normalisation des valeurs de données différentes. Last but not least, est particulièrement important compte tenu de la représentation de la provenance de la dépendance du système avec des sources plus souples telles que le commentaire social sur le Web. Dans les bases de données traditionnelles et généralement aux documents électroniques, la fiabilité et l'intégrité des données considérées comme une donnée, un cas qui ne peut être pris pour acquis dans le Web. Le consommateur de la connaissance est d'une importance capitale pour être au courant de l'origine des données. A cet effet, l'entité Provenance conserve les attributs requis (auteur, la source, la première date de publication. Licence, etc.). Sur la base des principes généraux proposés de la modélisation des données de provenance dans notre modèle, nous avons adopté l'approche dans laquelle les données et les annotations coexistent dans le même système, l'ajout d'informations supplémentaires à la base de données, mais sans la nécessité de recalcul (Cheney et al., 2009). est donné à la figure 2. La figure Une représentation des entités de base et leurs relations. 2: High Level Immobilier 2.0 Schema. Après l'intégration du schéma suit le processus de chargement des données provenant des sources. Ceci est un processus complexe et coûteux qui comprend l'intégration de personnalisation des sources hétérogènes dans un format commun, le nettoyage et le contrôle de la qualité de l'ensemble de données résultant selon des règles métier spécifiques et, enfin, le chargement au dépôt. Pour toutes ces raisons, nous avons adopté une approche dans laquelle les transformations de données ont lieu en dehors du serveur de base de données. Les points forts de cette approche est la réduction du temps de développement que seules les données pertinentes à la solution est extraite et traitée par le serveur de base de données, alors que le serveur de base de données reste libre de tâches prétraiter jusqu'à ce qu'une nouvelle mise à jour majeure se produit. En ce qui concerne le mécanisme de mise à jour concerne, la couche d'intégration peut planifier des campagnes à travers la couche rampants pour les données agrégées de manière asynchrone. les tâches de mise à jour sont utilisées hors ligne lorsque le système passe à un mode de maintenance. la politique de mise à jour diffère en fonction du type de données mises à jour par exemple - 431 - Tirer parti du Web 2.0 pour les mises à jour Informé de la propriété immobilière Immobilier Services et tâches archivées sont programmées sur une base plus fréquente que l'Open Data. Par la suite, nous décrivons les modules pour les tâches de traitement de l'assurance qualité et le texte. Module de qualité la qualité des données est pas une option mais une contrainte pour le bon fonctionnement du référentiel. les problèmes de qualité des données semblent introduire encore plus de complexité et de la charge de calcul pour le processus de chargement du dépôt. Ce module est responsable du traitement des duplications de données et le bruit dans POIs extraites par des heuristiques et des mesures de similarité. Le processus commence par la résolution des conflits basée sur la fiabilité des sources (par exemple Google Maps est considéré comme la source la plus fiable). Dans la deuxième étape, les problèmes avec les catégories manquantes sont résolus à l'aide des informations de contexte alors que dans la troisième longitude de stade et les valeurs de latitude sont recoupés avec l'adresse du POI par géocodage et géocodage inverse. Enfin, une combinaison pondérée des mesures de similarité des chaînes telles que Leveinstein, NeddlemanWunch, SmithGooth, Soundex pour ne citer que quelques-uns, conduit à l'action CRUD appropriée. Traitement de texte Une grande partie des données disponibles, qui contiennent des informations précieuses, sont fournies dans un format de texte brut non structurées telles que des descriptions de leurs biens immobiliers et le contenu social. A partir d'une réelle description de la propriété immobilière, des informations sur les caractéristiques et les équipements peuvent être extraites comme le nombre de chambres, nombre de salles de bains, région de résidence de propriété (par exemple, adresse, quartier), les installations de chauffage / refroidissement, etc. De plus, de la production d'innombrables du contenu social celles faisant référence à une nouvelle Gion, le quartier ou un POI doivent être détectés de manière aspects et opinions sur l'emplacement spécifique à révélé. Pour y parvenir le texte brut est analysé par un pipeline d'outils de traitement du langage naturel. Le pipeline comprend l'identification des langues, la segmentation de la phrase, la normalisation du texte, une partie de marquage de la parole, l'extraction des entités nommées candidats à travers des modèles et enfin un processus d'homonymie. Le processus d'homonymie est nécessaire comme une étape d'amélioration de la précision afin d'éliminer les annotations positives fausses et pour la résolution dans les cas où plusieurs sites partagent le même nom. Pour l'étape de désambiguïsation grandes bases de connaissances Web (DBPedia, geonames) ainsi que le contexte et les informations de profil (si existant) ont été utilisés. Ce processus a été très difficile en raison de la nature à court, informel et le plus souvent allusive du texte. En plus de cela, nous avons traité un texte écrit en deux langues à savoir l'anglais et le grec. Le degré de difficulté est augmentée en cas de grec, car de toute évidence est une langue moins étudiée dans une perspective de calcul avec un nombre limité d'outils disponibles. 3.3 Services layer Calque services accueille quatre modules de base qui permettent la fourniture d'une première Of- une sorte de valeur ajoutée RE services utilisateur final, sur la base des technologies et des techniques éprouvées Web 2.0. De façon plus détaillée le module Geo-Navigator permet l'intégration avec des services tiers (par exemple Google Maps, Street View, OpenStreetMap), ce qui permet l'exploration à distance des propriétés de RE dans l'intérêt et la représentation visuelle du contenu social et spatio-temporelle autour d'eux. Un tel contenu est alimenté dans le système Immobilier 2.0 à travers le contenu module de navigation sociale, offrant un accès direct aux commentaires et opinions exprimées dans les médias sociaux populaires en ce qui concerne les zones entourant la propriété et les POI à proximité, ainsi que le Interrogation spatiale / temporelle du module qui gère les formulation des prédicats spatiales et temporelles impliquant POIs, Open Government données et caractéristiques de la propriété. Ce dernier représente l'entrée principale - 432 - K. Papantoniou et al. source pour le module de projection RE valeur qui exploite une grande variété de jeux de données (caractéristiques de propriété native et les caractéristiques spatiales pertinentes, les données contextuelles et sociales basées sur la localisation, spatio-temporel caractérisé données ouvertes) pour activer et mobiliser un processus d'analyse de régression multiple, qui tourner le définit Immobilier 2.0 hédonique de prix Modèle pour l'évaluation objective et la projection à court terme des valeurs de propriétés de RE sélectionnées. Sur la base des services de base des services plus sophistiqués construiront afin de fournir des fonctionnalités plus avancées comme en détail dans les scénarios du cas d'utilisation. 3.4 Mise en œuvre Du point de vue de la mise en œuvre, pour le dépôt de la couche d'intégration du SGBD PostgreSQL a été sélectionné en combinaison avec l'extension PostGIS pour soutenir la représentation des objets géographiques et la mise en œuvre ultérieure des requêtes spatiales complexes. En outre, l'extension ltree de PostgreSQL a été utilisé pour la représentation de la hiérarchie des catégories de POI pour l'optimisation des requêtes connexes. Pour les tâches de géoréférencement (par exemple de géocodage et géocodage inverse), nous comptons sur les applications parties tierces à savoir Google géocodage 6 et le Nominatim collaboratif 7. La communication entre les couches obtenues grâce à des services Web RESTful assurant l'interopérabilité et l'extensibilité du système. La majeure partie du système est construit avec les technologies Java. 3.5 démonstration du système Dans la section suivante, nous présentons brièvement certaines des fonctionnalités du premier prototype de notre système 8. La première capture d'écran représente les propriétés immobilières à plusieurs paramètres d'options de recherche (région, critères de prix, services, etc.) et une vue d'ensemble des résultats de recherche. L'interface permet à l'utilisateur d'appliquer la proximité et des recherches complexes sur l'espace d'informations consolidées. Lors de la deuxième capture d'écran, les résultats de recherche enrichis sont représentés: (a) une figure interactive . 3: Immobilier 2.0 Options de recherche 6. 7. https://developers.google.com/maps/documentation/geocoding/ http://wiki.openstreetmap.org/wiki/Nominatim/ 8. http: // re2- vm-win.imis.athena-innovation.gr/ - 433 - Tirer parti du Web 2.0 pour Informed carte Immobilier services où POIs environ à la propriété immobilière sont représentés selon les préférences de l'utilisateur dans le menu de droite avec (b) POI à proximité Catégories (c) RE multimédia de propriété (d) données ouvertes liées (e) aperçu (f) description détaillée (g) commodités. FIGUE. 4: Immobilier 2.0 Aperçu des résultats 4 liés au travail Même si le marché immobilier est l'une des principales forces mobiles dans le monde entier, la pénétration IT dans les agents RE portefeuille de services est encore faible. L'état actuel dans les petites agences immobilières (principalement locale) est la présence sur le Web par des sites Web simples montrant des données statiques et des images sur leurs annonces immobilières et avec des fonctionnalités de recherche basés sur simple, critères de spécifiques propriété, carte manquant visualisations ou exploration POIs. Grands agents RE et les banques, ont mis au point et utiliser des applications logicielles internes, processus limité statique et les volumes de données assez complexes se référant principalement à des statistiques relatives à l'activité financière de RE, les taux de transaction, les zones de valeur et des données juridiques. L'extraversion de ces services est limitée à la fourniture d'études statistiques concernant les aspects de l'évolution et financiers du marché immobilier. Enfin, la propriété importante inscription des sites offrent des services de recherche plutôt limités, qui ne couvrent pas l'ensemble des besoins des clients pour la fourniture d'informations, et donc, ne pas permettre et soutenir la mise en place d'un cadre intégré pour la valeur ajoutée des services de RE disposition qui réduit au minimum effort de la clientèle et facilite les transactions et les investissements RE. Donc, au mieux de notre connaissance, c'est le premier système qui intègre des modèles d'affaires immobilier les données immobilières traditionnelles avec un contenu généré par les utilisateurs des réseaux sociaux en ligne, la modélisation des externalités qui affectent la valeur des biens immobiliers. Les mécanismes spécifiques que nous développons en Immobilier 2.0 sont Spanning dans de nombreux domaines - 434 - K. Papantoniou et al. notamment: l'extraction de commentaire social, la reconnaissance et à l'enrichissement POI, appariement d'entités, le traitement du langage naturel, la gestion des données spatio-temporelle, exploration du Web et l'archivage Web, l'estimation de l'indice des prix à la maison. Dans le contexte de l'estimation réelle des prix immobiliers un travail récent est celui de (Sun et al., 2014) qui propose une nouvelle méthode de prévision réelle de l'indice des prix de l'immobilier en introduisant le facteur de comportement humain dans le modèle de prévision. A cet effet, ils combinent le sentiment de nouvelles quotidiennes en ligne et les données Google moteur de recherche de requête, de manière à construire un modèle d'exploration de données intégré qui a des capacités de prévision. Ensuite, dans le domaine de la reconnaissance POI, l'enrichissement et l'extraction de contenu social, un travail connexe est celle de (Cano et al., 2011) qui démontre une méthodologie de modélisation de la perception collective d'un POI. Dans ce travail, après les phases de l'extraction de contenu social et l'enrichissement sémantique ultérieure des POI en exploitant les applications Web 2.0, ils effectuent un triplification de POI par leur ontologie LinkedPOI pour permettre une représentation visuelle des points d'intérêt. Il y a un grand volume de travail au cours des dernières années pour l'extraction de l'information localisée du texte de contenu social. Beaucoup d'entre eux sont basés sur le traitement des langues naturelles, mais leurs œuvres ne sont applicables en langue anglaise. Une exception, est le travail de la langue portugaise de (Santos et al., 2012) qui présente de multiples façons de lieux d'enrichissement, en appliquant des techniques de traitement du langage naturel et d'extraction d'information sur les contenus récupérés à partir du Web et exploite en outre des bases de connaissances Web telles que Wikipedia et Wiktionnaire comme un mécanisme de validation. Enfin, pour la Déduplication de POI et appariement d'entités, il y a beaucoup de méthodolo- proposées logies, l'un des plus récents que les démons trats haute précision et le rappel des résultats est le (Zheng et al., 2010) qui repose sur une approche fondée sur l'apprentissage machine. Dans cette approche, les caractéristiques proposées afin de trouver les différences entre deux points d'intérêt, étaient similitude de nom, d'adresse similitude, ressemblance de la catégorie, ainsi que les mesures correspondantes. Dans notre système, nous avons créé des scénarios basés sur des catégories POIs afin d'éviter des techniques coûteuses d'apprentissage de la machine. 5 Conclusions et travaux futurs Le présent document a présenté l'approche Immobilier 2.0 du point de vue de la conception d'un système et axé sur notre solution pour l'acquisition, l'intégration et l'interrogation des données relatives immobilier à la mise en œuvre de nouveaux et avancés RE e-services tels que l'exploration des propriétés de RE en utilisant les préférences liées spatiales, temporelles et de contenu, l'analyse des propriétés en utilisant le contexte de localisation et d'analyse en immobilier au fil du temps. Dans notre contexte les données de l'immobilier ne se limitent pas aux caractéristiques intrinsèques d'un bien, mais comprennent des aspects importants tels que points d'intérêt, le contenu social et Open Data. L'approche globale tire parti du Web 2.0 de multiples façons telles que pour l'acquisition de données, géoréférencement, visualisations etc. Les travaux en cours comprend une vaste évaluation basée sur l'utilisateur, ainsi que des améliorations des méthodes individuelles (par exemple, le module de la qualité, la projection des prix des logements) et la mise en œuvre de plus avancée services à l'utilisateur final. D'un point socio-économique de vue, les objectifs du système pour fournir un avantage concurrentiel aux agences immobilières en offrant des services de géo-données personnalisés qui peuvent intégrer à l'infrastructure existante avec un coût minimum d'effort et sans aucun investissement en capital ou l'entretien initial. - 435 - Tirer parti du Web 2.0 pour Informed services immobiliers 6 Acquittement Ce travail a été soutenu par les autorités nationales « COOPERATION 2011 » programme, projet avec le code 11SYN_1_531 intitulé « Informed immobiliers Services: Tirer parti du Web 2.0 ». Références Anselin (1988). Spatial Econometrics: Méthodes et modèles. Boston: Kluwer Academic Publishers. Batini, C., C. Cappiello, C. Francalanci et A. Maurino (2009). Les méthodes d'évaluation et d'amélioration de la qualité des données. ACM Comput. Surv. 41 (3), 16: 1-16: 52. Cano, A. E., G. Burel, A.-S. Dadzie et F. Ciravegna (2011). Topiques: Un outil de visualisation sémantique émergentes de POIs basées sur les flux de conscience sociale. 10e Conférence internationale Web sémantique. Cheney, J., L. Chiticariu et W.-C. Tan (2009). Provenance des bases de données: Pourquoi, comment et où. A trouvé. Trends bases de données 1 (4), 379-474. Geltner, D. et P. H. (2007). Un ensemble d'indices pour le commerce immobilier commercial basé sur la véritable base de données des prix de transaction de capitaux, centre mit pour les biens immobiliers. Rapport technique, MIT Center for Real Estate, Data Laboratory Immobilier commercial - CREDL. Hill, A. et W. M. (2012). Points de base d'intérêt. Rapport technique, W3C. Holly, S., M. Pesaran et T. Yamagata (2010a). Un modèle spatio-temporelle des prix des maisons aux Etats-Unis. Journal of Econometrics 158 (1), 160-173. M1 - 1. Holly, S., M. H. Pesaran et T. Yamagata (2010b). Diffusion spatiale et temporelle des prix des maisons au Royaume-Uni. Rapport technique. Santos, J., A. Alves, F. C. Pereira, et P. Abreu (2012). enrichissement sémantique de lieux pour la langue portugaise. INForum2012. Soleil, D., C. Zhang, W. Xu, M. Zuo, J. Zhou, et Y. Du (2014). DOE médias Web ont des preuves opinions- de prévision du marché de l'immobilier. 18e Conférence du Pacifique Asie sur les systèmes d'information PACIS 2014. Van Dijk, B., P. Franses, R. Paap et D. van Dijk (2011). Modélisation de prix des maisons régionales. Journal of Applied Economics 43, 2097-2110. Zheng, Y., X. Fen, X. Xie, S. Peng et J. Fu (2010). Détecter les enregistrements presque dupliqués dans des ensembles de données de localisation. 18 ACM SIGSPATIAL Conférence internationale sur les progrès dans les systèmes d'information géographique. - 436 - G - Session Web 2.0 Tirer parti Industrielle pour les services immobiliers en connaissance de Katerina Papantoniou, Marios-Lazaros Athanasiadis, Irini Fundulaki, Christos Georgis, Yannis S tavrakas, Michalis Troullinos, Anastasios Tsitsanis"
263,Revue des Nouvelles Technologies de l'Information,EGC,2015,Linked Data Annotation and Fusion driven by Data Quality Evaluation,Dans cet article nous présentons une approche de fusion de données fondée sur l'utilisation d'informations sur la qualité des données pour résoudre les éventuels conflits entre valeurs.,"Ioanna Giannopoulou, Fatiha Saïs, Rallou Thomopoulos",http://editions-rnti.fr/render_pdf.php?p1&p=1002086,http://editions-rnti.fr/render_pdf.php?p=1002086,en,"Les données liées Annotation et Fusion tirée par les données d'évaluation de la qualité Ioanna Giannopoulou (1), Fatiha Saïs (1) et Rallou Thomopoulo (2) (1) LRI (Université Paris Sud et CNRS) Bât. 650 Ada Lovelace, F-91405 Orsay Cedex, France (2) INRA-SupAgro, LIRMM (Université Montpellier 2, CNRS et Inria) 2 place P. Viala, F-34060 Montpellier cedex 1, France Résumé. Dans ce travail, nous nous intéressons à ce problème de données de sion, à partir de jeux de données réconciliées dont les objets sont liés aux relations sameAs sémantiques. Nous essayons de fusionner les informations souvent contradictoires de ces objets réconciliés afin d'obtenir des représentations unifiées qui ne contiennent que la meilleure information de qualité. 1 Introduction liaison de données, également connu comme la réconciliation des données Ferrara et al. (2013); Saïs et al. (2009); Pernelle et al. (2013), est le processus par lequel deux descriptions d'objet sont examinées afin de déterminer si elles se réfèrent à la même entité dans le monde réel, et si oui, à les relier entre eux. Ensuite, la fusion de données englobe l'effort d'acquérir un seul objet homogénéisés en fusionnant les informations des objets individuels liés. Les objets marqués du hibou: sameAs peuvent contenir différentes, des valeurs contradictoires ou incohérentes dans leurs propriétés. Pour chaque propriété doit être choisie la plus valeur appropriée. La fusion des données est une étape essentielle pour éviter la redondance, regroupant les meilleures informations de qualité et de donner des réponses cohérentes aux utilisateurs, dans l'environnement de données liées. La recherche sur le problème de la fusion des données a commencé il y a plus de deux décennies dans le domaine des bases de données relatio- nal Bleiholder et Naumann (2008). Cependant, nous examinons la fusion des données du point de vue RDF, on remarque que les spécificités des mécanismes RDF ne peuvent pas être pris en compte dans les solutions proposées par les experts de bases de données relationnelles. Trois approches ont été proposées pour la fusion de données en RDF Saïs ET Thomopoulo (2008); Saïs et al. (2010); Flouris et al. (2012); Mendes et al. (2012). Ces différentes approches tentent d'évaluer la qualité de chaque valeur, en tenant compte de diverses mesures en fonction de la valeur elle-même et / ou ses métadonnées. Dans ce travail, nous sommes intéressés à explorer le problème de la fusion de données. Notre méthode com- bine différents critères de qualité en fonction de la valeur et sa source de données et exploite autant que possible la sémantique de l'ontologie, les contraintes et les relations. De plus, nous créons un méca- nisme de fournir des explications sur la qualité de chaque valeur, estimée par notre système. Pour ce faire, nous générons des annotations utilisées à des fins de traçabilité et d'explication. Notre approche est décrite en détail dans la section 2. Une première évaluation est présentée dans la section 3. Enfin, la section 4 conclut le papier et donne des travaux futurs. - 257 - Fusion Linked Data 2 Solution proposée Inspiré par les approches citées dans la section 1, nous construisons un système qui tente d'offrir une solution automatisée solide au problème de la fusion. En ce qui concerne la description des méthodes de traitement décrites dans Bleiholder des conflits et Naumann (2008), notre approche est automatique et applique une stratégie de résolution des conflits. Nous montrerons qu'il est en même temps une instance basée et une stratégie basée sur les métadonnées. Comme il sera montré dans les sections suivantes, cette solution (i) être adapté à chaque propriété, (ii) exclure les valeurs qui sont peu plausible selon des règles spécifiques, (iii) exploiter, chaque fois que pos- sible, les connaissances de l'ontologie (contraintes , relations, etc.) et (iv) traiter nativement des propriétés à valeurs multiples. Plus important encore, il va stocker les informations qui ont conduit à chaque décision de fusion, afin d'explications d'offre sur comment et pourquoi une valeur a été choisie. 2.1 Méthodologie des valeurs non plausibles Detect. Nous utilisons la notion de vraisemblance afin d'exclure les valeurs qui sont irrationnelles, selon certaines mesures ou règles logiques prédéfinies. Cette étape est basée sur les éléments suivants: Fréquence. Les mesures d'homogénéité et d'occurrence f réquence dans les sources de données telles que définies dans Saïs et Thomopoulo (2008), sont utilisés ici dans le but de détecter l'invraisemblance: si la fréquence d'occurrence est inférieure à un seuil prédéfini, la valeur est considérée comme peu plausible. contraintes de domaine. Certaines contraintes de domaine et frappe de propriété restrictions peuvent également être utilisés pour détecter des valeurs non plausibles. Par exemple, si l'âge de la propriété est typé « xsd: ger nonNegativeInte- », doit alors être considérée comme une valeur négative de l'âge comme peu plausible. Calculer la valeur score de qualité. Pour les valeurs qui ne sont pas considérées comme peu plausible, un score de qualité est calculée. Nous croyons que les aspects de la valeur elle-même (par exemple, combien de fois il apparaît dans un ensemble spécifié) et les aspects de la qualité de sa source de données peut être tout aussi important, et qui est la raison pour laquelle, nous a donné lieu à une liste des éléments suivants des critères : L'homogénéité d'une valeur. La fréquence d'apparition d'une valeur parmi toutes les valeurs d'une propriété au sein du groupe de références réconciliés. Sa fréquence d'occurrence. La fréquence d'apparition d'une valeur parmi toutes les valeurs pour une propriété dans l'ensemble des données. Si une valeur apparaît plusieurs fois dans un jeu de données pour la même propriété, il est plus fiable car il est moins susceptible de contenir des erreurs d'orthographe. La fraîcheur de sa source. La dernière mise à jour de la source de données. Les sources de données qui sont récemment mises à jour ont tendance à être considéré comme plus fiable. La fiabilité de sa source. Les utilisateurs peuvent donner explicitement leur préférence pour un ou ano ther source de données. Ces préférences sont utilisées pour fixer une valeur de fiabilité dans [0..1]. Si aucune prefe- rence est donnée, toutes les sources de données sont considérées comme aussi fiables. Pour une description plus détaillée sur l'homogénéité, la fréquence d'occurrence et la RNS eau douce, voir les définitions Saïs et Thomopoulo (2008). Le score de qualité globale d'une valeur donnée est la moyenne de ces critères est calculée. peuvent être utilisés en effet, d'autres fonctions d'agrégation comme une moyenne pondérée, au maximum, un minimum, etc. Le score est alors normalisé et représenté sur une échelle qualitative avec les valeurs suivantes: - 258 - Giannopoulou et al. excellent, moyen, faible. Comme nous le verrons ci-dessous, cette échelle servira pour interroger les infor- mations sur l'ontologie de métadonnées. relations Découvrez. Pour toute valeur qui n'est pas considérée comme peu plausible, le système tente de découvrir comment la valeur est liée aux autres valeurs possibles pour la propriété. Les relations possibles sont: plus précis: un contrôle d'inclusion de chaîne est appliquée ici, ainsi que l'interrogation d'une base de connaissances pour détecter les relations possibles de subsomption pour les valeurs hiérarchiques pour sélectionner le Lue plus précis va-. Exemples: Paris est plus précis que la France, « 10/05/1999 » est plus précis que « 1999 ». Synonyme: plusieurs API pour détecter les synonymes sont utilisés pour déterminer si les deux valeurs sont synonymes. Exemple: l'Angleterre est synonyme du Royaume-Uni. Dans le cas où la propriété est explicitement défini comme monovalué, la valeur avec le meilleur score de qualité est choisie. Dans le cas contraire, les deux valeurs sont conservées dans la propriété. Incompatible: le système fait référence à une liste de règles d'experts pour détecter d'éventuelles lités incompatibi- logiques entre les valeurs d'une propriété ou plusieurs propriétés. Par exemple, si nous déclarons dans une base de connaissances qu'un hasFunction de propriété est fonctionnel et: hasFunction (X, '' primature '') ∧hasFunction (X, '' Président '') ⇒ ⊥ alors si le hasFunction de propriété a ces deux valeurs , ces derniers sera déclarée incompatible. Selon les rapports, le score de qualité est affectée positivement ou négativement, par un bonus ou un malus. Les valeurs sont ensuite triées par leur score de qualité. Pour les propriétés monovalué, la valeur avec le score le plus élevé de qualité est choisie. Pour les propriétés à valeurs multiples, toutes les valeurs plausibles sont conservés. Pour chaque valeur, le système stocke toutes les informations qui ont conduit au calcul du score, ou la raison de invraisemblance. 2.2 Provenance des décisions Fusion Depuis le calcul de la sc de qualité le minerai et la sélection de la valeur appropriée est plus complexe, nous nous rendons compte que l'offre des explications sur la provenance des décisions de fusion est une fonction utile. Pour ce faire, nous avons trouvé le mécanisme de réification, également utilisé dans Saïs et Thomopoulo (2008), plus souple et adapté à nos besoins de représentation. Le mécanisme de réification permet d'enrichir les déclarations RDF en ajoutant de nouveaux éléments. En substance, il offre la possibilité de créer une ontologie des métadonnées décrivant triplets RDF existants. L'ontologie de métadonnées de fusion de données. Nous présentons une méta-données de la fusion de données ontologie afin d'utiliser le mécanisme de réification RDF pour annoter des données fusionnées par les informations de qualité qui sont exploitées pour atteindre les décisions de fusion (voir la figure 1). Rdf: classe Statement est enrichie par la propriété d'objet hasQuality. La principale qualité de classe a trois propriétés d'objet qui organisent les différents aspects de qualité. Les groupes de propriétés hasCriteria jusqu'à toutes les mesures utilisées pour calculer le score de qualité, les hasRelations rassemble les relations de la valeur avec d'autres valeurs, et les hasIndicators contient les informations restantes. Un document de métadonnées conforme à cette ontologie est produite par le système à la suite de l'algorithme de fusion de données. Il fournit une description des mesures de la qualité de la valeur et les raisons pour lesquelles il a été choisi ou exclu (par exemple, la règle qui viole, les valeurs qui sont plus précis que cela, etc.). Cependant, l'objectif initial du document de métadonnées doit être automatiquement interrogé. En fonction de l'échelle de qualité d'une valeur (excellent, moyen, mauvais), les différentes requêtes sont destinées à fournir une « histoire » expliquant les aspects de la qualité de la valeur. - 259 - Fusion Linked Data Fig. 1 - Les métadonnées ontologies Exemple. Un extrait du document de métadonnées est affiché ci-dessous. Nous utilisons le DFA rythme names-, debout pour l'annotation de fusion de données. Dans cet exemple, les deux valeurs de la DFA propriété: prenom sont présentés et annotés avec toutes les informations utiles concernant leur qualité. Plus précisément, les Jacues de valeur peu plausible ne contient que les informations sur son mogeneity HO-, qui est très faible (0,015) et, par conséquent, rend compte de son exclusion de la liste des valeurs plausibles. De l'autre côté, la valeur plausible Jacques, toutes les informations ning le calcul de nécessaires sur son son score de qualité sont contenus dans le fichier. Pour cet exemple, si le système interroge le document de métadonnées pour les Jacues de valeur, la réponse serait « La valeur est implau- sible en raison de l'apparence très faible dans les sources de données Cause possible:.. Faute d'orthographe », tandis que pour la valeur Jacques il sera « la valeur est la seule plausible. Il a une combinaison d'apparition élevée dans la source de données et de haut niveau de confiance sur sa source de données ». PERSONNE-17123430093 rdf: type PhysicalPerson v1 rdf: type Valeur q1 rdf: type Qualité c1 rdf: type Critères PERSONNE-17123430093 DFA: prenom v1 v1 DFA: hasValue Jacques v1 DFA: isImplausible faux ... PERSONNE-17123430093 DFA: prenom v2 v2 rdf: Type de valeur q2 rdf: Type qualité c2 rdf: Type Critères v2 DFA: hasValue Jacues v2 DFA: isImplausible vrai v2 FMO: hasQuality q2 q2 FMO: hasCriteria c2 c2 FMO: hasHomogeneity 0,015 à 260 - Giannopoulou et al. 3 expériences Dans cette section, les principales sources de données qui sont utilisées pour tester notre approche sont fournis par le partenaire INA du projet ANR Qualinca. Nous avons examiné un ensemble de groupes de 10819 cas réconciliés (par paire liée en utilisant owl: liens sameAs) qui représentent les personnes célèbres français et les avis contenus où ils sont impliqués. Dans le tableau 1. (a), nous montrons la répartition des groupes instances réconciliés. Ces exemples sont décrits à l'aide des propriétés différentes, comme aPourNom, aPourT itreCollection, aPourDateDiffusion, et ainsi de suite. Tableau (a) Tableau (b) la personne # instances Jacques Martin 10288 Philippe Bouvard 264 Daniel Prevost 214 Frederic Martin 26 Emmanuel Petit 12 Luis Fernandez 7 Michel Leclerc 6 Virginie Lemoine 2% #values ​​valeurs #distinct 1 4588 - #isImplausible = « true » 9370 64,23% #isImplausible = « false » 5218 34,76% #qualityValue = « excellent » 2 0,04% #qualityValue = « medium » 3233 61,95% #qualityValue = « pauvres » 1983 38% TAB. 1 - Table (a) des groupes d'instances rapprochées; Tableau (b) Résultats du premier fusion de données dans le tableau 1. (b), nous détaillons les premiers résultats obtenus par notre approche de fusion de données. A partir du fichier d'annotation nous avons extrait le nombre de valeurs distinctes, le nombre de valeurs qui sont détectés comme peu plausible et ceux qui sont plausibles grâce au calcul de la fréquence. L'échelle à trois valeurs (excellent, moyen, mauvais) a été validé par les experts du domaine, car il capture l'idée intuitive de pointage neutre / positif / négatif. Il pourrait être plus précis (avec 5 valeurs, par exemple), mais ce serait encore un raffinement de l'échelle à trois valeurs. Pour la valeur de qualité qui est calculée, nous avons utilisé trois valeurs de seuils pour les déterminer: - si qualityScore ≥ 0,67 puis qualityValue = « excellent ». - si 0,33 <qualityScore> 0,67 puis qualityValue = « medium ». - si qualityScore ≤ 0,33 alors qualityValue = « pauvres ». Ce que nous pouvons observer de ces résultats est que plus de 64% des valeurs sont détectés comme peu plausible et considéré comme inapproprié pour les propriétés correspondantes. En outre, à partir des valeurs plausibles près de 62% des valeurs ont une valeur de qualité est moyenne ce qui est de consolider les résultats de la première étape en ce qui concerne la sélection des valeurs plausibles. Des expériences plus qualitatives sont nécessaires pour mieux qualifier les raisons pour lesquelles 38% des valeurs ayant une valeur de mauvaise qualité comme plausible apparaissent. 4 Conclusion Dans ce travail, nous avons présenté un effort pour étudier en profondeur le problème de la fusion des données dans le contexte des données liées. Nous avons montré que notre approche apporte des compléments solides - 261 - Fusion de données liées sur le calcul d'un score de qualité de valeur et comprend l'idée originale de garder la trace de la provenance des décisions de fusion. L'annotation ontologie nous avons proposé afin de représenter les aspects de la qualité de valeur, offre la possibilité d'interroger pour obtenir des réponses automatique spécifiées et des explications. Nous avons réalisé une première mise en œuvre du système et étudié plusieurs ensembles de données bibliographiques ainsi que des moyens d'évaluer nos résultats d'expérience. Plusieurs directions seront à explorer. En particulier, il serait intéressant d'examiner différents scénarios pour exploiter les données fusionnées, ainsi que l'utilisation d'autres critères de qualité. Nous pourrions aussi expérimenter différentes combinaisons de critères. D'autres expériences permettront de définir des seuils génériques pour nos variables et de découvrir la meilleure combinaison de poids pour le calcul du score de qualité. 5 Remerciements Ce travail est soutenu par l'Agence Nationale de la Recherche française: « La qualité et Interopera- bilité des grands catalogues de documents » projet (QUALINCA-ANR-2012-CORD-012-02). Références Bleiholder, J. et F. Naumann (2008). La fusion des données. ACM Comput. Surv. 41 (1). Ferrara, A., A. Nikolov, et F. Scharffe (2013). liaison de données. J. Web Sem. 23, 1. Flouris, G., Y. R. andMaria Poveda-Villalon andPablo N. Mendes, et I. Fundulaki (2012). En utilisant la provenance pour l'évaluation de la qualité et de la réparation des données ouvertes liées. En Dans Actes du 2ème atelier conjoint sur les connaissances Evolution et Ontologie Dynamics (EvoDyn-12). Mendes, P. N., H. Mühleisen, et C. Bizer (2012). Sieve: évaluation de la qualité des données liées et fusion. Dans Actes du commun EDBT / CIDC Ateliers 2012, Berlin, Allemagne, 30 Mars, 2012, p. 116-123. Pernelle, N., F. Saïs, et D. Symeonidou (2013). Une approche de découverte automatique de clés pour la liaison de données. J. Web Sem. 23, 16-30. Saïs, F., N. Pernelle, et M. Rousset (2009). La combinaison d'une logique et une méthode numérique pour la réconciliation des données. J. données Sémantique 12, 66-94. Saïs, F. et R. Thomopoulo (2008). Fusion de référence et l'interrogation flexible. Dans Actes à pied, à Internet Systems Meaningful: OTM 2008 confédérés international Conférences, CoopIS, DOA, Gada IS et ODBASE 2008, Partie II, pp. 1541-1549. Saïs, F., R. Thomopoulo, et S. Destercke (2010). Ontologie entraîné par fusion de référence possibiliste. Dans Actes à pied, à Internet Meaningful Systems, OTM 2010 - Confé- déclassée conférences internationales: CoopIS, IS, DOA et ODBASE, Partie II, pp 1079-1096.. Résumé Dans cet article nous Présentons Une approche de fusion de l'sur Données utilisation Fondée d'informations sur la qualité des Données verser les éventuels conflicts Résoudre Entre facts. - 262 - D - Sémantique ET ontologies données liées Annotation et Fusion entraînées par des données d'évaluation de la qualité Ioanna Giannopoulou, Fatiha Saïs, Rallou Thomopoulo"
267,Revue des Nouvelles Technologies de l'Information,EGC,2015,Mining Classes by Multi-label Classification,"We propose a new approach to mine potential classes in news documents by examining close relationship between new classes and probability vectors of multiple labeling of the documents. Using EM algorithm to obtain the distribution over linear mixture models, we make clustering and mine classes.","Yuichiro Kase, Takao Miura",http://editions-rnti.fr/render_pdf.php?p1&p=1002066,http://editions-rnti.fr/render_pdf.php?p=1002066,en,"Classes minières par Multi-label Classification Yuichiro KASE *, Takao MIURA ** * Département des sciences avancées, yuichiro.kase.7n@stu.hosei.ac.jp ** Département des élus. De & Elect. Eng., Miurat@hosei.ac.jp HOSEI Université 3-7-2 KajinoCho, Koganei, Tokyo, Japon 184-8584 CV. Nous vous proposons une nouvelle approche de mes classes potentielles dans les documents d'information en examinant relation étroite entre les nouvelles classes et des vecteurs de probabilité de marquage multiple des documents. En utilisant l'algorithme EM pour obtenir la distribution par rapport aux modèles de mélange linéaires, nous faisons le regroupement des classes et des mines. 1 Motivation nuage Récemment systèmes par Internet ont été largement répandu pour que nous puissions arriver à énorme quantité d'informations complexes facilement et rapidement. Cependant, nous pouvons attraper à peine avec les changements à l'intérieur et la plupart des informations disparaissent immédiatement ce qu'ils sont précieux. Très souvent, nous aimons classer les informations dans les classes qui viennent de cours donnés à l'avance. Une classe peut être obtenue grâce à la reconnaissance humaine par laquelle on peut imaginer ce qui se passe en utilisant des classes. Étant donné que chaque classe correspond à de certain concept, on peut voir ce qu'un mot ne signifie une fois que nous savons que le mot appartient à la classe. Dans ce travail, nous discutons problème de classification multi-étiquette et comment trouver des cours potentiels. classification multi-classe signifie un processus pour mettre l'information dans l'une des catégories multiples. Toute information dans une catégorie part des aspects communs qui caractérisent la catégorie donnée à l'avance, a appelé une classe et son nom une étiquette. La classification automatique nous permet d'extraire les règles par apprentissage inductif. Nous examinons une collection d'histoires (valeurs d'attributs avec des étiquettes, des données de formation appelée), puis extrait caractéristiques spécifiques aux classes (Han et Kamber, 2011). La recherche de la classification multi-label a été initialement motivée par la difficulté d'ambiguïté concept rencontré dans la catégorisation de textes. En fait, chaque document peut appartenir à plusieurs thèmes (étiquettes) simultanément et quelques documents contient une seule partie. L'une des approches typiques est la classification probabiliste (Kita, 1995), étant donné que les de classification traditionnelle sultats dépendent fortement des données de formation. Le plus important est qu'il ya peu corpora, même si nous voyons un énorme quantité d'informations sans étiquette (données brutes). Ici, dans ce travail, nous adoptons une approche semi-supervisée dans un cadre de probabilité. Ici nous nous concentrons notre attention sur un fait que la façon dont les classes sont constituées. Tout article de nouvelles de contestation internationale des « navires de commerce » en Chine peut provenir de plusieurs labels de la politique, l'économie, ainsi que l'histoire et la culture. Chaque catégorie porte son propre sens, bien qu'il contienne al combinaison pondérée des concepts de labels comme l'une des caractéristiques (Han et Kamber, 2011). Cela nous permet de définir de nouvelles classes pour de nouvelles catégories en donnant du poids - 77 - vecteurs (0,1, 0,2, 0,3, 0,4) sur ces étiquettes. Pour de nouvelles classes de mines en combinant les étiquettes sur les espaces de probabilité, nous pourrions avoir un nombre infini de combinaisons d'étiquettes en raison de nombre infini de poids. La principale contribution de ce travail se résume comme suit: (1) Nous pouvons mes classes potentielles basées sur la classification label Muli. (2) au moyen d'un modèle de mélange linéaire, on obtient les probabilités d'appartenance sur les étiquettes fournies. Ensuite, nous faisons le regroupement des probabilités d'étiquettes. (3) Nos expériences montrent que de nouvelles classes d'identifier de nouveaux aspects des classes potentielles qui diffèrent des étiquettes constitutives. Le reste du papier est organisé comme suit. Dans la section 2, nous décrivons clas- sification multi-étiquettes pour les documents et l'approche probabiliste, ainsi que des travaux connexes. L'article 3 contient un cadre de notre approche, y compris l'algorithme EM et le regroupement. L'article 4 contient quelques résultats expérimentaux. Dans la section 5, nous concluons cette enquête. 2 multi-label Classification Il y a eu de nombreux algorithmes de classification multi-étiquettes proposées jusqu'à présent. Ils se composent de deux types d'approche, un classement lgorithms et l'estimation probabiliste (Han et ber Kam, 2011). Le premier constitue la combinaison de classification et l'étiquette-ensemble (Tsoumakas et Ka- Takis, 2007). approche de classification contient le regroupement, le classement (Elisseeff et Weston, 2002), l'entropie (arbre de décision) et la traduction des résultats binaires en multi-étiquettes (Rifkin et Klautau, 2004). Une difficulté se pose sur la dépendance entre les étiquettes comme indiqué dans (Zhang et Zhang, 2010) préoccupations d'estimation .Probabilistic sur la façon d'estimer les paramètres de certaines fonctions de distribution de probabilité. Fondamentalement, nous comptons des fréquences de documents et faire des classificateurs basés sur eux. Étant donné que nous estimons une étiquette c qui rend P (c | x) au maximum, nous devons avoir P (x | c) × P (c) par le théorème de Bayes. La plus simple est un classificateur Naive Bayes (NB). où à la fois P (x | c) et P (c) peut être obtenue rapidement par des fréquences. classificateurs Naive devrait dépendre des données de formation et nous supposons modèle de probabilité et l'apprentissage semi-supervisé. Algorithme Expectation Maximisation (EM) a été discuté pour le document basé sur la classification probabilité multinomiale. Au cours de chaque étape, nous appliquons une estimation MAP pour obtenir de nouveaux paramètres, mais ils ont examiné la classification multi-classe. Ensuite, pour la classification multi-étiquettes, il y a eu une approche étiquette ensemble de manière probabiliste. McCallum (McCallum, 1999) a discuté de la classification multi-étiquettes en utilisant EM algorithme basé sur la probabilité Gauss N (μ, σ). Ils ont examiné les mélanges de distributions normales sur toute la combinaison d'étiquettes et d'environ les étiquettes de la probabilité maximale. De toute évidence, il prend du temps beaucoup à cause de nombre exponentiel de la combinaison. Ueda (Ueda et Saito, 2003) a proposé une nouvelle approche pour décrire des documents par plusieurs étiquettes, compte tenu de toutes les étiquettes sous forme de mélanges de sujets et tous les sujets que la distribution de probabilité multinomiale sur les mots. Ils ont estimé les distributions de probabilité en utilisant l'algorithme EM et propose 2 modèles de relation d'étiquettes, PMM1 et PMM2 mais il reste encore quelques problèmes de dépendances d'étiquettes. En utilisant le modèle de sujet, Wang a proposé un certain modèle de relation inter entre les étiquettes (Wang et al., 2008). Bien que l'approche d'allocation Latent Dirichlet ne peut pas modéliser la situation directement, ils ont introduit l'étiquette-vecteurs qui peuvent être générés de manière multinomial et a examiné la performance. - 78 - 3 Estimating multi-étiquettes pour classer les documents d pour la classification multi-étiquettes sur les étiquettes L = {L1, .., Ln}, nous introduisons un vecteur de probabilité d⃗ = (c1, .., cn), Σ ck = 1 , cj ≥ 0 pour décrire la cj de probabilité de d sur une étiquette Lj, ​​j = 1, ..., n. Nous aimons obtenir le vecteur de probabilité d⃗ sur plusieurs étiquettes sur L au moyen de l'apprentissage semi-supervisé. Puisque P (d) = Σ j P (Lj) P (d | Lj) par la marginalisation, nous aimons estimer PLJ (d) = P (d | Lj) avec un poids λLj = P (Lj). Note cj = P (Lj | d) = P (Lj) P (d | Lj) / P (d). Formellement notre classement fonctionne bien par un modèle de mélange linéaire sur les étiquettes (Kita, 1995). Soit X une variable aléatoire qui correspond à un document et la probabilité d'événement est généré par un mélange aléatoire: P (X) = Σ c λcPc (X) où Cp (X) signifie une probabilité de X provenant de la distribution de probabilité multinomiale d'un étiquette c et Xc une probabilité de choix de c indépendant de X. Nous supposons un mot w passe temps XW dans un document d de c selon la distribution de probabilité multinomiale Pc (d) avec un mot probabilité pw d'une manière naïve Bayes: Pc (d) = e! Πwxw! Πwp XW w, nd = Σ w∈d XW. Pour estimer les probabilités Pc (X) et les coefficients Xc, nous améliorons plusieurs paramètres θ (des fonctions de distribution de probabilité) au cours de l'algorithme EM jusqu'à ce que la convergence de telle sorte que le PC (X) = p (X | c, θc) est un multinomial fonction avec des paramètres θc (Han et Kamber, 2011). En appliquant l'estimation du maximum de vraisemblance plusieurs fois, nous arrivons à l'état stable BE- la cause de chaque EM itération ne diminuera pas la probabilité. Finalement, nous devons avoir une collection de probabilités d'appartenance d'une manière cohérente. Doigt de pied Stimate eux à une itération, nous ob- TAIN nouveaux paramètres thetav de θ en maximisant une des probabilités a posteriori (MAP) des fonctions de distribution multinomiale. L'histoire va avec le processus probabiliste, un mélange linéaire des distributions multinomiales sur les mots en fonction des étiquettes données à l'avance. Chaque document conserve la probabilité en fonction de chaque étiquette, et nous supposons mot selon les probabilités se pose de mélange. Nous estimons toutes les probabilités postérieures des étiquettes constituant P (d | Lj), ainsi que les probabilités a priori P (Lj) au moyen de l'algorithme EM. Pour plus de détails, allez à (Han et Kamber, 2011). Notons que la classification multi-étiquettes fonctionne bien avec le concept d'identifier correctement dans les documents et un document peut contenir plusieurs thèmes sur plusieurs. Cela nous amène à grouper dans l'espace d'étiquettes et nous extrayons des grappes de documents en fonction des vecteurs de probabilité. C'est, chaque document porte certaine probabilité de chaque étiquette, qui décrit la répartition des thèmes qu'il contient dans une certaine mesure. Considérant un ensemble de probabilités comme un nouvel aspect, nous donnons une classe (éventuellement nouvelle) au document. Grâce à l'algorithme EM, on obtient la probabilité d'appartenance P (d | c) d'un document d et une étiquette c, ainsi que les probabilités de choix P (c) de c1, .., cann, Λ = (λ1, .., Xc ), Σ = 1 Ài, ≥ 0. Comme Ài tout document peut appartenir à plusieurs étiquettes en même temps, définissons d⃗ = (P (d | c1), ..., P (d | tcc)). Note P (d) = Σ λiP (d | ci) = Λ · d⃗ détient. Définissons la norme de Λ et d⃗ comme || Λ || · d⃗ = Λ · d⃗ / | Λ · d⃗ |. Étant donné une collection de documents d1, .., dN, nous faisons le regroupement de tous les documents en fonction des vecteurs de probabilité d⃗1, ..., d⃗N dans notre espace d'étiquettes. en K ensembles exclusifs de telle sorte que nous avons le minimum rk j Σ i || Λ · (d⃗ij - tj) ||, chaque groupe est marqué par les centres t1, .., tK. Dans cette enquête P (d | c) est générée au moyen de distribution de probabilité multinomiale sur les mots, tous les groupes décrivent le maximum de vraisemblance de l'adhésion de documents. - 79 - Résumons notre approche: Supposons multinomial fonction de distribution de probabilité de chaque étiquette. Étant donné les étiquettes {c1, ...,} cC, documents de formation L et documents de test T, nous générer des vecteurs de probabilité sur D = L ∪ T et des grappes de faire plus de marques pondérées. Nous prétraiter D à l'avance, comme l'élimination des mots vides et à endiguer. (1) En utilisant l'algorithme EM, nous estimons les probabilités de choix et les vecteurs de probabilité, (λ1, .., Xc et P (d | c1), ..., P (d | tcc)). (2) Nous faisons le regroupement de tous les documents exclusivement en fonction des probabilités de choix et les vecteurs de probabilité. (3) Nous avons mis une étiquette c, le centre, à chaque groupe avec une technique de marquage du cluster. 4 Les expériences Montrons des résultats expérimentaux dans deux aspects, la classification multi-étiquette et l'exploitation minière de classe. Nous examinons ModApte de Reuter sous-corpus-21578 Version 1.0 en sélectionnant le top 10 des étiquettes fréquentes, et le premier 1000 article en fin de test. Un tableau 1 contient les étiquettes. Après prétraiter les articles tels que et les mots vides égrappage, nous avons remplacé chaque numéro de chiffres par un mot spécial « * d ». Nous avons sélectionné 200 articles au hasard en formation avec 10 fois repeatition du processus EM. Comme mesure d'évaluation, nous examinons la précision et le rappel à l'appariement multi-étiquette (correspondant complet) et à la correspondance unique étiquette (seule). Les anciens moyens que nous disons correct que si toutes les étiquettes d'un article sont exactement estimés, alors que ce dernier moyen que nous disons correct si des étiquettes sont estimées. Nous appliquons Naive classification bayésienne (NB) comme référence. Nous examinons 200 articles et la fréquence des mots d'extrait par étiquette sous forme de données de formation. Nous faisons décision binaire par un certain seuil. Notez le résultat ne varient pas beaucoup avec plusieurs seuils. Dans un tableau 2, nous obtenons 669 articles au total et la précision est 0,669, tandis que NB montre seulement 8 articles (précision 0,008). Au Nouveau-Brunswick, les 8 articles appartiennent à une étiquette « commerce » aucun article correspondant à d'autres étiquettes. Un tableau 3 présente les résultats de rappel et de précision à chaque obtai étiquette défini par notre approche et la ligne de base (NB) où Ans, Corr, des moyens réponses avec Précipitation, Correcness et précision respectivement. Nous obtenons les moyennes 0,745 et 0,770 du rappel et la précision respectivement par notre approche, mieux que les moyennes et 0,674 0,103 (NB). Bien que les valeurs de rappel par NB sont mieux que notre approche, toutes les valeurs de précision de notre approche surperformer NB considérablement, disent 748% amélioré. Le résultat rappel montre 1,12 fois meilleurs et la précision 7.24 fois mieux. Au Nouveau-Brunswick, nous obtenons assez élevé rappel dans chaque étiquette qui provoque la précision plutôt pire. Dans un tableau 4, nous montrons tous les groupes (centres) et tout le nombre des articles dans chaque groupe. Nous obtenons 11 groupes non vides où (...) signifie que les parties dominantes. Il y a 10 groupes avec étiquette dominante unique et 1 groupe multi-étiquettes ( « gagner, acq » nouvelle classe). Une seule classe de « Earn, acq » se pose lorsque plusieurs étiquettes ( « gagner » et « ACQ ») sont dominants parmi les 11 groupes, mais nous obtenons la pire précision de correspondance complète. Notez que nous avons 76 articles corrects entre 162 articles affectés à la classe « gagner, acq ». En fait, nous obtenons 33 articles de l'étiquette « gagnent », 41 articles de « ACQ » et 2 articles de « gagner, acq ». En mains, nous voyons les articles de « gagner, acq » (contenant plusieurs étiquettes dominantes) diffèrent des articles dans les classes « gagnons » et « ACQ ». Ces articles d'une seule classe d'étiquettes ont un aspect de l'analyse économique, tandis que les articles des multilabel ont un aspect différent des tendances financières. Il semble préférable de définir une nouvelle classe. - 80 - Articles Étiquette Étiquette articles 507 gagnent 13 226 intérêt acq 11 grain, le blé, le maïs 55 brut 5 gagnent, acq 41 commerce 5 brut, navire 34 grain, de blé 2 gagnent, brut 29 fx d'argent, l'intérêt 2 grain, de blé, expédier 20 argent-argent-2 fx fx, le commerce 19 grain, maïs 1 acq, bateau 13 bateau 1 grain, le grain navire 13 1 blé, le maïs 1000 TAB. 1 - Les étiquettes des articles Étiquette articles appariées de précision (nôtre) gagnent 468 453 0,968 acq 172 153 0,890 gagnent / acq 162 2 0,012 commerce 30 17 0,567 navire 21 8 0,381 grain 17 3 0,176 brut 28 25 intérêt 0,893 31 4 0,129 argent-fx 19 4 0,211 blé 24 0 0,0 28 0 0,0 maïs (total) 1 000 669 0,669 (NB) commerce 30 8 0,267 (total) 1000 8 0,008 TAB. 2 - Multilabels Étiquette entièrement appariée Ans Corr rappel Ans Corr avec Précipitation Ans Corr rappel Ans Corr Prec (Ours) (NB) acq 232 198 0,853 334 232 0,695 232 165 0,711 708 165 0,233 maïs 31 15 0,484 28 15 0,536 31 21 0,677 750 21 0,028 brut 62 26 0,419 28 26 0,929 62 43 0,694 821 43 0,052 gagnent 514 508 0,988 630 530 0,841 514 350 0,681 466 350 0,751 grain 80 12 0,15 17 12 0,706 80 50 0,625 760 50 0,066 intérêt 42 22 0,523 31 22 0,710 42 28 0,667 759 28 0,037 d'argent fx 7 0,137 19 51 7 0,368 51 33 0,647 772 33 0,043 16 0,727 navire 21 22 16 0,762 22 14 0,636 713 14 0,0196 0,419 18 commerce 30 43 18 0,6 43 26 0,605 866 26 0,030 16 0,333 blé 48 24 17 48 28 0,583 0,708 760 28 0,037 (total) 1125 838 0,745 1162 895 0,770 1125 758 0,674 7375 758 0,103 TAB. 3 - Rappel / précision par étiquette 5 Conclusion Dans ce travail, nous avons proposé une nouvelle approche de la mine de classes potentielles. Nous avons introduit un modèle de mélange linéaire des fonctions de distribution multinomiale pour obtenir des probabilités d'adhésion sur les étiquettes, alors nous avons fait le regroupement des probabilités d'étiquettes. L'approche surclasse 1.12 fois mieux dans le rappel et 7,48 fois plus de précision pour l'adaptation d'une seule partie. Nous avons obtenu une nouvelle classe qui identifie de nouveaux aspects par rapport aux étiquettes constitutives. Elisseeff Références, A. et J. Weston (2002). Méthode du noyau pour la classification multi-étiquetés. Les progrès 14 NIPS, 681-687. Han, J. et M. Kamber (2011). Data mining: Concepts et techniques. Morgan Kauffman. Kita, K. (1995). modèles de langue probabilistes (en japonais). Université de Tokyo Press. McCallum, A. (1999). classification texte multi-étiquette avec un modèle de mélange formé par em. Atelier sur le texte d'apprentissage, AAAI. - 81 - Non / Articles gagner acq d'argent fx intérêt le commerce du grain brut maïs navire de blé 1/0 0,253 0,093 0,010 0,015 0,100 0,016 0,017 0,286 0,011 0,197 2/468 (1.000) 1.90E- 05 2.05E- 08 6.68E- 10 2.78E- 09 1.31E- 10 1.92E- 07 1.99E- 10 9.11E- 10 4.65E- 09 3/172 3.27E- 08 ( 1,000) 1.07E- 1.87E- 12 13 15 2.16E- 2.02E- 1.38E- 14 10 13 2.11E- 2.25E- 1.45E- 11 4/162 14 (0,558) (0,205) 0,023 0,033 0,020 0,036 0,037 0,029 0,025 0,033 24.5 30 1.84E- 2.99E- 1.35E- 47 48 48 1.95E- 1.27E- 34 2.10E- 2.47E- 48 46 (1,0) 48 1.47E- 4.97E- 1.58E- 12 6/0 58 5.82E- 6.43E- 59 60 60 9.48E- 5.75E- 1.02E- 60 59 59 1.05E- 8.12E- 9.48E- 60 1 7/0 0,313 0,115 60 0,013 0,019 0,011 0,435 0,021 0,016 0,038 0,019 0,215 8/0 0,009 0,013 0,008 0,079 0,014 0,014 0,011 0,625 0,013 0,088 0,085 9/0 0,004 0,005 0,056 0,006 0,006 0,110 0,004 0,637 21.10 7.54E- 2.77E- 60 60 61 3.06E- 4.51E- 61 2.74E- 4.83E- 61 61 5,00 E- 61 3.87E- 61 (1,0) 11/0 0,129 4.51E- 0,047 61 0,005 0,008 0,235 0,008 0,009 0,314 0,006 0,238 0,201 0,122 0,007 12/0 0,465 0,043 0,048 0,048 0,009 0,008 0,047 13/31 4.56E- 1.11E- 13 14 2.45E- 7.16E- 16 10 1.67E - 12 4.48E- 17 (1,000) 08 1.37E- 8.37E- 7.40E- 16 16 14/19 50 4.71E- 1.73E- 50 (1,0) 51 2.82E- 1.71E- 3.02E- 51 51 3.12E- 51 2.41E- 51 2.11E- 2.82E- 51 0,168 0,112 15/0 51 0,207 0,010 0,006 0,261 0,161 0,009 0,008 0,060 58 16/0 1.58E- 5.82E- 6.43E- 59 60 60 9.48E- 5.75E- 60 1,02 E- 59 1.05E- 8.12E- 59 60 1 9.48E- 5.07E- 60 17/28 11 36 1.20E- 1.10E- 39 (1,0) 36 1.22E- 6.38E- 1.35E- 34 38 36 1.40E- 5.19E- 1.87E- 35 37 18/17 59 2.58E- 9.48E- 60 1.05E- 1.54E- 60 60 (1,0) 60 1.65E- 1.71E- 1.32E- 60 60 60 1.16E- 6.65E- 27 19/28 46 1.93E- 7.10E- 7.84E- 47 48 47 1.16E- 1.55E- 1.24E- 23 47 47 1.28E- 1.21E- 8.66E- 21 48 (1,0) 41 1,78 20/30 4.85E- E- 41 1.97E- 42 2.90E- 1.76E- 42 42 (1,0) 42 3.21E- 2.49E- 42 2.18E- 2.90E- 42 42 TAB. 4 - Cluster Constituants Rifkin, R. et A. Klautau (2004). Dans la défense d'un contre-toute classification. Journal of Machine Learning Research 5, 101-141. Tsoumakas, G. et I. Katakis (2007). Classification multi-label: Une vue d'ensemble. J. de données maison et des Mines 3-3 entrepôt. Ueda, N. et K. Saito (2003). modèles de mélange pour le texte Parametric multi-étiquettes. Les progrès réalisés dans NIPS 15, 721-728. Wang, H., M. H. X., et Zhu (2008). Un modèle probabiliste pour générative classifi- cation multi-étiquette. Intn'l Conf. sur l'exploitation minière de données (CISM), 628-637. Zhang, M. et K. Zhang (2010). apprentissage multi-label en exploitant la dépendance à l'étiquette. connais- sances Discovery dans les bases de données (KDD). Résumé En examinant relation étroite entre les nouvelles classes et des vecteurs de probabilité de marquage multiple des documents, on obtient la fonction de distribution de probabilité de chaque étiquette de documents. Avec la prise en charge de la distribution multinomiale sur les mots, on applique l'algorithme EM pour obtenir la distribution. Ensuite, nous appliquons le regroupement des probabilités d'étiquettes aux classes de mines. - 82 - B - Classification, Clustering, Classes Similarité Mining par multi-label Classification Yuichiro Kase, Takao Miura"
278,Revue des Nouvelles Technologies de l'Information,EGC,2015,To initiate a corporate memory with a knowledge compendium: ten years of learning from experience with the Ardans method,"Ardans method ArdansSas (2006b) and technology ArdansSas (2006a) of knowledge capitalization and structuration are used with different industries (automotive, aerospace, energy, defence, steel, health, etc.) for more than a decade in France and Europe.The proposed solutions in knowledge management and especially in expertise capitalisation have set a lot of feedback over time. With a view toward ongoing improvement, what are the impacts of these feedbacks on the method nowadays? Put into practice into the industry, the return of investment of a capitalization campaign is inferred from the quality of the knowledge base delivered at the end of the campaign. Therefore, the method and the technology are intrinsically connected. How IT tools can assist with the quality diagnosis of the knowledge base?A comparative study was conducted on the basis of the method Mariot et al. (2007) exposed at EGC'2007. This article sets out the results of the changes and improvements of the method, in conjunction with the latest technical and scientific development on the one hand, and the change of the industry needs on the other hand.","Vincent Besson, Alain Berger",http://editions-rnti.fr/render_pdf.php?p1&p=1002103,http://editions-rnti.fr/render_pdf.php?p=1002103,en,"Pour lancer une mémoire d'entreprise avec un recueil de connaissances: dix années d'apprentissage de l'expérience avec la méthode Ardans Vincent Besson *, Alain Berger * * Ardans SAS «Le Campus» - Bâtiment B1 6 rue Jean-Pierre Timbaud 78180 Montigny-le-Bretonneux { vbesson, aberger}@ardans.fr, http://www.ardans.fr Résumé. méthode Ardans ArdansSas (2006b) et ArdansSas technologiques (2006a) de la capitalisation des connaissances et Structuration sont utilisés avec différentes industries (automobile, aéronautique, énergie, défense, acier, santé, etc.) pendant plus d'une décennie en France et en Europe. Les solutions proposées dans la gestion des connaissances et en particulier dans la capitalisation d'expertise ont mis beaucoup de commentaires au fil du temps. Dans une perspective d'amélioration continue, quels sont les impacts de ces rétroactions sur la méthode de nos jours? Mettre en pratique dans l'industrie, le retour de l'investissement d'une campagne de capitalisation est déduite de la qualité de la base de connaissances délivrée à la fin de la campagne. Par conséquent, la méthode et la technologie sont intrinsèquement liées. Comment les outils informatiques peuvent aider au diagnostic de la qualité de la base de connaissances? Une étude comparative a été réalisée sur la base de la méthode Mariot et al. (2007) exposée à EGC'2007. Cet article présente les résultats des changements et des améliorations de la méthode, en collaboration avec le dernier développement technique et scientifique d'une part, et le changement des besoins de l'industrie d'autre part. 1 Introduction industrie à l'intérieur de la gestion des connaissances a parfois des contraintes qui ne dépendent pas de la quantité de données que vous avez à traiter ou comment « propre » est les données brutes. La méthode décrite dans ArdansSas (2006b), MAKE, met l'accent sur les problèmes qui ont trois contraintes principales. Le premier est évidemment la pression financière où les projets de gestion des connaissances ont lieu. La seconde est la pression du temps: projet de gestion des connaissances sont conçus pour être fermés dans quelques jours ou semaines. Dans ces fenêtres de temps, les problèmes doivent être identifiés, une solution doit être conçu, construit et déployé. A partir de là le travail est terminé pour le consultant du savoir, et l'entreprise doit mener la deuxième phase (pour donner accès aux connaissances aux apprenants) sur son - 401 - Pour lancer une mémoire d'entreprise avec un recueil de connaissances propres. La dernière contrainte est la plus importante, sur le plan conceptuel: que pouvons-nous faire si les données ne sont pas écrites? Ces trois contraintes ont donné beaucoup de pression à la méthode depuis 2006. Cet article se concentrera sur ce qui a été changé jusqu'à présent. Il décrira le périmètre applicable à la section 2. Les articles 3, 4 et 5 se concentreront sur ce qui a changé la définition des matériaux de base et de la sélection, le processus et les produits respectivement. Les commentaires est basé sur de nombreux cas industriels, trois d'entre eux ont déjà été décrits dans certains documents: le projet ICARE (Berger et Mariot (2007)) avec PSA Peugeot Citroën, le projet MDOW (Verhagen (2010)) menée en collaboration avec EADS, et le projet ALPIN (Louis (2012)) mené en collaboration avec Air Liquide. 2 Périmètre Pour une meilleure compréhension de ce qui est discribed dans cet article, il est important de considérer les différents niveaux de gestion des connaissances dans l'industrie. Cette méthode doit être appliquée lorsque les données ne sont pas présentes sur l'Internet, ni dans aucun système de fichiers, mais à l'intérieur des cerveaux humains. Par conséquent, ces données ne sont pas accessibles. Mais les interactions entre les gens donnent directement ce qui est nécessaire: pas de données, mais la connaissance. Les interactions entre les gens sont la partie « gestion » dans la « gestion des connaissances ». L'objectif est d'identifier les connaissances, de structurer, de valider, de partager et de faire fructifier d'une manière dynamique. Cette gestion des connaissances est centrée sur l'homme, et commence des idées aux données (approche descendante) beaucoup plus que les données à la connaissance (bottom-up). La connaissance sera donnée ou expliqué par petits groupes de « connaissants » (par exemple, une équipe de recherche et de développement de cinq personnes) dans un domaine donné, et non par des centaines d'employés dans tous les domaines impliqués dans une organisation donnée. Cela est beaucoup une approche psychologique que sociologique. Ce point de vue particulier de la gestion des connaissances peut être appelée la gestion des compétences. Collecte automatique des connaissances ou la prise d'inférence à partir des informations (par exemple des pages html) est très différent de « manuellement » la création d'un lien logique entre les concepts, avec l'aide d'experts, et afin de transférer les connaissances dans un but. Les mots-clés sont de MAKE « expertize », « à la main », et « à dessein ». L'un des modèles souvent utilisés pour classer le contenu de l'esprit humain est la suivante: les données est l'entité symbolique minimale; lorsque les données sont traitées pour être utile (quand il répond aux questions qui, quoi, quand, lorsque des questions), il devient une information; lorsque les données et informations répondre à la question de la façon dont il se savoir; quand il apprécie la question pourquoi, il est être- compréhension; lorsque cette compréhension est évaluée, il devient sagesse. Ce modèle présente deux inconvénients pour la gestion des connaissances appliquées dans l'industrie: le premier est le mot « données » qui peuvent se référer à des données à l'esprit et les données stockées informatiques, ce qui est source de confusion. Le second a lieu dans la situation: quand les gens parle et prend connaissance de l'autre (disons qu'il ya au moins un d'entre eux la compréhension des choses, créant ainsi la connaissance), il est pas vraiment utile de données preuve de discrimination, des informations et des connaissances à mieux comprendre ce que vous apprenez. Bien au contraire, la compréhension est faite lorsque des données, informations et connaissances à l'état fondu sont dans une soupe quand proportions sont bien définies. Données sans signification est inutile et le sens est dans un contexte. Le modèle sous-jacent de cette méthode est plus empruntée à psy- chologie cognitive: la communication et la compréhension sont faites par le partage et la création de représentations. Les représentations utiles pour identifier, structurer, validate, partager et leur dit fructifier « connaissance », et ils ne prennent que dans l'esprit. Dans cet article, la connaissance terme fait référence à une collection de - 402 - V. Besson et A. Berger représentations qui ont un sens et une valeur dans un champ d'intérêt donné. L'enregistrement de ces connaissances sur un papier ou dans une mémoire d'ordinateur fait de nouvelles « données », pas plus. La partie intéressante est lorsque l'ordinateur peut afficher toutes les représentations recueillies sous une forme qui donne d'autres représentations. L'émergence de connaissances à partir des connaissances recueillies peuvent se produire lorsque les gens voient les résultats. Par exemple, lorsque la visualisation des données peut être interprétée comme une carte des connaissances, il est possible d'évaluer rapidement la quantité de connaissances doit être appris dans un domaine d'intérêt vers un autre. Il y a des méta-connaissances car il y a des méta-données. Un exemple de cette méta-connaissances section décrit 5. L'objectif de la méthode MAKE est de donner une solution qui améliore la collecte de connaissances des petits groupes de connaissants, diffusion des connaissances aux plus grands groupes d'apprenants et de la croissance des connaissances. La méthode MAKE est entièrement décrite dans ArdansSas (2006b), et Berger et coton (2011b), cet article ne se concentrera pas sur mais en Ontario ses changements et évolutions. Quand vient le temps de partager, il est aujourd'hui difficile de concevoir une solution non orientée ordinateur. Mais quelle que soit la solution, un processus de conception pour créer ou configurer un outil selon un certain champ à l'intérieur d'un écosystème d'une entreprise nous conduit à données de connaissances de nierie. Dans cette méthode, la gestion de l'expertise et l'ingénierie des connaissances sont étroitement liées. 3 Matériaux de base Depuis 2006, deux objectifs principaux ont été identifiés lorsque la nécessité d'une étude de gestion de l'expertise se traduit: le stockage des connaissances et le partage des connaissances. Le stockage des connaissances est l'acte d'identification, de structuration, de validation et de connaissances sur l'enregistrement du cerveau. médias robustes doivent être étudiés pour résister à « l'érosion des données » (par exemple le fait que certains formats de fichiers deviennent trop vieux pour être lu par le logiciel en cours). En fait stocker connais- sances enregistre des représentations en des données de la manière que peop le à l'avenir pourrait créer des représentations aussi proches que celles initiales en lisant ces données. Le défi est d'obtenir les représentations les plus proches aussi vite que possible. Lors de la mémorisation des connaissances, les données resteront comme au cours des années. Le partage des connaissances est l'acte d'identification, de structuration, la validation des connaissances, d'initier une masse critique de contenu curated qui susciter l'intérêt et attirer les gens sur le terrain. La deuxième étape consiste à concevoir une solution pour partager les connaissances, pour former les gens à utiliser la solution, et pour rendre le groupe de connaissants et le groupe d'apprenants des connaissances en les faisant fructifier Interact. 3.1 Lors de la mémorisation des connaissances de connaissances Stockage semble apparaître quand beaucoup de connaissances ont été accumulées dans un certain domaine, mais ne sera pas plus utile à court ou à moyen terme. Mais pas utile à court terme ne signifie pas sans valeur à long terme. La connaissance peut être déclaré comme stratégique dans 10, 15 ans ou plus. le matériel et le format de stockage de données durable sont en effet très important. Mais sur la base des données stockées, sera la prochaine génération d'apprenants en mesure de faire de bonnes des représentations? Vocabulaire peut changer, les références aux technologies disparues peuvent se produire, etc. Une façon d'éviter tout malentendu autant que possible est de mettre la cohérence autant que possible. Cette question est abordée dans la section 4. L'idée est qu'un concept est moins difficile à comprendre s'il - 403 - Initier une mémoire d'entreprise avec un recueil de connaissances figure. 1 - Un schéma simple de ce qui est des connaissances, des représentations et des données dans MAKE. sont les relations expliquées avec d'autres concepts: un mot inconnu peut parfois être compris dans une phrase. Un autre scénario a été observé: une entreprise a un grand savoir-faire dans une tech- nologie très précis en raison des efforts de recherche et de développement qui lui sont données au cours des 10 dernières années, mais ne sait pas si le marché est encore prêt. Une stratégie peut consister à prendre le risque de la conception d'un nouveau produit basé sur cette technologie, mais un autre est d'attendre que le mouvement de concurrent. Si l'entreprise dispose d'indicateurs fiables sur les activités de son concurrent, il peut choisir de lancer la conception dès que le concurrent le fait, économiser beaucoup d'argent dans le marketing des études sans prendre aucun risque. Mais cela est seulement possible si l'entreprise peut rappeler très rapidement les connaissances stockées. Dans ce cas, le temps de récupération est la clé du succès. La connaissance et Structuration un bon outil pour y accéder rapidement semble être les principales solutions. Ils sont respectivement des sections discutées 4 et 5. Pour le plan de stockage, il faut deux matériaux de base: le groupe de connaissants et un gestionnaire de connaissances. Le groupe de connaissants est un petit groupe de personnes qui est appelé « personnes clés » dans un certain domaine. Il a été observé que ces personnes sont appelées des « experts » et ont parfois plus de 30 ans d'expérience dans le domaine. Ils sont généralement dans le niveau supérieur de la hiérarchie dans l'entreprise, mais il est en raison de leur expérience, non seulement leurs graduations. Leur expérience a évidemment été acquise sur un long travail dans la production ou la R & D. Le gestionnaire de connaissances est le deuxième volet pour démarrer un projet de gestion des connaissances. La tâche de gestionnaire de connaissances est de donner un coup de pouce à la boîte de dialogue avec le groupe des connaissants et les aider à verbeux leurs connaissances. Le gestionnaire de connaissances devra concevoir une planification avec les connaissants de mettre en réunion des séances d'entrevues. Certains de ces entretiens auront lieu avec tous les groupe de connaissants et d'autres auront lieu que face à face lorsque des sujets très spécifiques ont parlé. Les futurs groupe d'utilisateurs ne sait pas au moment de l'aide pour trouver la solution technique. Pour le stockage des plans, la solution est généralement un livre ou un site Web statique. - 404 - V. Besson et A. Berger Ces solutions sont exposées à la section 5. En général, le leader de ce genre de projet est rarement appelé « Knowledge Manager » s'il vient de l'intérieur du cabinet. 3.2 Lorsque le partage des connaissances Le besoin de partage des connaissances vient quand les entreprises veulent une mémoire vivante, con- traire au stockage p lan. Les aspects de l'enregistrement et la transmission sont en temps réel. La connaissance est qualifiée de « critique en ce moment » et pour l'avenir. De plus, la société est dans l'ambiance de la collecte et le partage des connaissances le long du chemin. Cela signifie que le groupe des apprenants et la solution technique sont beaucoup plus importants que dans le plan de stockage. La solution technique est exposée section 5. Le groupe de l'apprenant est le peuple qui devront étudier les données afin d'obtenir le connais- sances. Pour le plan de partage, il est très important de les connaître et de les faire participer à l'élaboration de la solution. Ils ne feront pas partie de la discussion entre les experts et le gestionnaire de connaissances, mais ils sont prêts à expliquer leurs besoins pour concevoir la meilleure interface entre les données recueillies et les utilisateurs finaux. Ils expliqueront ce qu'ils ont besoin d'apprendre le plus vite possible. Le groupe de connaissants est retenue et ont beaucoup d'expérience. Le groupe de l'apprenant est généralement plus grand, plus jeunes et ont beaucoup d'énergie pour la création. travail d'équipe lisse et la communication entre ces deux groupes non seulement améliorer le partage des connaissances, mais la production de connaissances et de l'innovation. 3.3 Conclusion Depuis 2006 deux concepts où consolidés: la disponibilité du groupe du Connaisseur et les compétences du gestionnaire de connaissances. La disponibilité de temps de groupe de propriétaires est l'une des clé de la réussite. Habituellement, les connaissants devront participer aux sessions du projet de gestion des connaissances ainsi que leur travail quotidien habituel. Si connaissants ne peuvent pas libérer assez de temps pour le projet, le projet ne peut être mené. Dans toutes les sociétés prospectées en dix ans pour des projets de gestion des connaissances, seule une proportion négligeable d'entre eux ont déjà ou sont prêts à avoir leur propre agers de connaissances ou à traction humaine ingénieurs de la connaissance, du moins en France. Habituellement, le Knowledge Manager est le gestionnaire de la qualité, le gestionnaire méthode, ou le responsable de l'amélioration continue. A l'intérieur, cette proportion mince, un seul d'entre eux (Commisariat à l'Energie Atomique et aux énergies alter- natifs) ont différents gestionnaires de connaissances qui ont des compétences précises selon le groupe des connaissants qu'ils auront à gérer. Ils sont des chercheurs qui ont un emploi à temps partiel du gestionnaire de connaissances. La plupart des entreprises prospectées n'ont pas gestionnaire de connaissances. Leur stratégie est de services de prêt de consultants lorsque le besoin identifié. Cela signifie qu'il est très rare que la société en question recevra un gestionnaire de connaissances avec des compétences dans le domaine à partager ou stockés. De plus, tous les projets de gestion des connaissances des dix dernières années concernées partage ou le stockage des connaissances très précises, mais pas des connaissances générales dans un grand champ. Il est évidemment préférable d'avoir un gestionnaire de connaissances qui est un expert dans le sujet, mais il est impossible d'avoir autant de gestionnaires de connaissances que de nombreux domaines les entreprises devront créer un projet dans. - 405 - Pour lancer une mémoire d'entreprise avec un recueil de connaissances Ces observations ont confirmé que les gestionnaires de la connaissance devraient avoir un haut degré dans la science ou la technologie pour comprendre les connaissants, mais ils doivent avoir des compétences axées sur l'homme dans l'organisation et la gestion; « compétences humaines » sont obligatoires pour créer une bonne dynamique de groupe et un sens de l'engagement. Les projets de gestion des connaissances dans des domaines non techniques ou scientifiques sont très rares par rapport aux anciens. Last but not least, la moitié des projets de gestion des connaissances ont été marqués « confidentiel » par les industriels au cours des dix dernières années. Le succès d'un projet de gestion des connaissances entre les deux industriels commence par ces trois mots-clés: la compréhension, la confiance et la mobilisation. Procédé 4 Dans la deuxième section a été définie la notion de gestion de expertize, qui est plus précise que la gestion des connaissances dans le paradigme de la méthode Ardans. Soit dit en passant, « gestion des connaissances » peut être utilisé comme « gestion des compétences » dans cet article. La capitalisation des connaissances est l'épine dorsale d'un projet de gestion de expertize. connaissance capitalizat ion est l'activité de la compréhension, l'appropriation et la reformulation de la connaissance donnée par le Connaisseur au gestionnaire de connaissances. Ardans SAS fournit trois consultants KM uniquement dédiés à diriger des projets de capitalisation expertize. Ces projets impliquent d'un à dix experts ou connaissants et se concentrer sur un domaine scientifique ou technique spécifique d'intérêt. Ardans a également conçu un outil de technologie de l'information utilisée par ses consultants pour modéliser et conserver ce qu'ils ont appris des experts. Ce même outil peut être utilisé pour maintenir les informations stockées à un niveau mis à jour au cours des années à travers une interface web. Cet outil a été conçu il y a plus de dix ans et encore en cours de modification. Une équipe de programmeurs est au courant des besoins et des commentaires du Consultan. Un à un projet (sur consultant et un expert) est généralement conçu pour être fermé en trois ou quatre mois. Les experts expliquent les connaissances à tirer profit au cours des sessions demi jours. La question du partage des connaissances ou le stockage des connaissances peut être considérée comme produisant ex pertise ou le maintien expertize. Expertize est produit lorsque de plus en plus d'apprenants deviennent connaissants, ou lorsque les données recueillies croît avec l'hypothèse de cette dis les données sont lues et compris (cela signifie que les apprenants continuent d'apprendre). Expertize est maintenue lorsqu'au moins toutes les données enregistrées peuvent être reproduites par au moins une knower. la sauvegarde des données ne suit pas le savoir. Les données enregistrées doivent être obtenu « hors du réfrigérateur » une fois de temps pour être sûr qu'il y ait au moins un cerveau qui maintient la connaissance. On peut en conclure que si les connaissances sont partagées en permanence, il est enregistré. Les solutions techniques utilisées pour partager les connaissances sont aussi importantes que les outils utilisés pour stocker et afficher les données. Ardans ne fournit pas des systèmes experts. 4.1 Identification des connaissances La première rencontre entre le gestionnaire de connaissances et l'équipe de connaissants est de définir la limite de l'étude en dénombrant sujets. Les sujets vont générer des éléments de connaissance, les liens entre ces éléments, mots-clés, la hiérarchie des mots-clés et des liens entre les éléments de connaissances et mots-clés. La visualisation de ces éléments identifiés dans les grappes peut aider beaucoup l'organisation du projet. Certains groupes peuvent être affectés à certaines personnes et leur étude peuvent être commandés à temps pour optimiser la gestion du temps. - 406 - V. Besson et A. Berger figure. 2 - Le cycle Omega comme expliqué dans ArdansSas (2006b). Il est l'épine dorsale de la méthode MAKE. 4.2 Quelle que soit la connaissance Structuration la stratégie choisie et les solutions techniques peuvent être, il semble que le principal défi est de concevoir des modèles qui peuvent aider à structurer le Connaisseur ses pensées, et le gestionnaire de connais- sances de comprendre et de les organiser en plusieurs parties. Ces parties appelées « connaissances », élé- ments ou des articles, peuvent parfois être trouvés sur Internet, dans l'intranet de l'entreprise ou même dans une bibliothèque. Ce qui ne peut être trouvé est les liens entre les éléments de connaissances, liens du sens et de fournir beaucoup de connaissances et de compréhension par eux-mêmes. Le réseau de liens entre les éléments est nommé net « Fi-Fi » (du mot français fiche que moyen article). La règle est de mettre en place moins de liens que possible et de ne conserver que ceux qui semble être obligatoire. peut être utilisé la même stratégie lors de la création du réseau « Fi-V » (V en français vues), le filet entre les éléments de connaissance et la hiérarchie des mots clés, comme décrit par Berger et coton (2011a). Quelle quantité de données (nombre de lignes par exemple) peuvent être stockées dans un élément de connaissance ne suit aucune règle stricte. Cela dépend du sujet de l'article, sur le terrain et sur le Feel- ment sur ce qui est juste nécessaire pour comprendre et ce qui peut être mis dans un autre article. Si le gestionnaire de connaissances ne sait pas si un article est trop long, il est admis que si elle prend plus d'une page lors de l'impression, il est pas à portée de main. La même stratégie est utilisée pour évaluer le nombre d'éléments de connaissances pour créer. La règle est de créer autant moins des éléments de connaissances que possible sans obtenir les trop grands à la fin du processus. La stratégie de conception du filet, le poids des articles, les articles cycle de vie « Fi-Fi » et leur nombre relatif n'a jamais été remis en question. En revanche, trois autres aspects de la Structuration de la connaissance ont été interrogés depuis 2006: - 407 - Pour lancer une mémoire d'entreprise avec une figure de recueil des connaissances. 3 - Structuration des connaissances dans MAKE et AKM. le nombre de différents articles de Structuration, des articles Structuration et le nombre de mots clés qui sont utilisés pour des articles de référence. Ces mots clés sont nécessaires pour établir un langage, un vocabulaire et une grammaire de base (hiérarchie). Ils sont de plus en plus utilisés comme clés de recherche efficaces. Les différents types ou classes de connaissances sont utilisées pour classer les éléments de connaissance. Leur but est d'aider le gestionnaire de connaissances et le Connaisseur de formaliser les idées et de garder une sorte de schéma sur la façon d'écrire les connaissances. Le gestionnaire de connaissances va alors créer un article d'une classe et cet article héritera ses Structuration de cette classe. Il va alors écrire les données de la connaissance qu'il a acquis de l'Omniscient. Dans Mariot et al. (2007), beaucoup de classes par défaut (parfois appelés modèles articles) ont été fixés. Aujourd'hui, cette stratégie n'est plus utilisé et plus flexible est préféré: la création de modèles moins d'articles que possible et essayer de commencer par les classes qui ont prouvé leur utilité. Les cinq classes qui sont le plus souvent utilisées sont fondamentales, processus, Feedback, solution technique et document. classe de document est uniquement utilisé pour décrire des références (par exemple de livres, articles scientifiques) afin de rendre une bibliographie et de lier certains principes de base à leurs justifications. classe fondamentale est utilisée par exemple pour expliquer les connaissances théoriques minimales pour comprendre les principes de fonctionnement de base de certaines solutions techniques, ou de comprendre un champ théorique donné. Les classes diffèrent au moins les uns des autres dans leur Structuration. Une classe peut contenir des champs qui sont classés et ont un but. Par exemple, une classe peut avoir la turation struc- suivante: Titre, du corps, et la section Liens pour relier quelques références. On a constaté que la conception de modèles avec trop de champs est trop compliqué pour le gestionnaire des connaissances pour écrire les pensées et pour les apprenants à comprendre l'élément de connaissance. Parfois, une idées ne peuvent pas être - 408 - V. Besson et A. Berger tronqués de la même manière. Par exemple, dans Mariot et al. (2007), le modèle de référence a 9 champs avec est maintenant considéré comme inutilisable. En revanche, une exception est le modèle de processus qui aide beaucoup Structuration aux idées d'ordre: Titre, entrée, transformation et production. Au détriment d'un moins grand nombre de classes, il a été observé la nécessité d'avoir un des mots clés assez riche arbre aux articles preuve de discrimination. En fait, les modèles d'articles doivent être aussi permanente que possible afin d'éviter les modifications des articles déjà créés en raison de la modification de leur modèle. Les liens entre les articles et les mots-clés peuvent évoluer si l'arbre évolue. Cela est souvent le cas lors de la phase de capitalisation. Un nouveau concept a augmenté depuis 2006 qui aide beaucoup le processus de capitalisation: une vision holistique de « Fi-Fi ». Ce point de vue est un graphique qui fournit en un seul coup la structure entière d'une base de données. Il est genarated par le programme GEPHI Bastian et al. (2009). 4.3 validation des connaissances Chaque élément de connaissance doit être validée par au moins un Omniscient. Cela peut être clouée par l'état du flux de travail. Rien n'a changé depuis 2006 et un flux de travail attaché à un article donné a été remarqué comme l'un des plus important outil pour la validation des connaissances et même la gestion des articles. Lorsque la capitalisation est faite avec plus d'un Connaisseur, la stratégie de validation semble être l'un des éléments que la taille du projet de calendrier de. Voici un scénario de capitalisation de base: la plupart des articles sont créés le plus tôt possible (mais laissé vide) afin de partager le vol- ume pari de travail ok Ween. connaissants Chaque Connaisseur travaillera sur ces articles avec le gestionnaire de connaissances et que l'ancien pense qu'il a atteint un niveau approprié de maturité, l'article est soumis à l'approbation du Connaisseur. Frome ici, deux stratégies peuvent être choisies: la validation de la population des articles est partagée par le nombre de connaissants, ou une politique de validation croisée peut être effectuée. Pour une politique de validation croisée, l'article est d'abord validé par le Connaisseur qui a travaillé le plus sur, puis donné aux autres connaissants pour un contrôle supplémentaire. Les dix dernières années, il a été observé que plusieurs experts capitalisations est très apprécié par les connaisseurs comme ils veulent que leurs collègues de vérifier, d'apprécier et de terminer leur travail. Cette stratégie renforce la collaboration entre les personnes et génère beaucoup de discussions entre connaissants. Le deuxième point est que l'on peut supposer qu'une qualité de validation multi-expert est préférable qu'une validation unique expert. Le principal inconvénient est que cela prend deux fois plus de temps pour valider s'il y a deux connaisseurs, triples temps s'il y a trois knowers et ainsi de suite. Cela génère un impact significatif sur le calendrier par rapport au cycle de capitalisation « pure » elle-même. Ainsi, la stratégie de validation doit être discuté lorsque le projet est conçu. 5 Produits Le but de tout projet de gestion des connaissances est de créer connaissants. Le modèle est basé sur l'hypothèse que les transits de connaissances et de Connaisseur à un apprenant, à travers une solution tech- nique qui est le vecteur de la connaissance. Ces projets peuvent être comparés que les connaissances de transmission écrire une histoire. Connaissants raconte l'histoire, le gestionnaire de connaissances fait le plan, les chapitres, demande les images, et l'écrit. Ensuite, pour Ardans le projet est généralement plus, sauf pour les entreprises qui veulent continuer de capitaliser au fil du temps. Ardans ne forme pas les apprenants. - 409 - Pour lancer une mémoire d'entreprise avec un recueil de connaissances Compte tenu du niveau élevé de connaissances nécessaires pour comprendre ce qui a été généralement capitalisé, la solution ne se fait pas à être formé, mais fait pour être une référence. Les apprenants ne peuvent pas l'utiliser sur une base quotidienne, mais quand ils ont besoin des réponses précises à leur question dans un certain domaine. L'accès se fait par les apprenants en période d'apprentissage intense, comme une nouvelle conception de produits, par exemple. Cette solution technique est si important pour le succès de ce genre de projet que l'on peut appeler un produit. Le consultant se concentrera sur la conception de ce produit. Une fois que le produit est fabriqué, l'entreprise peut stocker ou partager. L'équipe Ardans construit la solution, mais ne gêne pas combien de temps il sera stocké ou comment les utilisateurs seront gérés à l'utiliser. Les gestionnaires du savoir initier le processus de capitalisation et de fournir la base de données. Les ressources humaines et les technologies de l'information des équipes de l'entreprise doivent appliquer la meilleure politique pour promouvoir ce qui a été construit. La solution est conçue pour éviter toute période de formation pour pouvoir l'utiliser. 5.1 Solution de stockage Plan La génération d'un livre de connaissances traditionnelles (livre papier ou livre électronique en format imprimable) semble rester en faveur des solutions web hypertexte. Ces sites statiques sont souvent écrits en HTML pur pour améliorer l'érosion des logiciels diminuant vie. Habituellement, cette solution ne coûte rien par rapport à celui proposé pour la solution de partage des connaissances. Cependant, il a été observé que Industrials a créé une utilisation émergente de cette solution qui est un mélange de plan de stockage et de régime de participation. Leur stratégie peut être expliquée par le scénario sui- vants: ils lancent un projet de l'année y capitalisation et un site Web de la connaissance est créé sur le thème choisi. Ils demandent à leurs élèves d'utiliser le site, autant que possible, afin de sauver la connaissance. Année y + n (n est compris entre 3 à 10 ans), ils demandent une mise à jour de ce site la commande d'un nouveau projet de capitalisation sur le même sujet. Cette solution leur donne plus de flexibilité dans la gestion de leur portefeuille. L'inconvénient est que les apprenants ne sont pas formés de manière dynamique, ce qui implique tha t aucune connaissance est créée et partagée à travers la solution. Si la connaissance est créée, les données ne sont pas mis à jour dans la solution donc il n'y a pas de visibilité de toute évolution. 5.2 solution de partage le plan Cette solution est techniquement beaucoup concurrentiel et coûte généralement plus. L'une des Ardans est décrite dans ArdansSas (2006a) et consiste en une interface web qui peut être affiché sur un ordinateur, relié à une base de données qui stocke les données. Cette application Web prend en charge l'ensemble du projet de gestion des connaissances, de la phase de capitalisation à la phase d'apprentissage, y compris les mises à jour dynamiques, et sur la validation des flux. Ce genre de solutions faire en sorte que la connaissance est enregistrée, de nouvelles connaissances peut être créé, écrit et validé, puis enregistré à nouveau en partage et d'apprentissage. La structure principale de cette solution n'a jamais été mise en doute. Cependant, quelques nouvelles tionalities fonc- ont été ajoutés. L'interface a été dupliqué pour offrir un simple et un plus riche respectivement nommé « Recherche simple » et « Recherche avancée ». L'interface simple peut être utilisé par des personnes non formées et ont les fonctionnalités de base. Cela donne plus de flexibilité aux industriels. La visualisation de données holistique est intégrée à la solution. Pour l'instant, il est possible d'exporter un filet d'articles choisis (articles et leur « Fi-Fi » net) avec leurs mots clés ou non (net « Fi-V ») associées dans un fichier texte formaté graphml. Ces dernières années, la visualisation des données - 410 - V. Besson et A. Berger pour la représentation holistique du savoir ont prouvé son utilité pour l'identification des connaissances (début et mi-vie du projet), la cartographie des connaissances pour l'orientation stratégique (fin du projet). Lorsque les données reflétant la connaissance est représenté visuellement dans son ensemble, de manière compréhensible, les industriels qualifient de connaissances acquises contrôlée. La vue holistique donner aux industriels un outil pour conduire leurs politiques de transfert de connaissances et d'alimentation de la base de données. 6 Conclusion La méthode a prouvé sa robustesse et rien n'a changé de façon spectaculaire depuis 2006. Une dynamique collective des différents groupes doivent être pris en charge par le gestionnaire de connaissances. Seule une simplification des structures des articles a été réduit en complexité. Les solutions techniques ont été achevées. Les nouvelles fonctionnalités correspondent aux nouveaux usages (comprimés et différents types de tailles d'écran) et des représentations graphiques globales sont nées grâce à des logiciels comme GEPHI. Ce logiciel est présenté par Bastian et al. (2009). 7 Discussion Les changements apportés à la méthode permet une meilleure adapter la demande industrielle et les contraintes de coûts et de temps. Mais le souffre « fonction de remise en forme » d'un manque de visibilité: bien que les gains de coûts et de temps peuvent être mesurés, il est impossible d'évaluer pour les industriels scientifiquement si une solution est tout à fait mieux que le précédent. Il est impossible de définir deux séries de projets sur lesquels deux méthodes différentes sont appliquées et mesurer avec précision les différences avec les statistiques tielles inferen-. Les projets sont donné un à la fois, et ils ont donc des différences parfois trop, il est impossible de les comparer sur la variable relativement indépendante. Pour l'instant, les seules évaluations sont le retour sur investissement moyen au fil des ans et la confiance du gestionnaire de connaissances qui peuvent proposer des solutions qui semble mieux correspondre la demande et les produits existants, à leur point de vue. Références ArdansSas (2006a). Ardans Knowledge Maker: introduction, Principes et philosophie implan- Dans tes de gestion environnement this des connaissances heuristiques. Article de technique de synthèse, Ardans (www.ardans.fr/doc/AST2006-354%20NKM%20AKM%20v1_3.pdf). ArdansSas (2006b). Ardans faire: méthode d'de la mémoire 'élaboration en continu collective. Article de technique de synthèse, Ardans (http: //www.ardans.fr/). Bastian, M., S. Heymann, et M. Jacomy (2009). Gephi: Un logiciel open source pour explorer et manipuler des réseaux. Berger, A. et J. Cotton (2011a). Construire une mémoire collective de l'entreprise: la gestion des connaissan bureaux. Bulletin de l'AFIA (n ° 72), 70-73. Berger, A. et P. Cotton, JP. & Mariot (2011b). Au Début du Accompagner 21ème siècle les organisations Dans la mise en place d'Une gestion des connaissances heuristiques: retour d'expérience. - 411 - Pour lancer une mémoire d'entreprise avec un recueil de connaissances Berger, A. et C. Mariot, P. & Coppens (2007). Faire vivre un métier Dans l'Référentiel indus- trie: le Système de gestion de connaissances heuristiques icare. Louis, A. (2012). Alpin, demande Une collaboration de partage de connaissances heuristiques, des techniques de Autour des documents de l'ingénierie clés d'air génie liquide. Mariot, P., G. C., et A. Cotton, JP. & Berger (2007). Méthode, et modèle de capitalisation Ardans outil des connaissances heuristiques. Verhagen, CJM. & Curran, R. (2010). modélisation ontologique du domaine de la fabrication des composites de l'aéronautique. Méthode Ardans ArdansSas CV (2006b) et ArdansSas technologiques (2006a) des connaissances nagement ma- et surtout dans la capitalisation d'expertise ont mis beaucoup de commentaires au fil du temps. Dans une perspective d'amélioration continue, quels sont les impacts de ces rétroactions sur la méthode de nos jours? Mettre en pratique dans l'industrie, le retour de l'investissement d'une campagne de capitalisation est déduite de la qualité de la base de connaissances délivrée à la fin de la campagne. Par conséquent, la méthode et la technologie sont intrinsèquement liées. Comment les outils informatiques peuvent aider au diagnostic de la qualité de la base de connaissances? Une étude comparative a été réalisée sur la base de la méthode Mariot et al. (2007) exposée à EGC'2007. Cet article présente les résultats des changements et des améliorations de la méthode, en collaboration avec le dernier développement technique et scientifique d'une part, et le changement des besoins de l'industrie d'autre part. - 412 - G - Session Initialiser Avec un Industrielle d'expertise Une recueil d'entreprise mémoire: de retour Dix Années d'expérience de la Ardans Vincent Besson méthode, Alain Berger, Jean-Pierre Cotton, Aline Belloni, François Vexler"
279,Revue des Nouvelles Technologies de l'Information,EGC,2015,Towards Linked Data Extraction From Tweets,"Millions of Twitter users post messages every day to communicate with other users in real time information about events that occur in their environment. Most of the studies on the content of tweets have focused on the detection of emerging topics. However, to the best of our knowledge, no approach has been proposed to create a knowledge base and enrich it automatically with information coming from tweets. The solution that we propose is composed of four main phases: topic identification, tweets classification, automatic summarization and creation of an RDF triplestore. The proposed approach is implemented in a system covering the entire sequence of processing steps from the collection of tweets written in English language (based on both trusted and crowd sources) to the creation of an RDF dataset anchored in DBpedia's namespace.","Manel Achichi, Zohra Bellahsene, Dino Ienco, Konstantin Todorov",http://editions-rnti.fr/render_pdf.php?p1&p=1002100,http://editions-rnti.fr/render_pdf.php?p=1002100,en,"Vers l'extraction de données liés De Tweets Manel Achichi *, * Dino Zohra Bellahsene Ienco **, Konstantin Todorov * * Université Montpellier 2, LIRMM firstname.lastname@lirmm.fr, ** Irstea Montpellier, UMR TETIS dino.ienco@teledetection.fr Résumé . Des millions d'utilisateurs de Twitter postent des messages tous les jours pour communiquer avec d'autres utilisateurs en matière d'information en temps réel sur les événements qui se produisent dans leur environ- nement. La plupart des études sur le contenu des tweets ont mis l'accent sur la détec- tion des sujets émergents. Cependant, au mieux de notre connaissance, aucune approche a été proposé de créer une base de connaissances et enrichir automatiquement informa- tions provenant de tweets. La solution que nous proposons est composé de quatre phases principales: identification, classification sujet de tweets, tion automatique summariza- et création d'un triplestore RDF. L'approche proposée est mise en œuvre dans un système couvrant toute la séquence de traitement étapes de la collection de tweets écrits en langue anglaise (basée à la fois confiance et les sources de la foule) à la création d'un ensemble de données RDF ancré dans l'espace de noms de DBpedia. 1 Introduction L'un des objectifs du Linked Open Data (LOD) initiative est de la structure et des données d'interconnexion sur le Web en utilisant les technologies du web sémantique, comme la description des ressources travail DE CADRE (RDF), prenant ainsi le web d'aujourd'hui jusqu'à un nouveau niveau où les deux données sont interprétables et accessibles par les humains et les machines (Bizer et al., 2009). Un effort considérable a été fait dans cette direction tout au long des deux dernières années. Cependant, de nombreuses sources d'informations précieuses sur le Web restent encore inexplorées, bien qu'ils contiennent des données utiles qui peuvent être bénéfiques pour le projet LOD. Dans cet article, nous nous concentrons sur le support social Twitter qui fournit une plate-forme pour la publication de messages courts (tweets) de longueur maximale de 140 caractères. Le réseau a connu une popularité croissante à travers les dernières années, devenant une source importante de nouvelles informations sur de nombreux événements importants, mis à disposition en temps réel, souvent même avant sa diffusion à travers les canaux traditionnels de radiodiffusion. La principale motiva- tion de notre travail est de permettre l'intégration des informations qui circulent tous les jours à travers le flux Twitter sur le Web des données. Nous vous proposons une méthode pour l'extraction des données pertinentes de Tweets, leur conversion en RDF et leur stockage dans un triplestore RDF avec l'objectif final de leur publication sous forme de données ouvertes liées. Notre approche couvre toute la chaîne de traitement suite à un flux de travail bien défini, qui sera présenté dans la section 3. - 383 - vers l'extraction de données liées De Tweets 2 Travaux connexes Une grande famille d'approches connexes se concentre sur l'extraction de la relation du texte, divisé en les basé sur la dépendance et celles basées sur l'analyse syntaxique. On peut citer des systèmes tels que la réverbération ou le dePoe multilingue de ce dernier groupe et CLAUSIE (Corro et Gemulla, 2013) (Fader et al., 2011) (Gamallo et al, 2012). - de l'ancien. Le OLLIE système (Schmitz et al., 2012) est basée sur des modèles de relation qui sont extraits par réverbération. Il existe deux grandes catégories d'approches pour extraire triplets RDF à partir du texte. La première catégorie exploite les connaissances de base de déduire des faits nouveaux de texte. Un exemple est la méthode proposée dans (Anantharangachar et al., 2013), où triplets RDF sont extraits à l'aide de dictionnaires spécifiques de domaine induits par une ontologie existante. Les méthodes appartenant à la deuxième catégorie appliquent généralement une analyse sémantique du texte (Exner et Nugues, 2012; Augenstein et al, 2012;.. Cattoni et al, 2012). Dans (Cattoni et al., 2012), une infrastructure à grande échelle à des ressources multimédias magasin et Interlink est présenté. Le système est capable de savoir l'importation et annoter sous forme de RDF, associant automatiquement les ressources à des entités et la création de nouvelles connaissances sous la forme de triplets RDF. Une particularité de ce système est l'association des informations de contexte à chaque resou géré rce. On distingue deux approches de tweets summarization - le premier VIDES pro- groupe un sac de termes comme un résumé, tandis que le second extraits représentatifs (Cataldi et al, 2013;; Benhardus et Kalita, 2013 ioudakis Math- et Koudas, 2010.) tweets. Les approches du second groupe sont plus appropriés pour nous, car ils conservent une sorte de cohérence structurelle et de concision dans le résumé dérivé. Dans (Sharifi et al., 2010), un ensemble de tweets est résumée par une phrase dérivée par une représentation graphique des mots (co) se produisent dans une collection de tweets. (Chua et Asur, 2013) récupérer le plus de tweets pertinents comme le résumé d'un tweets col- lection, basée sur des modèles thématiques. Deux autres techniques sont proposées dans (Olariu, 2013). Le premier premier se confond tous les tweets dans un graphe de mots (de façon similaire à (Chua et Asur, 2013)) et calcule ensuite une fonction de pointage pour sélectionner un chemin du graphique comme un résumé possible. Le second sélectionne les mots les plus fréquents suite (phrase). 3: vue d'ensemble de l'approche Notre chaîne de traitement couvre l'ensemble du processus de la constitution d'un corpus Twitter pour la génération d'un triplestore de RDF (Figure 1). Constituer un corpus de tweets. Tweets écrits en anglais sont d'abord recueillies auprès de sources de confiance - comptes Twitter des médias établis (par exemple, la BBC). Ces tweets sont ensuite regroupés le sujet sage, en utilisant des techniques d'identification des sujets, formant des groupes homogènes. Nous avons choisi les K-means algorithme des pour sa simplicité et son efficacité. Nous avons mis en K à la valeur la plus faible qui maintient le rapport {distance moyenne / moyenne distance inter-Clusters intra-groupe} sous un seuil donné. De plus, pour chaque sujet, nous collectons tweets bondé de comptes utilisateurs ordinaires, qui sont utilisés pour enrichir l'information contenue dans tous les sujets. Enfin, nous ne gardons que les tweets étroitement liés aux sujets que nous voulons représenter, à savoir, les tweets contenant des mots-clés extraits précédemment (le plus de mots fréquents dans chaque groupe). Filtrage et pré-traitement. Un élément de filtrage est appliquée à la fois fiable et sources de foule. L'objectif est de ne garder que les tweets qui contiennent des entités nommées, et en particulier ceux qui ont DBpedia URIs, en préparation de l'enchaînement des (pas encore) extraites triplets RDF à - 384 - M. Achichi et al. FIGUE. 1 - Le flux de travail du système. le web des données. Les tweets sont bondées en plus filtrés par rapport à leur adéquation à un sujet donné à l'aide d'un ensemble de mots clés décrivant chaque sujet. De plus, tous les tweets sont prétraitées en supprimant hashtags, urls et retweets et par lemmatizing le texte. Classification des tweets bondés. Nous avons introduit un module de classification texte, basé sur le principe de l'analyse des sentiments, afin de classer les tweets entassés dans deux catégories: (i) fait, y compris une information neutre et objective que nous voulons garder et utiliser pour enrichir les tweets de confiance, et (ii) les autres, y compris l'opinion, messages privés, etc. Nous avons utilisé l'outil coreNLP. Ici, nous partons du principe que d'un tweet qui transmet des informations factuelles (ou un morceau de nouvelles) est dépouillé à la fois un sentiment positif et négatif. Cependant, l'évaluation de la polarité d'un tweet n'a pas été suffisante pour juger de son objectivité. Comme critères de classification supplémentaires, nous vérifions l'absence de l'ensemble des caractéristiques suivantes dans chaque tweet: (i) le symbole « @ » indiquant un message privé, (ii) des lettres répétitives ou abus de ponctuation, (iii) les smileys qui reflètent une mindstate, et (iv) mal orthographié mots. Résumé automatique. Après un corpus qui, soit présentent tweets par sujet, nous procédons à générer un résumé de ce corpus, afin d'éliminer les informations redondantes et inutiles. L'algorithme que nous avons développé génère automatiquement un résumé d'un sujet sous la forme d'un ensemble cohérent et concis de tweets. Il faut deux paramètres en entrée: un ensemble de tweets correspondant à un sujet et un seuil de similarité tweets de commande (σ). Tout d'abord, l'algorithme construit une Undir pondérée ète graphique (G) lorsque les sommets sont les tweets et un bord existe entre deux sommets si la similarité du cosinus entre les deux tweets correspondant est - 385 - Vers lié Extraction des données à partir Tweets supérieure à σ. De plus, l'algorithme passe pour extraire les cliques maximales de G, en se fondant sur l'algorithme de Bron-Kerbosch bien connu. L'hypothèse ici est qu'une clique maximale représente un groupe d'informations potentiellement redondantes qui peut être représenté par un seul tweet. L'algorithme PageRank est ensuite utilisé pour attribuer aux scores des sommets au sein de chaque clique et sélectionnez celui avec le score le plus élevé comme un résumé de la clique. Si nous avons plusieurs tweets avec le même score, l'algorithme sélectionne la plus longue. Le résumé du sujet entier est donnée par l'ensemble des tweets représentant chaque clique maximale dans le graphique sujet. De tweets à RDF. Enfin, la dernière étape consiste à transformer chaque résumé en un graphe RDF. Nous avons été inspirés par l'état de l'art approche à l'extraction RDF du texte, LODifier (Augenstein et al., 2012), car elle repose sur l'idée d'ancrer les informations sémantiques extraites dans l'espace de noms de DBpedia. Notre approche consiste en quatre étapes principales: (1) L'analyse sémantique. La sémantique de chaque phrase est modélisé comme un (ensemble de) triple (s) du type <sujet, verbe, objet>. L'algorithme donné (Rusu et al., 2007) a été adoptée et adaptée en raison de son efficacité combinée à la Stanford Parser. Cet algorithme fonctionne bien avec des phrases simple clause simple mais ne parvient pas à gérer correctement des phrases de plusieurs clauses. Nous vous proposons l'algorithme itératif clause de fractionnement suivant: (1) Appliquer la dépendance analyse sur l'expression complexe. (2) Pour chaque relation de dépendance de type « nsubj » (sujet nominal), récupérer toutes les relations de ses arguments, à l'exception des relations de type « nsubj ». (3) Répéter (2) pour chaque relation récupéré jusqu'à ce qu'aucune nouvelle relation est ajoutée. (4) Pour toutes les dépendances enveloppées par une relation de type « nsubj », extrait et organiser tous les mots dans l'ordre croissant des numéros qui leur sont associés. Ces chiffres indiquent l'ordre des mots dans la phrase originale. (2) Disambiguation. Avant d'attribuer un URI DBpedia à un mot, nous choisissons son sens le plus approprié dans un contexte donné. Nous avons appliqué une méthode couramment utilisée basée sur l'identification des synset dans WordNet. (3) Affectation URI DBpedia. Nous attribuons à chaque terme son URI DBpedia correspondant. (4) génération d'un graphe RDF. Au cours de l'analyse sémantique, les phrases ont été composées de- aux clauses simples et chaque clause a été structurée comme un triple du genre <sujet, verbe, objet>. Dans cette dernière étape, le système effectue une conversion de ces triplets dans un graphe RDF. Les arguments créés au cours de l'analyse sémantique sont analysées pour identifier les tities en- nommées qui correspondent aux sujets de RDF et des objets et attribuer à chacun d'entre eux l'URI DBpedia correspondant. Pour ce faire, nous avons adapté au sujet et l'objet des triplets extraits des triplets existant dans le graphique DBpedia. 4 Prototypage et expériences sur réel Twitter données Nous utilisons la Twitter4J 1 API aux tweets virés de flux, le TextRazor 2 services aux entités Recon- de nize et les affecter successivement une URL Wikipedia, et la bibliothèque CoreNLP Stanford pour traiter le traitement et le sentiment du langage naturel une analyse. La collection de tweets de confiance est obtenue par exploration du média social le 9 Octobre 2014, une période de 24 heures. Nous avons suivi les comptes de BBC World, CNN, New York Times, New York Times du monde, et Briser News.We gardé ces tweets qui contiennent au moins une entité nommée qui correspond à un URI DBpedia. Cette première collection est composée de 1. http://twitter4j.org/en/index.html 2. https://www.textrazor.com/ - 386 - M. Achichi et al. 125 tweets. La collection de tweets bondés est récupéré le 10 Octobre 2014 en tenant compte des mots-clés extraits des tweets de confiance. Dans notre jeu de données, nous avons pu détecter 50 dif sujets férents des tweets de confiance. Voici un exemple d'un sujet composé de 5 tweets: {(1) Natation a fait Michael Phelps un athlète dominant, mais il ne pouvait pas le guider en dehors de la piscine; (2) USA Swimming annonce une suspension de 6 mois pour Michael Phelps après l'arrestation conduite avec facultés affaiblies; (3) Michael Phelps suspendu par USA Swimming; (4) Michael Phelps a reçu une suspension de six mois à compter de la natation; (5) La nageuse Michael Phelps suspendu pour six mois.}. Pour cet exemple, nous avons récupéré 1 325 messages courts provenant de sources bondées en utilisant des mots-clés de tweets de confiance. Successivement, nous classons les tweets comme nouvelles ou d'autres en appliquant les règles de classification présentés à la section 3. Le tableau 1 présente les résultats de la classification. Nous obtenons une précision de 76,22%. Nous analysons également le comportement de notre méthode en ne considérant que les nouvelles de classe. À cette fin, nous calculons de précision, rappel et F-mesure pour cette classe obtenir, respectivement, 0,628, 0,722 et 0,672. Cette évaluation souligne la qualité de notre stratégie et montre que notre méthode est capable de détecter environ 2 sur 3 tweets contenant des informations factuelles. Nouvelles Classe prédites Autres biens de classe Nouvelles 323 124Other 191 687 TAB. 1 - Matrice de confusion obtenue par notre stratégie de classement sur les tweets collectés. Sujet prédicats objet Michael Phelps Michael Phelps athlète être reçu suspension Michael Phelps Michael Phelps Baltimore arrêter la boisson d'arrêt Michael Phelps Michael Phelps prendre pause suspends la natation TAB. 2 - Des exemples de triplets RDF auto- matiquement extrait par notre cadre. Nous notons que les faibles valeurs (entre 0,3 et 0,5) du paramètre correspondent de σ à un nombre élevé de cliques maximales, alors que la tendance des valeurs comprises entre 0,5 et 0,95 est tout à fait stable. Dans le tableau 2, nous montrons plusieurs triplets produits par notre méthode avec σ égal à 0,65. Ces triplets représentent des informations qui est induite automatiquement par l'ensemble des tweets sur le sujet Michael Phelps. Au total, 10 triplets ont été extraits dont 4 triplets sont identiques. Le petit nombre de triplets extraits est dû au fait que la collecte des tweets a été réalisée pendant 24 heures seulement. Tous les triplets RDF sont ensuite stockés dans un magasin triple 3, prêt à être publié et reliés entre eux sur le web des données. 5 Conclusion L'approche que nous proposons est complète - nous prenons en entrée le flux hétérogène de tweets circulant tous les jours à travers le milieu social et nous affichons un triplestore RDF ne contenant que des informations factuelles sur les entités qui vivent dans l'espace de noms de DBpedia. Parmi les contributions originales de notre cadre, nous soulignons: i) l'utilisation de deux principales sources de données - confiance (tweets provenant des médias traditionnels établis) et la foule (tweets des utilisateurs ordinaires). Cette dernière source est utilisée pour enrichir les données recueillies auprès de l'ancien. Afin de limiter la redondance 3. Virtuoso, http://virtuoso.openlinksw.com/ - 387 - Vers Linked Extraction de données De Tweets dans les informations recueillies et préparer le corpus de tweets pour la phase d'extraction triplets RDF, ii) nous introduisons un nouvelle approche pour générer automatiquement un résumé d'un ensemble de tweets, correspondant à un sujet donné. Comme les résumés extraits peuvent être riches et structurellement complexes, iii) nous concevons une stratégie de diviser les tweets en plusieurs morceaux simples clause d'information afin d'identifier facilement les composants d'un triple RDF. Enfin, toute l'approche est mise en œuvre dans un prototype modulaire. Références Anantharangachar, R., S. Ramani et S. Rajagopalan (2013). Ontologie extraction de l'information guidée de texte non structuré. IJVeST 4 (1), 19. Augenstein, I., S. Pado, et S. Rudolph (2012). Lodifier: Génération des données liées du texte structuré ONU-. Dans l'ESWC, pp. 210-224. Benhardus, J. et J. Kalita (2013). Streaming détection de tendance sur Twitter. IJWBC 9 (1), 122-139. Bizer, C., T. Heath et T. Berners-Lee (2009). Les données liées - l'histoire jusqu'à présent. IJSWIS 5 (3), 1-22. Cataldi, M., L. Caro D. et C. Schifanella (2013). Détection sujet personnalisé émergents basé sur un terme modèle de vieillissement. ACM TIST 5 (1), 7. Cattoni, R., F. Corcoglioniti, C. Girardi, B. Magnini, L. Serafini, et R. Zanoli (2012). Le knowledgestore: un système de stockage basé sur l'entité. En LRGC, p. 3639-3646. Chua, F. C. T. et S. Asur (2013). Résumé automatique des événements de médias sociaux. En ICWSM. Corro, L. D. et R. Gemulla (2013). Clausie: extraction de l'information ouverte sur la base clause. Dans WWW, pp. 355-366. Exner, P. et P. Nugues (2012). extraction Entité: Du texte non structuré à DBpedia triplets rdf. En Wole. Fader, A., S. Soderland et O. Etzioni (2011). L'identification des relations pour l'extraction de l'information ouverte. En EMNLP, p. 1535-1545. Gamallo, P., M. Garcia et S. Fernández-Lanza (2012). Dépendance à base de l'extraction de l'information ouverte. Dans l'atelier sur Unsupervised et apprentissage semi-supervisé en PNL, pp. 10-18. Association de linguistique informatique. Mathioudakis, M. et N. Koudas (2010). Twittermonitor: détection de tendance sur le flux Twitter. En SIGMOD, p. 1155-1158. Olariu, A. (2013). classification hiérarchique dans l'amélioration des flux de summarization microblog. En CICling, pp. 424-435. Rusu, D., L. Dali, B. Fortuna, M. Grobelnik et D. Mladenic (2007). extraction Triplet de phrases. Dans Int. Multiconf. Société de l'information-IS, p. 8-12. Schmitz, M., R. Bart, S. Soderland, O. Etzioni, et al. (2012). Langue d'enseignement ouvert pour l'extraction de l'information. En EMNLP, pp. 523-534. Sharifi, B., M. Hutton, et J. K. Kalita (2010). Résumant microblogs automatiquement. Dans HLT-NAACL, pp. 685-688. - 388 - F - Analyse des Entreprises RSS vers l'extraction de données liés De Tweets Manel Achichi, Zohra Bellahsene, Konstantin Todorov, Dino Ienco"
280,Revue des Nouvelles Technologies de l'Information,EGC,2015,Ultrametricity of Dissimilarity Spaces and Its Significance for Data Mining,"Nous introduisons une mesure d'ultramétricité pour les dissimilaritées et examinons les transformations des dissimilaritées et leurs impact sur cette mesure. Ensuite, nous étudions l'influence de l'ultramétricité sur la comportement de deux classes d'algorithmes d'exploration de données (le kNN algorithme de classification et l'algorithme de regroupement PAM) appliqués sur les espaces de dissimilarité. On montre qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour les clusters, une augmentation d'ultramétricité genere regroupements avec une meilleure séparation. Une diminution de la ultramétricité produit groupes plus compacts.","Dan Simovici, Rosanne Vetro, Kaixun Hua",http://editions-rnti.fr/render_pdf.php?p1&p=1002068,http://editions-rnti.fr/render_pdf.php?p=1002068,en,"Ultramétricité des espaces dissimilitude et son importance pour l'exploration de données Dan Simovici *, Rosanne Vetro **, *** Kaixun Hua University of Massachusetts Boston * dsim@cs.umb.edu ** *** rvetro@cs.umb.edu @ kingsley cs.umb.edu Résumé. Nous introduisons une mesure de ultramétricité pour les espaces de dissemblance et d'examiner les transformations de dissemblances qui ont un impact de cette mesure. Ensuite, nous étudions l'influence de ultramétricité sur le comportement des deux classes d'algorithmes de données (ing classification kNN et regroupement PAM) appliqués sur les espaces de dissemblance. Nous montrons qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour le regroupement, l'augmentation ultramétricité avec assorti- terings générer une meilleure séparation. L'abaissement produisent ultramétricité grappes plus compacts. 1 Introduction ultramétriques se produisent dans l'étude des algorithmes de classification ascendante hiérarchique, les arbres génétiques phylo-, nombres p-adiques, certains systèmes physiques, etc. but Notre est d'évaluer le degré de ultramétricité des espaces de dissemblance et d'étudier l'impact du degré de ultramétricité sur la performance de la classification et le regroupement algorithmes de. La mesure ultramétricité des espaces métriques a préoccupé un certain nombre de chercheurs (par exemple, dans (Rammal et al, 1985).); Toutefois, les mesures proposées sont utilisables dans le cas de mesures spé- ciales et sont liés à la ultramétrique subdominant attaché à une mesure qui nécessite le calcul d'un regroupement unique lien ou un arbre Spanning minimal. Nous vous proposons une mesure native alter- appelée faible ultramétricité qui peut être appliquée au cas plus général des espaces de dissemblance. Un espace de dissimilarité est une paire (S, d), où S est un ensemble et d: S × S - → R est une fonction telle que D (x, y)> 0, D (x, x) = 0, et d (x, y) = d (y, x) pour x, y ∈ S. Nous supposons que tous les espaces de dissemblance considérés sont finis. Un triangle en (S, d) est un triplet (x, y, z) ∈ S3. Pour simplifier la notation, on note t = (x, y, z) par xyz. La cartographie d est un quasi-métrique si elle est une dissimilarité et il satisfait à la triangulaire inégali- lité d (x, y) 6 d (x, z) + d (z, y) pour x, y, z ∈ S. en outre, si d (x, y) = 0 implique x = y, alors d est une métrique. - 89 - ultramétricité des dissimilarités Un quasi-ultramétrique est une dissemblance d: S × S - → R> 0 qui satisfait l'inégalité d (x, y) 6 max {d (x, z), d (z, y)} pour x, y, z ∈ S. Si, en outre, d (x, y) = 0 implique x = y, alors d est un ultramétrique. Dans la section 2, nous présentons une mesure de ultramétricité pour les espaces de dissemblance et une variante plus faible de cette mesure qui est mieux d'un point de vue informatique. Ensuite, nous examinons les transformations de dissemblances qui affectent ultramétricité. L'influence de ultramétricité de dissemblances sur la performance des classificateurs est examinée à la section 2 en utilisant les k plus proches voisins des classificateurs. L'article 4 est consacré à l'étude de l'impact de ultramétricité sur la compacité et la séparation cluster. 2 Evaluation de ultramétricité dans les espaces dissimilarité Soit r un nombre non négatif et soit Dr (S) l'ensemble des dissemblances définies sur S qui satisfont à l'inégalité d (x, y) r 6 d (x, z) r + d ( z, y) r pour x, y, z ∈ S. On notera que chaque dissimilarité appartient à l'ensemble D0; une dissemblance dans D1 est un semimetric. Laissez D∞ = ⋂ r> 0DR. Si d ∈ D∞, alors d est un ultramétrique. En effet, soit d ∈ D∞ et supposons que D (x, y)> d (x, z)> d (z, y). Ensuite, d (x, y) 6 d (x, z) (d (x, z)) r 1 + (d (y, z)) 1r pour chaque r> 0. Puisque limr → ∞ d (x, z ) (1 + (d (y, z) d (x, z)) r) 1r = d (x, z), il en résulte que d (x, y) 6 d (x, z) = max {d ( x, z), d (z, y) pour x, y, z ∈ S, ce qui nous permet de conclure que d est un ultramétrique. Il est facile de vérifier que r 6 s implique (d (x, z) r + d (z, y) r) 1r> (d (x, z) s + d (z, y) s) 1 s (voir ( Simovici et Djeraba, 2014), Lemme 6.15). Ainsi, si r 6 s, nous avons l'inégalité Ds ⊂ Dr Soit (S, d) un espace de dissemblance et laisser t = xyz un triangle. À la suite de la notation de Lerman (Lerman, 1981), nous écrivons Sd ( t) = D (x, y), Md (t) = d (x, z), et Ld (t) = d (y, z), si d (x, y)> d (x, z)> d (y, z). Définition 2.1. Soit (S, d) un espace de dissemblance et laisser t = xyz ∈ S3 un triangle. Le ultramétricité de t est le nombre ud (t) défini par ud (t) = max {r> 0 | Sd (t) r 6 Md (t) r + Ld (t) r}. Si d ∈ Dp, nous avons p 6 ud (t) pour chaque t ∈ S3. La notion de ultramétricité faible que nous sommes sur le point de présenter a des avantages de calcul sur la notion de ultramétricité, en particulier du point de vue de la manipulation des transformations de mesures. Le faible ultramétricité du triangle t, wd (t) est donnée par wd (t) =    1 log2 Sd (t) Md (t) si Sd (t)> Md (t) ∞ si Sd (t) = Md (t). Si wd (t) = ∞, alors t est un triple ultramétrique. Le faible ultramétricité de l'espace de dissimilarité (S, D) est le nombre w (S, d) définie par w (S, d) = médiane {wd (t) | t ∈ S3}. - 90 - Dan Simovici et al. La définition de w (S, d) élimine l'influence des triangles dont ultramétricité est une valeur aberrante, et donne une meilleure idée de la propriété globale de ultramétrique (S, d). Pour un triangle t, nous avons 0 6 Sd (t) -md (t) = (2 1 wd (t) - 1) Md (t) 6 (2 1 w (S, d) - 1) Md (t) Ainsi , si wd (t) est suffisamment grande, le triangle t est presque isocèle. Par exemple, si wd (t) = 5, la différence entre la longueur du côté le plus long Sd (t) et le côté médian Md (t) est inférieur à 15%. Pour chaque triangle t ∈ S3 dans un espace que nous avons dissemblance ud (t) 6 wd (t). En effet, depuis Sd (t) ud (t) 6 Md (t) ud (t) + Ld (t) ud (t) nous avons Sd (t) ud (t) 6 2md (t) ud (t), qui est équivalent à ud (t) 6 wd (t). Ensuite, on discute des transformations de dissemblance qui influent sur les ultramétricité des liens dissimilari-. Théorème 2.2. Soit (S, d) un espace de dissemblance et f: R> 0 - → R> 0 une fonction strictement croissante sur R> 0. Si la fonction g: R> 0 - → R> 0 donné par g (a) = {f (a) si a> 0, 0 si a = 0 est strictement décroissante, alors la fonction e: S × S - → R> 0 définie par e (x, y) = f (d (x, y)) pour x, y ∈ S est une dissimilarité et wd (t) 6 nous (t) pour chaque triangle t ∈ S3. Preuve. Il est immédiat que e (x, y) = e (y, x) et E (x, x) = 0 pour x, y ∈ S. Soit t = xyz ∈ S3 un triangle. Depuis Sd (t)> Md (t) et g est strictement décroissante, g (Sd (t)) 6 g (Md (t)), ce qui implique f (Sd (t)) Sd (t) 6 f (Md ( t)) Md (t). Puisque f est une fonction strictement croissante que nous avons Se (t) = f (Sd (t)) et moi (t) = f (Md (t)). Cela nous permet d'écrire: Se (t) Me (t) = f (Sd (t)) f (Md (t)) 6 Sd (t) Md (t). Par conséquent, wd (t) = 1 log2 Sd (t) Md (t) 6 1 log2 Se (t) Me (t) = nous (t). Exemple 2.3. Soit (S, d) un espace de dissimilarité et soit e la dissemblance définie par e (x, y) = d (x, y) r, avec 0 <r <1. Si f (a) = ar, alors f est strictement croissante et la fonction g: R> 0 - → R> 0 donné par g (a) = {f (a) si a> 0, 0 si a = 0 = {ar-1 si a> 0, 0 si a = 0 est strictement décroissante. Par conséquent, le faible ultramétricité nous (t) est supérieur wd (t), où e (x, y) = (d (x, y)) pour r x, y ∈ S. - 91 - ultramétricité de dissimilarités Exemple 2.4. Soit f: R> 0 - → R> 0 est définie par f (a) = aa + 1. Il est facile de voir que f est strictement croissante sur R> 0 et g (a) = {1 1 + a si a> 0, 0 si a = 0 est strictement décroissante sur le même ensemble. Par conséquent, le faible ultramétricité d'un triangle augmente lorsque d est remplacé par e donnée par e (x, y) = d (x, y) 1 + d (x, y) pour x, y ∈ S. Exemple 2.5. La Schoenberg transformation d'une dissemblance d décrit en (Deza et Laurent, 1997) est la dissemblance e: S2 - → R> 0 définie par e (x, y) = 1- e-kd (x, y) pour x, y ∈ S. soit f: R> 0 - → R> est la fonction f (a) = 1 - e-ka qui est utilisé dans cette transformation. Il est immédiat que f est une fonction strictement croissante. Pour un> 0 nous avons g (a) = 1-e -ka un, ce qui nous permet d'écrire g '(a) = e-ka (ka + 1) - 1 a2 pour une> 0. En tenant compte de l'inégalité évidente ka + 1 <eka pour k> 0, il en résulte que la fonction g est strictement décroissante. Ainsi, les faibles ultramétricité d'un triangle par rapport à la Schoenberg transformation est supérieure à la ultramétricité faible sous la dissemblance d'origine. 3 Clas sification et ultramétricité L'algorithme k-plus proches voisins (kNN) est une méthode de classification qui est basée sur la mémoire et ne nécessite pas un modèle à ajustement. La classification est décidée selon une décision à la majorité simple parmi les plus échantillons de jeu de la même formation. Nous montrons que les performances de kNN appliquée à un espace de dissemblance (S, d) se dégrade avec l'augmentation de la ultramétricité de d. Cela se produit parce que l'augmentation de ultramétricité parmi les éléments de S favorise l'égalisation des distances. Nous commençons par un espace de dissemblance (S, d) et on obtient une nouvelle dissemblance d '= f (d), où f est l'une des transformations examinées dans la section 2. algorithme 1 encapsule le process ci-dessus. Il fonctionne kNNwith t fois la validation croisée et calcule la matrice de confusion générée pour chaque pli ainsi que l'erreur de classification cumulative de l'espace transformé. Nous limitons la précision de la transformée dissemblance d 'en prenant en compte, comme ob- servi dans (2008 Murtagh et al.) Qui ultramétricité peut diminuer avec l'augmentation de la précision. La limitation de la précision d 'à quelques chiffres décimaux favorise l'égalisation de ces distances. Nous avons utilisé dans nos expériences les ensembles de données disponibles Fisheriris et ionosphère de - 92 - Dan Simovici et al. Algorithme 1: Fonctionne kNN avec fonction de la distance transformée entrée: un espace métrique ou de dissemblance S = (M, d), le nombre de plus proches voisins k, le nombre de plis t et une fonction f, telle que f (d) = d ' et u <= u 'où u et u' sont les ultrametricities de S et S '= (M, D'), respectivement. Résultat: L'erreur de classification cumulative de l'espace transformé S 'd' ← f (d), limitée à une certaine partitionM de précision décimal en sous-échantillons de t pour i = 1 tot do formation = partition (i) Test .Formation = partition (i). testSize essai (i) = taille (test) kNN (formation, test, k, d ') err (i) = #misclassified objets retour cerr = somme (err) / somme (testsSize) Diss. Iris ionosphère cancer de l'ovaire k = 3 k = 5 k = 7 k = 3 k = 5 k = 7 k = 3 k = 5 k = 7 d 0,0467 0,0427 0,3860 0,1033 0,3701 0,3852 0,1403 0,1394 0,1431 0,0753 0,0567 D0.1 0,3875 0,1187 0,4097 0,3897 0,1454 0,1431 0,1477 0,2900 0,3000 d0.01 0,5211 0,2700 0,5239 0,5365 0,3574 0,3181 0,3000 TAB. 1: Moyenne de 10 calculs de l'erreur de classification produit par kNN en utilisant une validation croisée t-fiée fois stratigraphique, pour différentes valeurs de k et t = 10. ensembles https://archive.ics.uci.edu/ml/data / jeux de données et cancer de l'ovaire ob- CONTENUES du programme clinique Proteomics FDA NCI Databank (http://home.ccr.cancer.gov/ncifdaproteomics/ppatterns.asp). Nos expériences considérées comme un espace euclidien initial (S, d) où S correspond à l'un des ensembles de données décrits ci-dessus et D à la distance euclidienne. Nous avons d'abord testé notre méthode sur l'espace original et comparé les résultats avec les résultats générés par l'augmentation de ultramétricité de dissemblance d '= f (d), où f (a) = ar pour a> 0. On utilise à la fois t kNN -fois la validation croisée et avec stratifié t fois la validation croisée (où chaque pli a à peu près la même taille et à peu près les mêmes proportions de classe comme dans tout l'ensemble de données). Les distances transformées étaient limitées à 2 chiffres de précision décimale. L'erreur de classification obtenu est toujours plus élevée pour le cas de l'espace transformé (S, d '), dans les deux scénarios de validation. Dans le tableau 2, nous montrons les résultats pour trois valeurs de k (le nombre de voisins) en stratifié validation 10 fois. Des résultats similaires sont obtenus pour 5 plis dans les deux scénarios de validation. - 93 - ultramétricité dissemblances 4 L'impact de ultramétricité sur Cluster et séparation Compacité clustering évalue la validation et évalue la bonté des résultats d'un algorithme de clustering (Maulik et Bandyopadhyay, 2002). Nous avons utilisé des mesures de validation internes qui reposent sur des informations contenues dans les données, à savoir et de la compacité et de la séparation (Tang et al., 2005) (Tang et al., 2005; Zhao et Karypis, 2002). mesures Compacité bien quantifier les objets liés dans un clust er sont. Il fournit des informations sur la cohésion des objets dans un cluster individuel par rapport aux autres objets en dehors du cluster. Un groupe de mesures évaluer la compacité du cluster en fonction de la variance où des valeurs plus faibles indiquent une meilleure compacité. D'autres mesures sont basées sur la distance, par exemple au maximum ou une distance de moyenne par paires, et maximum ou de la distance en fonction du centre de la moyenne. La séparation est une mesure de caractère distinctif entre un groupe et le reste du monde. Les distances entre les centres de paires clusters ou les distances minimales entre les objets par paires dans différents groupes sont souvent utilisés comme des mesures de séparation. La compacité de chaque groupe a été évaluée en utilisant la dissemblance moyenne entre les observations du cluster et le médoïde du cluster. La séparation a été calculée en utilisant la dissemblance minimale entre une observation du groupe et une observation d'un autre groupe. Nous étudions l'impact de ultramétricité sur la compacité et la séparation des grappes en utilisant la partition autour de l'algorithme Medoids (PAM) (Kaufman et Rousseeuw, 1990) à se regrouper objets à l'origine dans l'espace euclidien et plus tard dans un espace de dissemblance transformé avec ultramétricité inférieur ou supérieur. Les expériences montrent que la transformation de la matrice de distance qui diminue l'ultra- metricity de l'espace euclidien d'origine peut effectivement améliorer la compacité, mais aussi de réduire la séparation des agrégats générés par PAM. Cependant, la compacité améliore à un rapport plus rapidement que la diminution de la séparation. Nous avons également observé que l'augmentation de la pro- ultramétricité duces l'effet inverse, la compacité dégradante et la séparation de plus en plus, à différents rapports. Dans ce cas, la compacité diminue dans un rapport plus rapidement que l'augmentation de la séparation. Soit (S, d) un espace de dissimilarité, (S, D ') l'espace de dissimilarité transformée, où d' = f (d) est obtenue en appliquant l'une des transformations décrites dans la section 2 et soit u et u 'soit les faibles ultrametricities de ces deux espaces de dissimilarité, respectivement. L'augmentation de ultramétricité de (S, d) à (S, d ') favorise l'égalisation des valeurs de dissimi- larité. Dans le cas de l'extrême, nous avons un espace ultramétrique où les distances par paires impliquées dans tous les triplets de points forment un triangle équilatéral ou isocèle. Explorer comment l'égalisation (ou le processus inverse) peuvent affecter la qualité de regroupement, une meilleure étude des ef- fets de l'augmentation (ou diminution) ultramétricité sur les résultats générés par un algorithme de clustering largement connu et robuste a été réalisée. Afin d'étudier l'impact de ultramétricité sur la compacité du cluster et la séparation, nous avons mis en place un algorithme qui exécute PAM sur les espaces d'origine et transformées, et calcule les mesure pour chaque groupe de S et S '. Nos expériences considérées comme un espace euclidien initial (S, d) où S correspond à un ensemble d'objets et d à la distance Minkowski avec l'exposant 2. Pour obtenir une comparaison valable de compacité et de la séparation, les grappes obtenues à partir d'un ensemble de données spécifiques doivent S contenir les mêmes éléments dans les espaces d'origine et transformées. - 94 - Dan Simovici et al. Dissemblances dx où x> 1 ont tendance à diminuer la ultramétricité de l'espace d'origine, alors que dissemblances où 0 <x <1 ont tendance à augmenter ultramétricité. mesures de validation des clusters existants et les critères actuels peuvent être affectés par diverses caractéristiques de données (Liu et al., 2010). Par exemple, les données à densité variable est difficile pour les algorithmes de regroupement plu- sieurs. Il est connu que k-moyens souffre d'un effet d'uniformisation qui tend à diviser les objets dans des tailles relativement égales (Xiong et al., 2009). De même, k-means et PAM ne sont pas une bonne performance lorsqu'ils traitent avec des ensembles de données de distribution asymétrique où les grappes ont des tailles inégales. Pour déterminer l'impact de ultramétricité en présence de l'une de ces caractéristiques, les expériences ont été réalisées compte tenu de 3 aspects différents de données: bonne separat ion, la densité et les distributions asymétriques en trois ensembles de données synthétiques nommées WellSeparated, DifferentDensity et SkewDistribution, respectivement. La figure 1 montre les données de synthèse qui a été généré pour chaque aspect. Chaque jeu de données contient 300 objets. Tableaux 2 montre les résultats pour les ensembles de données WellSeparated, DifferentDensity bution et SkewDistri-, respectivement. La mesure (compacité ou de séparation) Le ratio est calculé en divisant la mesure de l'espace transformé par la mesure de l'espace d'origine. Le com- ratio de mesure moyenne imputées au 3 groupes est présentée dans chaque tableau. On notera que le rapport de mesure moyenne est inférieure à un pour les espaces avec ité inférieure ultrametric- (obtenus avec dissemblances d5 et d10). Dans ce cas, le taux de compacité moyenne est également plus faible que le rapport moyen de séparation, qui montre que les transformations générées dissemblances de cluster intra- qui rétréci plus que les inter-grappes, par rapport aux différences d'origine. Dans les espaces avec ultramétricité supérieur (obtenu avec dissemblances D0.1 et d0.01), le rapport de mesure moyenne est supérieure à un. Le rapport de la compacité moyenne est également plus élevé que le rapport de séparation moyenne, montrant que les transformations ont généré des similitudes dis- intra-grappe qui ont élargi plus que les inter-munitions. Cela explique l'effet de l'égalisation obtenue avec l'augmentation ultramétricité. (A) bien séparés (b) densité différente (c) Figure Skewed de distribution. 1: données synthétiques contenant 3 éléments de données différentes: 1a: une bonne séparation, 1b et 1c densité différente: les distributions asymétriques figures 2a, 2b et 2c montrent la relation entre la compacité d'un taux de séparation pour chaque jeu de données. Sur la figure 2 on montre la relation entre les rapports de compacité et de séparation pour les trois ensembles de données synthétiques et pour l'ensemble de données Fisheriris qui présentent des modèles de variation similaires. Comme mentionné précédemment, les données avec des caractéristiques telles que différentes densités et différentes tailles de cluster pourraient imposer un défi pour plusieurs algorithmes de regroupement. - 95 - ultramétricité de dissemblances Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.159852 0.462208 d10 6.782346E-008 0,00000036 0,000004433 006 2.113074E-d5 0,000150339 0,00082451 0,002830298 0,006041492 5,493823933 D0.1 0,835946 0,955770 0,973616 2,073671831 6,433067888 d0.01 0,995943 2.161429967 Résultats pour un ensemble de données avec des grappes bien séparées Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.226611 0.883006 d10 0,000000299 1.225126E-006 0,0085821266 0,0067414224 d5 0,000414 0,001758 0,120677 0,101145 3,829475 1,019217 D0.1 1,247117 0,862157 0,968235 4,302930 1,002328 d0.01 1.234965 Résultats pour un ensemble de données avec des grappes avec des densités variées Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.152911 1.088650 d10 5.001356E-005 0,0001674944 0,0202263733 0,0185757406 d5 0,001707 0,005744 0,240466 0,220866 7,502117 1,042825 D0.1 0,957924 0,815746 0,966675 9,123531 1,004683 d0.01 0.922859 Résultats pour un ensemble de données de distributions asymétriques. Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 2.564313e-01 2.841621e-01-D10 4.495584e 07 1.753134e-06 1.171608e-05 4.123026e-05-d5 7.628527e 04 2.974881e-03 4.583216e-03 1.612888e-02 D0.1-01 8.664974e 3.379062e + 00 8.715969e-01 3.067252e + 00 d0.01 9.630195e-01 3.755467e + 00 9.858841e-01 3.469442e + 00 Résultats pour l'ensemble de données Fisheriris. LANGUETTE. 2: compacité Cluster et la séparation en utilisant PAM sur trois ensembles de données synthétiques et Fish- eriris. dissemblances. Les deux moyennes de rapport sont calculés par rapport à l'ensemble de données com- grappe valeurs de compacité et de séparation donnés par le d de dissimilarité d'origine. - 96 - Dan Simovici et al. (A) WellSeparated Data Set (b) DifferentDensity Data Set (c) SkewDistr Data Set (d) Fisheriris Data Set Fig. 2: Relation entre les taux de compacité et de séparation pour trois ensembles de données de synthèse et pour la Fishe riris ensemble de données Nous montrons un scénario dans lequel PAM, lorsqu'il est appliqué à l'espace euclidien d'origine, ne fonctionne pas bien. Néanmoins, nous sommes en mesure d'améliorer les résultats du PAM en appliquant une trans- formation qui diminue la ultramétricité de l'espace d'origine et en cours d'exécution PAM sur l'espace formé de trans. Considérons l'ensemble de données présenté sur la figure 3a qui a été produite par synthèse dans un espace euclidienne par paire avec distance d par trois distributions normales avec viation de- type similaire, mais des densités différentes. Il a 300 points au total, avec le groupe le plus dense comprenant 200 points et les deux autres contenant 75 et 25 points. Notez que les groupes un peu clairsemés sont également situés très près les uns des autres. Différents symboles sont utilisés pour identifier les trois distributions distinctes. la fonction objectif de PAM tente de minimiser la somme des dissemblances de tous les objets à leur plus proche médoïde. Cependant, il peut ne pas partitionner les données dans les distributions d'origine lorsqu'ils traitent avec les données de densité différentes depuis la scission du groupe les plus denses peuvent se produire. Dans notre exemple, PAM fait exactement cela et combine aussi les deux groupes rares qui ne sont pas bien séparés. Notez que k-means contrairement à (qui effectuent aussi ne pas bien dans ces scénarios, mais peuvent éventuellement trouver la bonne partition - 97 - ultramétricité de dissemblances en raison du caractère aléatoire de la sélection des centres de gravité), PAM sera échoue le plus probable en raison du déterminisme de son BUILD et les étapes de SWAP combinées et le choix de la fonction objective. Pour explorer l'effet positif de l'augmentation de la compacité intra-groupe généré par de nouveaux espaces avec ultramétricité plus bas sur les données contenant ces caractéristiques, nous avons appliqué les mêmes transformations avec des exposants entiers positifs à la matrice ob- CONTENUES de distance euclidienne originale d. Les résultats montrent une amélioration significative du regroupement. La figure 3b montre le résultat de l'application PAM pour regrouper les données synthétiques avec dissemblance d. Notez que le résultat de Tering assorti- ne correspond pas à une partition ressemblant à des distributions qui ont été utilisées pour générer les données. Les figures 3d et 3c montrent que PAM omet également de fournir une bonne partition avec dissemblances; d 0,1 et d 0,01 puisque l'augmentation ultramétricité favorise l'égalisation des dissemblances qui peut dégrader encore plus les résultats. Notez cependant que les partitions obtenues par PAM en utilisant la d5 et la forme d10 dissemblances groupes similaires à ceux géné- erated par les distributions originales. En effet, l'augmentation de la compacité aide PAM à créer des limites qui sont conformes aux distributions normales d'origine. (A) de synthèse des données (b) d (c) d 0,01 (d) d 0,1 (e) d 5 (f) d 10 FIG. 3: La figure 3a montre les données synthétiques générées à partir de distributions de densité différente. 3b à 3f montrent les résultats de l'APM en utilisant la distance euclidienne d et d'autres dissemblances obtenus par des transformations sur d. Le tableau 3 montre les mesures et ratios pour cet ensemble de données. La figure 4 montre la relation entre les rapports de compacité et de séparation. 5 Conclusions et Poursuite des travaux Nous avons examiné l'influence de ultramétricité des espaces de dissemblance en ce qui concerne le classement et le regroupement. - 98 - Dan Simovici et al. Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.138692 0.460486 d10 1.295368e-09 9.339889e-09 0,011426 0,024814 d5 2.868980e-05 2.068598e-04 0,104837 0,227665 D0.1 0,842801 6,076787 0,816082 1,772218 d0.01 0,974571 7,026878 0,978284 2,124458 TAB. 3: Jeu de données comprenant des grappes ayant une densité différente. FIGUE. 4: Relation entre Compacité et ratios de séparation pour les données test Nous avons montré qu'il existe une variation inverse entre ultramétricité et performances indiquées des classificateurs. L'augmentation de cette mesure, obtenue par des transformations appliquées à l'espace d'origine, favorise l'égalisation des distances. Cette égalisation augmente le niveau d'incertitude au cours du processus de classification et dégrade la qualité du résu LTS généré par des classificateurs. Pour le regroupement, l'augmentation de ultramétricité génère clusterings avec une meilleure séparation. Comment- jamais, il diminue également la compacité plus rapide que l'augmentation de la séparation. L'abaissement metricity ultra- produit des grappes plus compactes, mais pas aussi bien séparés que dans l'espace d'origine. Dans ce cas, la compacité croît à un taux plus rapide que la diminution de la séparation. Il existe de nombreuses applications qui peuvent bénéficier de cette étude. Par exemple, en changeant le ultramétricité de l'espace d'origine peut aider à trouver des modèles dans les données qui ne sont pas conformes au comportement attendu, dans un exemple classique de détection des anomalies. L'impact de trametricity sur différents algorithmes ul- de classification hiérarchique semble également un sujet prometteur de l'enquête. Références Deza, M. M. et M. Laurent (1997). La géométrie des coupes et des mesures. Heidelberg: Springer. - 99 - ultramétricité de dissemblances Kaufman, L. et P. J. Rousseeuw (1990). Groupes de données dans Finding - Introduction à l'analyse Cluster. New York: John Wiley & Sons. Lerman, I. C. (1981). Classification et analyse des Ordinale Données. Paris: Dunod. Liu, Y., Z. Li, H. Xiong, X. Gao et J. Wu (2010). La compréhension des mesures de validation des clusters internes. En 2010 IEEE 10e Conférence internationale sur l'exploration de données, p. 911-916. IEEE. Maulik, U. S. et Bandyopadhyay (2002). Évaluation de la performance de certains clusters et indices algorithmes de validité. IEEE Transactions sur le modèle d'analyse et de la machine Intelli gence 24 (12), 1650-1654. Murtagh, F., G. Downs, et P. Contreras (2008). classification hiérarchique des ensembles de données tridimensionnelles élevées massives, en exploitant l'incorporation ultramétrique. SIAM Journal sur le calcul scientifique 30 (2), 707-730. Rammal, R., J. C. A. d'Auriac, et D. Douçot (1985). Sur le degré de ultramétricité. Le Journal de Physique - Letteres 45, 945-952. Simovici, D. A. et C. Djeraba (2014). Outils mathématiques pour l'exploration de données (deuxième éd.). London: Springer. Tang, P., M. Steinbach et V. Kumar (2005). Introduction à l'exploration de données. Lecture, MA: Addison-Wesley. Xiong, H., J. Wu, et J. Chen (2009). K-means par rapport à la validation des mesures: une perspective de distribution de données. IEEE Transactions on Systems, Man et Cybernétique, Partie B: Cybernétique 39 (2), 318-331. Zhao, Y. et G. Karypis (2002). L'évaluation des algorithmes de classification hiérarchique pour les ensembles de données de docu- ment. Dans Actes de la onzième Conférence internationale sur l'information et la gestion des connaissances, pp. 515-524. ACM. Nous introduisons CV Une ultramétricité Mesure d'affluer les dissimilaritées et les transformations des examinons dissimilaritées et sur l'impact their this mesure. Salle de bains, l'influence NOUS étudions de L'ultramétricité-sur-la de Deux cours COMPORTEMENT d'exploration de d'algorithmes Données (le kNN de classification et Algorithme l'algorithme de PAM Regroupement) sur les appli- QUES espaces de dissimilarité. Sur exists Une montre variation Qu'il inverse Entre ultramé- tricité et la performances des classificateurs. Pour les groupes, Une augmentation d'ultramétricité genere with a better regroupements séparation. Une diminution de la ultramétricité pro- duit, plus compacts Groupes. - 100 - B - Classification, Clustering, Similarité ultramétricité des espaces dissimilitude et son importance pour l'exploration de données Dan Simovici, Rosanne Vetro, Kaixun Hua"
289,Revue des Nouvelles Technologies de l'Information,EGC,2015,Using Social Conversational Context For Detecting Users Interactions on Microblogging Sites,"Dans ce travail, nous proposons une nouvelle méthode de détection des conversations sur les sites des réseaux sociaux. Cette méthode est basée sur l'analyse et l'enrichissement de contenu dans le but de présenter un résultat informatif basé sur les interactions des utilisateurs. Nous avons évalué notre méthode sur corpus recueillis de réseau social lié à des sujets spécifiques, et nous avons obtenu des bons résultats.","Rami BELKAROUI, Rim Faiz, Aymen Elkhlifi",http://editions-rnti.fr/render_pdf.php?p1&p=1002101,http://editions-rnti.fr/render_pdf.php?p=1002101,en,"En utilisant le contexte social conversationnelle Pour les utilisateurs sur les sites détecter les interactions Microblogging BELKAROUI * Rami, Rim FAIZ **, *** * ELKHLIFI Aymen LARODEC, ISG Tunis, Université de Tunis, Tunisie rami.belkaroui@gmail.com ** LARODEC, IHEC, Université de Carthage, Tunisie rim.faiz@ihec.rnu.tn *** LALIC, Université Paris Sorbonne, France Résumé aymen.elkhlifi@paris4.sorbonne.fr. À l'époque actuelle, les services de micro-blogging comme Twitter, donne per- PLE la possibilité de communiquer, d'interagir, de collaborer entre eux, répondre aux messages des autres et créer des conversations. Ces services peuvent être considérés comme référentiel d'information très large contenant des millions de messages texte généralement organisés en réseaux complexes impliquant les utilisateurs qui interagissent entre eux à des moments précis. Plusieurs travaux ont proposé des outils pour la recherche de tweets ont porté uniquement pour récupérer les tweets pertinents. Par conséquent, les utilisateurs ne sont pas capables d'explorer les résultats ou récupérer les tweets les plus pertinents en fonction du contenu, et peut se perdre ou se sentir frustrés par la surcharge d'information. Dans cet article, nous vous proposons une nouvelle méthode pour récupérer la conversation sur les sites ging microblog- en particulier Twitter. Il est basé sur l'analyse du contenu et l'enrichissement du contenu. Le but de notre méthode est de présenter un résultat plus d'information par rapport au moteur de recherche classique. La méthode proposée a été mis en œuvre et évalué en le comparant aux moteurs Google et Twitter recherche et nous avons obtenu des résultats très prometteurs. 1 Introduction Les dernières années, les gens sont de plus en plus communicatif grâce à l'expansion des services et des applications multi-plateformes telles que les blogs, les forums et les réseaux sociaux qui établit les milieux sociaux et de collaboration. Ce comportement conduit à une accumulation d'une énorme quantité d'informations. Parmi ces plates-formes sont soi-disant microblogs. En outre, les services de micro-blogging (Boyd et al., 2010) donne aux gens la possibilité de communiquer, d'interagir, de collaborer entre eux, répondre aux messages des autres et créer des conversations. Alors que les gens qui communiquent partagent différents types d'informations comme la connaissance commune, les opinions, les émotions, les ressources d'information et de leurs goûts ou aversions. L'analyse de ces communications peut être utilisation- FUL pour des applications commerciales telles que le suivi des tendances, la gestion de la réputation et la diffusion de nouvelles. En outre, l'une des principales caractéristiques des services de micro-blogging est que les utilisateurs ne sont pas limités à des contenus de produire; ils peuvent participer indirectement à des conversations avec d'autres - 389 - Utilisateurs sur la détection des interactions utilisateurs Sites Microblogging par goût et partager les messages de l'utilisateur. Plusieurs travaux ont proposé des outils pour la recherche de tweets ont porté uniquement pour récupérer les tweets pertinents. Par conséquent, les utilisateurs ne sont pas capables d'explorer les résultats ou récupérer les tweets les plus pertinents en fonction du contenu, et peut se perdre ou se sentir frustrés par la surcharge d'information. En outre, trouver de bons résultats sur les sujets donnés doit prendre en compte l'ensemble du contexte. Cependant, le contexte peut être dérivé des interactions utilisateur. Ce document propose une conversation méthode de récupération qui peut être utilisé pour extraire la conversation de Twitter. La comparaison avec les méthodes actuelles, le nouveau proposé de ne pas extraire seulement répondre directement tweets, mais aussi les tweets pertinents qui pourraient être retweets ou des commentaires et d'autres interactions possibles. L'extrait de méthode messages étendus au-delà de la conversation classique. Le reste du document est organisé comme suit: nous commençons par présenter les travaux connexes Adresse- ing récupération de conversation sur les sites de micro-blogging. Dans la section 3, nous vous proposons notre méthode permet d'extraire le contenu des interactions de l'utilisateur. Les résultats d'expérimentation et d'évaluation sont présentés dans la section 4. La dernière section présente un résumé de nos dicrections de travail et futurs. 2 Travaux connexes récupération de conversation est un nouveau paradigme de recherche pour les sites de micro-blogging. Il résulte de l'intersection de la recherche d'information et analyse des réseaux sociaux (SNA). La plupart des micro services de ging blog- fournissent un moyen de récupérer les informations pertinentes (Jabeur et al, 2012;. Cherichi et Faiz, 2013), mais pas la capacité de fournir toute discussion de tweets. Il y a eu peu de recherches antérieures traitant spécifiquement de la détection de la conversation. De plus, les approches existantes de récupération de conversation pour les sites de micro-blogging (Cogan et al, 2012;.. Magnani et al, 2010, 2011, 2012) ont porté jusqu'à présent sur le cas particulier d'une conversation formée par tweets répondre directement. Magnani et al. (2011) ont proposé un modèle d'arbre par utilisateur pour récupérer des conversations de microblogs. Ils ont considéré que les tweets qui répondent directement à d'autres tweets par l'utilisation de @sign comme marqueur de addressivity. L'inconvénient est que cette méthode ne tient pas compte des tweets qui ne contiennent pas le @sign. De même (Cogan et al., 2012) ont proposé une méthode pour construire des graphiques de conversation, formés par les utilisateurs ayant répondu aux tweets. Dans ce cas, un tweet ne peut répondre directement à un autre tweet. Cependant, les utilisateurs peuvent participer indirectement à des conversations commu- nautés en commentant, aimer, partager les messages de l'utilisateur et d'autres interactions possibles. Dans (Kumar et al., 2010), les auteurs sont concentrés sur différents aspects des conversations de micro-blogging. Ils ont proposé un modèle simple qui produit des structures de conversation de base en tenant compte de l'identité de chaque membre de conversation. D'autres travaux connexes (Huang et al., 2010) mettant l'accent sur différents aspects de la conversation de micro-blogging, qui traitent respectivement de marquage de conversation et l'identification des sujets. 3 Twitter Conversation Méthode de détection: Tcond Nous proposons une méthode qui combine un ensemble de fonctions de conversation et les messages texte échangés directement afin d'en extraire les messages étendus au-delà de la conversation classique. De plus, nous avons défini une conversation comme un ensemble de courts messages texte posté par un utilisateur à horodatage spécifique sur le même sujet. Ces messages peuvent être directement répondu à d'autres utilisateurs en utilisant « @username » ou indirectement par goût, retweeter, commenter et d'autres interactions possibles. Dans la partie suivante, nous présenterons plus de détails sur nos deux étapes d'approche. - 390 - R.Belkaroui et al. 3.1 Étape 1: Construire une conversation directe Dans cette étape, nous cherchons à recueillir tous les tweets en réponse directement à d'autres tweets. De toute évidence, une réponse à un utilisateur commence toujours par « @username ». Notre objectif dans cette étape est de créer l'arbre de réponse. Le processus de construction de l'arbre de réponse se compose de deux algorithmes exécuter en parallèle algorithme récursif de recherche de racine et de l'algorithme de recherche itérative. Algorithme 1 récursive Root Finder (A: twitter) Soit T un tweet recueilli de Twitter (de tweet ID) alors que (Ti = racine!) Faire Extrait Ti- 1 par champ correspondant ""en réponse à id état"" fin tandis que A: twitter = R: twitter - 1 Soit T0 est la racine (premier tweet publié) de la conversation C et T est un seul tweet récupéré la conversation. Considérons Ti du ​​type de T. Tweet Un tweet peut avoir trois types: racine, répondre ou retweeter. L'objectif de la racine récursif Finder algorithme est d'identifier la racine de conversation T0 donné T. Notez que lorsque l'algorithme démarre, ||T || est inconnu. Une fois, la T0 racine de conversation a été établie, l'algorithme de recherche itérative est utilisé pour rechercher le reste de la conversation C en recherchant tous les tweets adressés à l'aide Ti champ correspondant « en réponse à id d'état ». Il est géré de façon répétée jusqu'à ce que certaines conditions, ce qui indique que la conversation a pris fin, sont respectées. 3.2 Étape 2: Tweets indirects pertinents Nous définissons de nouvelles fonctionnalités qui peuvent aider à détecter les tweets liés indirectement à une même conversa- tion. Le but est de tweets extrait qui peuvent être pertinents à la conversation sans l'utilisation du @symbol. Nous utilisons les notations suivantes dans la suite: • ti est un ensemble de tweets présents dans la conversation directe (tweets en réponse à d'autres tweets directement). • tj est un tweet qui peut être indirectement lié à la conversation. Les caractéristiques que nous avons utilisées sont les suivantes: • En utilisant la même URL: En partageant une URL, un auteur enrichissement I'infor tion publié dans son tweet. Cette fonction est appliquée aux tweets Collectionnez qui partagent la même URL. P1 (ti, tj) = {1 si t contient la même URL. 0 autrement. (1) • hashtags similarité: Le symbole #, appelé hashtag, est utilisé pour marquer un sujet dans un tweet ou de suivre la conversation. Tout utilisateur peut classer ou suivre des sujets avec hashtags. Nous avons utilisé cette fonctionnalité aux tweets virés qui partagent les mêmes hashtags. - 391 - Utilisateurs sur la détection des interactions Microblogging Sites P2 (ti, tj) = {1 si t contient le même hashtag. 0 autrement. (2) • Différence Tweets Heure: La différence de temps est une caractéristique très importante pour la détection de tweets liés indirectement à conversa- tion. Nous utilisons l'attribut de temps pour éliminer efficacement les tweets ayant une grande distance en termes de temps par rapport à la racine de conversation. • Tweets Les dates de publication: attribut Date sont très importants pour détecter les conversations. Les utilisateurs ont tendance à poster des tweets sur le sujet de la conversation dans un court laps de temps. La distance euclidienne a été utilisée pour calculer la date de publication de deux postes similaires sont. • Contenu: On calcule la similitude textuelle entre chaque élément de tj, ti prenant la valeur maximale de la mesure de similarité entre deux messages. La similitude entre les deux éléments est calculée en utilisant la similarité cosinus TF-IDF bien connu, sim (ti, tj). • Fonction similarité: Enfin, la similitude entre les tweets indirectement liés à la conversation et les tweets qui sont présents dans l'arbre de réponse est calculée par une combinaison linéaire entre leurs attributs. 4 Expériences et résultats L'expérience suivante a été conçu pour recueillir des connaissances sur l'impact de nos résultats sur les utilisateurs finaux. Pour cette expérience, nous avons sélectionné trois événements et interrogé notre ensemble de données en utilisant Google 1, moteur de recherche Twitter 2 et notre méthode (Tcond). Ensuite, nous avons demandé un ensemble de 100 évaluateurs pour évaluer les résultats top-10 de toutes les tâches de recherche avec trois niveaux de pertinence, à savoir très pertinents (valeur égale à 2), pertinente (valeur égale à 1) ou non pertinent (valeur égale à 0 ). Afin de mesurer la qualité des résultats, nous utilisons Normalisée à prix réduits mulative Gain Cu- (NDCG) à 10 pour tous les événements jugés. De plus, nous avons utilisé une seconde qui est la mesure de précision en haut 10. L'ensemble de données a été obtenu par la surveillance du système de micro-blogging Twitter messages sur la période Juillet-Août 2013. nous avons utilisé, en particulier, un échantillon d'environ 113 000 messages contenant des tendances mots-clés sujet en utilisant l'API de streaming de Twitter. les sujets ont été déterminés Trending directement par Twitter, et nous avons sélectionné les plus fréquents ceux au cours de la période de surveillance. 1. www.google.com 2. Search.twitter.com. - 392 - R.Belkaroui et al. 4.1 Résultats expérimentaux et d'interprétation Résultats Nous comparons notre méthode de récupération de conversation avec les résultats renvoyés par Google et par le moteur de recherche Twitter en utilisant deux paramètres à savoir le P @ 10 et le NDCG @ 10. De cette comparaison, nous avons obtenu les valeurs résumées dans le tableau 1 où l'on remarque que notre méthode permet de surmonter les résultats donnés par les deux Google et Twitter. La raison de ces valeurs prometteuses est le fait que nous associons un ensemble de fonctionnalités de conversation et méthode des réponses directes pour récupérer la conversation peut avoir un impact significatif sur l'évaluation des utilisateurs. P @ 10 (moyenne%) NDCG (moyenne%) Task1 Google 59,62 56,86 Twitter 65,73 59,71 73,28 64,52 Tcond Task2 Google 57,31 56,02 Twitter 62,78 58,45 67,27 62,73 Tcond Task3 Google 63,21 66,52 Twitter 65,88 68,46 77,27 69,33 Tcond TAB. 1 - Table des valeurs pour le calcul de notre exemple de travail se concentrant sur les trois sélections de messages, nous observons que toutes les conversations obtenues avec notre méthode reçoivent des scores plus élevés avec par rapport à la sélection de Google et Twitter. Selon les commentaires libres de certains utilisateurs et suite à l'analyse qualitative des postes dans les trois sélections, nous pouvons voir que Google et Twitter ont reçu des notes inférieures non pas parce qu'ils renfermaient des postes jugés moins intéressants, mais parce que certains postes ont été considérés comme non pertinents en ce qui concerne le sujet recherché. Concentration sur les trois messages sélections nous observons que toutes les sélections de conversations obtenues avec la recherche de twitter a des scores plus élevés en ce qui concerne la sélection de Google. Ces résultats nous conduisent vers une interprétation plus générale des données recueillies. Il semble que l'utilisation des indicateurs sociaux ont un impact significatif sur l'intérêt degré de l'utilisateur dans les messages récupérés. De plus, le processus de conversations de récupération réseaux sociaux diffère de la recherche d'information sur le Web traditionnel; il comporte des aspects de la communication humaine, comme l'intérêt degré dans la conversation explicitement ou implicitement exprimée par les personnes qui interagissent. 5 Conclusion Ce travail a exploré une nouvelle méthode pour détecter la conversation sur les sites de micro-blogging: une activité de recherche de formation in- exploitant un ensemble de fonctions de conversation en plus des messages texte échangés directement pour récupérer la conversation. Nos résultats expérimentaux ont mis en évidence de nombreux points intéressants. Tout d'abord, y compris les caractéristiques sociales et le concept de conversation directe - 393 - Utilisateurs détecter les interactions sur les sites Microblogging dans la fonction de recherche améliore la pertinence des tweets Informativité et fournit également sultats re- qui sont considérés comme plus de satisfaction par rapport à une tâche de recherche traditionnelle tweet. les travaux futurs poursuivra des recherches sur les aspects de la conversation en incluant les aspects de la communication humaine, comme le degré d'intérêt pour la conversation et leur influence / popularité en recueillant des données provenant de sources multiples de réseaux sociaux en temps réel. Références Boyd, D., S. Golder et G. Lotan (2010). Tweet, tweet, retweet: aspects conversationnels du retweet sur Twitter. Dans Actes de 2010 43e Hawaii Conférence internationale sur les sciences du système, HICSS '10, Washington, DC, Etats-Unis, pp. 1-10. IEEE Computer Society. Cherichi, S. et R. Faiz (2013). Nouvelle mesure métrique pour l'amélioration des résultats de la recherche dans les microblogs. Dans Actes de la 3e Conférence internationale sur le Web Intelligence, l'exploitation minière et Sémantique, WIMS '13, New York, NY, USA, pp. 24: 1-24: 7. ACM. Cogan, P., M. Andrews, M. Bradonjic, W. S. Kennedy, A. Sala et G. Tucci (2012). construction et nouvelle analyse des graphiques de conversation twitter. Dans Actes de l'atelier international Premier ACM sur Hot Topics sur la recherche interdisciplinaire Réseaux sociaux, Hot- sociale '12, New York, NY, USA, pp. 25-31. ACM. Huang, J., K. M. Thornton et E. N. Efthimiadis (2010). marquage conversationnel sur Twitter. Dans Actes de la 21e conférence de l'ACM sur Hypertext et hypermédia, HT '10, New York, NY, USA, pp. 173-178. ACM. Jabeur, L. B., L. Tamine, et M. Boughanem (2012). Uprising microblogs: Un modèle de récupération de travail Net- bayésienne pour la recherche tweet. Dans Actes du 27 ACM Symposium annuel sur l'informatique appliquée, New York, NY, USA, pp. 943-948. ACM. Kumar, R., M. Mahdian, et M. McGlohon (2010). Dynamique des conversations. Dans Ings Proceed- du 16 SIGKDD conférence internationale sur la découverte des connaissances et l'exploration de données, KDD '10, New York, NY, USA, pp. 553-562. ACM. Magnani, M., D. Montesi, G. Nunziante et L. Rossi (2011). récupération de conversation de twit- ter. Dans Actes de la 33e conférence européenne sur les progrès dans la recherche d'information, ECIR'11, Berlin, Heidelberg, pp. 780-783. Springer-Verlag. Magnani, M., D. Montesi et L. Rossi (2010). analyse de la propagation de l'information sur un site de réseau social. Dans N. Memon et R. Alhajj (Eds.), ASONAM, p. 296-300. IEEE C.S. Magnani, M., D. Montesi et L. Rossi (2012). récupération de conversation pour les sites de micro-blogging. Dans Information.Retrieval Journal, Volume 15, pp. 354-372. Springer Netherlands. Dans CV travail CE, nous proposons Une nouvelle méthode de détection des conversations Sur les lieux des réseaux sociaux. This is méthode sur l'analyse basée et l'enrichissement de contenu in the mais de Presenter un sur basons Résultat les informatif interactions des utilisa- teurs. Nous Avons transforme en é sur notre valeur corpus méthode de réseau linked recueillis SOCIALA des sujets Spécifiques, et nous des bures Avons Obtenu Résultats. - 394 - F - Analyse des réseaux sociaux en utilisant le contexte social conversationnelle Pour les utilisateurs sur les sites détecter les interactions Microblogging Rami Belkaroui, Rim Faiz, Aymen Elkhlifi"
292,Revue des Nouvelles Technologies de l'Information,EGC,2015,Visualizing Shooting Spots using Geo-tagged Photographs from Social Media Sites,"Hotspots, à laquelle de nombreuses photographies ont été prises, pourraient être des lieux intéressants pour beaucoup de gens faire du tourisme. Visualisation des hotspots révèle les intérêts des utilisateurs, ce qui est important pour les industries telles que la recherche et du marketing touristiques. Bien que plusieurs techniques basées sociaux-pour hotspots extraction indépendamment ont été proposés, un hotspot a une relation à d'autres hotspots dans certains cas. Pour organiser ces hotspots, nous proposons une méthode pour détecter et de visualiser les relations entre les hotspots. Notre méthode proposée détecte et évalue les relations de taches de tir et sujets photographiques. Notre approche extrait les relations à l'aide de sous-hotspots, qui sont fendus d'un hotspot qui comprend des photographies de différents types.","Masaharu Hirota, Masaki Endo, Shohei Yokoyama, Hiroshi Ishikawa",http://editions-rnti.fr/render_pdf.php?p1&p=1002076,http://editions-rnti.fr/render_pdf.php?p=1002076,en,"Visualisant Spots de prise de vue à l'aide de photographies marquées Geo de Sites Social Media Masaharu Hirota *, Masaki Endo *, Shohei Yokoyama **, Hiroshi Ishikawa * * Graduate School of Design Système, Tokyo Metropolitan University Hino, Japon {Hirota-Masaharu, Endou, ishikawa- hiroshi}@sd.tmu.ac.jp, ** école supérieure des sciences et de la technologie, l'Université de Shizuoka Hamamatsu, Japon, yokoyama@inf.shizuoka.ac.jp Résumé. Hotspots, au cours de laquelle de nombreuses photos ont été prises, pourraient être in- intéres- places pour beaucoup de gens à faire du tourisme. Visualisation des points chauds révèle les intérêts des utilisateurs, ce qui est important pour les industries telles que la recherche et le marketing touristiques. Bien que plusieurs techniques à base sociale pour les points chauds ont indépendamment extraction été proposés, un point d'accès a une relation avec d'autres points chauds dans certains cas. Pour organiser ces points chauds, nous proposons une méthode pour détecter et visualiser les relations entre les points chauds. Notre Détecte méthode proposée et évalue les relations de points de tir et sujets photographiques. Notre ap- proche extrait les relations à l'aide de sous-points chauds, qui sont divisés à partir d'un point d'accès qui comprend des photographies de différents types. Nous démontrons notre approche en découvrant les relations à l'aide de métadonnées photographiques tels que les étiquettes, la entation de la photographie, et les lieux de photographie de Flickr. 1 Introduction Selon la popularité croissante des appareils mobiles tels que les appareils photo numériques et les téléphones intelligents, de nombreuses photos prises par les photographes ont été téléchargés sur les services Web de partage de photos tels que Flickr 1 et Panoramio 2. Récemment ces dispositifs ont inclus des systèmes de positionnement global intégré ( GPSs). En les utilisant, les photographes peuvent facilement prendre des photos avec des métadonnées photographiques telles que les informations de localisation et l'orientation de la photographie. Par- ticularly, des photographies avec une fonction photo-orientation sont devenus nombreux récemment (Zheng et al., 2011). De plus, de nombreuses photos sur les sites de médias sociaux ont des métadonnées qui sont annotés par les utilisateurs par le biais de l'étiquetage social. Beaucoup de gens pourraient prendre des photos de sujets tels que les paysages en fonction de leurs propres térêts in-. Ensuite, ils pourraient télécharger ces photos sur les sites de médias sociaux. Comme beaucoup d'endroits où les photographies ont été prises, ces lieux pourraient aussi être des lieux intéressants pour de nombreuses per- PLE ou visite à visiter la ville. Comme cela est décrit dans cet article, nous définissons des endroits tels que les points chauds. Figure 1. http://www.flickr.com/ 2. http://www.panoramio.com/ - 167 - Spots Visualizing Prise de vues avec photographies marquées Geo de Sites médias sociaux Big Ben London Eye Hotspot Fig. 1 - Exemples de points chauds. 1 présente des exemples de points chauds extraits en utilisant notre méthode proposée à partir de photographies prises autour de Big Ben. Dans cette figure, les polygones noirs représentent les zones de points chauds extraits. Certaines méthodes ont été proposées pour les points chauds d'extraire des données à partir de photographies sur les sites de partage de photos (Crandall et al, 2009;. Kisilevich et al, 2010;.. Shirai et al, 2013;. Hirota et al, 2014). Les points chauds extraits pourraient refléter les intérêts des personnes, ou être utiles pour la recherche marketing, analyse spatiale, et ainsi de suite. L'analyse de ces lieux est important pour les industries telles que celles liées au tourisme (Sengstock et Gertz, 2012;. Kisilevich et al, 2010). En outre, les systèmes de recommandation d'attraction touristique tels que (Lu et al, 2010;. Et Grefenstette Popescu, 2011) peuvent utiliser cette approche. En présentant les points chauds aux personnes qui visitent une ville pour la première fois, notre approche aide le tourisme. Bien que de nombreuses méthodes pour extraire et les points chauds de Visualize de sites de médias sociaux ont été proposés, ces études visualisent les intérêts des utilisateurs, mais ils ne portait pas spécifiquement sur les relations (Crandall et al, 2009;.. Kisilevich et al, 2010;. Shirai et al, 2013). Par conséquent, nous extrayons les relations entre les points chauds. Hotspots dans des études antérieures sont extraites de façon indépendante, mais certains points chauds sont liés. Cependant, l'extraction et la visualisation des relations entre les points d'accès est important d'organiser des points d'accès, et t o trouver la nature des points chauds. Par exemple, la figure 1, autour de Big Ben à Londres, les points chauds ont été extraits à l'intérieur de Big Ben et autour de Big Ben. Les photographies prises autour de Big Ben ont été prises dans le sens de Big Ben. avant There-, ces points chauds ont un certain rapport au lieu de tir et le sujet photographique. Comme cela est décrit dans cet article, on extrait ces relations de points de tir et sujets photographiques. - 168 - M.Hirota et al. Nous définissons un point de tir comme un type de point d'accès dans une zone où les photos ont été prises dans le sens des lieux intéressants, comme un point de repère à un endroit qui est éloigné de là. Points de tir de l'extraction des sites sociaux révèle des endroits attrayants pour les touristes, basé sur l'activité des utilisateurs. En outre, la plage dans laquelle les gens sont en mesure de prendre des photos d'un point de repère est important pour les attractions touristiques et des systèmes de recommandation de routage. Par conséquent, dans ce PA- par, nous examinons plus précisément l'extraction et la visualisation des relations entre des points de tir et sujets photographiques de points d'accès des sites de médias sociaux. De plus, on extrait les relations en utilisant des sous-points d'accès. Un sous-hotspot est un point d'accès qui comprend des photographies de différents types. Il est divisé à des types distincts de point d'accès. Par exemple, la figure 1, autour de Big Ben, plusieurs zones sont apparentes où les photographies ont été prises pour célèbres monuments: Big Ben et le London Eye (une célèbre grande roue géante près de la Tamise). Hotspots sont extraits de cette région. Le point d'accès marqué par un cercle violet dans cette figure est associée à des photographies prises à la fois Big Ben et le London Eye. Par conséquent, les points chauds sont de tir pour les deux points de repère. Pour extraire les relations depuis le point de tir à chaque point de repère, nous devons diviser un point d'accès à des sous-points d'accès en fonction des points de repère. Par conséquent, nous avons divisé un point d'accès en sous-points d'accès à l'aide de l'étiquetage social et les relations d'extraction. Le reste de l'article est organisé comme suit. La section 2 présente les travaux liés à ce sujet. La section 3 présente une description de notre méthode proposée aux relations calculate de sujet et de la place de tir à l'aide d'orientations photographie, lieux de photographie, et l'étiquetage social. De plus, nous décrivons les sous-points chauds de points chauds extraits à l'aide de l'étiquetage social. L'article 4 explique plusieurs exemples de notre système proposé et présente une analyse des résultats. La section 5 conclut le document avec une analyse des résultats et des travaux futurs. 2 Travaux connexes 2.1 Extraction des points chauds à partir de sites de partage de photos Certaines méthodes ont été proposées pour les points chauds de l'extrait des nombreuses photos qui sont disponibles sur les sites de partage de photos. Ces méthodes utilisent des algorithmes de clustering à base de densité tels que dbscan (Ester et al., 1996) aux points chauds d'extrait d'un énorme ensemble de données de photographie. Cran- dall et al. a présenté une méthode pour extraire des repères et points d'accès en utilisant une technique de classification basée sur de nombreuses photographies marquées géo disponibles sur Internet (Crandall et al., 2009). En outre, Kisilevich et al. ont proposé une méthode pour extraire un point d'accès en utilisant la densité d'emplacements de graphes photo- en fonction des résultats de clustering (Kisilevich et al., 2010). Ces méthodes traitent chaque hotspot comme indépendant. Cependant, certains points d'accès sont liés à d'autres points chauds tels que des sujets graphiques photo- et points de tir. Shirai et al. ont proposé une méthode pour extraire un point d'accès et pour calculer le rapport des points d'accès (Shirai et al., 2013). Pour découvrir un large domaine d'intérêt, cette approche infère la relation entre les points d'accès en fonction de l'emplacement de la photographie et de l'orientation. Cependant, d'autres relations entre les points d'accès peuvent exister tels que des taches de tir et sujets photographiques. En outre, bien que ces études extraient hotspots à partir de sites de médias sociaux, ils ne considèrent pas le contenu des points chauds. Comme cela est décrit dans cet article, nous détectons les sous-points d'accès basés sur l'étiquetage social à partir d'un point d'accès pour extraire les relations du sujet et de la place de tir entre les sous-points chauds à considérer ph orientation Otograph. - 169 - Visualizing Spots de tir à l'aide de photographies marquées Geo de Sites sociaux Relations avec les médias sur Londres points chauds Extrait des yeux en utilisant les points chauds de la fracture grille de regroupement aux sous-points d'accès à l'aide des relations de l'étiquetage social Extrait du point de tir et sujet photographique sous-hotspot Hotspot Hotspot sur Big Ben hotspot sous-sur London Eye Relation Big Ben figure. 2 - Vue d'ensemble de notre méthode proposée. 2.2 Visualisation basée sur l'orientation de la photographie Selon photographies avec une orientation de photographie sont devenus plus couramment disponibles, l'orientation de la photographie est utilisée pour extraire les points chauds. Lacerda et al. proposé un procédé d'extraction à l'aide de points d'accès orientations de photographie (Lacerda et al., 2012). Cette méthode calcule les intersections entre les lignes d'orientation photographique de plusieurs phies de pho-. Les intersections sont regroupées à l'aide dbscan. En outre, Thomee et al. proposé une méthode pour l'examen des inexactitudes affectant les mesures de localisation GPS (Thomée, 2013). Nous considérons cette approche comme efficace pour l'extraction de sujets photographiques, mais cette approche a une limitation des zones d'extraction où les photographies ont été prises comme des points chauds. De plus, les données GPS actuelles servent uniquement à identifier l'endroit où la photo a été prise. La plupart des appareils ne sont pas équipés de capteurs pour mesurer l'orientation. En conséquence, les points chauds d'extrait, l'approche de l'utilisation de l'orientation de la photographie est pas applicable pour la zone où les photos énormes ont été prises. Par conséquent, nous appliquons un algorithme de classification basé sur les localisations pho- de points d'accès à tographe extrait. Dans la littérature connexe, certaines méthodes pour estimer l'orientation de la photographie ont été présentées, même pour les photos qui semblent avoir aucune orientation photographique (Park et al, 2010;. Huang et al, 2012.). Notre méthode utilise les photos avec l'orientation de la photographie aux relations d'extraction. Par conséquent, nous nous attendons à utiliser l'orientation photographie estimée pour augmenter la précision de notre méthode. 3 Méthode proposée Nous proposons une méthode pour détecter les relations de sujets photographiques et des points de vue à l'aide de l'orientation photographie, l'emplacement et l'étiquetage social. Par conséquent, nous utilisons des photos pour lesquelles le résultat obtenu comprend des informations relatives à l'orientation de la photographie, l'emplacement, - 170 - M.Hirota et al. et les étiquettes. Nous présentons un aperçu de notre approche à la figure 2. Pour détecter les relations de lieu de tournage et sujet photographique, notre approche comprend les étapes suivantes. 1. Pour un domaine particulier, nous obtenons un grand nombre de photos de Flickr. 2. Nous appliquons un algorithme de classification basé sur une grille des photographies obtenues sur la base des emplacements de pho- tographe pour extraire les lieux où de nombreuses photos ont été prises comme des points chauds. 3. Utilisation de points chauds extraits, nous groupent les balises de photos en fonction de leur distri- bution géométrique. nous utilisons des balises en cluster extrait de diviser les points chauds extraits à différents sous-points d'accès. 4. Calcul de la pertinence des sous-points d'accès en utilisant l'emplacement du point d'accès et d'orientation, pour extraire la relation entre le point de vue et le sujet photographique. Nous décrivons les détails relatifs aux différentes étapes ci-dessous. 3.1 Extraction des points chauds Identifier hotspot lieux où de nombreuses photos ont été prises, nous avons examiné en particulier le nombre d'emplacements de photographie pour trouver les endroits où les photos sont prises par de nombreuses personnes. Pour ce faire, nous dressons la carte des photos qui ont un emplacement de photographie à une grille à deux dimensions. Ces photographies sont mises en correspondance avec les coordonnées comme suit. y = mhauteur - (Lat - Latmin) * mhauteur Latmax - Latmin (1) x = mlargeur - (Lng - Lngmin) * mlargeur Lngmax - Lngmin (2) Ici, Lat représente la latitude de la photographie (GPSLatitude dans Exif). Lng est la longitude de la photographie (gpslongitude en Exif). Latmax, Latmin, Lngmax et Lngmin représentent respective- ment les valeurs maximales et minimales de Lat et Lng. En outre, mhauteur et mlargeur sont la hauteur et la largeur de la grille (Ceci est décidé au moyen d'un paramètre d pour l'ajustement que le nombre de cellules que nous voulons faire dans cette procédure.). Par conséquent, chaque cellule de la grille obtenue comprend une photographie prise dans l'intervalle. Utilisation de la grille obtenue, on extrait les cellules pour lesquelles le nombre de photos de sa cellule y com- est supérieure au seuil MinP. Les cellules extraites à proximité, qui sont mutuellement connectées des cellules, sont reliés. Chaque groupe de cellules jointes est défini comme un cluster. Pour extraire les points chauds de la grille obtenue, on applique la grille-Clustering Axis DÉCALÉE al gorithme (Chang et al., 2009) à la grille obtenue. Cette méthode de classification est basé regroupement densité-réseau avec une stratégie de partitionnement décalée axe afin d'identifier les zones à forte densité. Un avantage impor- tant de cette méthode est le réglage de la dynamique de la taille des cellules d'origine dans la grille et à la réduction de la faiblesse des frontières de cellules, en déplaçant la grille d'origine dans chaque dimension de l'espace de données après les grappes générées à partir de cette la grille d'origine sont obtenus. La procédure de la grille-Clustering Axis-Shifted comprend les étapes suivantes: 1. Extraction de grappes par le procédé utilisant la procédure décrite ci-dessus. Cette Tering assorti- est notée C1. - 171 - Visualizing Spots de tir à l'aide de photographies marquées Geo de Sites médias sociaux ben, grand ben, oeil d'horloge, London Eye, roue tesson, pont gratte-ciel ferroviaire, la gare, la pierre de train, plafond, rivière musée, la Tamise tour TAB. 1 - Exemples de balises en cluster. 2. La grille utilisée pour extraire C1 est transformé. La grille est décalée de la moitié de la distance d / 2 dans chaque dimension. Pour obtenir de nouveaux résultats de classification, la procédure d'extraction des grappes est appliquée à nouveau à la grille décalée. Ce nouveau résultat de regroupement décalée est désignée par C2. 3. Clustering résultat C1 est révisé en utilisant C2. Nous trouvons chaque groupe chevauchée dans les deux résultats de regroupement, où C1a ∩ C2b ̸ = ∅, C2b ∈ C2, et C1a ∈ C1. C1 est modifiée, en joignant les grappes superposées extraites. Par conséquent, le résultat de classification finale comprend C1 ∪ C2. Enfin, chaque groupe extrait est défini comme un point d'accès. Chaque point chaud comprend des photographies du cluster. 3.2 Clustering balises pour créer des sous-points chauds Après extraction de points chauds photos, nous diviser chaque point d'accès en sous-points chauds BE- la cause d'un point d'accès a souvent plusieurs photographies prises dans le sens du sous-Ject photographique. Par conséquent, pour cette étude, nous en déduisons sous-points d'accès pour chaque sujet d'un tel point d'accès basé sur les étiquettes que les photos ont. Lorsque nous faisons des sous-points d'accès, nous organisons des balises similaires qui resent les mêmes significations repré- en groupes tels que « big ben » et « ben ». Photographies avec chaque balise en cluster dans un hotspot peut révéler un aspect des significations associées au hotspot. Pour organiser des tags similaires, nous regroupons l'étiquetage social en utilisant des vecteurs de caractéristiques hotspot à base de vecteurs de caractéristiques géo-spatiales (Zhang et al., 2012). Les différents types de balises de photographies ont différentes distributions géographiques. Le vecteur de caractéristique géospatiale d'une étiquette est représentée par un géo-bin, qui comprend des cellules d'une grille ayant le nombre de photos avec le tag. En conséquence, la dimension du vecteur est extrêmement important. Cependant, la grille obtenue à partir de photographies avec des étiquettes sont rares. Par conséquent, pour diminuer le coût de calcul, on calcule les vecteurs de caractéristiques en utilisant les points chauds extraits au lieu des bacs géo. Pour calculer une fonction géo vecteur pour Tagi, nous comptons les photos avec le tag qui ont été prises dans une zone de point chaud h en U (g, t). Nous comptons les utilisateurs qui ont demandé une étiquette dans un point chaud au lieu des photographies. De ce fait, nous évitons les utilisateurs de haute activité de polarisation de la distribution (Ahern et al., 2007). Ensuite, nous appliquons L2-normalisation au vecteur pour calculer la géo-bin vi (t) de l'étiquette t comme suit. vi (t) = U (g, t) √Σ | H | j = 1 U 2 (j, t) (3) Ici, | H | est le nombre de points chauds extraits dans la section 3.1. Il a été décrit dans un rapport antérieur (Zhang et al., 2012) que cette fonction L2-normalisation mieux. Par conséquent, nous utilisons la même méthode. Après des vecteurs géo-caractéristiques de chaque balise sont calculés, nous regroupons balises basées sur la ity similaire- des distributions géographiques en utilisant le regroupement hiérarchique. Ensuite, nous utilisons - 172 - M.Hirota et al. Procédé pour calculer la distance entre les grappes. En outre, pour arrêter les étapes agglomératives, nous utilisons un paramètre de critère: la distance minimale est de cluster supérieur au critère. Des exemples de balises en cluster sont présentés dans le tableau 1. Enfin, nous définissons un sous-ensemble de photographies avec des balises d'un cluster dans i-ième salut hotspot comme un sous-point d'accès IHV de salut. h s i comprend des photographies qui ont une étiquette de la s-ième cluster ou plus. 3.3 Extraction des relations de place de tir et sujet photographique aux relations d'extrait de taches de tir et sujets photographiques, nous calculons la pertinence des sous-points d'accès en fonction de l'emplacement géométrique extraits. Si les photos en sous-hotspot HSI ont été prises à des sujets photographiques inclus dans HSJ, puis h i s est un spot de tir de sj h. Ces points chauds sous ont une relation entre HSI et sj h. Pour confirmer l'orientation de s i h à j de h, les caractéristiques de trois types sont utilisés: la distance, l'orientation et le rapport de photographies. Tout d'abord, on calcule la distance entre Hubeny HSI et sj h parce que les photographies liées à l'endroit de tir auraient été prises à proximité d'un point d'accès qui comprend un sujet photographique. La distance ds Hubeny (i, j) est calculé comme ds (i, j) = √ (M * dP) 2 + (N * cos (P) * dR) 2. (4) à l'intérieur, P signifie la latitude moyenne de centroïdes de hsi et S j h. En outre, dP et dR représentent respective- ment les différences de latitude et la longitude de centroïdes de HSI et sj h. M représente le rayon de courbure d'un méridien. N est un rayon de courbure pour un premier vertical. Ensuite, nous vérifier si l'orientation des photographies sous-point d'accès IHV sj h. Hs je pourrais être un endroit de tir pour prendre des photos de sujet dans HSJ si de nombreuses photographies de h i s face à la direction de HSJ. , On calcule donc le degré d'orientation de s i h à j de h. Tout d'abord, on calcule l'orientation principale de HSI. Nous avons divisé les valeurs d'orientation de photographie h i est de 20 ° et compta les photos de chaque catégorie. Nous utilisons des métadonnées (gpsimgdirectionref ou gpsimgdirection dans Exif) que l'orientation de la photographie. La classe max MDSI est l'orientation principale de HSI. On calcule la moyenne de ces classes si le nombre de classes max est supérieur à un. Ensuite, on calcule l'orientation de ldsij de barycentre (x1, y1) de HSI barycentre (x2, y2) HSJ comme ldsij = tan -1 cosy2 * sin (x2 - x1) cosx1 * siny2 - siny1 * cosy2 * cos ( x2 - x1). (5) Le degré o (i, j) est d'orientation de hsi à S j h est calculée comme os (i, j) = | MDSI - ldsij |. (6) on calcule le rapport des photographies de hsi et S j h. Cette fonction permet d'éviter le cas où un autre endroit de tir est situé au milieu de la tache de tir HSI et le sujet photographique en HSJ. Éviter ce cas est difficile en fonction des caractéristiques décrites précédemment. Dans le domaine des sujets photographiques tels que un point de repère, les gens ont pris de nombreuses photos à l'intérieur ou à proximité. Le nombre de photographies dans les lieux de tournage pour le sujet est à peu près identique ou inférieure à la sous-hotspot extraite de la zone. Les rs de rapport (i, j) de photographies de hsi et HSJ est calculé comme rs (i, j) = | HSJ | | HSI | . (7) - 173 - Prise de vues avec Visualizing Spots photographies marquées Geo de Social Media Sites Big Ben London Eye Trafalgar Square Charing Cross a b c d e f g h i j Relation Hotspot Point de repère Fig. 3 - Relation Big Ben (Londres de jeu de données). Big Ben London Eye a b c d e f g h i j Trafalgar Square Charing Cross Relation Hotspot Landmark FIG. 4 - Relation pour London Eye (Londres de jeu de données). PREVOIT, | HSI | et | HSJ | est le nombre de photographies de HSI et HSJ. Enfin, on calcule la pertinence de NRE (i, j) à j HSI et h comme NRE ( i, j) = √ ds (i, j) * os (i, j) * 1 rs (i, j). (8) La valeur plus petite de rels (i, j) est la pertinence de forte hsi à S j h. Ici, nous éliminons la relation du point chaud du sujet photographique à un autre point d'accès. Par conséquent, nous détectons le point d'accès qui comprend des sujets photographiques de points chauds extraits sur s-ième balises en cluster. Comme décrit ici, nous définissons le point chaud relation recueilli de nombreux autres points chauds comme un point d'accès qui comprend un sujet photographique. Le point d'accès d'un sujet photographique est plus | NER | * 0,5, où || NER | est le nombre de relations sur s-ième balises en cluster. Nous visualisons NRE de relation (i, j) de moins que le seuil réglable sur Google Maps 3. 4 Expériences Cette section présente une description des expériences menées en utilisant notre méthode proposée. Nous présentons et discutons de plusieurs exemples de détection des rapports de points de tir et sujets graphiques photo-. Les photographies pour les expériences sont obtenues à partir des résultats de recherche photographiques de Flickr. Ces photos contiennent des métadonnées Exif de latitude (gpslatituderef, GPSLatitude), la longitude (gpslongituderef, gpslongitude), des étiquettes et des horodateurs. Figures 3 et 4 présentent les résultats des relations de tir des taches et des sujets photographiques dans la région de Big Ben et le London Eye. Nous avons utilisé des photos prises au cours de 1,123,550 3. https://www.google.co.jp/maps/ - 174 - M.Hirota et al. 1 Janvier 2010-31 Juin 2014 et pris à Londres (latitude: -0,450439-0,148315 et la longitude: 51,301643-51,669361). Les paramètres de classification sont définis comme d = 200 et MINp = 20. Dans ces figures, les émissions de polygones noirs lieu extrait en tant que point chaud. La ligne fléchée montre la relation. La flèche à un point d'accès du point de tir à un autre point d'accès d'un sujet photographique. Ici, aucune différence existe entre les couleurs de ces lignes. Ceci est seulement une représentation facile à vue. De plus, ces chiffres, plusieurs lieux célèbres et monuments existent dans ce domaine: Big Ben (point orange), le London Eye (point rouge), Trafalgar Square et Charing Cross (point vert). La figure 3 montre les nombreux points chauds avec des lignes au hotspot fléchés qui comprend Big Ben. Dans les points chauds reliés par des lignes, le point d'accès qui inclut Big Ben (d sur la figure 3) a photogra- phies prises autour de Big Ben. D'autres (a-c, e-g dans la figure 3) des photos prises dans le sens de Big Ben. Par exemple, les points chauds de b comprennent les sept photographies prises à 60- 120 °, 0 ° est au nord, ce qui augmente avec la rotation dans le sens horaire. Ici, les lignes entre certains points d'accès (tels que i et j) et point chaud d ne sont pas tirées. Ces points chauds inclure des photos de Big Ben, mais quelques photos parmi celles-ci ont pas les métadonnées d'orientation. Par conséquent, nous ne pouvons pas calculer les relations entre ces points chauds. En outre, l'utilisation des fonctions de la distance, l'orientation et le rapport, la relation de point chaud à f d est extrait de manière appropriée. Par exemple, si l'on extrait les relations entre les points d'accès en fonction de la distance entre les points chauds ou orienta- tion, la relation du point chaud f est extrait comme face e hotspot. Notre méthode extrait les rela- tions entre le lieu de prise de vue et sujet photographique en utilisant trois caractéristiques. En outre, sur la figure 4, les deux points d'accès (D et F) ont fléché lignes à point chaud h. Pourquoi certains points d'accès ont aucun rapport avec hotspot h est la même raison que celle du nombre de photos avec des métadonnées tion orienta-, comme décrit précédemment. Hotspot f a deux types de photographies prises à Big Ben et le London Eye. Le lieu de hotspot f est un lieu de prise de vue pour prendre ces points de repère. En outre, par rapport aux figures 3 et 4, en tirant des taches et des sujets photographiques, points chauds h et d sont intervertis en fonction de ce rapport sur un sujet extrait nous. Par conséquent, les points chauds de division aux sous-points d'accès basés sur l'étiquetage social, puis d'extraire les relations de chaque type de sujet photographique est important. Ensuite, nous insistons sur l'efficacité de la combinaison de trois caractéristiques: la distance, l'orientation et le rapport de le nombre de photos. Ici, nous montrons les résultats des rapports de points de tir et sujets photographiques obtenus à partir d'un autre ensemble de photographies prises à New York. Nous avons utilisé des photos prises 1,300,315 candidathèque pendant 1 Janvier 2010-31 Juin 2014 et pris à New York (latitude: 40.700422 - 40.804324 et Longitude: -74.028454 - -73,929577). Les paramètres de classification sont définis comme d = 500 et MINp = 50. Les figures 5, 6, 7 et 8 montrent des points d'accès et des relations extraites à travers le pont de Brooklyn orange et Manhattan Bridge le blanc dans ces figures. Les relations de la figure 5 ont été extraites à partir de trois éléments de l'équation 8. En outre, la relation sur les figures 6, 7 et 8 ont été extraits en fonction de chaque caractéristique. La figure 5 montre de nombreux points chauds liés à b hotspot, dans lequel le centre du pont de Brooklyn existe. Ces résultats montrent que ces lieux tirent des points pour prendre des photos du pont de Brooklyn. Dans la figure 6 en fonction des orientations photographiques, il est évident que certains points d'accès ont une relation à différents points d'accès tels que les points chauds et un g, par rapport à la figure 5. Toutefois, en effet, de nombreuses photographies prises dans ces points chauds sont pris dans la direction du centre du pont de Brooklyn. La seule raison pour laquelle les relations basées sur l'extraits en orientation sont prises dans le sens d'un point d'accès pourrait être l'erreur de métadonnées d'orientation mesurée par GPS. De plus, si les métadonnées sont mesurées improprement, en termes de - 175 - Visualizing Spots de prise de vue en utilisant des photographies marquées par Geo Sites de Social Media a b c d e f g h i Relation Hotspot Fig. 5 - relations extraites à partir de trois caractéristiques combinées (ensemble de données de New York). a b c d e f g h i Relation Hotspot Fig. 6 - relations extraites la base des orientations (ensemble de données de New York). a b c d e f g h Relation Hotspot Fig. 7 - basé sur les relations extraites les points chauds séparant de distance (ensemble de données de New York). a b c d e f g h Relation Hotspot Fig. 8 - relations extraites en fonction du rapport entre le nombre de photographies (ensemble de données de New York). composition photographique, le sujet photographique n'a pas été positionné au centre de la photographie. Un écart séparant le centre de photographies et de mesure peuvent se produire. En raison de ces lacunes et en raison de l'imprécision de la mesure, nous pensons que l'extraction des relations basées uniquement sur l'orientation est insuffisante. En outre, la figure 7, les points chauds montrent une relation à un point d'accès plus proche. En conséquence, une relation qui photographies ont été prises au pont de Brooklyn ou Manhattan Bridge n'est pas extrait. Dans la figure 8, tous les points d'accès ont des relations à point d'accès b. Cependant, hotspot c la figure 5 a une relation hotspot h car hotspot c comprend des photographies du pont de Brooklyn, mais plus de photos de Manhattan Bridge. Par conséquent, pour extraire les relations du point de tir et sujet photographique, ces caractéristiques qui sont appliquées aux points d'accès ne sont pas réalisables. Les résultats montrent que notre méthode basée sur les relations entre les caractéristiques combinées de manière efficace. 5 Conclusions Nous avons proposé une méthode pour extraire et visualiser la relation d'un spot de tir et un sujet photo- tographiques en utilisant des photographies avec des informations de localisation liées aux sites de partage de photos. - 176 - M.Hirota et al. Nous extrayons les points chauds de lieux où de nombreuses photos ont été prises. Ensuite, nous avons divisé les points chauds en sous subdivisées ex-points d'accès en fonction des distributions géométriques de l'étiquetage social. Pour cal- culer les relations, notre approche utilise l'orientation photographique, les relations de position des points d'accès, et le rapport entre le nombre de points chauds. Nous avons présenté quelques exemples de résultats obtenus à partir Flickr utilisant notre méthode. Dans plusieurs cas, nos méthodes étaient suffisantes pour détecter et visuo ize les relations de points de tir et sujets photographiques. De plus, nous avons discuté de la disponibilité de l'utilisation de trois caractéristiques pour extraire les relations. Des études futures seront menées pour estimer les métadonnées telles que l'orientation photographique et l'étiquetage social à l'augmentation"
304,Revue des Nouvelles Technologies de l'Information,EGC,2014,Automatic correction of SVM for drifted data classification,"Concept drift is an important feature of real-world data streams thatcan make usual machine learning techniques rapidly become unsuitable. Thispaper addresses the problem of sudden concept drift in classification problemsfor which standard techniques may fail. To this end, support vector machines(SVMs) are automatically corrected to cope with a new suddenly drifted dataset.Results on real-world datasets with several types of sudden drift indicate that themethod is able to correct the SVM in order to better classify the new data afterthe concept drift, using a correction based on the difference between the initialdataset and the new drifted dataset, even when the new dataset is small.","Alexandra Degeest, Benoît Frénay, Michel Verleysen",http://editions-rnti.fr/render_pdf.php?p1&p=1001942,http://editions-rnti.fr/render_pdf.php?p=1001942,en,"Correction automatique de SVM pour dérivait classification des données Alexandra Degeest *, **, Benoît Frenay *, Michel Verleysen * * machine Learning Group, ICTEAM, Université Catholique de Louvain, Place du Levant 3, 1348 Louvain-la-Neuve, Belgique ** ISIB , Haute-Ecole Paul-Henri Spaak Rue Royale 150, 1000 Bruxelles, Belgique alexandra.degeest@uclouvain.be, degeest@isib.be Résumé. la dérive Concept est une caractéristique importante de flux de données réelles qui peuvent faire des techniques habituelles d'apprentissage de la machine deviennent rapidement inadaptés. Ce document porte sur le problème de la dérive concept soudaine des problèmes de classification pour lesquels des techniques standard peuvent échouer. A cet effet, le soutien des machines vectorielles (SVM) sont automatiquement corrigées pour faire face à un nouveau dériva soudain ensemble de données. Résultats sur des ensembles de données du monde réel avec plusieurs types de dérive brutale indiquent que la méthode est en mesure de corriger le SVM afin de mieux classer les nouvelles données après la dérive du concept, en utilisant une correction en fonction de la différence entre l'ensemble de données initial et le nouveau dérivé ensemble de données, même lorsque le nouvel ensemble de données est faible. 1 Introduction Concept dérive dans les flux de données du monde réel est une préoccupation standard. la dérive Concept se produit lorsque la distribution sous-jacente des données change au fil du temps. Il peut se produire de plusieurs façons: la dérive soudaine (modification brusque des données distribution statistique), la dérive progressive (période de temps où deux distributions différentes sont actifs), la dérive progressive (modification progressive des données distribution statistique) ou des contextes récurrents (réapparition périodique des différentes distributions statistiques) (Zliobaite, 2010). Cet article traite de la dérive soudaine. Les raisons pour lesquelles la dérive concept soudain se produit dans des ensembles de données sont nombreuses et diverses. À titre d'exemple, dans les applications industrielles, les données peuvent dériver soudainement en raison de l'entretien du moteur, car l'outil a été déplacé ou parce que les nouvelles données ont été recueillies par une autre personne. L'apparition de la dérive du concept dans les flux de données perturbe souvent les techniques habituelles d'apprentissage de la machine et, par conséquent, apporte la nécessité de développer de nouvelles techniques d'apprentissage ou d'adapter ceux qui existent déjà. machines à vecteurs de support (SVM) de sont un outil puissant, couramment utilisé pour la classification des données de grande dimension. Cependant, SVM peuvent rapidement devenir inutiles lorsque les données sont sujettes à la dérive du concept. L'objectif de cet article est de proposer une méthode pour corriger automatiquement le modèle SVM face à la dérive soudaine d'un problème de classification. L'objectif est double. Tout d'abord, l'objectif est de nous permettre d'utiliser le premier ensemble de données (avant la dérive) pour l'apprentissage du modèle, ce qui est particulièrement utile lorsque le second ensemble de données (après la dérive) est faible. Le deuxième objectif est de corriger la décision hyperplan de mieux classifier de nouveaux ensembles de données, même lorsque leur distribution statistique (un peu) diffère de la distribution du premier ensemble de données. Deux hypothèses ont été faites dans ce travail. Tout d'abord, les étiquettes des données sont inconnues dérivaient. En second lieu, le jeu de données est faible a dérivé - 311 - Correction automatique de SVM pour dérivait figure de classification des données. 1 - Des types structuraux de dérive concept, inspiré par Zliobaite (2010). par rapport au premier ensemble de données. L'article 2 est une brève revue de la littérature sur les différents types de dérive du concept. Le problème et la méthode proposée sont décrites respectivement dans les sections 3 et 4. La section 5 présente les résultats expérimentaux alors que la section 6 conclut le travail. 2 dérive Concept Concept dérive se produit lorsque la distribution des données change au fil du temps. Ce phénomène se produit fré- quently dans les flux de données du monde réel. Pour faire face à la dérive du concept, de nouvelles méthodes d'apprentissage de la machine doivent être développées ou techniques communes doivent être améliorées. Plusieurs aperçus approfondis sur la dérive concept ont été écrits (Zliobaite, 2010; Tsymbal, 2004). En consé- ING Zliobaite (2010) devraient être plus adaptées au problème lui-même, les techniques de dérive de concept, car trop d'études recherchent une réponse universelle à des dérives en général, ce ne sont pas opt assez iMAL. En effet, il existe plusieurs types de dérive concept (voir la figure 1), tels que la dérive soudaine, la dérive progressive, la dérive progressive et contextes récurrents (Zliobaite, 2010). Chaque type de dérive est associée à une classe spécifique des problèmes réels et a besoin d'une approche spécifique. Ce document fait face à des problèmes de dérive brutale; ceux-ci sont communs dans l'industrie et la médecine, par exemple. dérive brutale peut se produire en raison de l'entretien du moteur, le déplacement de la machine ou une modification non souhaitée dans des conditions environnementales. Par exemple, si un médecin enregistre un ECG à partir d'un premier patient lundi et d'un deuxième patient mardi, les conditions environnementales peuvent avoir changé (température, l'emplacement des sondes...). Les deux propriétés de jeux de données (distribution...) Sera différent, qui peut entraîner une dérive. Cependant, il serait vraiment intéressant de pouvoir utiliser l'ensemble de données à partir du premier ensemble de patients avec l'ensemble de données du deuxième patient, sachant combien il peut être difficile d'obtenir un ensemble de données de formation suffisamment grande. 3 Énoncé du problème Cet article traite du problème de la dérive concept soudaine des problèmes de classification. La principale question est « Comment garder et corriger un modèle M1, a appris la base de données D1, afin de l'utiliser pour évaluer un concept voguaient une autre base de données D2? » Deux hypothèses principales sont faites dans ce travail. La première est que les étiquettes du dérivé données D2 ne sont pas connus; ce qui correspond à des situations réalistes. Pour le premier jeu de données D1, avant la dérive, un spécialiste est disponible pour classer les résultats au cours des périodes de formation et de validation. Cependant, pour le deuxième jeu de données D2, après la dérive, par exemple, après un déplacement d'outil, aucun spécialiste est plus disponible et il serait trop coûteux d'appeler un expert pour donner les étiquettes chaque fois qu'il ya une légère dérive sur la distribution statistique. La seconde hypothèse est que le jeu de données a dérivé D2 est beaucoup plus faible - 312 - A. Degeest et al. que l'ensemble de données undrifted D1. Cela correspond également à une situation commune: après la dérive, le modèle doit encore bien performer, même si le nouvel ensemble de données D2 est uniquement composée de quelques cas. Il est donc intéressant de trouver une méthode qui fonctionne sur un petit ensemble de données non marqué. SVM sont déjà utilisés pour la détection de dérive concept (Campigotto et al, 2010;. Dries et al, 2009;.. Klinkenberg et al, 2000) et pour l'apprentissage progressif (Rüping, 1999). Le travail le plus proche de ce document, par Yang et al. (2007), utilise également SVM à face dérive brutale. La différence entre fonda- mentale Yang et al. (2007) et ce travail est que Yang et al. (2007) a besoin de données étiquetées dans l'ensemble de données a dérivé pour adapter le classificateur. En outre, il convient de noter que Yang et al. (2007) a besoin d'un pré-traitement consistant à sélectionner certains cas, dans l'ensemble de données, ce qui pourrait être risqué si l'ensemble de données est faible. L'originalité du procédé décrit dans le présent document est qu'il adapte le modèle directement hyperplan aux données dérivé non marquée, basée sur les distributions a dérivé et de données undrifted. Il n'y a pas besoin d'une sélection hasardeuse de données de D2, et il n'y a pas besoin d'étiquettes sur ces données dérivaient. Cela a l'avantage d'offrir une solution simple et rapide correction au modèle, une fois que les données ont dérivé. Il est rapide parce que le modèle n'a pas besoin d'être formé à nouveau; en particulier les méta-paramètres du modèle ne doivent pas nécessairement être sélectionné et validé à nouveau, après la dérive du concept, ce qui peut prendre du temps. 4 Support Vector Machine avec correction de données Drifted Cette section présente la principale contribution de cet article. Une méthode pour modèles SVM corrects face à la dérive soudaine est décrite. Pour les modèles de SVM corrects, la méthode utilise une correction en fonction de la différence entre l'ensemble de données initial et le nouvel ensemble de données dérivait. Que D1 représente l'ensemble de données undrifted avec des instances marquées et D2 le jeu de données sans étiquettes ont dérivé. Le modèle M1 apprend que SVM de D1. Les méta-paramètres du SVM sont sélectionnés par validation croisée. Ensuite, la classification des M1 sur D1 est évaluée. Sans modifier M1, le modèle est évalué sur le jeu de données D2 a dérivé, qui peut entraîner un taux d'erreur de classification plus grande, en raison de la dérive brutale. Le principe de l'algorithme de correction proposé est alors d'utiliser les valeurs de décision de M1 sur D1, appelé M1 (D1) et de M1 sur D2, appelé M1 (D2). Les valeurs de décision représentent les distances entre l'hyperplan de séparation et de chaque instance de l'ensemble de données. Chaque instance (x, y) des ensembles de données D1 est constitué d'un vecteur de particularité x ∈ Rn et une étiquette de classe y. Soit ω le vecteur de poids et b le seuil. La distance entre l'hyperplan et chaque instance de l'ensemble de données est ensuite | ωx + b | / || ω || où ωx + b est l'équation d'hyperplan, conduisant à la fonction de décision f (x) = signe (ωx + b). Avant la dérive, le hyperplan optimal est celui avec la marge maximale de séparation entre les classes de l'ensemble de données undrifted D1. Les valeurs de décision M1 (D2) représente la distance entre le même hyperplan, calculé sur D1, et chaque instance du jeu de données D2. Une fois que les valeurs de décision ont été calculées sur D1 et D2, les fonctions de densité de probabilité sont des estimations sur les deux ensembles. La figure 2 montre un exemple, avec une droite M1 (D1) et une ligne en pointillés pour M1 (D2). des fonctions de densité de probabilité des valeurs de décision sont utilisés parce qu'ils sont un moyen facile de caractériser un tracé à une dimension des différences entre les fonctions de densité de probabilité de M1 (D1) et M1 (D2). Lorsque les fonctions de densité de probabilité ont été estimés, la distance entre eux peut être mesurée. A cet effet, l'intégrale de la différence quadratique entre les deux fonctions de densité est calculée. L'étape suivante de la procédure consiste à modifier le modèle afin de mieux - 313 - Correction automatique de SVM pour dérivait classification des données figure. 2 - Méthode pour corriger un modèle SVM face dérive brutale; voir le texte pour plus de détails. adapter les données dans D2. A cet effet, les valeurs de décision M1 (D2) sont décalés par α (devenant M1’ (D2)) afin de minimiser la distance entre M1’ (D2) et M1 (D1). Décaler les ues de la décision équivaut à décaler l'hyperplan SVM perpendiculairement à lui-même, ou, de manière équivalente, pour décaler les données D2 dans la même direction. La valeur optimale de α est obtenu par une descente de gradient sur la distance entre les deux fonctions de densité de probabilité. Ce facteur de correction est ensuite utilisée pour corriger l'hyperplan SVM afin d'obtenir le modèle corrigé M1’ . L'application du nouveau modèle M1’ sur des données D2 devrait se traduire par des performances nettement améliorées par rapport à l'aide du modèle M1 sur D2. La nouvelle fonction de décision de M1’ est f »(x) = signe (ωx + b + α). La figure 2 décrit le procédé entier. Les flèches simples représentent l'utilisation des données undrifted D1 et les flèches en pointillés représentent l'utilisation de dérivé de données D2. 5 expériences Les expériences de cette section montrent que le déplacement du hyperplan SVM, basé sur un facteur tive CORREC- α, sans apprentissage supplémentaire, peut atteindre rapidement et de bons résultats face à la dérive soudaine, même sur un petit jeu de données D2 et l'absence de données marquées est disponible dans cet ensemble de données. Pour ces expériences, deux bases de données réelles, les oiseaux (Jacques et al., 2010) et Crabes (cloche Camp- et al., 1974) ont été utilisés. La première base de données Oiseaux se compose d'oiseaux de la même espèce avec différentes origines géographiques. Cinq variables morphologiques ont été mesurées sur 206 oiseaux de chaque espèce. A partir de ces variables, le modèle apprend à classer les oiseaux en deux groupes: les hommes et les femmes. La seconde base de données de crabes a 100 lignes et 6 colonnes, décrivant des 5 mesures morphologiques sur les crabes des deux sexes. Le modèle apprend aussi à classer les oiseaux en deux groupes: les hommes et les femmes. Sur ces bases de données, différentes amplitudes de dérive concept ont été artificiellement ajouté, afin de valider la méthodologie proposée. Base de données des oiseaux. Pour la première expérience, la taille de D2 a été limitée à 80 cas en D1 a gardé ses 206 cas. Les 80 cas de D2 ont été sélectionnés parmi e au hasard e deux classes (hommes et femmes) et la proportion initiale des classes ont été respectées. Sur ces cas, sept amplitudes différentes de dérive concept ont été ajoutés, à partir d'une très petite dérive à une modification importante des données. Pour chaque fonction, correspond la dérive (fixe) à 9, 18, 24, 26, 30, 36 et 47% de l'écart-type caractéristique. Figure 3 - 314 - A. Degeest et al. FIGUE. 3 - gauche: erreurs de classification pour plusieurs amplitudes de dérive sur les oiseaux (en haut) et les bases de données Crabes (en bas), avant la correction (gris) et après correction (noir) du modèle M1 sur le jeu de données a dérivé D2. A droite: les erreurs de classification pour plusieurs tailles de D2. montre l'amélioration des performances de classification après la correction de SVM a été AP- retors. Pour la seconde expérience, une amplitude fixe de la dérive a été fixé à une des caractéristiques de 20% assez réaliste l'écart-type D2. Huit différentes tailles de jeu de données D2 ont été utilisées: 24, 50, 80, 100, 120, 150, 180 et 206. La figure 3 montre que le pourcentage d'erreur de classification chute après la correction pour toutes les tailles d'ensembles de données. Crabes Base de données. Des expériences similaires ont été réalisées sur la base de données Crabes. Pour la première expérience, quatre amplitudes différentes de la dérive de conception ont été ajoutés à D2: 3, 6, 13 et 20% de l'écart-type caractéristique. La taille de D2 a été réglée sur 40 cas, tandis que D1 a conservé ses 100 instances. Les 40 cas de D2 ont été choisis au hasard parmi les deux classes et la proportion initiale des classes ont été respectées. La figure 3 montre qu'une amélioration bonne pourcentage après la correction a été obtenue pour toutes les amplitudes de dérive. Pour la seconde expérience, une amplitude fixe de la dérive a été maintenu 15% de l'écart-type des fonctions. Ensemble de données ont été utilisées plusieurs tailles de D2: 20, 40, 60, 80 et 100. La figure 3 montre que le pourcentage d'erreur de classification baisse après la correction pour chaque taille de données. 6 Conclusions Ce document propose une méthode pour corriger automatiquement la machine à vecteurs de support algo- rithme face à la dérive soudaine d'un problème de classification. Pour classer correctement la nouvelle sans étiquette dérivait données, la méthode proposée permet d'utiliser le modèle appris sur les données initiales (BE- - 315 - Correction automatique de SVM pour dérivait dérive antérieure de classification des données) et de corriger directement sa décision hyperplan en utilisant la distribution statistique des instances dérivé. Les expériences ont montré que le déplacement du hyperplan SVM, en fonction d'un facteur de correction, sans apprentissage supplémentaire ni validation méta-paramètres, peut obtenir des résultats simples et bons face à la dérive soudaine. Le concept de dérive CV Est caracteristique UNE des flux de importantes Les Donnees REELLES Qui peut les techniques RENDRE Vite de classiques d'apprentissage inadaptées la machine. Cet article du Traité concept de dérive les Dans soudain de classification where Problèmes les techniques classiques évincés peuvent échouer. A this fin, les SVMs visage Sont au automatiquement corrigés NEM de driftées Données soudainement. Les Résultats sur des Bases de données REELLES types de Avec la dérive Différents montrent Que la soudain is capable de méthode le SVM Corriger de better classificateur AFIN les après le Données nouvelles dérive, en juin correction Utilisant sur la basée l'ensemble difference between initial de Données et le NEM Drifte, si same-ci Celui est petit. - 316 -"
305,Revue des Nouvelles Technologies de l'Information,EGC,2014,Broad Data: What happens when the Web of Data becomes real?,"“Big Data” is used to refer to the very large datasets generated by scientists, to the manypetabytes of data held by companies like Facebook and Google, and to analyzing real-time dataassets like the stream of twitter messages emerging from events around the world. Key areasof interest include technologies to manage much larger datasets (cf. NoSQL), technologies for the visualization and analysis of databases, cloud-based data management and dataminingalgorithms.Recently, however, we have begun to see the emergence of another, and equally compellingdata challenge – that of the “Broad data” that emerges from millions and millions of rawdatasets available on the World Wide Web. For broad data the new challenges that emerge includeWeb-scale data search and discovery, rapid and potentially ad hoc integration of datasets,visualization and analysis of only-partially modeled datasets, and issues relating to the policiesfor data use, reuse and combination. In this talk, we present the broad data challenge anddiscuss potential starting points for solutions. We illustrate these approaches using data froma “meta-catalog” of over 1,000,000 open datasets that have been collected from about twohundred governments from around the world.",Jim Hendler,http://editions-rnti.fr/render_pdf.php?p1&p=1001905,http://editions-rnti.fr/render_pdf.php?p=1001905,en,"Large données: Qu'est-ce qui se passe lorsque le Web des données devient réel? Jim Hendler * * Rensselaer Polytechnic Institute, États-Unis hendler@cs.rpi.edu, http://www.cs.rpi.edu/ Hendler Résumé « Big Data » est utilisé pour désigner les ensembles de données très importants générés par les scientifiques, à la plusieurs pétaoctets de données détenues par des sociétés comme Facebook et Google, et à l'analyse des actifs de données en temps réel comme le flux de messages twitter qui sortent d'événements à travers le monde. Les principaux domaines d'intérêt comprennent les technologies de gestion des ensembles de données beaucoup plus importants (voir NoSQL), technologies pour la visualisation et l'analyse des bases de données, la gestion des données dans les nuages ​​et les algorithmes datamining. Récemment, cependant, nous avons commencé à voir l'émergence d'un autre, et le défi des données tout aussi convaincante - que des « données larges » qui se dégage des millions et des millions de jeux de données brutes disponibles sur le World Wide Web. Pour les grandes données, les nouveaux défis qui émergent Clude in- données Web échelle de recherche et de découverte, rapide et potentiellement ad hoc intégration des ensembles de données, la visualisation et l'analyse des ensembles de données ne-partiellement modélisés, et les questions relatives aux CIES pour l'utilisation des poli- données, la réutilisation et la combinaison. Dans cet exposé, nous présentons le vaste défi de données et de discuter des points de départ potentiels des solutions. Nous illustrons ces approches à l'aide des données à partir d'une « méta-catalogue » de plus de 1.000.000 ensembles de données ouvertes qui ont été recueillies depuis environ deux cents gouvernements du monde entier. Biographie Jim Hendler est le Professeur Tetherless monde de l'informatique et des sciences cognitives, et le directeur de l'Institut pour l'exploration et des applications de données à l'Institut polytechnique Rensselaer. Il est également une filiale du corps professoral du Centre des arts du spectacle multimédia expérimental (EMPAC), est administrateur de fiducie de bienfaisance Web Science du Royaume-Uni et est un profes- seur invité à l'Université DeMontfort à Leicester, Royaume-Uni. Hendler, membre de l'IEEE, AAAI, BCS et AAAS, est le premier informaticien pour siéger au Conseil d'examen des rédacteurs pour la science. Il a été nommé l'un des 20 professeurs les plus innovants en Amérique par le magazine Playboy et a été choisi comme un « expert Web Internet » par le gouvernement américain. - 1 -"
339,Revue des Nouvelles Technologies de l'Information,EGC,2014,Incremental learning with latent factor models for attribute prediction in social-attribute networks,"Dans ce travail, nous nous intéressons au problème de la prédiction d'attributs sur lesnoeuds dans un réseau social. La plupart des techniques sont hors ligne et ne sont pas adaptéesà des situations où les données arrivent massivement en flux comme dans le cas des médiassociaux. Dans ce travail, nous utilisons les modèles de variables latentes pour prédire les attributsinconnus des noeuds dans un réseau social et proposer une méthode pour mettre à jourincrémentalement le modèle avec des nouvelles données. Des expérimentations sur un jeu dedonnées issues des médias sociaux montrent que notre méthode est moins coûteuse en tempsde calcul et peut garantir des performances acceptables en comparaison avec les techniquesnon-incrémentales de l'état de l'art.","Duc Kinh Le Tran, Cécile Bothorel, Pascal Cheung Mon Chan",http://editions-rnti.fr/render_pdf.php?p1&p=1001916,http://editions-rnti.fr/render_pdf.php?p=1001916,en,"Apprentissage progressif avec des modèles de facteurs latents pour la prédiction d'attributs dans les réseaux sociaux attributs Duc Kinh Le Tran *, ** Cécile Bothorel * Pascal Cheung Mon Chan ** * UMR CNRS 3192 Lab-STICC Département LUSSI - Télécom Bretagne {duc.letran, cecile. bothorel}@telecom-bretagne.eu ** orange Labs {duckinh.letran, pascal.cheungmonchan}@orange.com Résumé. Dans cet article, nous nous intéressons au problème de la prédiction des attributs sur les noeuds dans un réseau social. La plupart des techniques existantes face à ce problème sont des techniques d'apprentissage hors ligne et ne sont pas appropriés dans des situations où les données massives viennent dans le flux comme les médias sociaux. Dans ce travail, nous utilisons des modèles de facteurs latents pour prédire les attributs inconnus des noeuds dans un réseau social et de proposer une méthode pour mettre à jour progressivement le modèle de prévision sur l'arrivée de nouvelles données. Les expériences sur un véritable jeu de données des médias sociaux montrent que notre méthode est plus rapide et peut garantir des performances acceptables par rapport à l'état de l'art des techniques non progressives. 1 Introduction et énoncé du problème Avec l'explosion des médias sociaux sur Internet au cours des dernières années, l'exploitation minière contenu des médias sociaux est devenu de plus en plus critique pour de nombreux domaines. L'un des défis de l'exploitation des médias sociaux est de savoir comment tirer parti de l'information relationnelle (par exemple les amitiés, les interactions entre les utilisateurs des médias sociaux) et les attributs simultanément (par exemple les intérêts utilisateurs, textuelles ou toute autre information supplémentaire). Un autre défi réside dans le fait que ces médias fournissent des flux vastes et continus de données. En utilisant des techniques d'apprentissage hors ligne, nous devons regrouper toutes les données disponibles du passé jusqu'à nos jours. Cette approche ne convient pas dans cette situation parce que (1) les nouvelles données viennent, la taille de l'ensemble de données se développe, il deviennent de plus en plus cher d'apprendre et d'appliquer le modèle (2), cette approche ne peut pas saisir la dynamique du flux de données : les anciennes données et les données récentes sont traitées uniformément. Dans cet article, nous abordons les défis en introduisant une méthode d'apprentissage progressif pour la tâche de prédire les attributs des acteurs sociaux dans un réseau social. Ce problème a de nombreuses applications du monde réel, par exemple pour prédire les intérêts ou passe-temps en utilisant les médias sociaux des utilisateurs. Nous construisons un graphique des interactions entre les utilisateurs des médias sociaux et d'enrichir le graphique avec des attributs définis sur les nœuds. Comme les données (noeuds, liens) arrivent comme un cours d'eau permanent, nous voulons des modèles de construction pour prédire périodiquement des attributs inconnus sur les nœuds. Pour formuler notre problème, nous adoptons le réseau attribut social (Yin et al. (2010)). Un réseau attribut sociale (SAN) contient un réseau social Gs = (Vs, Es) où Vs est l'ensemble de - 77 - l'apprentissage incrémental avec LFM dans les réseaux sociaux attributs des noeuds et Es est l'ensemble des arêtes. Le graphe social est complété par un graphe bipartite Ga = (Vs ∪ Va, Ea), appelé le graphe d'attribut, reliant les noeuds sociaux Vs avec des noeuds d'attribut dans Va. Les bords Es sont des liens sociaux et les bords de Ea (liaison nœuds sociaux et les nœuds d'attributs) sont des liens d'attributs. Il existe 2 types de lien d'attributs entre un noeud i et sociale d'un noeud d'attribut k: un lien positif si je possède l'attribut k; un lien négatif si je n'ai pas k et dans le cas où nous ne savons pas si i a k ​​ou non, il n'y a pas de lien entre i et k (chaînon manquant). Nous sommes intéressés par le problème de prédire les attributs des nœuds (i.e. la nature - positif ou négatif - des maillons manquants dans le graphe d'attributs Ga) dans le contexte de l'apprentissage progressif. Dans ce contexte, à chaque t pas de temps, nous avons un instantané G (t) du SAN, qui représente l'ensemble des données (noeuds et liens) disponibles à partir du passé jusqu'à t. En comparaison avec l'instantané précédent G (1 T-), G (t) a de nouveaux nœuds et de nouveaux liens. Les nouveaux nœuds peuvent être des noeuds sociaux ou des noeuds d'attributs (dans les expériences que nous considérons comme seuls les nouveaux nœuds sociaux en raison de la limitation de l'ensemble de données). On note ÀG (t) le SAN constitué des nouveaux liens qui viennent b een ajouté au pas de temps t. Le SAN G (t) contient deux composants (sous-réseaux), le premier composant est l'instantané G (t-1), et le second composant est ÀG (t). Nous formulons notre problème d'apprentissage progressif comme suit: supposons que nous avons construit un modèle Mt-1 pour prédire les attributs des noeuds au pas de temps t - 1, notre problème est de mettre à jour le modèle Mt-1 avec de nouvelles données (noeuds et liens ÀG (t)) pour prédire les attributs de noeuds inconnus au pas de temps t. Dans ce qui suit, nous passons en revue les modèles latents de facteur et de la matrice factorisation dans l'apprentissage des lots (section 2), puis proposer une approche d'apprentissage progressif basé sur ces techniques (section 3). Nous présentons des résultats expérimentaux encourageants dans la section 4. Enfin, dans la section 5 nous concluons et le point sur certaines orientations prometteuses dans les travaux futurs. 2 modèle de facteur Latent et de la matrice factorisation Comme indiqué précédemment, l'approche de l'apprentissage proposé dans le présent document est inspiré des modèles de facteurs latents (LFM) (Bartholomew et al. (2011)), qui ont longtemps été utilisés dans les statistiques et l'apprentissage de la machine. Un LFM est un modèle statistique qui représente chaque instance de données par un ensemble de variables latentes. Matrice de factorisation (MF) peut être considéré comme un procédé de modélisation de facteur latent dans lequel les variables latentes sont continues. L'idée de base de MF est de décomposer une matrice de données dimensionnelle dans des matrices de dimensions inférieures. Les techniques de MF ont également été étendues à gérer plusieurs matrices à la fois. Singh et Gordon (2008) introduit la matrice collective factorisation (CMF). CMF peut traiter des données relationnelles dans lesquelles il existe de nombreux types d'entités et de nombreux types de relations entre les entités, chaque type de relation est représentée par une matrice relationnelle. CMF tente de cartographier les entités dans un espace commun latent par factoriser simultanément plusieurs matrices relationnelles. Dans notre contexte de problèmes, nous avons deux matrices: la matrice adjacente au réseau social (noté S) et la matrice d'attributs (désigné par A où Aik est une valeur binaire indiquant si le lien d'attribut (i, k) est positif ou négatif ). Utilisation de CMF, on minimise: QCMF (U, P, G) = α Σ (i, j) ∈Es (Sij - uiuTj) 2 + Σ (i, k) ∈Ea (Aik - uipTk) 2 + λ (nsΣ i = 1 + ‖ui‖2 naΣ k = 1 ‖pk‖2) (1) - 78 - Le DK Tran et al. où Es est l'ensemble des liens sociaux, Ea est l'ensemble des liens d'attributs; U est la matrice constituée des vecteurs latents de tous les noeuds sociaux et de manière similaire, P est la matrice constituée des vecteurs latents de tous les noeuds d'attribut de G. Le paramètre α permet de régler l'importance relative du réseau social dans le modèle . Le troisième est un terme de régularisation de pénaliser les modèles complexes avec de grandes amplitudes de vecteurs latents. λ est un paramètre de régularisation. Li et Yeung (2009) ont proposé une autre extension de MF, relation appelée matrice régularisé factorisation (RRMF). RRMF exploite simultanément le graphe social et le graphe d'attribut en réduisant au minimum (avec les mêmes notations que dans l'équation 1): QRRMF (U, P, G) = α Σ (i, j) ∈Es Sij ‖ui - uj‖2 + Σ (i, k) ∈Ea (Aik - uipTk) 2 + λ (nsΣ i = 1 + ‖ui‖2 naΣ k = 1 ‖pk‖2) (2) On peut voir que ceci est en fait la factorisation de la matrice attribut A terme lors de l'ajout de régu- larization α Σ (i, j) ∈Es Sij ‖ui - uj‖2. Ce terme est appelé terme de régularisation relationnelle qui permet de minimiser les distances entre les nœuds sociaux connectés dans l'espace latent. L'approche RRMF suppose que les acteurs sociaux connectés ont tendance à avoir des profils similaires. 3 Apprentissage progressif avec des modèles de facteurs latents Dans le contexte d'apprentissage progressif définis à la section 1, nous avons besoin d'apprendre un modèle de prédiction (i.e. les caractéristiques latentes des nœuds) à chaque pas de temps. L'approche d'apprentissage par lots suggère que nous apprenions les caractéristiques latentes à chaque pas de temps en utilisant l'ensemble instantané G (t) U? (T), P? (T) = arg min U, P Q (U, P, G (t)) (3) où Q est l'un des deux fonctions objectifs définis ci-dessus (équation 1 et l'équation 2). Différente de la méthode d'apprentissage par lots, l'inc Procédé remental apprend un modèle qu'à partir de nouvelles données (à savoir SAN ÀG (t)) lors de la réutilisation de l'ancien modèle, i.e. latent caractéristiques des nœuds calculés dans le pas de temps précédent. Pour ce faire, nous minimisons la fonction objectif suivant: Qinc (U, P, t) = Q (U, P, ÀG (t)) + μ   Σ i∈Vs (t-1) ‖ui - u? i (t-1) + Σ ‖2 k∈Va (t-1) ‖pk - p k (t-1) ‖2   (4) où Vs (t-1) et Va (t-1) sont respectivement l'ensemble des noeuds sociaux et de l'ensemble de noeuds d'attribut dans le pas de temps précédent; u? i (t-1) et p? k (t-1) sont respectivement les vecteurs latents du noeud i sociale et le noeud d'attribut k appris dans le pas de temps précédent et μ est un paramètre du modèle. Cette fonction objective se compose de deux termes. Le premier terme est la fonction objective de MF sur la ÀG graphique incrémental (t). Le deuxième terme est un terme de régularisation pour minimiser les changements de caractéristiques latentes des mêmes nœuds entre les pas de temps. En réduisant au minimum les deux termes en même temps, nous apprenons caractéristiques latentes des nœuds à la fois des nouvelles données et - 79 - Apprentissage progressif avec LFM dans les réseaux sociaux des attributs caractéristiques latentes des noeuds existants du pas de temps précédent. On peut facilement voir que les caractéristiques latentes d'un nœud existant sont mis à jour si et seulement s'il y a de nouveaux liens qui s'y connectent. Le paramètre μ permet de régler la contribution du modèle précédent au modèle actuel. En termes d'optimisation, nous adaptons le Alternance moins Squared (Zhou et al. (2008)) algorithme minimizeQ dans l'équation 3 pour l'apprentissage orQinc lot dans l'équation 4 pour l'apprentissage progressif. L'idée de base de cet algorithme est de résoudre le moindre problème carré par rapport aux caractéristiques latentes d'un nœud à la fois jusqu'à la convergence. La complexité de l'algorithme dépend linéairement du nombre de termes au carré dans la fonction objectif, qui est le nombre total de noeuds et le nombre de liens dans le SAN. En d'autres termes, l'algorithme d'apprentissage a une complexité linéaire par rapport à la taille des données. En cas d'apprentissage progressif, lors de l'optimisation uniquement sur des données récentes (AG (t)), on peut gagner beaucoup en termes de coût de calcul. 4 expériences 4.1 Dispositif expérimental L'ensemble de données utilisé dans ces expériences est BlogCatalog, recueillie et utilisée par Tang et Liu (2011). Dans BlogCatalog 1, un blogueur peut indiquer ses liens avec d'autres blogueurs. En outre, lors de la présentation d'un nouveau blog, un blogueur précise les catégories du blog parmi un ensemble de catégories prédéfinies. Les intérêts d'un blogueur peut être déduit des catégories de ses blogs. L'ensemble de données ne contient qu'une petite parties de l'ensemble du réseau: 10312 blogueurs, 333983 relations entre blogueurs, 39 catégories, et chaque blogueur a sur 1,4 moyenne catégories d'intérêt. Nous pouvons construire un SAN sur cet ensemble de données où les blogueurs sont des acteurs sociaux et les catégories sont des attributs. Étant donné que nous n'avons pas un vrai flux de données, nous construisons des instantanés SAN artificiels à partir de cet ensemble de données statiques pour tester notre méthode d'apprentissage progressif. Nous construisons des instantanés SAN à 6 pas de temps dans notre expérience. Nous considérons que l'ajout de nouveaux nœuds sociaux à chaque pas de temps (l'ensemble des 39 noeuds d'attribut est fixe). Nous choisissons d'abord 50% des noeuds sociaux totaux et construire l'instantané SAN G (0) à partir de ces noeuds et tous les liens (liens sociaux et des liens d'attributs) qui les concernent. A chaque étape t ∈ temps {1, 2, 3, 4, 5}, nous prenons au hasard 10% des noeuds sociaux totaux (seuls nœuds qui ne sont pas encore prises). Nous ajoutons ces nœuds sociaux et leurs liens sociaux pour construire un nouvel instantané G (t). A propos des liens d'attributs, nous supposons que les attributs de nouveaux nœuds à t sont inconnus jusqu'à la prochaine étape t + 1. Notre objectif est de prédire les attributs inconnus des noeuds avec nos méthodes supplémentaires (incrémental CMF, RRMF incrémental) à chaque fois étape. Nous comparons nos méthodes d'apprentissage supplémentaires avec l'approche d'apprentissage par lots (i.e. en utilisant l'ensemble instantané G (t) à chaque pas de temps t). Trois méthodes d'apprentissage par lots sont utilisés pour comparer: l'apprentissage par lots avec CMF, RRMF et un autre état de l'art méthode appelée dimension sociale (SocialDim) (Tang et Liu (2011)). L'idée de base de cette méthode est de transformer le réseau social aux caractéristiques des noeuds en utilisant un algorithme de clustering de graphe (où chaque groupe, également appelé appelé une dimension sociale, cor- respond à une caractéristique), puis former un classificateur discriminante (soutien Vector machine (Cortes et Vapnik (1995))) en utilisant ces fonctions. Il a été démontré que le SocialDim surclasse d'autres méthodes de bien connu de la classification dans un réseau. Pour mesurer les performances des différentes méthodes de prévision, nous utilisons aire sous ROC courbe (AUC) (Bradley (1997)). A chaque pas de temps, l'AUC est calculée à partir de la prédiction 1. http://www.blogcatalog.com/ - 80 - D. K. Le Tran et al. les scores et les véritables étiquettes de tous les liens manquants. Nous mesurons également le temps de calcul de chaque méthode pour montrer un gain empirique de la complexité de notre méthode incrémentale. A propos des choix de paramètres, nous avons observé que les performances des méthodes LFM sont relativement stables avec des changements de λ, α et μ dans l'apprentissage par lots et l'apprentissage progressif. Nous avons mis en λ = 1,0, α = 1,0, μ = 100 pour CMF et λ = 1,0, α = 1,0, μ = 10 pour RRMF pour produire des résultats représentatifs pour chaque méthode dans notre expérience. Le nombre de facteurs latents est fixé à 50, à laquelle atteignent CMF et RRMF leurs performances stables maximales. 4.2 Expérience résultats 0 1 2 3 4 5 6 0,74 0,76 0,78 0,8 0,82 0,84 0,86 Temps étape t AUC lot SocialDim lot CMF Batch RRMF incrémental CMF incrémental RRMF (a) AUC 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 pas de temps t Batch Batch SocialDim CMF CMF Batch RRMF incrémental Incremental RRMF (b) le temps d'apprentissage FIG. 1: Performance et temps d'apprentissage de l'apprentissage progressif par rapport à l'apprentissage par lots Nous effectuons 5 courses et traçons l'AUC moyenne de chaque méthode dans chaque pas de temps à la figure 1a. Nous observons que les techniques d'apprentissage supplémentaires ne peuvent pas donner de meilleures performances que les méthodes d'apprentissage par lots dans toutes les étapes de temps. Cette observation est prévu: dans ce cadre expérimental, le « flux de données » est pas réel. Cependant, à la fois CMF et RRMF donnent presque les mêmes performances dans l'apprentissage des lots et l'apprentissage progressif (dans tous les cas, la différence ne dépasse pas 1%). En d'autres termes, avec LFM, nous pouvons apprendre progressivement le modèle de prédiction au lieu de l'apprentissage à partir de zéro sans aucune perte significative des performances. Lorsque l'on compare CMF et RRMF, nous voyons clairement que CMF est mieux. Nous pouvons également voir que les performances de nos techniques d'apprentissage supplémentaires ne sont pas trop éloignées de celles de la méthode de référence - SocialDim (différence de 4% dans le pire des cas). La figure 1B montre le temps d'apprentissage (en secondes) de chaque procédé testé. Pour être juste, toutes les méthodes sont mises en œuvre et exécutées dans Matlab sur la même machine (2.5GHz CPU et 4 Go de RAM). Les techniques d'apprentissage supplémentaires ont besoin d'apprendre un modèle du SAN G (0) (sans prédiction) à l'étape de temps 0, alors que les techniques d'apprentissage par lots ne ont pas besoin de cette étape. Mais dans les étapes suivantes de temps (1 à 5), les techniques supplémentaires ont toujours beaucoup plus petit temps d'apprentissage que celui des méthodes d'apprentissage par lots. Dans l'apprentissage par lots, le temps d'apprentissage de CMF et RRMF augmente rapidement après chaque pas de temps. Bien que le temps d'apprentissage des SocialDim augmente moins rapidement que celle du CMF et RRMF, il est encore très long par rapport à nos méthodes incrémentielles. - 81 - incrémental apprentissage avec LFM dans les réseaux sociaux attributs 5 Conclusion Motivé par les défis de l'exploitation minière des médias sociaux, nous avons proposé une méthode d'apprentissage progressif basé sur LFM. Deux alternatives (CMF et RRMF) inspirés de LFM ont été testés pour le problème de la prédiction d'attributs supplémentaires dans un réseau social. Notre algorithme d'apprentissage peut atteindre des performances relativement bonnes par rapport à la méthode de référence basée sur la dimension sociale, une méthode de classification non-incrémentale. Dans les travaux futurs, nous allons tester notre approche progressive sur des données réelles ruisseaux. Nous espérons que notre méthode d'apprentissage progressif peut capturer la dynamique des flux de données et donner de meilleures performances que l'apprentissage par lots. Nous considérons également des extensions possibles de nos modèles pour traiter des données plus complexes dans les médias sociaux, par exemple d'envisager d'autres types de noeuds et des liens dans le SAN, pour inclure des attributs sur les bords, pour traiter des liens dirigés, etc. Références Bartholomew, DJ, M. Knott, et moi Moustaki (2011). Modèles variables et Latent analyse factorielle: une approche unifiée. Série Wiley dans Probabilités et statistiques. Wiley. Bradley, A. P. (1997). L'utilisation de l'aire sous la courbe ROC dans l'évaluation des algorithmes d'apprentissage machine. Motif reconna. 30 (7), 1145-1159. Cortes, C. et V. Vapnik (1995). réseaux de soutien-vecteur. l'apprentissage de la machine 20 (3), 273-297. Li, W. et D. Yeung (2009). Relation régularisée matrice factorisation. Dans IJCAI-09, IJCAI'09, p. 1126-1131. Morgan Kaufmann Publishers Inc. Singh, A. et G. Gordon (2008). apprentissage relationnel via factorisation de matrice collective. En procédant de la 14e SIGKDD, numéro Juin, pp. 650-658. ACM. Tang, L. et H. Liu (2011). Tirer parti des réseaux de médias sociaux pour la classification. Data Mining et Knowledge Discovery 23 (3), 447-478. Yin, Z., M. Gupta, T. Weninger, et J. Han (2010). Un cadre unifié pour Link mandation En utilisant Walks Recom- au hasard. ASONAM '10, pp. 152-159. IEEE Computer Society. Zhou, Y., D. Wilkinson, R. Schreiber, et R. Pan (2008). À grande échelle parallèle filtrage collaboratif pour le prix Netflix. Dans Actes de la 4ème conférence internationale sur les aspects algo- rithmic à l'information et de gestion, AAIM '08, pp. 337-348. Springer-Verlag. Dans CV CE travail, nous nous intéressons au Problème de la attributes sur d'prédiction les Nœuds Dans un réseau social. La Plupart des techniques hors ligne et Sont ne pas Adaptées à Sont des situations where les en Données Arrivent flux massivement in the Comme des médias sociaux No CAS. Dans CE travail, nous les utilisons des variables de l'Latentes Modèles versent les at- tributs prédire des Inconnus Nœuds Dans un réseau sociale et proposer une méthode à jour verser incrémentalement Mettre le modèle Avec des nouvelles Données. Des questions Expérimentations de un jeu de des médias sociaux Données montrent Que notre méthode couteuse en is temps Moins calculation et des performances may ACCEPTABLES garantir en les techniques Avec comparaison non incrementales de l'état de l'art. - 82 -"
348,Revue des Nouvelles Technologies de l'Information,EGC,2014,Mining the Crowd,"Harnessing a crowd of Web users for data collection has recently become a wide-spreadphenomenon. A key challenge is that the human knowledge forms an open world and it is thusdifficult to know what kind of information we should be looking for. Classic databases haveaddressed this problem by data mining techniques that identify interesting data patterns. Thesetechniques, however, are not suitable for the crowd. This is mainly due to properties of thehuman memory, such as the tendency to remember simple trends and summaries rather thanexact details. Following these observations, we develop here a novel model for crowd mining.We will consider in the talk the logical, algorithmic, and methodological foundations neededfor such a mining process, as well as the applications that can benefit from the knowledgemined from crowd.",Tova Milo,http://editions-rnti.fr/render_pdf.php?p1&p=1001908,http://editions-rnti.fr/render_pdf.php?p=1001908,en,"Exploitation minière la foule Tova Milo * * Tel-Aviv milo@cs.tau.ac.il universitaire, http://www.math.tau.ac.il/ milo / Résumé Exploiter une foule d'utilisateurs sur le Web pour la collecte des données a récemment un phénomène très répandu. un monde ouvert et il est donc difficile Un défi majeur est que la forme la connaissance humaine de savoir quel genre d'informations que nous devrions rechercher. bases de données classiques ont abordé ce problème par des techniques d'exploration de données qui identifient les modèles de données intéressantes. Ces techniques, cependant, ne conviennent pas à la foule. Ceci est principalement dû à des propriétés de la mémoire humaine, comme la tendance à retenir les tendances simples et des résumés plutôt que les détails exacts. À la suite de ces observations, nous développons ici un nouveau modèle pour l'exploitation minière de la foule. Nous examinerons dans les discours les bases logiques, algorithmiques et méthodologiques nécessaires à un tel processus d'extraction, ainsi que les applications qui peuvent bénéficier de la connaissance extrait de la foule. Biographie Tova Milo a obtenu son doctorat diplôme en science informatique de l'Université hébraïque de Jérusalem, en 1992. Après avoir obtenu son diplôme, elle a travaillé à l'Institut de recherche de l'INRIA à Paris et à l'Université de Toronto et est revenu en Israël en 1995, d'intégrer l'Ecole des sciences informatiques à l'université de Tel-Aviv, où elle est maintenant professeur titulaire et le chef du département. Ses recherches portent principalement sur les applications de bases de données avancées, telles que l'intégration de données, XML et semi-structurées, des processus métier centrés sur des données et crowdsourcing, étudiant à la fois les aspects théoriques et pratiques. Tova a été président du programme de plusieurs conférences interna- TIONNELLES, y compris PODS, CIDC, VLDB, XSym et WebDB. Elle est membre de la Fondation VLDB et le CIDC conseil d'administration et est un éditeur de TODS, le VLDB Journal et les méthodes logiques en informatique Journal. Elle a reçu des subventions de la Science Foundation Israël, les Etats-Unis-Israël binationale Science Foundation, le Ministère israélien et français des sciences et de l'Union européenne. Elle est un ACM Fellow et récipiendaire du 2010 ACM PODS Alberto O. Mendelzon Test Award-de-temps et des prestigieux chercheurs chevronnés de l'UE ERC subvention. - 7 -"
358,Revue des Nouvelles Technologies de l'Information,EGC,2014,Representative training sets for classification and the variability of empirical distributions,"We propose a novel approach for the estimation of the size of trainingsets that are needed for constructing valid models in machine learning and datamining. We aim to provide a good representation of the underlying populationwithout making any distributional assumptions.Our technique is based on the computation of the standard deviation of the 2-statistics of a series of samples. When successive statistics are relatively close,we assume that the samples produced represent adequately the true underlyingdistribution of the population, and the models learned from these samples willbehave almost as well as models learned on the entire population.We validate our results by experiments involving classifiers of various levels ofcomplexity and learning capabilities.","Saaid Baraty, Dan Simovici",http://editions-rnti.fr/render_pdf.php?p1&p=1001940,http://editions-rnti.fr/render_pdf.php?p=1001940,en,"ensembles de formation représentatifs pour la classification et la variabilité des distributions empiriques Saaid Baraty, Dan Simovici Université du Massachusetts Boston sbaraty@cs.umb.edu, dsim@cs.umb.edu Résumé. Nous vous proposons une nouvelle approche pour l'estimation de la taille des ensembles de formation qui sont nécessaires pour la construction de modèles valides dans l'apprentissage de la machine et l'exploration de données. Notre objectif est de fournir une bonne représentation de la population sous-jacente sans faire des hypothèses distributifs. Notre technique est basée sur le calcul de l'écart-type des statistiques χ2- d'une série d'échantillons. Lorsque les statistiques successives sont relativement proches, nous supposons que les échantillons produits représentent de manière adéquate la véritable distribution sous-jacente de la population, et les modèles tirés de ces échantillons se comportent presque aussi bien que les modèles tirés de l'ensemble de la population. Nous validons nos résultats par des expériences impliquant des classificateurs de différents niveaux de capacités de complexité et d'apprentissage. 1 Introduction L'estimation d'une taille échantillon qui permet la conclusion d'un bon modèle est une partie importante du processus d'apprentissage. Nous cherchons à déterminer la taille minimale d'un échantillon qui est très susceptible d'être un représentant de « juste » de la population sous-jacente. Les modèles tirés de ces échantillons se comportent presque aussi bien que les modèles appris sur la population entière et toute augmentation de la taille de l'échantillon entraînerait une augmentation négligeable de la qualité des modèles. Notre objectif est de déterminer la taille des échantillons qui sont suffisantes pour faire en sorte que ces échantillons représentent Adenova quately la population sous-jacente. Ces échantillons sont utilisés comme la formation de jeux pour les modèles comparables en con- construction définies par leurs performances à celles déduites de l'ensemble de la population, mais sont moins chers à construire. Les articles 2 et 3 décrivent en détail notre approche pour trouver la taille d'un échantillon d'un ensemble de données et une population respectivement. Le travail expérimental est présenté à la section 4. 2 Estimation de la taille d'un échantillon de données Soit U = {u1, u2,. . . , Un} un ensemble d'attributs. L'ensemble des états possibles pour ui d'attribut, Dom (ui), est supposé être fini et est communément appelé domaine ui. La notion de domaine d'attributs est étendue aux ensembles d'attributs de définition de l'ensemble Dom (V) pour l'ensemble des attributs V ⊆ U comme Dom (V) = Πv∈V Dom (v). - 299 - Estimation de la taille des ensembles de formation d'un ensemble de données de U est un D multi-ensemble de tuples t ∈ Dom (U). La multiplicité d'un membre de t D est la numberMD (t) qui est égal au nombre d'occurrences de tuple t en D. La taille de D | D | = Σt∈dom (U) MD (t). Laissez PU (t) indiquent la distribution de probabilité conjointe inconnue de U où t ∈ Dom (U). Définir le domaine λ-actif de U comme l'ensemble AdomU (λ) = {t ∈ Dom (U) | PU (t) ≥ λ}, où 0 ≤ λ <1 est un paramètre défini par l'utilisateur qui l'on se réfère sous le nom de seuil de valeur aberrante. Définition 2.1. NλS = (MS (t1),..., MS (tk)) est le vecteur de fréquence extraite du ple S pour λ de données. Si ELEM (D) -AdomDU (λ) 6 = ∅, nous ajoutons un tuple supplémentaire pour tenir compte des tuples consi- Ered comme des valeurs aberrantes, qui est, nous avons mis en k = m + 1 andMS (tm + 1) = | S | - Σm i = 1MS (ti); sinon nous fixons k = m. Étant donné que les tuples de l'échantillon S sont iid, nous pouvons considérer le vecteur de fréquence NλS pour un échantillon arbitraire S de taille fixe q de D en tant que vecteur aléatoire avec une distribution NλS ~ multinomiale (q, MD (t1) | D |,... , MD (tk) | D |), (1) où q = Σki = 1MS (ti) et MD (tk) = | D | - rk-1 i = 1 MD (ti). Définir les statistiques des c 2-de l'échantillon S pour le seuil des valeurs aberrantes λwith concerne la répartition cible de probabilité p = (p1,..., Pk) en tant que X 2S (λ, p) = Σki = 1 (MS (ti) -qpi) 2 QPI. Nous nous référons à X 2S (λ, p) que les statistiques de c 2, parce que si NλS ~ multinomiale (q, p1,..., Pk) puis, q → ∞, la distribution de la variable aléatoire X 2S (λ, p ) converge dans la distribution de χ2-distribution avec k- 1 degré de liberté (Pearson, 1900). Nous utilisons X 2S (λ, p) en tant que mesure de la proximité NλS est en représentant la distribution cible p. Comme nous augmentons l'échantillon s ize q, par la forte loi des grands nombres, X 2S (λ, p) devient plus petite. Notre but est d'estimer q, la taille d'un échantillon de données, de telle sorte que les vecteurs de fréquence extraites des échantillons de taille q sont susceptibles de représenter étroitement la distribution empirique de D pour les tuples qui ne sont pas X-valeurs aberrantes. Par conséquent, nous indiquons la répartition cible à la distribution empirique des données et définir χ2-statistiques des données d'échantillon S pour le seuil des valeurs aberrantes λ par rapport à la distribution empirique de la série de données D à être X 2S (λ, D) = kΣ i = 1 (MS (ti) - qmd (ti) | D |) 2 qmd (ti) | D | = | D | q kΣ i = 1 M2S (ti) MD (ti) - q. Laissez-S1,. . . , Sz être des échantillons z tracé à plusieurs reprises d'une taille fixe q de D. Etant donné un seuil λ on calcule X 2Si (λ, D) pour chaque Si suivi par ce procédé est résumé dans l'algorithme 1, où σq est l'écart-type entre les valeurs X 2Si (λ, D) pour différentes i. 3 Une estimation itérative de la taille d'un échantillon d'une population Nous appliquons notre approche d'estimer sans avoir un ensemble de données à portée de main de la taille d'un échantillon juste d'une population. Depuis PU (t) est inconnue, nous supposons que AdomU (λ) = {t1,. . . , Tm} pour un certain seuil λ des valeurs aberrantes. - 300 - S. Baraty et D. Simovici algorithme 1: Le pseudo-code pour trouver la taille d'un ensemble de formation suffisante ensemble de données D. Taille de l'échantillon de q du plus petit au plus grand faire prélever des échantillons de données de taille q: S1,. . . , Sz avec remplacement de jeu de données D; calculer l'écart type de séquence σq X 2S1 (λ, D). . . , X 2SZ (λ, D); sortie: taille de l'échantillon q tel que pour toute taille de l'échantillon v ≥ q on a σq ≈ σv Définition 3.1. Le vecteur de fréquence extraites d'un échantillon de la population de seuil des valeurs aberrantes λ est MλS = (MS (T1), MS (tk)...) Où, comme dans la définition 2.1, il y a deux cas: (1) si Dom (U ) -AdomU (λ) = 6 ∅ nous ajoutons un tuple supplémentaire pour compte pour les tuples considérés comme des valeurs aberrantes, qui est, nous avons mis en k = m + 1 andMS (tm + 1) = | S | - Σm i = 1MS (ti), et (2) sinon, qui est, si Dom (U) = AdomU (λ), nous avons ensemble k = m. Officieusement, on considère un échantillon de taille q en tant que représentant λ-équitable de la population si MλS / q se rapproche étroitement de vrai vecteur de distribution des tuples de la population AdomU (λ). Similaire à la section précédente, nous traitons MλS pour l'échantillon de la population arbitraire S de taille q comme vecteur aléatoire MλS ~ multinomiale (q, PU (t1),..., PU (tk)), où PU (tk) = 1 -Σ k-1 i = 1 PU (ti). Cependant, les probabilités d'unité centrale (ti) pour 1 ≤ i ≤ k sont inconnus. Par conséquent, on définit la probabilité aléatoire vecteur p = (p1,..., Pk) pour représenter l'apparition d'une distribution de probabilité à k dimensions en tant que véritable distribution sous-jacente de la population. Puis, p est représenté par l'espace de probabilité (Ω, P (Ω), f) de la probabilité k dimensions des vecteurs de dis- tribution où le Ω d'espace d'échantillon est une norme (k - 1) -simplex. On notera que X 2S (λ, p) est une variable aléatoire lui-même avec des valeurs dans R≥0. Nous rapprochons les statistiques de-c 2 un échantillon de la population par rapport à la vraie distribution sous-jacente de la valeur attendue conditionnelle de X 2S (λ, p) étant donné que nous avons un autre échantillon de la même taille de la même population à portée de main. Cet échantillon conditionné se rapproche de la forme de la distribution de probabilité de p (deuxième distribution de commande) si elle est suffisamment grand pour représenter sans biais de la distribution sous-jacente de la population. Laissez-S1,. . . , S2Z soit une séquence d'échantillons indé- pendants de taille q établis uniformément au hasard par le remplacement de la population sous-jacente. On calcule l'espérance conditionnelle de statistiques sur les c 2 (statistiques CEC) E [X 2Si (λ, p) | Sz + i] pour 1 ≤ i ≤ z. Cette statistique est utilisée comme substitut pour les statistiques réelles de-c 2 Si par rapport à la distribution cible PU. Ensuite, nous nous calculons l'écart-type entre les statistiques CECS de Si + i données Sz pour 1 ≤ i ≤ z. Si q est suffisamment grand, alors les distributions de probabilité capturées par les fréquences extraites de Si et Sz + i serait similaire à PU et, par conséquent, semblable à l'autre. Par conséquent, la variation dans les statistiques CECS est attendues d' d pour être petit. Ensuite, observer que P (S` | p) = Πk j = 1 p MS` (tj) j. Si nous supposons en outre l'avant, p ~ Dirichlet (μ1,..., Μk) pour μ1,. . . , Μk> 0, alors, on peut montrer que f (p | S`) suit la distribution Dirichlet, Dirichlet d'ordre k ≥ 2. Nous attirons (avec remplacement) échantillons aléatoires simples S1 (α1,, ak...) ,. . . , S2Z de taille q de l'OP, où | OP | est un multiple dépendant de domaine de q et plus la taille du pool d'observation est relative à q, le plus fiable est la conclusion du processus. E [X 2Si (λ, p) | Sz + i] est évaluée pour chaque i. Si l'écart-type entre les attentes conditionnelles est suffisamment faible et se stabilise à une certaine valeur de q, alors nous choisissons cette valeur de q comme seuil de la taille - 301 - Estimating tailles d'ensembles de formation d'échantillons équitables ou des ensembles de formation / évaluation adéquats. Dans le cas contraire, nous augmentons q et répétez le processus. Dans ce processus itératif, nous devrons peut-être élargir le bassin d'observation pour vous assurer qu'il est un multiple de q substantielle. Comme nous ajoutons de nouvelles observations à notre piscine, nous avons besoin de mettre à jour Adom OP U (λ), l'ensemble de tuples à considérer selon des valeurs aberrantes λ seuil, puis k, le ber de nom- dimensions de l'espace de probabilité. Notez que, nous élargissons la piscine d'observation OP, Adom OP U (λ) devient une approximation plus proche de l'ensemble AdomU (λ). Le docode suivant explique pseudotronc le processus de recherche de la taille d'un échantillon juste d'une population comme expliqué dans cette section. Algorithme 2: Le pseudo-code pour trouver la taille d'un ensemble de formation suffisante d'une popu- lation. foreach taille de l'échantillon q du plus petit au plus grand faire si ¬ (| OP | >> q) puis développez l'OP tel que | OP | q; évaluer Adom OP U (λ) et de trouver k en fonction de cet ensemble; prélever des échantillons indépendants de la taille q: S1, S2,. . . , S2Z (avec remplacement) à partir OP; calculer l'écart-type σq de séquence E [X 2S1 (λ, p) | T1],. . . , E [X 2SZ (λ, p) | Tz]; sortie:. Taille de l'échantillon q tel que pour toute taille v ≥ q nous avons σq ≈ σv 4 Résultats expérimentaux Dans la première expérience, nous avons utilisé l'algorithme 1 pour estimer la taille d'un échantillon de données de la Bank Marketing Data Set (Moro et al, 2011), qui contient 45, 211 références (voir la figure 1). Pour λ = 0, l'écart-type chute à son niveau minimal lorsque q est égal à environ 5, 000 si un échantillon de taille 5, 000 est très susceptible de présenter fidèlement l'ensemble des données qui est de la taille 45, 211. Pour λ = 0,00039 un échantillon de formation de taille 2, 000 est adapté. Dans l'expérience suivante, nous avons évalué notre approche pour déterminer la taille d'un échantillon d'une représen- tant la population que résumée dans l'algorithme 2 pour = 0,005. Nous avons simulé le processus de collecte d'observations à partir d'une population, afin d'élargir le bassin d'observation en générant synthétiquement tuples de quatre attributs en utilisant une distribution multinomiale avec des paramètres choisis au hasard, | dom (U) | = 24 et λ = 0 et nous avons exécuté l'algorithme 2 avec z = 1000. Pour chaque q, nous avons généré une centaine d'échantillons de taille q à partir d'un classificateur ensemble de données synthétiques et utilisé WEKA pour apprendre un voisin de k-plus proche (k-NN) à partir de chaque échantillon. Ensuite, nous avons évalué la performance de prédiction des classificateurs à l'aide d'un ensemble de test fixe de taille 10, 000 qui est assez grand pour représenter la distribution sous-jacente sans biais du domaine. L'écart moyen et standard du pourcentage de cas correctement classés (CCI) sont présentés dans la figure 2. Des résultats similaires ont été obtenus pour les réseaux bayésiens. D'autre part, des expériences avec des classificateurs bayésiens naïfs donnent des résultats très différents à la figure 3. L'amélioration du pourcentage moyen de la CCI en raison de l'augmentation - 302 - S. et D. Baraty Simovici FIG. 1 - écart-type des statistiques d'échantillons-c 2 de données par rapport aux changements de q échan- taille de PLE pour les données marketing de la Banque. Chaque courbe correspond à une valeur particulière de valeurs aberrantes seuil X énumérés dans la partie droite. FIGUE. 2 - Moyenne et STD la taille de l'échantillon pour 20 NN q est beaucoup plus faible que dans les cas précédents et la moyenne pourcentage de CCI atteint son maximum au format échantillon q = 2, 000 puis diminue légèrement à un niveau constant par la suite. Enfin, l'écart type du pourcentage de l'ICC converge vers zéro plus lent que les deux cas précédents. Ces différences sont dues au fait que les classificateurs de Bayes naïfs dépendent moins de la distribution de probabilité conjointe globale de classificateurs k-NN et les réseaux bayésiens en raison de l'hypothèse d'indépendance naïve. Les résultats expérimentaux montrent qu'il n'a pas de sens d'aller au-delà de la taille que nous déterminons ici, parce que l'amélioration que nous gagnons dans la performance est istent insignifiante ou inex-. Si la taille évaluée de l'ensemble de la formation est prohibitif, alors, nous pouvons être en mesure de réduire l'approximation de taille de l'échantillon en analysant dans le contexte d'un classificateur spécifique. Références Pearson, K. (1900), Sur le critère selon lequel un système donné des écarts par rapport à la probable dans le cas d'un système corrélative de variables est tel qu'il peut être raisonnablement supposer - 303 - Estimating tailles d'ensembles de formation FIG. 3 - L'écart moyen et standard du pourcentage de cas correctement classés pour bayésiens naïfs classificateurs ont surgi d'un échantillonnage aléatoire, Philosophical Magazine, Vol. 50, no. 302, ser. 5, pp. 157-175. Moro, S., Laureano, R. et Cortez, P. (2005), en utilisant Data Mining Bank Marketing Direct: Une application de la méthodologie CRISP-DM, Actes de la Conférence sur la simulation et la modélisation européenne, pp 117-121.. Le Portugal. Schmidtmann, I., Marteau, G., Sariyar, M. et Gerhold-Ay, A. (2009), l'évaluation des Kreb- de NRW Schwerpunkt couplage d'enregistrements, Rapport technique, IMBEI Nous proposons Résumé Une nouvelle approach l'estimation de verser la taille des ensembles d'ap- prentissage Qui sont nécéssaires verser des construct Dans l'Modèles d'extraction valides de connais- saices. Nous Visons à provide Une bonne représentation de l'ensemble de sans faire des Données de Hypothèses Répartition. Notre technique sur le is basée de l'écart calcul type des χ2-statistique d'échan- Une série d'tillons. When les statistiques Suivantes Sont proches Relativement, nous supposons Que les representent Échantillons Produits la vraie répartition adéquatement sous-jacente de la popula- tion, et les bureaux de Tirés Modèles se Échantillons bien also Presqu'île comportent les Qué sur l'APPRI Modèles ensemble de la population. Les Résultats semestriels Nous validons par des travaux expérimentaux involving Une des clas- sificateurs Variété. - 304 -"
365,Revue des Nouvelles Technologies de l'Information,EGC,2014,The Hitchhiker's Guide to Ontology,"Artificial Intelligence has long had the dream of making computers smarter. For quite sometime, this vision has remained just that: a dream. With the development of large knowledgebases, though, we now have large amounts of semantic information at our hands. This changesthe game of AI. Computers have indeed become smarter. In this talk, we present the latestdevelopments in the field: The construction of general purpose knowledge bases (includingYAGO and DBpedia, as well as NELL and TextRunner), and their applications to tasks thatwere previously out of scope, The extraction of fine-grained information from natural languagetexts, semantic query answering, and the interpretation of newspaper texts at large scale.",Fabian Suchanek,http://editions-rnti.fr/render_pdf.php?p1&p=1001909,http://editions-rnti.fr/render_pdf.php?p=1001909,en,"Le Guide de l'auto-stoppeur de Ontologie Fabian Suchanek * * Telecom ParisTech, 46 rue Barrault, 75013 Paris fabian@suchanek.name, http://suchanek.name Résumé Intelligence artificielle a longtemps eu le rêve de rendre les ordinateurs plus intelligents. Depuis un certain temps, cette vision est restée juste que: un rêve. Avec le développement de grandes bases de connaissances, cependant, nous avons maintenant de grandes quantités d'informations sémantiques à nos mains. Cela change le jeu de l'IA. Les ordinateurs sont en effet devenus plus intelligents. Dans cet exposé, nous présentons les derniers développements dans le domaine: La construction de bases de connaissances à usage général (y compris YAGO et DBpedia, ainsi que NELL et TextRunner), et leurs applications à des tâches qui étaient auparavant hors de portée, l'extraction de fines informations grainé de textes en langage naturel, réponse de requête sémantique et l'interprétation des textes de journaux à grande échelle. Biographie Fabian M. Suchanek est Maître de Conférences (comparable à un professeur associé) à l'Institut Télécom ParisTech à Paris. Il a obtenu son doctorat à l'Institut Max-Planck de l'informatique sous la direction de Gerhard Weikum. Dans sa thèse, Fabian a développé entre autres la YAGO-Ontologie, une des plus grandes ontologies publiques, ce qui lui a valu une mention honorable du prix de thèse SIGMOD. Fabian était un postdoc chez Microsoft Research dans la Silicon Valley (rapports Rakesh Agrawal) et à l'INRIA Saclay / France (rapport à Serge Abiteboul). Il a continué comme le chef du groupe de recherche Otto Hahn « ontologies » à l'Institut Max-Planck pour l'informatique en Allemagne. Fabian a enseigné des cours sur le Web sémantique, extraction d'information et de représentation des connaissances en France, en Allemagne et au Sénégal. Avec ses étudiants, il travaille sur l'extraction de l'information, l'extraction de règles, alignement d'ontologies et d'autres sujets liés à de grandes bases de connaissances. Il a publié environ 40 articles scientifiques, entre autres à ISWC, VLDB, SIGMOD, WWW, CIKM, CIED, et SIGIR, et son travail a été cité plus de 2700 fois. - 9 -"
383,Revue des Nouvelles Technologies de l'Information,EGC,2013,A POS Tagger analysed in collaboration environments and literary texts,"Part-of-speech (POS) tagging is often used in other modules of natural language processing and therefore the results of this process should be as precise as possible. Many different types of taggers have been developed to improve the accuracy of the results in the field of literature or newspapers. Nowadays when the internet is widespread, the environments for online collaboration as chats, forums, blogs, wikis have become important means of communication. The purpose of this research is to analyse the results of tagging the words obtained from the labelling of the words from the online collaboration environments and literary texts with the corresponding parts of speech. In the case of POS tagging, the ambiguities arise due to the fact that a word may have multiple morphological values depending on context.","Dumitru-Clementin Cercel, Stefan Trausan-Matu",http://editions-rnti.fr/render_pdf.php?p1&p=1001847,http://editions-rnti.fr/render_pdf.php?p=1001847,en,"Un POS Tagger analysé dans des environnements de collaboration et de textes littéraires Dumitru-Clémentin Cercel * Ştefan Trăuşan-Matu ** * Université ""Politehnica"" de Bucarest, Département des sciences informatiques et en génie Splaiul Independenţei Bd., N ° 313, Bucarest, Roumanie clementin.cercel @ gmail.com ** Institut de recherche Académie roumaine pour l'intelligence artificielle 13 Septembrie Street, n ° 13, Bucarest, Roumanie Résumé. Partie du discours (POS) de marquage est souvent utilisé dans d'autres modules de traitement du langage et donc naturelle- Ural les résultats de ce processus devraient être aussi précis que possible. De nombreux types de tagueurs ont été développés pour im- prouver l'exactitude des résultats dans le domaine de la littérature ou dans les journaux. jours lorsque l'Internet De nos est très répandue, les environnements de collaboration en ligne comme les chats, les forums, les blogs, les wikis sont devenus des moyens importants de com- munication. Le but de cette recherche est d'analyser les résultats de marquage des mots obtenus à partir de l'étiquetage des mots de les ments de collaboration en ligne et des textes littéraires avec les parties correspondantes de la parole. Dans le cas de marquage POS, les ambiguïtés apparaissent en raison du fait qu'un mot peut avoir plusieurs valeurs morphologiques selon le contexte. 1 Introduction Partie du discours (POS) de marquage est le processus d'étiquetage grammaticale de chaque mot dans un texte avec sa partie correspondante de la parole. L'étiquetage peut également contenir des informations supplémentaires relatives aux caractéristiques morphologiques de la langue respective comme le nombre, le sexe, personne, temps et l'aspect du verbe. De nombreux types de tagueurs ont été développés pour améliorer la précision des ré- sultats dans le domaine de la littérature ou dans les journaux. De nos jours, lorsque l'Internet est très répandu, les environnements de collaboration en ligne comme les chats, les forums, les blogs, les wikis sont devenus d'importants moyens de communication. Le but de la recherche présentée dans le présent document effectue une analyse comparative de marquage POS dans les corpus de collaboration (en particulier pour les chats, Wikipedia et Twitter comme un exemple de microbloggins) et de la littérature (les textes du corpus Brown). Dans ce que nous voulons un trigramme HMM mis en œuvre tagger selon Jurafsky et Martin (2000) et Brants (2000). Le reste de cet article est structuré comme suit. Dans la section 2, nous passer brièvement en revue: l'état de l'art des approches de marquage POS, le modèle de Markov caché, la mise en œuvre du A POS Tagger analysé dans des environnements de collaboration et de textes littéraires trigramme HMM tagger et nous allons analyser les facteurs qui influent sur la performance des le tagger, en tenant compte des différences entre les textes littéraires, les wikis, les microbloggings et le chat Copora. Dans la section 3, nous analysons le résultat de notre travail. Dans la section 4, nous présentons les conclusions et d'identifier les possibilités d'une étude de suivi. 2 POS marquage Au fil du temps, le marquage de points de vente a été un sujet important de la recherche et de nouvelles méthodes ont mis au point afin d'améliorer la précision des résultats. En Jurafsky et Martin (2000) est fait une classification des méthodes utilisées dans la résolution POS-marquage dans trois catégories: méthodes stochastiques (méthodes probabilistes), des méthodes basées sur des règles et une combinaison des deux (tagueurs hybrides). En général, la construction d'un tagueur POS suit certaines phases (Voutilainen, 2005): tokens, lexique Recherche et résolution d'ambiguïté. Dans une étude publiée par l'Association de linguistique informatique, en ce qui concerne le problème de marquage de point de vente est effectué une analyse des résultats de plusieurs mises en œuvre de méthodes d'apprentissage automatique, à l'aide de la formation et de tester le Wall Street Journal corpus (Marcus et al., 1993 ). Pour la TnT Tagger proposé par Brants (2000) et basée sur HMM, on a obtenu une précision 96,46% de. Le SVMTool Tagger introduit par Giménez et Márquez (2004) et sur la base Support Vector Machine a obtenu une précision 97,16% de. La précision de Stanford Tagger 2.0 (Manning, 2011) nous ing le modèle d'entropie maximale était de 97,32%. L'étiqueteur LTAG-spinal décrit par Shen et al. (2007), en utilisant un algorithme basé sur l'apprentissage perceptron bidirectionnel a une précision 97,33% de. Le MORCE / COMPOST Tagger présenté dans Spoustová et al. (2009), a une précision de 97,44%, en utilisant une méthode basée sur l'apprentissage perceptron bidirectionnel. Un résultat avec une précision 97,50% a été obtenue par le SCCN Tagger proposé par Sogaard (2011), basé sur un modèle qui utilise condensées voisin le plus proche. En termes de mots inconnus qui ne sont pas dans le corpus de formation, la plus grande précision de 91,29% a été obtenue en utilisant la matière fondue Tagger décrit par (Denis et Sagot, 2009), et le résultat le plus insatisfaisant de 85,86% a été obtenu par la TnT Tagger . L'analyse des performances précédentes ont été réalisées uniquement (comme nous le savons) sur des textes qui peuvent être considérés comme étant dans la catégorie de la littérature. Comme nous l'avons mentionné, dans cet article, nous examinerons aussi le cas des textes trouvés dans des environnements de collaboration. L'Internet a été un facteur important dans le développement du langage informel à travers des environnements de com- munication, qui ont apporté des changements à la langue littéraire. La principale chose qui distingue tweets d'autres textes est que le message Twitter est jusqu'à 140 caractéris- TERs. Sont basées sur des discussions envoyer plusieurs messages courts entre les participants et donc une phrase ne peut pas être entièrement écrit dans un message d'instance. Il existe de nombreuses similitudes entre les textes de micro-blogging et les textes de chat. Une tendance est d'utiliser beaucoup d'abréviations et raccourcis (tels que « BRB » - « être de retour à droite ») (. Par exemple « UE » au lieu de « U.E ») ou de laisser tomber les apostrophes ou les arrêts complets après les abréviations. Les participants ont également omettent d'utiliser des lettres majuscules au début d'une phrase ou un nom et ils utilisent rarement diacritiques. Une coutume courante consiste à utiliser des émoticônes en place de mots afin d'exprimer des émotions et des sentiments. Contrairement à la langue littéraire, l'une des conséquences d'un message rapide est de mots mistype comme l'inversion des lettres, des lettres manquantes, joignant les mots et ainsi de suite (par exemple, « malade » au lieu de « je veux »). apparaissent fréquemment des fautes d'orthographe, ainsi que l'utilisation de l'accent par le caractère D. C. Cercel et Þ. répétition Trăuşan-Matu (par exemple ""biiigggg"" au lieu de ""grand""). Une autre particularité des microbloggings et les chats consiste dans les entrées qui sont agrammaticales bien plus que dans les œuvres littéraires. Les chatteurs ont souvent tendance à ne pas respecter l'ordre des mots dans une phrase ou les signes de ponctuation, bien que l'ordre des mots et ces séparateurs ont un rôle important dans la compréhension de la signification. Wikipedia est un site modifiable qui permet à ses utilisateurs d'ajouter, de modifier ou de remplacer son contenu. En contraste frappant avec le texte littéraire collective, cette caractéristique de Wikipédia d'être une création collaborative peut impliquer des modifications grammaticales du texte et donc, sa signification est successivement modifiée. Pour éviter les actes de vandalisme, l'accès aux articles d'édition peut être protégé, même verrouillé de telle sorte que certaines personnes sont en mesure de faire des changements. 2.1 Un modèle trigramme pour le marquage d'un POS HMM selon Manning et Schutze (1999) est spécifié par les paramètres (N, K, A, B, π). Pour le marquage de point de vente, les étiquettes sont représentés en tant qu'Etats HMM automate fini. Ainsi, N est le nombre d'étiquettes utilisées par le modèle, la taille K est le nombre de mots distincts du vocabulaire du modèle, πi est la probabilité que le premier mot de la séquence est marqué avec ti, aij est la probabilité que tj étiquette est précédée d'étiquette ti, BJK est la probabilité que le mot wj a la balise tj. Dans un HMM, la séquence des états générés par le procédé ne sait pas (il est caché), car on sait que la séquence d'observations (les mots dans notre cas). Etant donné une séquence de mots W = w1, w2,. . . , Wn, le processus de marquage POS implique la détermina- tion de la séquence la plus probable d'états que le modèle traverse, à savoir la séquence la plus probable d'étiquettes T qui maximise P (T | W) .Il resu lts que le modèle trigramme pour le marquage POS comme représenté sur Jurafsky et Martin (2000) est la suivante: T = argmaxt1 ... tn [nn i = 1 P (ti | ti-1, ti-2) P (wi | ti) ] P (tn + 1 | tn) (1) en raison du manque de données du corpus de formation, il peut arriver qu'une séquence de trigrammes apparaît dans le corpus de test, mais pas dans le corpus de formation et nous arrivons ainsi à établir à tort la probabilité de l'ensemble de séquences en tant que zéro. Même lorsque la séquence d'étiquettes semble trop peu de fois dans le corpus de formation, la probabilité calculée pour la séquence respective ne serait pas une estimation exacte. Dans notre application, nous avons utilisé une solution proposée par Thede et Harper (2011) qui donne des poids aux séquences de trigramme, bigramme et unigramme. La probabilité P (wi | ti) est obtenue en effectuant une analyse des suffixes en utilisant une base de lettre-modèle n-gram. Suffixes peut fournir une bonne indication de la partie associée de la parole d'un mot. Les suffixes des mots avec une fréquence inférieure ou égale à la fréquence de seuil sont utilisées pour construire une structure de données à savoir arbre suffixe. Ainsi, l'algorithme construit un arbre suffixe qui contient les suffixes pour les mots qui commencent par les minuscules, un autre arbre pour les mots commençant par une lettre majuscule et un arbre pour les mots qui commencent par les chiffres. L'application d'une approche en raison de Samuelsson et Reichl (1999) et Brants (2000), on calcule par interpolation la probabilité d'une certaine étiquette, où sont connus les dernières lettres i d'un mot de lettres L. La séquence la plus probable de balises étant donné la séquence observée des mots peut être résolu par une recherche de force brute évaluer la probabilité de chaque séquence possible des balises pour l'un POS Tagger analysé dans des environnements de collaboration et la séquence des textes littéraires des mots d'entrée, mais il a besoin d'un grand temps d'exécution. Afin d'obtenir de meilleurs résultats que nous avons utilisé dans notre implémentation de l'algorithme (Viterbi, 1967), qui est basée sur la programmation dynaming. 3 Évaluation Pour l'évaluation de la tagger, nous avons utilisé plusieurs corpus. Nous avons utilisé le corpus Brown qui est l'un des corpus les plus connus pour la langue anglaise décrit dans Francis et Kucera (1979). Le NPS chat Corpus v. 1.0 (Forsyth et Martell, 2007) a été créé en 2006 à partir de différentes salles de chat en ligne et contient des enregistrements d'une courte période un jour donné et se compose de 10,567 énoncés. WikiCorpus (Reese et al., 2010) représente une ressource sémantique lexicale disponible pour la communauté PNL. La partie anglaise du corpus contient de grandes portions de pages Wikipédia disponibles en 2006 (environ 600 millions de mots). En outre, nous avons utilisé Twitter corpus étiquetés avec des parties de la parole par Gimpel et al. (1993). Les sections de la rame test sections série Le nombre de mots de la formation mis en mots inconnus Le nombre de mots de l'ensemble de test de précision mots connus paroles de précision ca-cg ch-cr 608001 26245 553191 95,82 78,52 ck-cr ca-cj 857752 12719 303440 96.13 78,68 ca-cl cm-CR 985663 6398 175529 96,40 79,58 ca-cn cp-cr 1069475 2956 91717 96,65 79,63 ca-cp cr 1139497 934 21695 96,28 79,33 TAB. 1 - La précision de marquage pour le corpus Brown. Le pus cor- utilisé le nombre de mots de la formation mis en mots inconnus Le nombre de mots du test mis en mots connus de précision des mots de précision NPS chat 12694 7540 32314 92,77 62,99 27185 3270 17823 93,33 64,95 32646 2213 12362 92,94 67,78 39420 736 5588 94,22 63,45 42226 379 2782 95,46 60,15 WikiCorpus 1663043 579254 6277115 97,39 92,52 655532 153776 1358854 97,08 90,66 413137 2014386 257750 96,94 90,07 354488 466773 3594006 97,00 89,65 153937 575257 3594006 96,69 87,46 Twitter 14619 1901 7152 92,00 60,44 21771 1207 4823 91,48 64,70 23682 723 2912 91,18 68,46 TAB. 2 - La précision de marquage pour NPS chat, WikiCorpus, Twitter. Le tagueur a été formé pour chaque corpus et testé sur lui-même. Dans les tableaux 1 et 2 sont pré- senté les résultats obtenus pour l'étiqueteur lorsque le nombre de suffixes, et la fréquence de seuil D. C. Cercel et S. Trăuşan-Matu du mot s à partir de l'ensemble d'apprentissage, utilisés dans la construction des arbres de suffixes sont 2 et 4 respective- ment, ces valeurs étant déterminées expérimentalement. Les résultats du corpus Brown ont une assez bonne précision aussi bien pour les mots inconnus (79.69) et les mots trouvés dans l'ensemble de la formation (96,65% de). Les essais de la tagger sur le corpus de chat avait une précision de 92-95% pour les mots connus et une précision de 60-67% pour les mots inconnus. En utilisant le pus WikiCor- la précision était plus de 97% pour les mots connus et entre 89-92% pour les mots inconnus. 4 Conclusions Construire un POS tagger avec une grande précision pour certains environnements de collaboration en ligne est plus difficile, car il y a beaucoup de différences syntaxiques et sémantiques en comparaison avec les textes de la littérature ou dans les journaux. Dans cet article, nous avons identifié plusieurs différences que nous considérons comme les plus importants résultats de marquage de points de vente pour des environnements spécifiques collaboration en ligne tels que wikipedia, chats et micro-blogging. Dans une recherche future, nous allons procéder à l'analyse des résultats de l'tagger POS formés sur le vieux corpus et testé sur un ensemble de données récentes afin d'identifier les différences grammaticales. En outre, une direction de recherche intéressante en utilisant un POS tagger est l'analyse des différences de marquage grammatical des mots dans le corpus de textes entre langue maternelle et ses dialectes. Références Brants, T. (2000). Tnt - une statistique de partie du sppech tagger. Dans la 6e Conférence PNL appliquée, Seatle, WA. Denis, P. et B. Sagot (2009). Le couplage d'un corpus annoté et un lexique morpho pour l'état de l'art pos marquage avec moins d'effort humain. PACLIC. Forsyth, E. Martell et C. (2007). analyse lexicale et discours de dialogue de chat en ligne. Dans la première IEEE Conférence internationale sur la sémantique Computing (ICSC 2007), pp. 19-26. Francis, W. et H. Kucera (1979). Brown manuel de corpus. Rapport technique, Brown University, Département de linguistique, Providence, Rhode Island. Giménez, J. et L. Márquez (2004). Svmtool: Un générateur de tagger général pos sur des machines SUP- vecteur de port. Dans la 4ème Conférence internationale sur la langue des ressources et de l'évaluation (LREC'04), Lisbonne, Portugal. Gimpel, K., N. Schneider, B. O'Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman et D. Yogatama (1993). Une partie du discours pour le marquage twitter: Annotation, caractéristiques et expé- riences. Dans Actes de l'assemblée annuelle de l'Association pour tics informatique Linguis-. Jurafsky, D. et J. Martin (2000). Discours et traitement du langage: Une introduction à naturelle- traitement du langage Ural, linguistique informatique et la reconnaissance vocale. Upper Saddle River: Prentice Hall. Un POS Tagger analysé dans des environnements de collaboration et de textes littéraires Manning, C. (2011). marquage partie du discours de 97% à 100%: Est-il temps pour certaines linguistique? Dans la 12e Conférence internationale sur le texte intelligent et traitement linguistique computationnelle., Pp. 171-189. Manning, C. et H. Schütze (1999). Fondations de traitement du langage naturel statistique. Cambridge: MIT Press. Marcus, M., B. Santorini, et M. Marcinkiewicz (1993). Construire un grand corpus annoté de l'anglais: Le treebank penn. Linguistique informatique 19 (2). Reese, S., G. Boleda, M. Cuadros, L. Padró et G. Rigau (2010). Wikicorpus: Un sens multilingue wikipedia désambiguïsé mot: corpus. Dans la 7ème langue des ressources et de la Conférence d'évaluation (LREC'10), La Valette, Malte. Samuelsson, C. et W. Reichl (1999). Un modèle de langage basé sur la classe pour une grande reconnaissance vocale du vocabulaire extrait des statistiques partie de synthèse vocale. Dans IEEE ICASSP-99, pp. 537- 540. Shen, L., G. Satta, et A. Joshi (2007). apprentissage guidé pour la séquence bidirectionnelle la classification. Dans la 45e réunion annuelle de l'Association de la linguistique informatique (ACL). Sogaard, A. (2011). Semi-supervisé condensé le plus proche voisin pour une partie du discours de marquage. Dans la 49e réunion annuelle de l'Association pour la linguistique computationnelle: Human Te Lan- guage chnologies (ACL-HLT). Spoustová, D., J. Hajic, J. Raab, et M. Spousta (2009). la formation semi-supervisé pour la moyenne perceptron pos tagger. Au cours des 12 GACE. Thede, S. et M. Harper (2011). Un modèle de Markov caché de second ordre pour une partie du discours marquage. Lors de la réunion annuelle 37e de l'Association de linguistique informatique sur la linguistique informatique (ACL '99), pp. 175-182. Viterbi, A. J. (1967). limites d'erreur pour les codes convolutifs et asymptotiquement algorithme optimal. decod- ing IEEE Transactions on Théorie de l'information 13, 260-269. Voutilainen, A. (2005). Oxford Manuel de la linguistique informatique, chapitre ch. 11: Part-de-Speech Tagging. Oxford University Press. pp. 219-232. L'grammaticale résumé Étiquetage un is Souvent des Autres modules Composant du treatment du langage naturel les Do not also etre Résultats doivent possible Que précis. De types d'étiqueteurs grammaticaux Nombreux were Improving la répandrai développés des Résultats Dans précision le domaine de la littérature de la presse ous. De les jours de l'Internet, Quand est tres répandu, les Environnements de collaboration en ligne les clavardages Comme, les forums, les blogs, les wikis Devenus means are Importants de communication. Le mais de recherche this is d'analyse les Résultats obtenus Dans l'Étiquetage des partis du discours Pour Les corpus d'une collaboration en de Environnements ligne et le corpus de la littérature. Dans le Cas de « éti- quetage grammaticales, les ambiguités may surviennent lorsqu'un mot several facts Avoir en fonction du morphologiques Contexte."
395,Revue des Nouvelles Technologies de l'Information,EGC,2013,Detecting Academic Plagiarism with Graphs,"In this paper, we tackle the problem of detecting academic plagiarism, which is considered as a severe problem owing to the convenience of online publishing. Typical information retrieval methods, stopword-based methods and ngerprinting methods, are commonly used to detect plagiarism by using the sequence of words as they appear in the article. As such, they fail to detect plagiarism when an author reconstructs a source article by re-ordering and recombining phrases. Because graph structure ts for representing relationships between entities, we propose a novel plagiarism detection method, in which we use graphs to represent documents by modeling grammatical relationships between words. Experimental results show that our proposed method outperforms two n-gram methods and increases recall values by 10 to 20%.","Bin-Hui Chou, Einoshin Suzuki",http://editions-rnti.fr/render_pdf.php?p1&p=1001848,http://editions-rnti.fr/render_pdf.php?p=1001848,en,"Détection académique Plagiat avec des graphiques Bin-Hui Chou, Einoshin Suzuki Dept. Informatique, Université de Kyushu, au Japon chou@i.kyushu-u.ac.jp, suzuki@inf.kyushu-u.ac.jp Résumé. Dans cet article, nous nous attaquons au problème de la détection plagia- académique risme, qui est considéré comme un grave problème en raison de la facilité de mise en ligne sur l'autre. méthodes de recherche d'information typiques, des méthodes basées sur des méthodes et des termes courants ngerprinting, sont couramment utilisés pour détecter le plagiat en utilisant la séquence de mots tels qu'ils apparaissent dans l'article. A ce titre, ils ne parviennent pas à détecter le plagiat quand un auteur reconstitue un article source en re-commande et de phrases re- combinant. Parce que ts structure graphique pour représenter les relations entre les entités, nous proposons une nouvelle méthode de détection de plagiat, dans lequel nous utilisons des graphiques pour représenter des documents en modélisant les relations grammaticales entre les mots. Les résultats expérimentaux montrent que notre méthode proposée surpasse deux méthodes et des augmentations rappellent les valeurs n-gramme de 10 à 20%. 1 Introduction Publication en ligne offre une plate-forme pour les chercheurs de partager leurs résultats de recherche alors qu'il apporte également un effet secondaire grave, le problème de plagiat académique. Autrement dit, les étudiants ou les cher- cheurs copier tout le contenu ou une partie des passages des documents d'autres sans citation appropriée (Howard, 1995). Il est difcile pour les éditeurs de journaux et procédures pour découvrir tous les comportements de plagiat en raison de la limitation dans le temps et la quantité de publications. Une méthode de détection automatique matic peut être utilisé pour les emplois d'aide éditeurs et d'atténuer le problème. méthodes de détection de plagiat existants évaluent les similitudes de documents en utilisant des mots de contenu (Gustafson et al, 2008;. Hoad et Zobel, 2003), les mots vides (Stamatatos, 2011) ou du document empreintes digitales (Seo et Croft, 2008; Schleimer, 2003) . En commun dans la recherche d'information (IR), les méthodes (Grman et Ravas, 2011, Gustafson et al, 2008;. Hoad et Zobel, 2003) de mots vides jettez, par exemple, l'est, et considérer le contenu restant mots comme des mots significatifs. Ce genre de méthodes utiliser des séquences de mots pour représenter le contenu d'un document. Stamatatos (2011) estime qu'un plagiaire peut remplacer les mots de contenu pour éviter la détection, et a proposé de représenter des documents en supprimant les mots de contenu, mais en conservant les mots vides. Seo et Croft (2008); Schleimer (2003) l'utilisation hash des morceaux de longueur xé en tant que document. Empreintes digitales Les deux passages sans citation copie et paraphraser directement sont considérés comme le plagiat démie acadé- (Howard, 1995, Rosamond, 2002). Ici, nous cherchons à détecter les documents plagiés où l'on paraphrases du texte à partir d'autres documents par des phrases re-commande ou modiers altérant. La plupart des approches existantes utilisent des séquences de mots tels qu'ils apparaissent dans le document pour représenter le document afin qu'ils ne parviennent pas à détecter ce type de plagiat. Détection académique Plagiat avec des graphiques pour détecter ce genre de plagiat, nous considérons ce qui représente un document en modélisant tionships entre les paires de rela- mots dans le document. En capturant les relations entre les mots, nous sommes toujours en mesure de détecter le plagiat, même si un plagiaire en grande partie modifie l'ordre des phrases. Dans cet article, nous proposons une nouvelle méthode de détection de plagiat par la représentation des documents avec des graphiques. Dans notre méthode, chaque document est transformé en structure graphique en fonction des relations Tical syntac- entre les mots. Nous détectons un plagiat si les deux graphiques contiennent des sous-graphes similaires. Les résultats expérimentaux montrent que notre méthode est plus efficace que les méthodes existantes pour détecter le plagiat de paraphraser. 2 Motivation et problème Denition 2.1 Motivation Prenons un exemple indiqué dans le tableau 1, où le texte A est un extrait de (Shi et Malik, 2000) et le texte B est un texte que nous récrit du texte A par réordonner la phrases A et ajout / suppression de mots sans altérer son contenu. Textes A et B représentent le même concept, mais ont des expressions différentes dans leurs phrases et constructions. Nous considérons textes A et B en tant que document source et un document plagié, respectivement. LANGUETTE. 1 Exemple d'une source et des documents plagiés. (A) Texte A (Source) Nous proposons une nouvelle approche pour solv- le problème de l'ing groupement perceptuel dans la vision. Nos objectifs d'approche à l'ING impression y décompresser globale d'une image. Nous traitons la segmentation d'images comme un problème de partitionnement de graphe et de proposer un nouveau critère global, la coupe normalisée, pour segmenter le graphique. (B) Texte B (Plagiat) Dans cet article, nous traitons la tion de l'image, à savoir le problème de groupement perceptuel dans la vision, comme un problème de partitionnement de graphe et visent à extraire la pression globale im- d'une image. Nous vous proposons un nouveau critère global pour segmenter le graphique, appelé la coupe normalisée. Nous classons les méthodes existantes dans les méthodes typiques IR (Grman et ravas, 2011, Gustafson et al, 2008;. Hoad et Zobel, 2003), les méthodes ngerprinting (Schleimer, 2003, Seo et Croft, 2008) et une méthode basée sur mot vide- (Stamatatos, 2011). Les méthodes typiques d'IR évaluent souvent la similitude des deux documents par le texte de diviser en séquences de mots d'une longueur specied et en comparant le nombre de mots. Puisque l'ordre des phrases dans le texte B dans le tableau 1 est en grande partie changé, ce qui diminue le nombre de mots communs entre des séquences de mots, les méthodes typiques IR ne parviennent pas à détecter un tel type de plagiat. Les méthodes existantes (ngerprinting Schleimer, 2003, Seo et Croft, 2008) représentent un document en utilisant hash des morceaux de longueur xes. Les deux caractères et les mots peuvent être utilisés pour des morceaux, mais la plupart d'entre eux (Schleimer, 2003, Seo et Croft, 2008) considèrent mot n-grammes. Ainsi, l'ajout ou la suppression d'un petit nombre de mots modifie le résultat de hachage, ce qui rend les méthodes ngerprinting moins efficaces pour détecter le plagiat où un plagiaire modies en grande partie l'ordre des mots ou le phrasé (Seo et Croft, 2008). Stamatatos (2011) supprime tous les mots de contenu dans un B. Chou et E. Document Suzuki et a proposé la méthode n-grammes de mots interdits (SWNG). Depuis l'ordre de mots vides est modifiée ainsi dans le texte B dans le tableau 1, SWNG est pas efficace pour détecter le plagiat. Dans l'exemple, nous sommes motivés à inventer une méthode de détection de plagiat, où la représentation d'un document est basé sur les relations entre les mots au lieu de leurs positions d'occurrence. Précisément parlant, nous utilisons des graphiques pour modéliser les relations grammaticales entre les paires de mots. Notre méthode représente les structures syntaxiques de documents tels que notre méthode est capable de détecter le plagiat, même si les mots dans un document sont réarrangée. Comme le montre la figure 1, les textes du tableau 1 sont différentes, mais leurs structures de graphes sont similaires dans notre transformation. le segment gâterie propose dans résoudre comme image problème de la segmentation de la vision graphique nous d'extrait d'impression proposer critère d'approche (a) Graphique A partir du texte Un traitement segments nous problème de segmentation vision impression graphique propose du critère d'extrait d'image en tant que (b) Le graphique B du texte B figure . 1 Un exemple de modélisation de documents sous forme de graphiques. 2.2 Denition de détection Plagiat Nous formalisons ici le problème. Nous considérons que la détection de plagiat monolingue dans ce travail. L'entrée est un ensemble DSRC de documents de source et un ensemble dsusp de documents suspects, le nombre minimum k de nœuds communs dans un sous-graphe candidat, et la longueur maximale δ d'un chemin, dont nous parlerons dans la section suivante. Compte tenu d'une source et un document suspect, notre tâche est de décider s'il existe des passages plagiés dans le document suspect et découvrir leurs passages source correspondants si les passages plagiés existent. Nous adoptons la dénition dans (Potthast et al., 2010). Laissez un plagiat s = <splg, dplg, SSRC, DSRC> comme 4-tuple qui contient un splg de passage dans un document DPLG qui comprend la version plagié d'un certain ssrc de passage de source dans le document DSRC. Compte tenu dplg, la tâche d'un détecteur de plagiat est de détecter s en signalant une détection de plagiat r = <rplg, dplg, rsrc, d'src> qui consi m d'un rplg de passage plagié dans le document prétendument DPLG et son rsrc source dans d'src. A r de détection est deni: r détecte s ⇐⇒ splg ∩ rplg 6 = ∅, SSRC ∩ rsrc 6 = ∅, et DSRC = d'src. 3 Approche proposée 3.1 Vue d'ensemble Nous transformons chaque document à un graphique, dans lequel les relations syntaxiques sont conservées entre les mots. Après la transformation, les textes similaires devraient avoir des structures similaires afin que nous proposons une mesure de similarité et un algorithme d'appariement de graphes. Notre approche comprend quatre procédures dans chaque cycle de détection: le pré-traitement, trans- formation, la correspondance et les étapes de post-traitement. Dans l'étape de pré-traitement, on obtient des lemmes, des décalages et des longueurs des mots et la détermination des relations de coréférence des pronoms par le Plagiat Academic Detecting Stanford graphiques algorithme 1: Transformer un document à un graphique d'entrée: Un document d, qui contient s1, s2 ,. . . , Des phrases sn sortie: un graphe G = <V, E> 1 G ← ∅; 2 pour i ← 1 à n faire 3 Γ ← tous les Relations de dépendance en Si; 4 foreach r ∈ Γ faire 5 r.ψ de commutation font 6 cas nsubj nsubj (r, Γ, G); Pause; 7 cas xsubj xsubj (r, Γ, G); Pause; 8 cas iobj iobj (r, Γ, G); Pause; Agent 9 cas (r, Γ, G); Pause; 10 cas de préparation de préparation (r, Γ, G); Pause; 11 cas prepc prepc (r, Γ, G); Pause; 12 cas partmod Parmod (r, Γ, G); Pause; 13 MergeNodes (G); 14 retour G; CoreNLP (Klein et Manning, 2003. Raghunathan et al, 2010). Nous allons discuter des étapes et trans- formation correspondants dans les sections 3.2 et 3.3, respectivement. Dans l'étape de post-traitement, nous transformons les paires de découvertes semblables à leurs sous-graphes passages correspondants. Nous ob- TAIN un passage plagié en incluant des mots situés dans la gamme des compensations minimales et maximales des mots entre les nœuds du sous-graphe. Nous utilisons le cas d'un seul document suspect et un document source pour simplifier des discussions. Nous pouvons obtenir des résultats de dsusp et DSRC en effectuant des courses nsuspnsrc, où nsusp et nsrc représentent le nombre de documents dans dsusp et DSRC, respectivement. Laissez un document suspect et un document source soient dsusp et DSRC, respectivement. Laissez les graphiques générés par dsusp et DSRC dans l'étape de transformation soient G et H, respectivement. G et H sont des graphes orientés, et des noeuds et des arêtes qui représentent des étiquettes de mots. V (G) et E (G) l'ensemble de noeuds et l'ensemble des arêtes de G, respectivement. LG (v) et LG (x, y) représentent des étiquettes de noeud v et le bord (x, y) à G, respectivement. LG (v) est abrégé L (v) quand il n'y a pas d'ambiguïté. 3.2 Transformation de graphes Nous pensons que les noms sont des éléments essentiels dans une phrase et les verbes ou prépositions sont généralement liés à des noms. Ainsi, nous considérons intuitivement les noms et les verbes en tant que nœuds ou prépositions arêtes dans la transformation. parlant Intuitivement, si nous sommes en mesure de trouver un verbe ou préposition concernant deux noms, nous créons un lien dirigé entre eux. Au lieu de tirer directement des positions de mot, nous dérivons la représentation graphique du texte par la modélisation des relations grammaticales entre les mots. Algorithme 1 montre une vue d'ensemble de la transformation des phrases dans le document d au graphe G. On invente des procédures dans des lignes de 6 à 12 pour générer des noeuds et des arêtes B. Chou et E. Suzuki fonction PREP (r, Γ, G) Entrée: Une relation de préparation r = <ψ, wgov, wdep>, où ψ représente une chaîne de préparation avec la combinaison d'une préposition par un trait de soulignement (par exemple, prep_into), l'ensemble de Γ de relations de la phrase qui r appartient, et le graphique G = <V, E> sortie: Mise à jour graphique G 1 si r.wgov est un verbe, puis 2 R ← rFindDobj (r, r.wgov, Γ); 3 R ← R ∪ rFindNsubj (r, r.wgov, Γ); 4 R ← R ∪ rFindNsubjpass (r, r.wgov, Γ); 5, tandis que R 6 = ∅ do 6 L (v) ← pop (R) .wdep; L (u) ← r.wdep; 7 e ← (v, u); L (e) ← r.wgov; Insert 8 v, u et E à G; / * Lors de l'insertion d'un nœud, v, nous vérifions s'il existe L (v) entre les étiquettes de nœud existant dans G * / 9 d'autre si r.wgov est un nom puis 10 L (v) ← r.wgov; L (u) ← r.wdep; 11 e ← (v, u); L (e) ← la préposition après le trait de soulignement dans ψ; 12 Insertion v, u et E à G; sur la base des relations grammaticales, à savoir, les relations de dépendance (explaine d ultérieurement). Après un graphique est généré, nous autres noeuds de fusion selon leur relationships.1 coréférence Identifier les relations grammaticales entre les noms, nous utilisons l'analyseur Stanford (version 2.0.1) (Klein et Manning, 2003), qui fournit les relations de dépendance tapée Stanford (de Marn- Effe et Manning, 2008). Une relation de dépendance est une simple description de la relation grammaticale entre les mots dans le format de triplets, <ψ, wgov, wdep>, ce qui représente une ψ relation grammaticale entre un mot de gouverneur wgov et un wdep mot à charge. Une phrase est composée d'un ensemble ordonné de relations de dépendance. Par exemple, la peine étant donné la détection communautaire est le problème des nœuds en cluster dans un graphique dans les communautés, l'une des relations obtenues, relation <dobj, regroupement, noeuds> indique que mot nœuds est un objet direct de regroupement de mots. Relation <prep_into, regroupement, communautés> indique que les communautés de mots accompagnés en est un modier préposition de regroupement verbe. Parce que les relations de dépendance ne contiennent pas nécessairement une seule paire de noms et un verbe / préposition représentant la relation entre les noms, nous faisons référence à des relations multiples pour obtenir les noms connexes et leur verbe / préposition dans la plupart des cas correspondants. L'intuition derrière les règles heuristiques est que nous voulons capturer autant de noms que possible des sujets, des objets ou des compléments de phrases. parlant spéciquement, nous e paires de noeuds, chaque paire est associée à un verbe / préposition, en recherchant des relations nsubj, passe nsubj- ou dobj. Nous considérons que les relations de dépendance impliquées dans la partie du sujet, l'objet et le complément d'une phrase comprennent les relations nsubj, xsubj, iobj, agent, préparation, prepc et 1Suppose v est fusionnée à son homologue u. Nous copions tous bords adjacents de v u et nous négligeons v dans le graphique. Détection académique Plagiat avec des graphiques partmod.2 Chacune des relations a une règle de recherche correspondante comme montré dans l'algorithme 1. Fonction montre de préparation que nous e mots connexes quand donné une relation où ψ est prep (préposition modier). En préparation de la fonction, indique r.wgov mot de gouverneur wgov de relation r. rFindDobj (r, r.wgov, Γ) retourne toutes les relations dobj situé en face de r dans Γ dont chacun des wgov est r.wgov.3 De même, nous pouvons dénir rFindSubj (·) et rFindNsubjpass (·). pop (R) ressorte une relation de l'ensemble R des relations. Avec la fonction de préparation, nous générons un sous-graphe de la phrase, noeud into-- → communauté, lorsque les relations données <dobj, regroupement, noeuds> et <prep_into, regroupement, communautés>. Le reste des règles heuristiques sont présentés dans la figure 3. 3.3 Graphique Matching Si deux textes sont semblables, leurs structures de graphique transformées devraient être similaires. Ainsi, nous interprétons une détection de plagiat comme la découverte d'une paire de sous-graphes similaires dans ce document. Ci-dessous nous dénir la similitude des deux sous-graphes et de proposer un algorithme de découverte. 3.3.1 Similitude Denition En raison de la diversité des expressions en langage naturel, une dénition de correspondance exacte limite sa capacité de découvrir similaires dans une sous-graphes application réelle. Pour détecter des textes similaires qui ont des graphiques différents, nous considérons dans notre correspondance dénition inexacte des sous-graphes similaires. Envisager des peines, TextRunner est un système qui extrait de l'OIE et relationnelle tuples TextRunner extraits relationnels tuples. On remarque que même s'il existe des mots supplémentaires (par exemple, l'OIE et système) dans la phrase première, les deux phrases ont toujours le même sens que les mots supplémentaires sont entre les mots communs (par exemple, TextRunner, tuples) de telle sorte que les mots supplémentaires fonctionner comme modiers. Au niveau graphique, nous appelons mots comme des noeuds communs et des mots supplémentaires comme des noeuds hors du commun. Nous dénit similaires en permettant des sous-graphes noeuds hors du commun d'exister dans le chemin entre les nœuds communs. Laissez un sous-graphe de G et un sous-graphe H g et h, respectivement. Prenons le cas de deux nœuds communs pour l'instant. G et h sous-graphes sont similaires subgr APHS si elles satisfont à: 1. (Noeud de similarité) Lg (α) = Lh (a) ∧ Lg (β) = Lh (b) 4 2. (Chemin de similarité) chemin (α, β) est similaire au chemin (a, b), où a et b sont des noeuds communs à g, et α et β sont des noeuds communs h. Les quatre noeuds sont des noeuds communs. Chemin (x, y) représente le chemin le plus court entre x et y. Comme il existe des noeuds hors du commun, nous considérons la similitude des chemins au lieu des bords. Denition. (Chemin similarité) Soit chemin chemin (v1, vl) le plus court chemin du noeud v1 à Vn dans G, et le chemin chemin (u1, um) est le plus court chemin du noeud u1 um isoniazide Si 0,5 | Chemin (v1, vl) | ≤ δ, | Chemin (u1, um) | ≤ δ et L (1-vl, vl) = L (um-1, um), on dit que les chemins Chemin (v1, vl) et le chemin (u1, um) sont similaires. 2nsubj, nsubjpass, DOBJ, xsubj, iobj, prepc et partmod représentent sujet nominal, sous réserve nominale passive, objet direct, le contrôle de l'objet, l'objet indirect, propositionnelle clausal modier et participial modier, respectivement. 3FindDobj (r, r.wgov, Γ) retourne toutes les relations de dobj situées à l'arrière de r, utilisés dans d'autres règles. 4Pour simplifier l'explication, nous ignorons le problème des synonymes ici. En fait, nous considérons synonymes de mots dans notre approche en vérifiant l'ensemble des synonymes d'un mot avec WordNet (Miller, 1995). En d'autres termes, L (x) = L (y) si x est un synonyme de y et sont donc les étiquettes des bords. 5Si il y a deux chemins les plus courts, nous vérifions les deux. B. Chou et E. Suzuki Algorithme 2: Vue d'ensemble de l'algorithme de découverte Entrée: Un graphe suspect G, un graphique de la source H, K, δ sortie: paires (g, h), de sous-graphes similaires entre G et H 1 (Vs, nous ) ← {v ∈ V (G), u ∈ V (H) | v et u sont des noeuds communs; il existe une relation de mise en correspondance une à une entre v et u}; 2 tandis que PopSeeds (Vs, nous) 6 = ∅ faire 3 (vs, nous) ← PopSeeds (Vs, nous); 4 vs si nous ne sommes pas inclus dans une paire quelconque de (g, h), puis 5 (g, h) ← jeu (G, H, vs, vu, δ); 6 si SimNodesNum (g, h) ≥ k puis 7 sortie (g, h); g ← ∅; h ← ∅; Soit p1 désigne chemin α → → x1 · · · → xn → β, où α et β sont des noeuds communs tandis que chaque xi représente un noeud rare. De même, nous dénit chemin p2 comme → y1 → · · · → YM → b. Pour dénir la similitude entre P1 et P2, nous devons considérer: le nombre maximum de nœuds hors du commun que nous autorisons dans les chemins (à savoir, la longueur maximale des chemins), et les étiquettes des bords. Plus les noeuds hors du commun sont autorisés, les sous-graphes sont les moins dissemblables. Par conséquent, nous avons fixé un seuil, δ, pour déterminer la longueur d'un chemin entre les nœuds communs. Nous dénit L (1-vl, vl) = L (um-1, um) en raison de deux raisons. Les nombres d'arêtes de V1 à Vn et ceux de u1 um peuvent être différentes si l'on compare chaque paire d'entre eux est difcile. De plus, comme indiqué dans les phrases, considérer qu'un casse plagiaires une phrase à part en insérant des phrases ou des clauses modier. Le dernier verbe ou préposition dans les phrases insérés est le bord de connexion d'un noeud commun donc nous pensons qu'il est plus important que d'autres. 3.3.2 algorithme Découverte Notre but est de découvrir des paires (g, h) de sous-graphes similaires maximale entre un graphique suspect G et un graphique H source dans notre algorithme. Avec les denitions dans la section précédente, nous pouvons déterminer si g avec deux noeuds communs a et β est similaire à h avec deux noeuds communs a et b. Si g et h sont similaires, nous pouvons encore utiliser α et β, et a et b comme semences et la recherche dans leur proximité agrandir g et h par concaténer nouvellement acquise sous-graphes g 'et h', qui ont aussi deux nœuds communs, g et h, respectivement. L'algorithme 2 montre une vue d'ensemble de l'algorithme de découverte. Nous Rst ème toutes les paires de noeuds communs. Nous ne correspond pas à v à d'autres noeuds si v est mis en correspondance au noeud u de sorte que chaque paire de noeuds a une relation un-à-un rapport de correspondance. Etant donné une paire de noeuds communs, nous recherchons leur maximale similaires en match des sous-graphes (·). Puisque nous comparons mots du début à la fin des textes, PopSeeds (·) retourne nœuds communs dans leur ordre. Si un sous-graphe retourné a quelques noeuds communs, à savoir, moins de seuil k, nous le considérons comme une découverte banale que nous détruisons, vérifié en fonction SimNodesNum. SimNodesNum (· ) Renvoie le nombre de noeuds communs entre une paire de sous-graphes. montre match de la façon dont nous fonction recherche d'une paire de semblables. maximale des sous-graphes Notre straté- gie est de vérifier si nous pouvons développer un sous-graphe d'un noeud commun à un sous-graphe de deux noeuds communs et ainsi de suite, jusqu'à ce qu'aucun des noeuds les plus courants peuvent être inclus dans le graphique. Détection Academic Plagiarism avec correspondance graphes de fonction (G, H, vs, nous, δ) Entrée: G, H, les noeuds de semences vs et nous, la longueur d'un chemin δ sortie: Nodes V (g), V (h) de la sous-graphes g , h / * nous estimons que le passage d'un sous-graphe candidat de nœuds, qui ont des informations de décalages de caractères et longueurs pour que nous générons que les nœuds g et h dans notre mise en œuvre. * / 1 g ← ∅; h ← ∅; S1 ← ∅; S2 ← ∅; 2 V (g) ← V (g) ∪ {vs}; V (h) ← V (h) ∪ {nous}; S1 ← S1 ∪ {vs}; S2 ← S2 ∪ {nous}; 3 4 v répétition ← pop (S1); u ← pop (S2); (V, U) ← FindCandiNodes (G, H, v, u, δ); 5 foreach (x, y) ∈ (V, U) faire 6 s'il existe un chemin entre v et x && il existe un chemin entre u et y puis 7 si x et y ne sont pas inclus dans une paire quelconque de (g, h ) && PathSim (v, x, u, y), alors V 8 (g) ← V (g) ∪ {x}; V (h) ← V (h) ∪ {y}; 9 S1 ← S1 ∪ {x}; S2 ← S2 ∪ {y}; 10 jusqu'à ce que S1 = ∅, S2 = ∅; 11 V retour (g), V (h); FindCandiNodes (G, H, v, u, δ) nds noeuds communs qui sont proches de l'intérieur de la longueur de δ v ou u dans les graphiques sous-jacents obtenus par le remplacement de tous les bords dirigés avec des bords non orientés. Par exemple, FindCandiNodes (·) renvoie des noeuds adjacents du noeud v qui sont des noeuds communs dans la version non orienté de G lorsque δ = 1. Dans PathSim (v, x, u, y), on vérifie à la fois σ1 de similarité entre le chemin (v, x) et le chemin (u, y) et σ2 similitude entre le chemin (x, v) et le chemin (y, u). PathSim (v, x, u, y) renvoie vrai lorsque l'σ1 σ2 ou est vrai et retourne faux, sinon. 4 expériences Nous comparons notre méthode avec une méthode n-grammes naïf, qui utilise des mots de contenu comme la représentation d'un document, et un état de l'art méthode appelée SWNG (mots vides n-grammes méthode) (Stamatatos, 2011). Dans SWNG, θ, qui est une limite supérieure de longueur d'intervalle permis dans un passage, est établie pour être 100 comme l'indique l'auteur. Nous examinons de nombreuses valeurs de n utilisées dans la méthode et SWNG n-grammes naïfs dans nos expériences et nous montrons les résultats lorsque n = 5, 6 que nous considérons comme les meilleures performances. Dans notre méthode, k, qui est un seuil de décider du nombre minimal de nœuds communs requis dans un sous-graphe de sortie, est réglé pour être 3. Une valeur de δ implique que beaucoup de mots rares sont autorisés à être inclus dans les paires de sous-graphes similaires. Pour éviter la découverte de passages différents, nous avons mis δ à une faible valeur, δ = 2. B. Chou et E. Suzuki 4.1 ensembles de données et métriques d'évaluation Nous utilisons l'ensemble de données réseau DBLP-citation (Tang et al., 2008) nos expériences, où chaque enregistrement, par exemple, un article, des informations sont assignées généralement des auteurs, le lieu, Ørences abstraits, réfé- et publication year.From le réseau de citations de jeu de données, nous préparons deux ensembles de données, DBLP1 et DBLP2, utilisés pour examiner les performances des méthodes lorsque la totalité du contenu d'un document est plagié et qu'une partie du contenu d'un document est plagié, respectivement. Dans ensemble de données DBLP1, nous sélectionnons un ensemble de documents D, qui sont publiés dans les actes de conférences de haut niveau telles que KDD et utiliser leurs résumés DSRC comme documents de test dans les expé- riences. Pour chaque document d en D, nous préparons un document plagié correspondant dplg par des phrases de fractionnement, ré-organisation et des phrases re-commande sans modifier le contenu, en ajoutant des mots mineurs et en remplaçant au hasard des noms et des verbes avec des mots de leurs ensembles de synonymes par WordNet (Miller , 1995). Nous utilisons également des résumés Dref des documents cités dans les documents de D pour faux-fuyants. En résumé, les documents suspects dsusp comprennent tous les documents plagiés DPLG et Dref. | DSRC | = 20. | dsusp | = 89, où | Dref | = 69, qui sont des documents non plagiés, et | Dplg | = 20, qui sont des documents plagié. Dans ensemble de données DBLP2, les documents source sont les mêmes que dans DBLP1. Il y a 100 documents suspects, parmi lesquels 31 et 69 documents sont partiellement plagié et documents non plagié, respectivement. La moitié des phrases dans chacun des documents partiellement plagié sont extraits d'un document dans Dplg et la moitié d'un des documents correspondant à Dref. Nous utilisons les mesures de précision, le rappel, la granularité et une mesure globale combinant précision, rappel et granularité, PlagDet, qui sont proposés pour la détection de plagiat (Potthast et al., 2010), pour évaluer les résultats expérimentaux. Soit S l'ensemble des plagiats dans les documents suspi- cieuses d'un corpus, et que R représentent l'ensemble des détections de plagiat qui a des rapports de détection. Pour simplifier la notation, un plagiat s, s ∈ S, est représenté comme un ensemble de références à des caractères de dplg et DSRC qui forment les passages de la SSRC et. De même, une détection de plagiat r, r ∈ R, est représenté par r. prec (S, R) = 1 | R | Σ r∈R | Ss∈S (sur) | | R | rec (S, R) = 1 | S | Σ s∈S | Sr∈R (sur) | | S | où sur = s∩r si r Détecte de, et sur = ∅ autrement. Un score élevé de précision représente que bon nombre des passages détectés sont des passages plagié. Un score élevé de rappel représente que le détecteur identies de nombreux passages plagiés. Parce que les détecteurs de plagiat peuvent signaler ou plusieurs détections qui se chevauchent pour un plagiat unique, nous utilisons la mesure de granularité en plus de précision et de rappel. (. Pot- thast et al, 2010) La granularité est deni comme, gran (S, R) = 1 | SR | Σ s∈SR | R |, où SR ⊆ S sont des cas détectés par R dans les détections et Rs de R sont de s détections. Un score élevé de granularité représente que de nombreux segments sont présentés comme détections du même passage plagié. La combinaison de la précision, le rappel et la granularité, PlagDet est deni comme PlagDet (S, R) = F1 log2 (1 + gran (S, R)), où F1 = 2 · prec (S, R) · rec (S, R ) prec (S, R) + rec (S, R). Les valeurs de précision, le rappel et PlagDet, sauf plage de granularité de 0 à 1. La valeur minimale et idéal pour la granularité est égal à 1. 4.2 Résultats expérimentaux Les figures 2 (a) et 2 (b) montrent des résultats expérimentaux sur les ensembles de données DBLP1 et DBLP2, re - pectivement. Des chiffres, nous voyons que notre méthode surclasse SWNG et la méthode naïve grammes n- dans les expériences de détection des deux passages totalement et partiellement plagié. Détection Academic Plagiarism avec précision les graphiques de rappel PlagDet Granularité 0 0,0 0 0,5 1 0,0 1 0,5 2 0,0 0 0,0 0 0,5 1 0,0 Proposition SWNG (n = 5) SWNG (n = 6) n-grammes ( n = 5) n-grammes (n = 6) (a) DBLP1 résultats de précision de rappel PlagDet Granularité 0 0,0 0 0,5 1 0,0 0 0,0 0 0,5 1 0,0 0,5 1 Proposition SWNG (n = 5) SWNG (n = 6) n-grammes (n = 5) n-grammes (n = 6) (b) DBLP2 ​​résultats FIG. 2 résultats expérimentaux sur les ensembles de données DBLP1 et DBLP2. Bien que notre méthode a tendance plus grande à obtenir des résultats qui se chevauchent que SWNG, qui est observée à partir des valeurs de granularité, notre méthode permet d'obtenir des scores plus élevés sur la précision, le rappel et PlagDet que SWNG. Autrement dit, notre méthode montre sa compétitivité dans la détection des passages plagiés, qui sont en grande partie par des mots modied re-commande et des phrases. En comparant les deux figures, on constate que toutes les méthodes obtenir plus favorables ré- sultats dans les expériences de DBLP1 que dans les expériences de DBLP2. En effet, le problème de la détection des documents qui ont partiellement le contenu plagié est intuitivement plus difficile que le problème de la détection des documents que tout leur contenu sont plagié d'autres documents. En dépit de l'difculty du problème, les résultats de notre méthode sont encore supérieures à celles des SWNG et la méthode n-grammes. Nous examinons les graphiques que nous transformons du texte. Un graphique généré est parfois pas un graphe connexe en raison de la diversité des expressions en langage naturel. Étant donné que nos Détecte méthode passages plagiés en recherchant des noeuds communs le long de chemins dans le graphe, graphiques déconnectées peuvent provoquer des passages infructueux détections plagiés. Ceci est probablement la raison pour laquelle notre méthode ne permet pas d'atteindre des scores plus élevés sur le rappel de 0,6. Cependant, notre méthode est toujours efficace pour détecter le plagiat de réorganisées ou re-contre texte struits par rapport aux méthodes existantes, montrant l'avantage de la modélisation des relations entre les mots avec la structure graphique. 5 Conclusions et travaux futurs Nous avons proposé une méthode de détection de plagiat par la représentation des documents avec des graphiques. Nous transformons des documents à des graphiques en fonction des relations grammaticales entre les mots et découvrir des paires de sous-graphes similaires, dont chacun est une détection de plagiat. les résultats montrent que expérimentaux notre méthode améliore largement les valeurs de rappel et est plus efficace pour détecter le plagiat de paraphraser que les méthodes existantes. Dans notre travail futur, nous considérons comparer notre méthode avec d'autres méthodes fondées sur les graphiques utilisés pour détecter le plagiat de logiciels tels que GPLAG (Liu et al., 2006) et d'analyser la façon dont nos échelles de proposition en ce qui concerne le nombre et la taille des documents. Nous considérons aussi l'extension de notre méthode pour contrefaçon de brevet. B. Chou et E. Suzuki agent de fonctionnement (R, Γ, G) 1 R ← rFindNsubjpass (r, r.wgov, Γ); 2 Whiler 6 = ∅ faire 3 L (v) ← pop (R) .wdep; 4 L (u) ← r.wdep; 5 e ← (v, u); L (e) ← r.wgov; Insert 6 v, u et e TOG; Fonction partmod (r, Γ, G) 1 R ← toutes les relations à l'arrière de chacun des r whosewgov est r.wdep; 2 Whiler 6 = ∅ faire 3 L (v) ← r.wgov; 4 L (u) ← pop (R) .wdep; 5 e ← (v, u); L (e) ← r.wdep; Insert 6 v, u et e TOG; Fonction nsubj (r, Γ, G) si une r.wgov est un verbe, puis 2 R ← FindDobj (r, r.wgov, Γ); 3 Whiler 6 = ∅ do 4 γ ← pop (R); 5 L (v) ← r.wdep; L (u) ← γ.wdep; e ← (v, u); L (e) ← r.wgov; 6 V ← V ∪ {v}; V ← V ∪ {u}; E ← E ∪ {e}; 7 else if r.wgov est un nom puis 8 L (v) ← r.wdep; L (u) ← r.wgov; 9 e ← (v, u); L (e) ← soit; 10 V ← V ∪ {v}; V ← V ∪ {u}; E ← E ∪ {e}; Fonction xsubj (r, Γ, G) 1 R1 ← FindPrep (r, r.wgov, Γ); 2 whileR1 6 = ∅ faire 3 L (v) ← r.wdep; 4 L (u) ← pop (R1) .wdep; 5 e ← (v, u); L (e) ← pop (R1) .wgov; Insert 6 v, u et e TOG; 7 R2 ← FindDobj (r, r.wgov, Γ); 8 whileR2 6 = ∅ faire 9 L (v) ← r.wdep; 10 L (u) ← pop (R2) .wdep; 11 e ← (v, u); L (e) ← r.wgov; 12 Insertion v, u et e TOG; Fonction iobj (r, Γ, G) 1 R ← rFindNsubj (r, r.wgov, Γ); 2 Whiler 6 = ∅ faire 3 L (v) ← pop (R) .wdep; 4 L (u) ← r.wdep; 5 e ← (v, u); L (e) ← r.wgov; Insert 6 v, u et e TOG; Fonction prepc (r, Γ, G) si une r.wgov est un verbe, puis 2 R ← rFindDobj (r, r.wgov, Γ); 3 R ← R ∪ rFindNsubj (r, r.wgov, Γ); 4 R ← R ∪ rFindNsubjpass (r, r.wgov, Γ); 5 R '← FindDobj (r, r.wdep, Γ); 6 Whiler 6 = ∅ do 7 γ1 ← pop (R); 8 Whiler '6 = ∅ do 9 γ2 ← pop (R'); 10 L (v) ← γ1.wdep; 11 L (u) ← γ2.wdep; 12 e ← (v, u); L (e) ← r.wdep; 13 Insertion v, u et e TOG; 14 else if r.wgov est un nom puis 15 R ← FindDobj (r, r.wdep, Γ); 16 Whiler 6 = ∅ faire 17 L (v) ← r.wgov; 18 L (u) ← pop (R) .wdep; 19 e ← (v, u); L (e) ← r.wdep; FIGUE. 3 Règles pour l'agent de fonctions, partmod, nsubj, iobj, xsubj, prepc Remerciements. Ce travail a été soutenu par la JSPS KAKENHI Grant Numéro 24800049, 21300053. Références de Marneffe, M.-C. et C. D. Manning (2008). Le Stanford dactylographié dépendances repré- sentation. Dans Proc. CrossParser, p 18.. Grman, J. et R. Ravas (2011). La mise en œuvre améliorée pour trouver Texte dans Similitudes grands ensembles de données - Bloc-notes pour PAN à CLEF 2011. Dans Proc. LA POÊLE. Détection académique Plagiat avec des graphiques Gustafson, N., S. M. Pera, et Y.-K. Ng (2008). Nowhere to Hide: Trouver plagié docu- ments Basé sur la peine similarité. Dans Proc. WI-IAT, pp. 690696. Hoad, T. C. et J. Zobel (2003). Les méthodes d'identification et Versioned ments. Plagié Docu Confiture. Soc. Inf. Sci. Technol. 54 (3), 203215. Howard, R. M. (1995). Plagiats, Authorships, et la peine de mort académique. Collège Anglais 57 (7), 788806. Klein, D. et C. D. Manning (2003). Précis Parsing Unlexicalized. Dans Proc. ACL, p. 423430. Liu, C., C. Chen, J. Han, et P. S. Yu (2006). GPLAG: Détection des logiciels Plagiat par la dépendance du programme d'analyse de graphe. Dans Proc. KDD, p. 872881. Miller, G. A. (1995). WordNet: Une base de données lexicales pour l'anglais. Commun. ACM 38, 3941. Potthast, M., B. Stein, A. Barrón-Cedeño, et P. Rosso (2010). Un cadre d'évaluation pour la détection Plagiat. Dans Proc. COLING, p. 9971005. Raghunathan, K., H. Lee, S . Rangarajan, Chambers N., M. Surdeanu, D. Jurafsky et C. ning Homme- (2010). Un multi-passes pour Sieve Coréférence résolution. Dans Proc. EMNLP, p. 492501. Rosamond, B. (2002). Plagiat, normes académiques et la gouvernance de la profession. Politique 22 (3), 167174. Schleimer, S. (2003). Vanner: Algorithmes local pour le document Fingerprinting. Dans Proc. SIGMOD, p. 7685. Seo, J. et B. Croft (2008). Détection réutilisation Texte local. Dans Proc. SIGIR, p. 571578. Shi, J. et J. Malik (2000). Coupes et Normalisé Segmentation d'images. IEEE Trans. PAMI, 888 905. Stamatatos, E. (2011). Sur la base de détection Plagiat Information sur la structure. Dans Proc. CIKM, p. 12211230. Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang et Z. Su (2008). ArnetMiner: Extraction et exploitation minière des réseaux sociaux universitaires. Dans Proc. KDD, p. 990998. Dans cet article résumé, nous nous intéressons au probléme de la plagiats des détection in the world académique, Qui est un véritable including en raison EAÜ de la d'Accès aux facilité Internet de publications. Les methods de recherche d'classiques informations en couramment utilisées Detection of plagiarism se sur les mots basent VIDES et l'identication de signatures, et les utilisent des mots Séquences soi Presentent Tels articles Qu'ils Dans Les. CÉS approaches ne par détectent pas les situations qui en découlent de plagiarism un lorsqu'un auteur ar- ticle reconstruit en réordonnant et réorganisant les phrases. Dans CE Contexte, la structure de juin, plus graphè is Adaptée representer les relations verser Entre les Entités. Nous proposons AINSI Une nouvelle méthode de détection de nous plagiarism Dans Laquelle des Graphes répandrai utilisons representer des documents en modélisant les relations grammaticales Entre les mots. Les Résultats expéri- Que la Taux montrent Que nous proposons méthode Depasse deux methods de n-grammes et le AUGMENTE par des descendeur facts de 10 à Allant 20%."
413,Revue des Nouvelles Technologies de l'Information,EGC,2013,Non-disjoint grouping of text documents based Word Sequence Kernel,"This paper deals with two issues in text clustering which are the detection of non disjoint groups and the representation of textual data. In fact, a text document can discuss several themes and then, it must belong to several groups. The learning algorithm must be able to produce non disjoint clusters and assigns documents to several clusters. The second issue concerns the data representation. Textual data are often represented as a bag of features such as terms, phrases or concepts. This representation of text avoids correlation between terms and doesn't give importance to the order of words in the text. We propose a non supervised learning method able to detect overlapping groups in text document by considering text as a sequence of words and using the Word Sequence Kernel as similarity measure. The experiments show that the proposed method outperforms existing overlapping methods using the bag of word representation in terms of clustering accuracy and detect more relevant groups in textual documents.","Chiheb-Eddine Ben N’Cir, Afef Zenned, Nadia Essoussi",http://editions-rnti.fr/render_pdf.php?p1&p=1001843,http://editions-rnti.fr/render_pdf.php?p=1001843,en,"Non-disjoints regroupant des documents de texte basé sur Word séquence noyau Chiheb-Eddine Ben N'Cir *, Afef Zenned **, Nadia Essoussi *** * LARODEC, ISGT, Université de Tunis chiheb.benncir@isg.rnu.tn ** LARODEC , ISGT, Université de Tunis afef.zenned@gmail.com *** LARODEC, ISGT, Université de Tunis nadia.essoussi@isg.rnu.tn Résumé. Ce document traite de deux questions dans le regroupement de texte qui sont la dé- tection des groupes non-disjoints et la représentation des données textuelles. En fait, un document de texte peut discuter de plusieurs thèmes puis, il doit appartenir à plusieurs groupes. L'algorithme d'apprentissage doit être en mesure de produire des grappes non disjoints et ayants documents à plusieurs clusters. La deuxième question concerne la représentation des données. les données textuelles sont souvent représentés comme un sac de fonctionnalités telles que les termes, expressions ou concepts. Cette représentation du texte évite la corrélation BE- termes d'interpolation et ne donne pas d'importance à l'ordre des mots dans le texte. Nous vous proposons une méthode d'apprentissage non supervisé capable de détecter des groupes qui se chevauchent dans le document texte en considérant le texte comme une séquence de mots et en utilisant la séquence de mots noyau comme mesure de similarité. Les expériences montrent que les surclasse de la méthode proposée par les méthodes existantes qui se chevauchent en utilisant le sac de mot en repré- sentation termes de précision et de détecter le regroupement des groupes les plus pertinents dans les documents textuels. 1 Introduction regroupement de texte est une application importante dans le domaine des informations de recherche (IR). Il consiste à regrouper les documents similaires dans le même groupe, alors que des documents différents doivent appartenir à différents groupes sans utiliser des catégories prédéfinies. Cette définition peut être une question cruciale dans de nombreuses applications de la vie réelle de regroupement de texte où un document doit être attribué à plus d'un groupe. Cette question se pose naturellement parce qu'un document peut discuter de plusieurs sujets et peut appartenir à plusieurs thèmes. Par exemple, un article de journal concernant la par- ticipation d'un football à la sortie d'un film d'action peut être groupé avec les deux catégories Sports et Cinéma. De nombreuses méthodes de classification ont été proposées pour résoudre le problème de la détection des groupes non disjoints dans les données. Ce type d'application est arbitré comme le regroupement de chevauchement (Diday, 1984), (boursiers et al., 2011). Nos travaux concerne la détection des groupes à base de k-means de gorithme. Les méthodes existantes qui se chevauchent, lorsqu'il est appliqué au texte le regroupement de documents (Cleuziou, non-disjoints regroupant des documents texte 2008), utilisez la représentation modèle vectoriel (VSM) pour l'ensemble des documents. Cette représenta- tion est basée sur l'hypothèse que la position relative des jetons sont en tête sans rapport avec la perte de corrélation avec des mots adjacents et à la perte des informations concernant les positions de mots. La perte de l'information et de la perte de corrélation entre les mots adjacents influent sur la qualité des grappes obtenues. Nous vous proposons dans cet article, d'utiliser un modèle structuré pour la représentation de texte lors de groupes qui se chevauchent detect- ING en fonction des séquences de mots. Cette représentation, prend en compte les renseignements sur les positions des mots et a l'avantage d'être plus indépen- dent linguistique. Nous vous proposons une méthode qui se chevauchent en mesure de détecter les groupes concernés dans les données textuelles sur la base de mots de séquence du noyau comme une mesure de similarité. Le présent document est organisé comme suit: présente Sect.2 la séquence de mots du noyau et les méthodes de classification qui se chevauchent existantes, présente alors Sect.3 la méthode WSK à base de KOKM que nous vous proposons. Les expériences sur les différents ensembles de données sont décrites et discutées dans Sect.4. Enfin, sect.5 présente conclusion et les travaux futurs. 2 Contexte Dans le modèle vectoriel (VSM), chaque document de texte est représenté par un vecteur de jetons (mots) où la taille du vecteur est déterminé par le nombre de jetons différents dans tous les documents D. Chaque documents dj se transformeront dans un vecteur: dj = (W1J, W2J, ..., w | T | j), où T est l'ensemble des termes T = (t1, ..., t | T |) (ou jetons) qui apparaît au moins une fois dans le corpus (| T | est la taille du vocabulaire), et WKJ représente le poids (fréquence ou de l'importance) du terme tk dans le document dj. Les documents dont les vecteurs sont proches les uns des autres en fonction de jetons fréquences sont considérées comme un contenu similaire. Cette représentation est basée sur l'hypothèse que la position relative des jetons a peu d'importance qui conduit à la perte de corrélation avec les mots adjacents et conduisant à la perte des informations concernant les positions de mots. Le « n-grammes » représentation de texte permet de résoudre le problème de la perte d'information positions de mots en considérant indépendam- ment document texte sous forme de séquences de n caractères consécutifs (syllabes ou mots). L'ensemble des n-grammes est obtenu par l'extraction de toutes les sous-séquences possibles ordonnées de n caractères consécutifs (syllabes ou mots) le long du texte. Cette repré- sentation des fils de texte à une grande dimension caractéristiques de séquences représentant le document texte. Ce problème est résolu dans les tâches de récupération d'informations en utilisant des machines du noyau sur des séquences de texte. De nombreux noyaux connus sous le nom chaîne du noyau tels que le n-grammes du noyau (Leslie et al., 2002), String Subsequence noyau (Lodhi et al., 2001) et Word séquence noyau (Cancedda et al., 2003) sont proposés dans la littérature. Ces noyaux de retour du produit intérieur entre docu- ments mis en correspondance dans un espace de représentation dimensionnelle. Ce produit intérieur est calculé sans calculer explicitement des vecteurs de caractéristiques. 2,1 WSK: Word séquence noyau WSK est défini comme une extension de la chaîne de séquence noyau (SSK) proposé par Lodhi et al. (2001) qui mesurent la similarité entre deux phrases ou deux documents en fonction du nombre de séquences de caractères partagés entre eux. Cancedda et al. (2003) étend la Benn'cir C. et al. noyau SSK et propose le noyau WSK qui mesurent la similarité entre deux séquences à base de mot plutôt que le caractère. Que Σ l'alphabet qui consiste dans l'ensemble des mots qui existent dans tous les documents, soit S = s1s2s3 ... s | s | la séquence de mots avec | S | est la longueur de S, soit u = s [i] une sous-séquence de S avec s [i] = si1 ..sij ..sin où si1 et sij dans cette sous-séquence ne sont pas nécessairement contigus dans S, le mappage de fonction φ pour la phrase s dans l'espace caractéristique est donnée par la définition de φu pour chaque u ∈ Σn que: φu (s) = Σ i: u = s [i] λl (i), (1) où l (i) est la longueur de sous-séquence s [i] en s avec l (i) = en - i1 + 1 et λ est le facteur de décroissance servant à pénaliser des sous-séquences non contiguës. Ces caractéristiques mesurent le nombre d'occurrences de sous-séquence dans la phrase u s les pondérant en fonction de leur longueur. Donc, étant donné deux chaînes s1 et s2, le produit intérieur des vecteurs de caractéristiques est obtenue en calculant la somme de toutes les communes séquences: Kn (s1, s2) = Σ u∈Σn φu (s1) φu (s2) = Σ u∈ Σn Σ i: u = s1 [i] Σ j: u = s2 [j] λl (i) + l (j). (2) L'objectif de cette représentation est de conserver les informations concernant les positions des mots et de garder le sens linguistique des termes en utilisant des mots ordonnés comme des unités atomiques. Par exemple, les termes « fils-frère » désignent une signification particulière qui peut être perdu si elle est cassée. De plus, le nombre de fonctions par document est réduit, car il utilise des séquences de mots plutôt que des séquences de caractères. 2.2 Méthodes existantes qui se chevauchent sur la base des k-means algorithme existant k-moyens méthodes basées étendent les méthodes de classification floue et possibilistes à pro- duire des grappes se chevauchent. Exemple de ces procédés sont les c-moyen de logique floue (Bezděk, 1981) et les méthodes c-possibilistes moyens (Krishnapuram et Keller, 1993). Ces méthodes ont besoin d'un traitement post-traitement pour générer des clusters durs et se chevauchent par seuillage adhésions clusters. Des méthodes plus récentes sont basées sur l'adaptation des algorithmes k-moyens à la recherche de couvertures optimales (Cleuziou, 2008), (Cleuziou, 2009), (BenN'Cir et al., 2010). Contrairement à k-means floue et possibilistes, ces méthodes produisent des grappes qui se chevauchent dur et n'a pas besoin d'aucun traitement post-traitement. le critères optimisés par ces méthodes recherchent des groupes qui se chevauchent optimales. Une méthode récente proposée désigné comme le noyau se chevauchent k-moyens (KOKMφ) (BenN'Cir et Essoussi, 2012), se prolonge le noyau k-moyens pour détecter non disjoints et non linéairement séparables grappes. Par une application implicite des données à partir d'un espace d'entrée à un niveau supérieur, éventuellement infi- nie, l'espace de fonction, KOKMφ regards pour la séparation dans l'espace des fonctions et résout le problème de chevauchement avec les clusters non-linéaire et les séparations non sphériques. Compte tenu de l'ensemble des observations X = {xi} Ni = 0 avec xi ∈ Rd, et N est le nombre d'observations. Soit C le nombre de couvertures et φ (xi) la représentation du xi d'observation dans un espace de dimension hauteur d'une transformation non linéaire φ: xi 7 → φ (xi) ∈ F. Le KOKMφ non-disjoints regroupant des documents de texte méthode introduit la contrainte de chevauchement (une observation peut appartenir à plus d'un cluster) dans la fonction objectif qui minimise une erreur locale sur chaque observation définie par la distance entre l'observation et son image dans la fonction l'espace: J ({πc} Cc = 1) = Σ xi∈X ∥φ (xi) - im (φ (xi)) ∥2, (3) où im (φ (xi)) est l'image de l'observation xi et est définie par le centre de gravité de grappes prototypes à laquelle appartient xi. Si l'observation xi est attribué à un seul groupe, l'image est équivalent au représentant du groupe suivant. La fonction objectif est calculée sans effectuer de manière explicite le mappage non linéaire φ en utilisant la fonction du noyau Kij = φ (xi) .φ (xj) évaluer le produit scalaire dans l'espace de caractéristique entre xi et xj: J ({πc} Cc = 1 ) = Σ xi∈X [Kii - 2 Li CΣ c = 1 Pic · KIMC + (1 Li) 2 CΣ c = 1 l = 1 CΣ PicPil · Kmcml]. (4) 3 Solution proposée: KOKM basée WSK Pour détecter des groupes non disjoints de documents texte séquentiel, nous proposons une méthode de chevauchement arbitré comme « WSK à base KOKM » en utilisant WSK comme une mesure de similarité entre documents structurée. Compte tenu d'un ensemble de documents D = {DQ} | D | q = 1 où chaque document DQ est défini dans l'espace de fonction par le u φu coordonnées (DQ), qui mesure le nombre d'occurrences de séquence u dans le document DQ pondéré selon il est longueurs. La méthode proposée consiste à la minimisation d'une erreur locale sur chaque document, où l'erreur locale est définie par la distance du noyau entre le document et l'image de lui. La fonction objective de la WSK à base KOKM est décrit par: J ({} πc Cc = 1) = Σ dq∈D ∥φ (DQ) - im (φ (DQ)) ∥2. (5) où C est le nombre de groupes qui se chevauchent et im (φ (dq)) est l'image du document dq défini par le centre de gravité de prototypes de clusters à laquelle le document dq appartient: im (φ (dq)) = CΣ c = 1 PQC · φ (mc) CΣ c = 1 PQC, (6) avec PQC est une variable binaire indiquant l'appartenance du document dq dans la grappe et c mc est le prototype du c du cluster dans l'espace de caractéristiques. Utilisation du noyau WSK défini dans l'équation 2 et en utilisant le noyau Trick, la fonction objective est effectué comme suit: C. Benn'cir et al. J ({πc} Cc = 1) = Σ dq∈D [Σ u∈Σn φu (dq) φu (dq) - 2 Lq CΣ c = 1 PQC · Σ u∈Σn φu (dq) φu (DMC) + (1LQ) 2 CΣ c = 1 l = 1 CΣ PqcPql · Σ u∈Σn φu (DMC) φu (DML)] = Σ dq∈D [Kn (dq, dq) - 2 Lq CΣ c = 1 PQC · Kn (dq, DMC) + 1 Lq 2 CΣ c = 1 l = 1 CΣ PqcPql · Kn (DMC, DML)], (7) où Lq = CΣ c = 1 PQC, Σn est l'ensemble de toutes possibles commandés mot-séquences de longueur n et de DMC est le prototype du groupe c. La minimisation de la fonction objective est effectuée localement par deux étapes principales itérer: la première étape concerne le calcul des prototypes clusters où chaque prototype est défini par le document qui minimise les distances avec les documents d'autres appartenant au même groupe. Ce calcul de prototypes est réalisée dans l'espace de représentation où le document dmc représentant prototype de c cluster est calculé comme suit: dmc = min q∈πc Σ j∈πc, j = q ̸ wj. Σ u∈Σn || φu (dq) - φu (dj) || 2 Σ j∈πc, j = q ̸ wj = min q∈πc Σ j∈πc, j = q ̸ wj [Kn (dq, dq) - 2.Kn (dq, dj) + Kn (dj, dj)] Σ j∈πc, j = q ̸ wj, (8) où wj est un poids de la Kern el la distance entre le document et le document DQ dj en fonction du nombre de groupes auxquels le document dj appartient. Ce poids est plus important si le document dj appartient à plus d'un cluster pour prendre en compte le fait que les documents qui se chevauchent dj ont une faible influence dans la détermination de prototype de cluster ainsi que le nombre d'augmentations d'affectation. La deuxième étape concerne l'attribution de plusieurs des documents à un ou plusieurs groupes. Cette étape est effectuée en utilisant une heuristique qui explore les ensembles combinatoires de signments as- possibles. L'heuristique consiste, pour chaque document, en grappes de tri du plus proche au plus éloigné du, assignant alors le document dans l'ordre défini en affectation minimise l'erreur cal lo- définie dans la fonction objective et il minimise la fonction objectif de trou. La règle d'arrêt de l'algorithme de base WSK KOKM se caractérise par deux critères: le grand nombre possible d'itérations ou l'amélioration minimale de la fonction objective entre deux itérations. L'algorithme principal de WSK à base KOKM est décrite comme suit: non-disjoints regroupant des documents de texte algorithme 1 WSK basé KOKM (D, C, Tmax, ε) → {πc} Cc = 1 Exigent: D: ensemble de documents, Tmax: nombre maximum d'itérations, ε: l'amélioration minimale de la fonction objective, C: nombre de groupes. Assurer: 1: Initialisation des prototypes de clusters avec un des documents au hasard, des documents attribuer et valeur dérive de la fonction objectif J0 ({πc} Cc = 1) dans l'itération en utilisant l'équation 7. 0 2: Calcul des prototypes de grappes en utilisant l'équation 8. 3: documents Affectez un ou plusieurs groupes. 4: Calculer Jt ({πc} Cc = 1) en utilisant l'équation 7. 5: if (t <tmax et Jt-1 ({πc} Cc = 1) - Jt ({πc} Cc = 1)> ε) puis 6 : passer à l'étape 2. 7: 8 autres: Retour les adhésions clusters {} πc Cc = 1 dans l'itération t. 9: end if 4 expériences et discussions expériences ont été effectuées sur l'ordinateur avec 4 Go de RAM et 2,1 GHz Intel Core 2 Duo. Les données sont prétraitées en supprimant quelques mots d'arrêt. La représentation modèle vectoriel de chaque ensemble de données est construit en utilisant le « module prétraiter texte WEKA » où est calculé en utilisant la technique TF * IDF les fréquences d'apparition des mots. Pour la mise en œuvre de la Parole séquence du noyau, nous utilisons la définition récursive de la WSK définie par Cancedda et al. (2003) basée sur la technique de programmation dynamique. L'avantage de cette mise en œuvre est de réduire la complexité de temps et d'effectuer WSK sans extraire implicitement des séquences de mots. La complexité du temps de calcul noyau WSK entre deux uments doc- d1 et d2 est réduite à O (n | d1 d2 || |) où n est la longueur de la séquence utilisée et | di | est le nombre de mots dans le document di. La complexité de calcul du procédé de WSK à base de KOKM est évaluée à O (N.C2.Nc) où N est le nombre de documents, C est le nombre de grappes et Nc est le nombre maximal de documents dans chacune des grappes. Des expériences ont été réalisées sur deux séries de données textuelles se chevauchant qui sont respectivement Reuters 1 et 2 Ohsumed ensembles de données. Nous avons utilisé un sous-ensemble de Reuters composé de 76 documents et un sous-ensemble de Ohsumed composé de 83 documents. Chaque document dans chaque jeu de données est marquée par une ou plusieurs étiquettes à partir d'un ensemble de 5 catégories où chaque catégorie contient 20 documents. Les résultats sont comparés en fonction de trois mesures de validation externe: précision, de rappel et de mesure F-. Ces mesures de validation tentent d'estimer si la prédiction des catégories est correcte par rapport aux vraies catégories sous-jacentes dans les données. Pour chaque ensemble de données, le nombre de groupes est défini sur le nombre de catégories sous-jacentes dans l'ensemble de données. Tableau 1 et le tableau 2 rapport des notes moyennes et la variation de l'écart-type de précision, rappel et F-mesure sur les pistes dix en utilisant des méthodes OKM et KOKMφ basé sur sentation de VSM par rapport à la WSK à base KOKM méthode proposée (nous fixons n = 2 la longueur de 1. voir http://kdd.ics.uci.edu/databases/reuters-transcribed/reuters-transcribed.html 2. cf. http://disi.unitn.it/moschitti/corpora/ohsumed-first -20000-docs.tar.gz C. Benn'cir et al. Avec issue Sans issue Méthodes de précision Rappel F-mesure de précision Rappel F-mesure OKM 0275 ± 0,01 ± 0968 ± 0,01 0,429 0,03 0,275 ± 0,01 ± 0968 ± 0,03 0,429 0,01 KOKMφ (linéaire) 0275 ± 0,01 ± 0955 ± 0,04 0427 0,02 0,275 ± 0,01 ± 0,958 ± 0,04 0,428 0,01 KOKMφ (polynomiale) 0275 ± 0,01 ± 0955 ± 0,04 0427 0,01 0275 ± 0,01 0,958 ± 0,04 0,428 ± 0,01 KOKMφ (RBF σ = 10) 0274 ± 0965 0,01 ± 0,03 0,02 ± 0426 0274 ± 0968 0,01 ± 0,03 0427 ± 0, 02 KOKMφ (RBF σ = 108) 0275 ± 0955 0,01 ± 0,05 0,02 ± 0427 0275 ± 0958 0,01 ± 0,04 ± 0,01 0428 WSK à base de KOKM 0.499 ± 0,04 0670 ± 0, 12 0569 ± 0,04 ± 0,05 0,458 ± 0698 ± 0,02 0553 0,04 TAB. 1 - Comparaison entre OKM et KOKMφ représentation basée VSM avec WSK basé KOKM sur Reuters Dataset. Avec issue Sans issue Méthodes de précision Rappel F-mesure de précision Rappel F-mesure OKM 0274 ± 0799 ± 0,03 ± 0,36 0,396 ± 0,04 0,262 ± 0,04 0761 0,378 ± 0,32 0,02 KOKMφ (linéaire) 0297 ± 0798 ± 0,10 ± 0,36 0,417 ± 0,11 0,297 ± 0,795 0,10 ± 0,36 0,416 0,11 KOKMφ (polynomiale) 0297 ± 0798 ± 0,10 ± 0,35 0,417 0,297 0,10 ± 0,10 ± 0795 0,36 ± 0,11 0417 KOKMφ (RBF σ = 10) 0248 ± 0980 0,01 ± 0,02 0,01 ± 0396 0248 ± 0983 0,01 ± 0,02 0396 ± 0, 01 KOKMφ (RBF σ = 108) 0262 ± 0835 ± 0,04 0,40 ± 0,03 0385 0260 0840 ± ± 0,04 0,40 ± 0,03 0383 WSK à base de KOKM 0308 0696 ± 0,03 ± 0, 08 0421 ± 0,02 ± 0,312 ± 0,02 0641 0,06 0,420 ± 0,03 TAB. 2 - Comparaison entre OKM et KOKMφ représentation basée VSM avec WSK basée KOKM sur Ohsumed Dataset. des séquences de mots et de λ = 0,9 la valeur du facteur de décroissance). Les résultats sont comparés avec et sans sans issue. Pour chaque course, toutes les méthodes sont calculées avec la même initialisation des semences pour garantir que toutes les méthodes ont les mêmes conditions expérimentales. Les valeurs en gras correspondent aux scores obtenus. Mieux Le F-mesure obtenue avec WSK à base KOKM est caractérisé par une haute valeur com- épurée des procédés qui se chevauchent sur la base de la représentation VSM. L'amélioration de la F-mesure est induite par l'amélioration de la précision. Par exemple, dans les données Reuters définir la précision obtenue à l'aide WSK à base de KOKM est 0,458 tout en utilisant des méthodes et KOKMφ OKM la précision obtenue max est 0,275. L'amélioration de la précision est réalisée avec et sans ming tige-. Les rappels obtenus avec KOKMφ et méthodes OKM sont caractérisées par une valeur élevée (le rappel obtenu avec OKM dans la série de données Reuters est équivalent à 0,968). Ces valeurs élevées de rappel est expliquée par la façon dont les observations et Assigner OKM KOKMφ à tous les groupes en raison du problème de dominance diagonale. Par exemple, dans le jeu de données Reuters, où la mensionality di- de la matrice de VSM est très clairsemée (1482 mots), OKM et KOKMφ ont la question de la domination diagonale et attribue donc chaque observation à tous les groupes. Les résultats obtenus prouvent la conclusion théorique que l'examen du texte comme une séquence de mots améliore la précision de classification par rapport à la représentation VSM. Le sens de lan- gues naturelles dépend des séquences de mots, et les séquences de mots fréquents peuvent fournir des informations précieuses sur compact et structures de documents. En fait, les méthodes fonction du noyau à base (Sac de Word noyau ou Word séquence noyau) surclassent méthode non du noyau. Ces résultats non-disjoints regroupant des documents texte prouvent que la recherche de la séparation entre les clusters dans un espace caractéristique est mieux que la recherche de la séparation dans l'espace d'entrée. Séparabilité entre documents peut être améliorée lorsque les documents sont mis en correspondance avec un espace de représentation. 5 Conclusion Nous proposons dans cet article la méthode WSK à base KOKM qui est capable de détecter des groupes non disjoints de documents textuels séquentiels basés sur le noyau mot séquence en tant que mesure de simi- larité. La détection groupes qui se chevauchent en considérant le texte comme une séquence de mots im- prouve la qualité des groupes obtenus par rapport à la représentation VSM du texte. Préliminaires résultats obtenus sur des ensembles de données Reuters et Ohsumed prouvent l'efficacité de la méthode proposée par rapport aux méthodes qui se chevauchent en utilisant la représentation de VSM. Cette méthode proposée peut être appliquée pour beaucoup d'autres domaines d'application où les besoins de données textuelles à être affecté à plus d'un cluster. Nous prévoyons de réaliser des expériences dans d'autres ensembles de données réelles qui se chevauchent où les séquences de texte sont plus pertinentes que les séquences de Reuters et Ohsumed datasetd comme dans la détection des groupes de données biologiques textuelles. Références BenN'Cir, C. et N. Essoussi (2012). Chevauchement reconnaissance des modèles avec des séparations linéaires et non linéaires à l'aide de noyaux définis positifs. International Journal of Computer cations Appli- (IJCA). BenN'Cir, C., N. Essoussi et P. Bertrand (2010). Kernel se chevauchent k-moyens de regroupement dans l'espace des fonctions. À la Conférence internationale sur la découverte des connaissances et la recherche d'information KDIR, Valencia, SPA, pp. 250-256. SciTePress Bibliothèque numérique. Bezdek, J. C. (1981). Reconnaissance des formes avec algoritms fonction objectif floue. Plenum Press 4 (2), 67-76. Cancedda, N., E. Gaussier, C. Goutte et J. Renders (2003). les noyaux de séquences de mots. Journal of Machine Learning Research 3, 1059-1082. Cleuziou, G. (2008). Une version étendue de la méthode k-moyens de regroupement se chevauchent. Dans la Conférence internationale sur la reconnaissance des formes ICPR, Floride, États-Unis, pp. 1-4. IEEE. Cleuziou, G. (2009). Okmed et wokm: deux de OKM verser variantes la classification recou- vrante. Revue des Nouvelles Technologies de l'Information, CÃl'paduÃĺs Édition 1, 31-42. Diday, E. (1984). Les commandes et les grappes qui se chevauchent par des pyramides. Rapport technique 730, INRIA, France. Fellows, M. R., J. Guo, C. Komusiewicz, R. Niedermeier, et J. Uhlmann (2011). données basées sur le graphique en cluster avec des chevauchements. Optimization Discrète 8 (1), 2-17. Krishnapuram, R. et J. M. Keller (1993). Une approche possibiliste de regroupement. actions IEEE Trans- sur les systèmes Fuzzy 1, 98-110. Leslie, C. S., E. Eskin et W. S. Noble (2002). Le noyau du spectre: Un noyau de chaîne pour la classification des protéines svm. Dans Symposium du Pacifique sur Bioinformatique, p. 566-575. C. Benn'cir et al. Lodhi, H., N. Cristianini, J. Shawe-Taylor et C. Watkins (2001). Texte classication en utilisant le noyau de la chaîne. Le Journal of Machine Learning Research 2, 419-444. Ce travail résumé Problématiques deux parents traite à la classification des Données textuelles. La Première problèmatique des Se APPLIQUER La détection non-disjoints Groupes. En effet, un docu- ment Textuel may Aborder several et par thématiques Différentes la suite il Appartenir à Doït several Groupes. L'algorithme d'apprentissage Doït of this Tenir compte et Doït contrainte each Document à attributeur un OU à several Groupes differents. La concerns la Deuxième problématique des Données textuelles modélisation. Les Données Mode- textuelles lisées Sont sous Souvent formes vectorielles. This forme de la représentation Négligé Entre les Annoter corrélation et ne donne à l'importance Aucune d'apparitions des Ordre Dans le texte mots. Nous proposons de classification Une méthode non capable de supervisee des Détecter en Recouvrements Groupes Avec modélisant le texte sous forme de de Séquences Annoter. Le WSK de (Word séquence noyau) is used Comme de mesure Entre les séquences similarité de Annoter. Les Expérimentations la performances réalisées montrent de la proposed par rapport méthode aux methods recouvrantes to vary Qui la modélisation utilisent vectorielle. Page blanche"
425,Revue des Nouvelles Technologies de l'Information,EGC,2013,"Towards a New Science of Big Data Analytics, based on the Geometry and the Topology of Complex, Hierarchic Systems","My work is concerned with pattern recognition, knowledge discovery, computer learning and statistics. I address how geometry and topology can uncover and empower the semantics of data. In addition to the semantics of data that can be explored using Correspondence Analysis and related multivariate data analyses, hierarchy is a fundamental concept in this work. I address not only low dimensional projection for display purposes, but carry out search and pattern recognition, whenever useful, in very high dimensional spaces. High dimensional spaces present very different characteristics from low dimensions, I have shown that in a particular sense very high dimensional space becomes, as dimensionality increases, hierarchical. I have also shown how in hierarchy, and hence in an ultrametric topological mapping of information space, we track change or anomaly or rupture.In this presentation, the first theme discussed is that of linear time hierarchical clustering with application to sky survey data in astronomy, and to chemo-informatics. The second theme discussed is computational text analysis. It is interesting to note that J.P. Benzécri's original motivation was in language and linguistics. In my text analysis work, I have taken the dictum of McKee (Story : Substance, Structure, Style and the Principles of Screenwriting, Methuen, 1999) that ""text is the sensory surface of a work of art"" and show just how this insight can be rendered in computational terms. This leads to demarcating, tracking, statistical modelling, visualizing, and pattern recognition of narrative. In an application to collaborative writing, I developed an interactive framework for critiquing, and assessing fit and appropriateness of content, on the basis of semantics, leading to books that were published as e-books, having been written by school children in a few days of collaborative class work. In many aspects of this work, hierarchy expresses both continuity and change in the textual narrative or in the narrative of chronological events.",Fionn Murtagh,http://editions-rnti.fr/render_pdf.php?p1&p=1001813,http://editions-rnti.fr/render_pdf.php?p=1001813,en,"Vers une nouvelle science de Big Data Analytics, basée sur la géométrie et la topologie du complexe, Hiérarchique systèmes Fionn Murtagh * * Royal Holloway, Université de Londres fionn@cs.rhul.ac.uk~~V~~singular~~3rd 1 Résumé Mon travail porte sur la reconnaissance des formes, découverte de la connaissance, l'apprentissage informatique et des statistiques. Je adresse comment la géométrie et la topologie peut découvrir et Empower la sémantique des données. En plus de la sémantique des données qui peuvent être explorées avec la correspondance lyse Ana- et analyses de données à plusieurs variables connexes, la hiérarchie est un concept fondamental dans ce travail. J'adresse non seulement faible projection dimensionnelle à des fins d'affichage, mais effectuer une recherche et PAT- reconnaissance de sternes, chaque fois utile, dans des espaces très haute dimension. des espaces de grande dimension présentent des caractéristiques très différentes de faibles dimensions, Nous avons montré que dans un sens particulier l'espace de dimension très élevée devient, l'augmentation de la dimensionnalité, hiérarchique. J'ai aussi montré comment dans la hiérarchie, et donc dans une cartographie topologique ultramétrique de l'espace d'information, nous suivons le changement ou anomalie ou rupture. Dans cette présentation, le premier thème abordé est celui de la classification hiérarchique de temps linéaire avec application aux données de l'enquête de ciel en astronomie, et de chimio-informatique. Le deuxième thème abordé est l'analyse de textes de calcul. Il est intéressant de noter que la motivation originale de J.P. Benzécri était en langue et linguistique. Dans mon travail d'analyse de texte, j'ai pris le dictum de McKee (histoire: Substance, structure, Style et les principes de la scénarisation, Methuen, 1999) que « le texte est la surface sensorielle d'une œuvre d'art » et de montrer à quel point cette idée peut être rendu en termes de calcul. Cela conduit à démarcation, le suivi, la modélisation statistique, la visualisation et reconnaissance de forme du récit. Dans une application à l'écriture collaborative, j'ai développé un cadre interactif pour faire la critique et l'évaluation de l'ajustement et la pertinence du contenu, sur la base de la sémantique, conduisant à des livres qui ont été publiés sous la forme d'e-livres, ayant été écrit par les enfants de l'école dans quelques jours des travaux de classe de collaboration. Dans de nombreux aspects de ce travail, la hiérarchie exprime à la fois la continuité et le changement dans le récit textuel ou dans le récit chronologique des événements. 2 Biographie Fionn Murtagh est professeur d'informatique à l'Université de Londres, au Royal Holloway (http: //www.cs.rhul.ac.uk/home/fionn). Au cours des 5 dernières années, il dirigeait les programmes de financement de la Science Foundation Ireland sur une vaste zone, y compris web sémantique et le capteur, les énergies renouvelables, la nanotechnologie et les télécommunications. Dans le passé, il a occupé la pleine Vers une nouvelle science du professeur Analytics Big Data de postes Computer Sciences à l'Université Queen de Belfast, et à l'Université d'Ulster. Pendant 12 ans, il a servi au département de l'Agence spatiale européenne des sciences spatiales, à l'Observatoire européen austral, à Munich. il a été pendant de nombreuses années un professeur auxiliaire à l'Observatoire astronomique de Strasbourg, Université de Strasbourg. De nombreuses positions visi- ting ont eu lieu au Centre commun de recherche, Ispra, en Italie et au Département de la statistique, Université de Washington. Fionn Murtagh diplômes de sont en mathématiques et en sciences de l'ingénieur (BA, BAI), une maîtrise ès sciences en informatique, tous de Trinity College de Dublin, un doctorat de cycle en mathé- 3ème Statistiques de l'Université matique P.M. Curie, Paris 6, et un HDR en reconnaissance de formes en astronomie de (aujourd'hui) Université de Strasbourg. Fionn a été président de la société de classification (anciennement la société de classification de l'Amérique du Nord), et président de la société de classification britannique. Il est membre élu de l'Académie royale d'Irlande, membre de l'Association internationale pour la reconnaissance des formes, et membre de la British Computer Society. Fionn Murtagh est rédacteur en chef de l'ordinateur Journal, la revue phare de la société informatique de la Britannica (publié par Oxford Universit y Press). Il est membre des comités de rédaction de diverses revues, y compris la reconnaissance des formes, Journal de classification et Neu- rocomputing. Il a publié 6 monographies de recherche, 125 articles de revues, 135 autres documents dans les compilations de livres et comptes rendus de conférences, et il a édité de nombreux livres et revue de questions particulières. Son numéro est Erdös 2."
430,Revue des Nouvelles Technologies de l'Information,EGC,2013,Unsupervised Video Tag Correction System,"We present a new system for video auto tagging which aims at correcting and completing the tags provided by users for videos uploaded on the Internet. Unlike most existing systems, we do not learn any tag classifiers or use the questionable textual information to compare our videos. We propose to compare directly the visual content of the videos described by different sets of features such as Bag-of-visual-Words or frequent patterns built from them. Then, we propagate tags between visually similar videos according to the frequency of these tags in a given video neighborhood. We also propose a controlled experimental set up to evaluate such a system. Experiments show that with suitable features, we are able to correct a reasonable amount of tags in Web videos.","Hoang-Tung Tran, Elisa Fromont, François Jacquenet, Baptiste Jeudy, Adrien Martins",http://editions-rnti.fr/render_pdf.php?p1&p=1001867,http://editions-rnti.fr/render_pdf.php?p=1001867,en,"résultat / 5nnSlimBaseline-all-both.eps Unsupervised vidéo Tag correction.Système Hoang-Tung Tran *, Elisa Fromont *, François Jacquenet *, Baptiste Jeudy *, Adrien Martins * * Laboratoire Hubert Curien, UMR CNRS 5516 18 Rue du Professeur Benoît Lauras, 42000 Saint-Etienne {hoang.tung.tran, elisa.fromont, francois.jacquenet}@univ-st-etienne.fr {baptiste.jeudi, adrien.martins}@univ-st-etienne.fr Résumé. Nous présentons un nouveau système de marquage automatique vidéo qui vise à recting cor- et en complétant les balises fournies par les utilisateurs pour les vidéos téléchargées sur Internet. Contrairement à la plupart des systèmes existants, nous n'apprenons pas classificateurs d'étiquette ou d'utiliser les informations textuelles discutables pour comparer nos vidéos. Nous vous proposons de comparer directement le contenu visuel des vidéos décrites par différents ensembles de fonctionnalités telles que le sac-de-visuels mots ou motifs fréquents construits d'eux. Ensuite, nous propageons les balises entre les vidéos visuellement similaires en fonction de la fréquence de ces balises dans un quartier de vidéo donné. Nous vous proposons également une exper- contrôlée imental mis en place pour évaluer un tel système. Les expériences montrent que des caractéristiques appropriées, nous sommes en mesure de corriger une quantité raisonnable de balises dans des vidéos sur le Web. 1 Introduction Les moteurs de recherche classiques à base de texte offrent déjà un bon accès à des contenus multimédias dans le monde en ligne. Cependant, ils ne peuvent pas indexer le nombre important de vidéos en ligne à moins que ces vidéos sont soigneusement annotés avant d'être mis sur le Web. Cependant, les notations an- fournies par l'utilisateur sont souvent erronées, à savoir sans rapport avec la vidéo (par exemple pour augmenter le nombre de vues de la vidéo), et incomplète. Pour remédier à ces inconvénients, nous allons nous concentrer sur la tâche de ting progammation un système automatique pour améliorer les annotations de vidéos sur le Web. Il y a déjà eu beaucoup d'efforts pour les vidéos annoter (par exemple (Morsillo et al., 2010), (Shen et al., 2011)). Cependant, la plupart des systèmes proposés utilisent des concepts limités (tags) et une formation in- supervisée pour apprendre un ou plusieurs classificateurs pour marquer un jeu de données vidéo. Ces approches semblent donc inappropriées pour une vidéo sur un grand site comme Youtube où le nombre de balises possibles est illimité et où les vraies étiquettes sont inaccessibles a priori. Nous aimerions donc proposer une approche non supervisée basée sur la comparaison du contenu visuel des vidéos pour propager les balises des vidéos voisines en fonction de leur fréquence textuelle. Dans cette ap- proche des principaux verrous scientifiques résident i) dans le choix des caractéristiques qui seront utilisées pour faire des comparaisons sans surveillance pertinentes, ii) dans la méthode de comparaison elle-même, iii) dans le processus de propagation et iv) dans l'évaluation de la ensemble du système. Un examen des travaux connexes concernant les problèmes ci-dessus mentionnés est brièvement donnée dans la section 2. Dans la section 3, nous décrivons en détail comment appliquer les données techniques minières ainsi que notre méthode proposée pour comparer des vidéos. Les expériences faites jusqu'à présent sont présentés à la section 4 et nous concluons à l'article 5. Système de correction de la balise vidéo Unsupervised 2 Cadre général et les travaux connexes Trouver les caractéristiques pertinentes (étape 1 et 2). La première étape de notre procédé consiste à décomposer une vidéo en une séquence d'images clés (en utilisant par exemple (Zhuang et al., 1998)). Ensuite, nous DE- Scribe la vidéo sur la base des images. Différentes fonctionnalités sont généralement les mieux adaptés pour différentes tâches. La tendance actuelle en vision par ordinateur est de concaténer différents types de fea- tures bas niveau dans un vecteur de grande dimension qui sera ensuite utilisé pour résoudre les tâches de vision. Par exemple, on peut utiliser des histogrammes de distribution de pointe, des moments de couleur ou de la couleur de la texture ondelettes autocorrélation relograms (Moxley et al., 2010), histogrammes de gradient (HOG) ou des fonctions audio orienté, LAB et histogrammes de couleur HSV global, Haar ou Gabor vaguelettes ( voir (Morsillo et al., 2010)). Une autre technique très populaire est de construire un sac de mots visuels (BoVW) à partir des vecteurs de caractéristiques à faible niveau Inal initiatrices (voir (Yang et al., 2007)). Cependant, lorsque vous utilisez seulement contenu visuel pour comparer les vidéos, les caractéristiques mentionnées ci-dessus pourraient ne pas être assez discriminante. techniques minières fréquentes de modèle sont de plus en plus souvent utilisés dans la vision informatique commu- nauté pour obtenir de meilleures caractéristiques (voir par exemple (Sivic et Zisserman, 2004), (Yuan et al., 2011) et, plus récemment, (Fernando et al. , 2012)). Les approches reposent souvent sur des informations de classe pour pouvoir sélectionner un ensemble compact de caractéristiques pertinentes de la sortie des algorithmes d'exploration. similitudes entre le calcul de vidéos (étape 3). Même si une vidéo est considérée comme une séquence d'images, les variations de la durée des vidéos ou du nombre d'images clés les rendent plus difficiles à comparer. Une première méthode consiste à prendre la moyenne des histogrammes des trames (par exemple (Yang et Toderici, 2011)), pour produire une description unique pour l'ensemble vidéo. L'histogramme peut être seuillée pour enlever un peu de bruit potentiel. Voici les fonctions de distance classique (par exemple L1) peuvent être utilisées pour estimer la similitude entre les vidéos. Même si cette méthode est efficace, on perd beaucoup de l'information disponible en faisant la moyenne de tous les cadres. La seconde approche consiste à comparer des paires d'images clés, par exemple, calcul de la similitude entre les deux trames les plus semblables des vidéos comme dans Moxley et al. (2010). La comparaison des deux vidéos est faite en utilisant une paire unique de cadres et aucune information séquentielle est prise en compte. La dernière utilise des cadres identiques communs (mais différents en termes de mise en forme, points de vue, les paramètres de l'appareil photo, etc.) appelés près de double pour comparer des vidéos (voir par exemple (Zhao et al., 2010)). Ces deux exemplaires près ne peut être trouvé dans toutes les vidéos. procédure de propagation de marque (étape 4). Comme la plupart des systèmes de marquage auto vidéo apprendre classificateurs multiples, l'étape de propagation de l'étiquette n'est pas nécessaire. Cependant, la méthode de la double-près présentée dans Zhao et al. (2010) utilisent cette procédure de propagation sur laquelle repose la nôtre. Pour chaque V vidéo, une liste des balises possibles pertinentes est obtenue à partir des k plupart des vidéos similaires (en utilisant un algorithme voisin K-le plus proche). Après cela, une fonction de score est appliquée pour chaque étiquette d'estimer la pertinence de cette étiquette selon un V vidéo donné. Cette fonction de score dépend de la fréquence d'étiquette, le nombre d'étiquettes associées à une vidéo, et la similitude vidéo. Enfin, seuls les tags avec un plus grand score à un seuil sont considérés comme appropriés pour la V vidéo. 3 Amélioration du système de marquage automatique proposé caractéristiques proposées Comme il est expliqué à la section 2, nous pouvons utiliser de nombreuses fonctionnalités possibles pour décrire une vidéo et ceci est un point crucial de travailler sur une propagation de l'étiquette correspondant à la fin Tran et al. du procédé. Nous vous proposons d'utiliser BOVW construit à partir des descripteurs EIPD (Lowe, 2004) obtenus régulièrement dans chaque image-clé d'une vidéo que nos fonctionnalités de bas niveau. Nous voulons ensuite d'utiliser un algorithme d'exploration de modèle pour extraire mieux que l'on appelle des fonctionnalités de niveau moyen pour comparer nos vidéos. La plupart des algorithmes proposés dans la prise de la littérature en tant que vecteurs binaires d'entrée. Comme l'a expliqué Fernando et al. (2012), doit être fait avec soin le « binarisation » du BOVW d'origine. Nous vous proposons d'utiliser un simple taille égale-bin discrétisation (avec un nombre de cases égal à 4) pour chaque mot visuel pour transformer notre histogramme d'origine dans un vecteur binaire. En outre, les données minières sortie des techniques un grand nombre de modèles (exponentielle du nombre de dimensions des vecteurs binaires). Ces motifs peuvent être filtrés à l'aide des informations surveillées comme, par exemple, représenté sur Fernando et al. (2012). Cependant, dans notre cas, aucune information supervisée est disponible ainsi critères différents doivent être proposés. Nous avons donc décidé d'utiliser l'algorithme SLIM (Smets et Vreeken, 2012). Cet algorithme optimize un critère basé sur la longueur Description minimum pour réduire le nombre de modèles de sortie à ceux qui « bien » Compresser les données. Elle emploie une heuristique simple et précise pour estimer le gain ou le coût de l'ajout d'un candidat au motif de sortie ensemble. Si F est l'ensemble des motifs fréquents obtenus en utilisant SLIM, nous construisons un vecteur binaire V de taille | F | pour chaque image clé. Dans ce vecteur, V (i) est réglé sur 1 si le motif i de F apparaît dans cette image-clé et 0 sinon. Étant donné que le nombre de motifs en F peut encore être grand, nous utilisons également une analyse en composantes principales (ACP de) pour réduire la dimension du vecteur V. Enfin, le vecteur décrivant chaque image clé est soit que l'histogramme BOVW, seul le vecteur V des motifs SLIM (réduite par PCA) ou ces deux vecteurs concaténés. vidéo asymétrique proposée mesure de similarité La première étape de notre méthode consiste à calculer toutes les similitudes entre toutes les paires d'images clés des vidéos. Ensuite, on calcule la moyenne de toutes les similitudes maximum correspondant à une vidéo. En d'autres termes, pour chaque image-clé d'une vidéo A, nous cherchons dans toutes les images clés de vidéo B pour le score correspondant le plus élevé et nous enregistrons par paires cette valeur. Ensuite, on calcule la moyenne de toutes les valeurs enregistrées pour toutes les images clés de la vidéo A pour revenir le score de similitude de la vidéo A vers la vidéo B. Si l'on note A (i) la keyframe ième A et | A | le nombre d'images-clés dans A, puis la carte SIM (A, B) = 1 / | A | Σ i sim max j (A (i), B (j)). La similitude sim (A (i), B (j)) entre les trames est juste l'inverse d'une distance entre les vecteurs représentant les trames. 4 expériences Nous avons d'abord effectué une série d'expériences sur des ensembles de données d'image pour évaluer la ingness interest- des motifs fréquents que les caractéristiques, les différentes distances et la méthode PCA sur le motif de sortie histogramme. En raison du manque d'espace, ces expériences ne sont pas présentés ici, mais ils ont montré que i) les motifs fréquents (FP) peuvent être caractéristiques intéressantes par rapport à de simples sac de mots si elles sont soigneusement choisis; ii) la distance L1 peut être une bonne mesure de distance pour comparer deux vecteurs haut dimensions, qui décrit une vidéo (il est préférable que le noyau de tion habituelle utilisée dans intersections vision par ordinateur pour comparer les histogrammes); iii) un PCA où nous gardons des composants assez pour expliquer 90% de la variance peut contribuer à réduire la dimensionnalité des vecteurs de caractéristiques sans endommager la précision. Système de correction de balise vidéo non surveillée 0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 pe rc en ta ge ofe rr ou vi de os pourcentage de bruit introduit Ajouté BoW bruit tous deux 0 5 10 15 20 25 30 0 5 10 15 20 25 30 pourcentage de bruit introduit pe rc en ta ge ofe rr ou vi de calcul de la moyenne os - 10NN - Seuil = 7 BoW motif fréquent deux Fig. 1 - Résultat de l'algorithme de correction d'étiquette sur un ensemble de données vidéo réel (à gauche) et un synthétique (à droite) en utilisant uniquement sac de caractéristiques de mot ou sac de mots et motifs fréquents. La deuxième série d'expériences pour but de proposer un nouveau protocole expérimental pour évaluer la méthode de propagation de la balise. Nous utilisons d'abord un 51 vidéos ensemble de données réelles tirées d'un ensemble de données de référence des vidéos YouTube (Cao et al., 2009). Chaque vidéo est décomposé en images clés. Il y a environ 27 images clés pour une vidéo. La dimension du vocabulaire SIFT-BOVW est 1000. Une vidéo est donc représentée par une matrice qui contient, pour toutes les images clés de la vidéo de l'histogramme de mots visuel qui décrit la trame. Nous avons gardé cet ensemble de données assez petit pour être en mesure d'évaluer manuellement l'intérêt des étiquettes originales et celles propagées pour chaque vidéo. Les 51 vidéos ont été choisis de telle sorte qu'ils appartiennent à 4 sujets pour faire en sorte que cet ensemble de données contient des paires de vidéos similaires et des paires de vidéos différentes. Nous étiquetterons manuellement les vidéos avec 35 balises. Comme les résultats sur cet ensemble de données ne sont pas concluants, nous avons créé un ensemble de données synthétique de 182 vidéos construites à partir de 7 vidéos très différentes de l'ensemble de données précédente. Dans les deux cas, étaient intéressés à évaluer les caractéristiques à base de motifs fréquents par rapport aux caractéristiques à base BOVW. propagation Tag Un ensemble de données vidéo est un triplet (V, T, étiquette) où V est l'ensemble des vidéos V = {v1, ..., vn}, l'ensemble des balises possibles i s T = {t1, ... tm} et l'étiquette est une relation sur V × T tel que balise (v, t) est vrai si et seulement si la vidéo v a tag t. Notre procédure d'évaluation est alors: - ajouter un peu de bruit sur les balises, par exemple, choisir un bruit proportion 0 <p <1 et réalise une fonction tag bruyante tagnoisy telle que, pour chaque t ∈ T et v ∈ V, avec une probabilité p que nous avons : tagnoisy (v, t) = ¬tag (v, t) (c.-à retourner la valeur d'une variable donnée, avec une probabilité p); - appliquer notre technique de correction d'étiquette, la sortie de l'étape de correction de la balise est tagcorr; - calculer la proportion des balises incorrectes après l'étape de correction comme: err (étiquette, tagcorr) = {‖ (v, t) ∈ V × T | tag (v, t) = 6 tagcorr (v, t)} ‖ / (‖V ‖.‖T‖) Le cas idéal est err (tag, tagcorr) = 0. Notez que err (tag, tagnoisy) ≈ p. Cela signifie que dès que err (étiquette, tagcorr) <p, il y a des balises moins incorrectes sur le jeu bruyant après l'étape de propagation de l'étiquette que précédemment. Dans la figure. 1, nous traçons l'erreur ERR (étiquette, tagcorr) contre la valeur de p. Lorsque la courbe est en dessous de la ligne diagonale, nous pouvons affirmer que notre algorithme a diminué le nombre de balises incorrectes. Les résultats sur l'ensemble de données réel Nous avons appliqué notre procédure d'évaluation sur le réel 51 vidéos ensemble de données présenté au début de la présente section. Nous les résultats sur fait la moyenne 100 pistes pour chaque niveau de bruit. Les résultats sont présentés sur la Fig. 1 (à gauche). Pour presque tous les niveaux de bruit, le nombre d'étiquettes incorrectes est plus élevé après l'algorithme de correction que précédemment. Ces erreurs peuvent être le résultat de l'algorithme de correction ou le fait que la distance calculée entre les vidéos ne Tran et al. reflètent pas la réelle similitude des vidéos. En particulier, le nombre de vidéos que nous utilisons est assez faible. Dans un jeu de données de millions de vidéos, k devrait être beaucoup plus semblables que dans notre petit jeu de données les plus proches voisins d'une vidéo donnée (et donc des balises très similaires). Un autre problème réside dans les balises elles-mêmes: notre algorithme utilisent la similitude visuelle entre les vidéos pour corriger les tags. Ainsi, il peut être efficace que sur les étiquettes qui sont en corrélation avec le contenu visuel. Résultats sur l'ensemble de données de synthèse Le nombre maximum de balises pour cet ensemble de données est de 182 * 7 = 1274. Cela signifie que lors de l'ajout de 5% du bruit dans l'ensemble de données, 63 valeurs de balise sont retournées dans le jeu de données (certaines balises sont ajoutées, certaines ont été retirées ). Ensuite, pour construire la vidéo de synthèse, nous 1) choisir au hasard entre 2 et 4 vidéos de l'ensemble de données vidéo réelle; 2) choisir au hasard des cadres de chacune des vidéos réelles choisies. L'ensemble de trames ainsi obtenues est la vidéo synthétique; 3) Marquer cette vidéo de synthèse avec A si elle contient des images de la vidéo A, avec B si elle contient des images de la vidéo B et ainsi de suite. Chaque vidéo de synthèse a donc entre 2 et 4 balises sur 7 balises possibles. Par cette construction, si deux vidéos synthétiques partagent par exemple l'étiquette A, cela signifie qu'ils contiennent tous deux cadres similaires extraites de la vidéo réelle A. De plus, par construction, chaque étiquette est associée au contenu visuel de la vidéo. Nous évitons donc le dernier problème rencontré avec l'ensemble de données réelles. Pour un niveau de bruit compris entre 0 et 30%, on voit sur la figure. 1 (à droite) que la proportion de balises incorrectes diminue de manière significative. Par exemple, à un niveau de bruit de 20%, la proportion d'erreur après correction de l'étiquette est d'environ 16%. L'algorithme a ainsi éliminé environ un quart des erreurs introduites par le bruit. Notez que pour un niveau de bruit plus élevé, le nombre de balises incorrectes est trop grand pour attendre l'amélioration des résultats par la propagation de l'étiquette. L'analyse des résultats Bien que donnant des résultats très prometteurs sur la propagation de l'étiquette comme le montre la figure. 1 (à droite), les dernières séries d'expériences sur les jeux de données vidéo des questions l'utilité de notre méthode de comparaison vidéo par paires et du niveau élevé proposé fréquent poncifs. En effet, les résultats en utilisant la comparaison par paires introduites dans la section 3 sont similaires à ceux obtenus à l'aide d'une simple moyenne des cadres bien que celui plus tard est plus efficace de calculer. Fig. 1 montre de également que les motifs fréquents construits à l'aide l'algorithme SLIM n'améliore pas la comparaison de l'étiquette par rapport aux simples caractéristiques de BOVW. La combinaison des deux vecteurs de caractéristiques donne également des résultats similaires qui montre que pour les vidéos, au contraire que pour les images, les modèles calculés par l'algorithme SLIM ne semblent pas donner des informations supplémentaires par rapport à la BOVW dont ils sont construits. 5 Conclusion Nous avons présenté un système de marquage automatique sans supervision complète qui corrige et Pletes com- tags originaux sur les vidéos. Le système semble efficace, surtout lorsque le nombre de vidéos dans l'ensemble de données est suffisamment élevé pour avoir un quartier suffisamment pertinent pour chaque vidéo. Cependant, les nouvelles fonctionnalités proposées et la procédure de comparaison vidéo par paire ne semblent pas améliorer nos résultats par rapport aux méthodes de référence. Comme les travaux futurs, nous pose donc pro- de prendre en compte l'information séquentielle dans la vidéo pour créer de meilleures fonctionnalités de haut niveau et de prendre en compte la position spatiale des caractéristiques des cadres. Nous envisageons également de travailler sur l'évolutivité du système proposé pour lutter contre les jeux de données réels plus importants. système de correction tag vidéo Cao Références non surveillée, J., Y. Zhang, Y. Song, Z. Chen, X. Zhang, et J. Li (2009). Mcg-webv: Un ensemble de données de référence pour l'analyse vidéo sur le web. Rapport technique, TIC-09-001 MCG. Fernando, B., E. Fromont et T. Tuytelaars (2012). L'utilisation efficace de l'exploitation minière pour la classification fréquente itemset d'image. Dans Conférence européenne sur l'ordinateur Vision, pp. 214-227. Lowe, D. G. (2004). image distinctive de caractéristiques keypoints échelle invariantes. International Journal of Computer Vision 60 (2), 91-110. Morsillo, N., G. S. Mann et C. Pal (2010). échelle Youtube, grande annotation vidéo vocabulaire. Dans Recherche vidéo et des mines, Volume 287 des études en informatique Intelligence, pp. 357- 386. Springer. Moxley, E., T. Mei, et B. Manjunath (2010). annotation vidéo par le renforcement et la recherche graphique minière. IEEE Transactions on Multimedia 12 (3), 184-193. Shen, J., M. Wang, S. Yan, et X.-S. Hua (2011). marquage multimédia: passé, présent et futur. Dans Actes de la 19e conférence internationale ACM sur le multimédia, pp. 639-640. Sivic, J. et A. Zisserman (2004). Les données vidéo MINING en utilisant des configurations de point de vue invariance régions de fourmis. Vision par ordinateur et reconnaissance de formes (1), pp. 488-495. Smets, K. et J. Vreeken (2012). Slim: l'exploitation directe des modèles descriptifs. En SIAM Conférence nationale sur l'exploration de données inter, p. 236-247. Yang, J., Y. Jiang, A. Hauptmann et C. Ngo (2007). L'évaluation de sac-visuels Mots de sentations de la classification de la scène. Dans l'atelier international sur la recherche d'information multimédia, p. 197-206. ACM. Yang, W. et G. Toderici (2011). l'apprentissage tag discriminante sur des vidéos youtube avec des sous-balises latentes. Dans Vision par ordinateur et reconnaissance, p. 3217-3224. Yuan, J., M. Yang, Y. et Wu (2011). Exploitation minière modèles de co-occurrence discriminants pour la reconnaissance visuelle. En CVPR: Conf. sur Vision par ordinateur et reconnaissance, p. 2777-2784. Zhao, W., X. Wu et C. Ngo (2010). Sur l'annotation de vidéos web par la recherche en double quasi efficace. IEEE Transactions on Multimedia 12 (5), 448-461. Zhuang, Y., Y. Rui, T. Huang et S. Mehrotra (1998). extraction adaptatif de trame en utilisant la clé classification non supervisée. Dans Int. Conf. sur le traitement de l'image (1), pp. 866-870. Nous proposons un résumé nouveau Système de marquage de vidéos Vasant automatique à et Corriger les « Agenda item automatiquement des tags » Fournis Par les LORs de la Utilisateurs mise en ligne d'Une nouvelle vidéo sur Internet. Au Contraire des Systèmes existants, nous ne décidons de pas l'informations UTILISER textuelle Fourni par fausse possiblement les techniques de ni Utilisateurs d'apprentissage supervisez verser les décisions de viles. Nous comparons le contenu visuel Directement des vidéos en nous Basant sur des attributes discriminants APPRI LORs D'une étape de motifs de fouille Fréquents. Ce papier decrit also Une méthode simple, de la propagation des balises ENTR e et vidéos un visuellement proches protocole d'EVALUER expérimental permettant notre approche."
437,Revue des Nouvelles Technologies de l'Information,EGC,2012,Antipattern Detection inWeb Ontologies: an Experiment using SPARQL Queries,"Ontology antipatterns are structures that reflect ontology modelling problems because they lead to inconsistencies, bad reasoning performance or bad formalisation of domain knowledge. We propose four methods for the detection of antipatterns using SPARQL queries. We conduct some experiments to detect antipattern in a corpus of OWL ontologies.","Catherine Roussey, Oscar Corcho, Ond&#711;rej Šváb-Zamazal, François Scharffe, Stephan Bernard",http://editions-rnti.fr/render_pdf.php?p1&p=1001177,http://editions-rnti.fr/render_pdf.php?p=1001177,en,"Détection AntiModèle dans le Web ontologies: une expérience utilisant SPARQL requêtes Catherine Roussey *, Oscar Corcho **, Ondřej Šváb-Zamazal ***, François Scharffe ****, Stephan Bernard * * Irstea / Cemagref, 24 Av. des Landais, BP 50085, Aubière, France ** Ontology Engineering Group, Universidad Politécnica de Madrid, Espagne *** Groupe d'ingénierie des connaissances, Université d'économie de Prague, République tchèque **** LIRMM, Université de Montpellier, France Résumé. Ontologies sont des structures qui les anti reflètent l'ontologie de modélisation des problèmes car ils conduisent à des incohérences, les mauvaises performances de raisonnement ou mauvaise formalisation des connaissances de domaine. Nous vous proposons quatre méthodes de dé- tection des requêtes en utilisant les anti SPARQL. Nous menons des expériences pour détecter dans un corpus antimodèle d'ontologies OWL. 1 Introduction Le concept de modèle de modélisation des connaissances ou modèle de conception de l'ontologie est utilisée pour faire référence à des solutions de modélisation qui permettent la résolution de la modélisation des connaissances récurrente ou l'ontologie des problèmes de conception, Presutti et al. (2008). Antipatterns sont des modèles qui sont inefficaces ou loin de timal OP- dans la pratique, ce qui représente les pires pratiques sur la façon de structurer et de concevoir une ontologie. Il existe plusieurs outils qui peuvent être utilisés pour la détection de les anti. Pellint 1 porte sur la détection et la réparation de l'ontologie pour améliorer les anti-performances de raisonnement. Des outils tels que décrits dans Workbench Explication Horridge et al. (2008), ou SWOOP décrit dans Kalyan- pur et al. (2005), fournir des justifications des incohérences dans ontologies basées sur les sorties de raisonneurs DL. Cependant, toutes ces contributions ont besoin d'un raisonneur pour fournir leurs justifications. Nos méthodes de détection AntiModèle mettre en œuvre une approche plus générale, qui peut fonctionner sur tout antimodèle et peut être appliquée sans l'utilisation d'un raisonneur, ce qui est très utile avec de grandes ontologies et lorsque le nombre d'erreurs dans une ontologie est si grande que la précédente systèmes de justification ne sont pas en mesure de les gérer correctement, en fournissant les délais d'attente. Pour détecter la lected antimodèle sé-, nous l'avons transformé en ensembles de requêtes SPARQL. En général, correspondent à plusieurs anti-patterns requêtes, car ils sont des structures abstraites qui peuvent avoir plusieurs formes logiques lorsqu'elles sont exprimées dans la description Logiques (DL). De plus, nous avons proposé plusieurs méthodes de Tection dé-. Nous pouvons activer ou désactiver des inférences avant d'exécuter des requêtes SPARQL. Nous pouvons également transformer les ontologies originaux en une forme où les requêtes SPARQL plus simples peuvent être exécutés. Ce document est structuré comme suit. L'article 2 décrit brièvement l'antimodèle qui sera utilisé pour exécuter nos expériences. La section 3 décrit les méthodes que nous avons suivi pour exécuter les expériences. La section 4 décrit la configuration de l'expérience et les résultats du 1. http://pellet.owldl.com/pellint - 263 - AntiModèle expérimentation de détection. Enfin, la section 5 fournit quelques conclusions au travail, sur la base des résultats de l'expérience, et décrit les prochaines étapes à faire dans notre travail. 2 Un échantillon antimodèle OnlynessIsLoneliness (OIL) Un ensemble de modèles couramment utilisés par les experts du domaine dans leur mise en œuvre des tologies de OWL sont identifiés dans Corcho et al. (2009). Ces modèles ont donné lieu à des cours insatisfiables ou des erreurs de modélisation, en raison d'une mauvaise utilisation ou mauvaise compréhension des expressions DL. Dans cette section, nous allons décrire un antimodèle qui est celui qui, notre expérience a montré, est plus facile à comprendre et débogage par des experts du domaine. C3 v ∀R.C1; C3 v ∀R.C2; DISJ (C1, C2); (1) C3 ≡ ∀R.C1; C3 v ∀R.C2; DISJ (C1, C2); (2) C3 ≡ ∀R.C1; C3 ≡ ∀R.C2; DISJ (C1, C2); (3) Le développeur de l'ontologie a créé une restriction universelle à dire que les instances C3 ne peuvent être liés à la propriété R aux instances C1. Ensuite, une nouvelle restriction universelle est ajouté en disant que les instances C3 ne peuvent être liés à R aux instances C2, avec C1 et C2 disjoints. En général, cela est parce que le développeur de l'ontologie a oublié l'axiome précédent dans la même classe ou dans l'une des classes parent. 3 détection basée SPARQL-de l'huile antimodèle Dans cette section, nous décrivons les différentes méthodes que nous avons élaborées afin de dé- Tect les anti OWL ontologies au moyen de requêtes SPARQL, basées sur l'utilisation de l'PatOMat ontologie outil de détection de motif 2. Cette fait partie des outils de la suite d'outils de PatOMat, qui se concentre sur la détection de modèles dans ontologies et leur transformation. Cet outil de dé- tection est basée sur Jena 2.6.2 3 et 4 Granules 2.0.1 et permet le traitement d'un ensemble de requêtes SPARQL sur un ensemble d'ontologies, un rapport en termes de nombre de modèles détectés (requêtes de SPARQL résultats ) et les détails de chaque ontologie. Les axiomes sur lesquels l'outil de détection de motif est exécuté peut être les axiomes affirmés dans les ontologies, ou une combinaison des axiomes affirmés et inférées. Nous OWL requête ontologies au moyen d'un langage de requête (SPARQL) qui est agnostique sur le modèle de représentation des connaissances sous-jacente de OWL: nous Interrogation en fait la sérialisation RDF de OWL. D'autres options sont disponibles dans l'état actuel de la technique pour OWL ontologie appariement et la transformation modèle sont la langue OPPL et ses outils associés décrit par Iannone et al. (2009), ou plus récente syntaxe d'interrogation OWL Terp 5, basé sur la syntaxe OWL Manchester. Si SPARQL est le langage dédié à la requête triplets RDF, OPPL et Terp sont dédiés à interroger la sérialisation RDF des expressions OWL parce qu'ils contiennent des constructions OWL comme subClassOf, complementOf, disjointWith. Néanmoins pour faire 2. La version utilisée pour ce document est à l'adresse: http://eso.vse.cz/~svabo/patomat/detectionTool.zip 3. http://jena.sourceforge.net/ 4. http: // clarkparsia.com/pellet/ 5. http://clarkparsia.com/weblog/2010/04/01/pellet21-terp/ - 264 - C. Roussey et al. la construction de SPARQL requêtes plus facile, nous développons un traducteur de requête qui transforme une requête d'entrée, à l'aide de la SPARQL-DL syntaxe abstraite définie dans Sirin et Parsia (2007), dans une requête SPARQL. Transformer en requêtes SPARQL les anti-DL n'est pas une tâche triviale. Pour chaque antimodèle, plusieurs requêtes SPARQL-DL sont nécessaires pour détecter les occurrences dans AntiModèle définition de classe OWL. Les difficultés proviennent de plusieurs points: - Un antimodèle peut être associée à plusieurs formules logiques dans la syntaxe DL. Par exemple, nous avons présenté 3 formules pour les tipatterns de l'huile. - Certaines formules logiques sont composées de plusieurs axiomes atomiques. 6 Par exemple, les trois formules du antimodèle HUILE contient trois axiomes atomiques. - développeur Ontologie peut avoir un style de mise en œuvre très différente lors de la conception d'une ontologie OWL. Par exemple, certains développeurs préfèrent écrire longue définition de classe. Dans ce cas, une classe est définie par une conjonction de classes unamed: C v (∃R.X) u (∀R.Y) u .... D'autres préfèrent écrire de courtes définitions. Une classe est définie par un ensemble d'axiomes atomiques: C v C v; ∃R.X ∀R.Y; C v .... Ainsi, pour une formule anti-modèle, un axiome atomique peut être placé à différents endroits dans la définition de classe. - Un axiome atomique peut appartenir à la définition de classe ou peut être héritée d'une définition de la classe mère. - Un axiome atomique peut dire par le développeur de l'ontologie ou par un raisonneur inférer. Pour construire nos requêtes, nous imaginons d'abord les différentes versions de chaque formules AntiModèle en utilisant la syntaxe abstraite SPARQL-DL. Nous essayons d'imaginer où un axiome atomique peut dire par le développeur de l'ontologie dans une définition de classe. Nous limitons notre imagination aux définitions de classes qui ont au plus quatre conjonctions. Nous intégrons dans ces requêtes quelques-unes des conclusions qui devraient être faites par un raisonneur. Nous prenons en compte le fait que: - axiomes sont disjoints symetric DISJ (C1, C2) DISJ (C2, C1), - axiome disjoints peut être déduit d'une négation logique C1 v ¬C2 DISJ (C1, C2). Ensuite, nous traduisons automatiquement chaque requêtes SPARQL-DL dans les SPARQL. Nous générons automatiquement des requêtes SPARQL qui regroupe toutes les différentes versions. FIGUE. 1 - Les méthodes de détection AntiModèle 6. Nous avons défini un axiome atomique comme condition (v nécessaire ou suffisante ≡) associé à une classe appelée C en utilisant au plus un constructeur (∀, ∃, ¬ ou u). Tous les paramètres doivent être nommés. Un exemple d'axiome atomique peut être C v ∃R.X. - 265 - Détection AntiModèle Comme le montre la figure 1, nous allons maintenant décrire les quatre méthodes que nous avons suivi afin de détecter dans le corpus anti-patterns de l'ontologie: -Méthode 1: Utilisation de requêtes sur SPARQL Affirmé OWL Ontologie Axiomes. Dans cette ap- proche, nous prenons en compte que les moteurs de SPARQL en soi ne considèrent pas des déductions qui peuvent être faites avec ontologies OWL. Cependant, nous supposons qu'il y aura des cas où ontologies ne peuvent pas être traitées par un raisonneur ou les résultats raisonneur ne peuvent pas être obtenus dans un délai raisonnable. Cela se produit normalement avec de grandes ontologies ou ontologies avec un grand nombre de errors.- Méthode 2: Utilisation des requêtes sur SPARQL sur OWL matérialisées Déductions ontologies. Quand il est possible d'utiliser un raisonneur, nous matérialisent toutes les conséquences qui peuvent être faites par un raisonneur OWL sur les ontologies et exécuter des requêtes SPARQL sur les ontologies résultant, appelé matérialisé ontologies. - Méthode 3 et 4: Utilisation de requêtes SPARQL sur Transformed OWL ontologies. En raison de la complexité de la création d'un grand nombre de requêtes SPARQL pour un antimodèle et au fait que les différents développeurs d'ontologies peuvent avoir différents styles de mise en œuvre, nous vous proposons de suivre un processus en deux étapes où l'on applique des transformations avant d'exécuter les requêtes. Les transformations ont deux objectifs: harmoniser le style de mise en œuvre de l'ontologie et de simuler certains des axiomes par Soner inférer rai-. Les transformations actuelles que nous appliquons sont: - Lorsque l'ontologie contient C1 ≡ C2 où C1 et C2 sont nommés classes, nous ajoutons deux nouveaux axiomes C1 v C2 et C2 v C1. - Lorsqu'une classe nommée est définie par la conjonction des classes nommées ou unamed, avons-nous divisé en plusieurs axiomes plus simples. Prenons comme exemple la définition de la classe: C v X u Y, dans ce cas, nous ajoutons deux axiomes C v X et C v Y. - Lorsqu'une classe parent contient un axiome, on ajoute aussi dans sa catégorie enfant direct. Prenons comme exemple la définition de la classe: C1 v ∃R.X. Si C1.1 est un enfant direct de C1, C1 C1.1 v, on ajoute l'axiome C1.1 v ∃R.X. À ce moment-là, cette transformation ne se répète pas sur la hiérarchie des classes. Dans ce cas, nous avons exploré le comportement de la méthode de vérification de la requête SPARQL à la fois sur l'ontologie affirmé après transformation et l'ontologie matérialisée (également après transformation). 4 Trouver dans ontologies les anti-monde réel Dans cette section, nous décrivons les résultats de nos expériences avec un corpus d'ontologies de ceux ex subdivisées en secteurs public sur le Web et indexé par le moteur de recherche sémantique Watson 7. Nous allons d'abord décrire l'ontologie corpus, puis les résultats de l'application des méthodes de dif- férents décrites à la section 3 de ce corpus d'ontologie. 4.1 Les expériences Les ontologies utilisées dans notre expérimentation vient de notre expérience dans l'ontologie Debug- tâche Ging. Cinq d'entre eux ont déjà été utilisés pour la création et la mise à jour du catalogue présenté dans antimodèle Corcho et al. (2009). Il contient le HydrOntology (qui a 159 classes dont 114 sont unsatisfiables), le Forestal ontologies (qui a 93 classes dont 62 sont insatisfiable), l'ontologie Tambis (qui a 395 classes dont 112 sont insatisfiable), l'ontologie douce numérique (qui a 2364 classes dont deux sont insatisfiable) et l'ontologie Uni- versité de l'esprit Lab (qui a 29 classes dont 7 sont insatisfiable). Avis 7. http://watson.kmi.open.ac.uk/ - 266 - C. Roussey et al. que dans notre expérience Hydrontology et les ontologies Tambis ne peuvent pas être traitées par le raisonneur à granulés dans un délai raisonnable. Nous avons fait les expériences suivantes sur l'ensemble des ontologies, en utilisant les méthodes de détec- tion AntiModèle décrites à la section 3: 1. SP: Rechercher dans les ontologies d'origine (uniquement avec axiomes affirmées) à l'aide de requêtes SPARQL et aucune inférence. 2. SP + R: Recherche dans le matérialisée ontologies (et affirmé axiomes inférée) à l'aide de requêtes SPARQL après application d'un raisonneur (pellets). 3. SP_Trans: Appliquer des transformations sur les ontologies d'origine et de recherche (uniquement avec axiomes BRANCHÉ as-) à l'aide de requêtes SPARQL et aucune inférence. 4. SP_Trans + R: Appliquer des transformations sur les ontologies d'origine et de recherche dans le rialisation Maté de ces ontologies harmonisées. Dans ces expériences, nous utilisons aussi le mot-clé MANUEL pour faire référence au processus de détection manuelle à l'aide des outils de débogage de base fournis par les éditeurs de l'ontologie. Cette méthode de détection définit une ligne de base par rapport à ce qui peut être détecté sur l'état actuel de la technique. Nous avons également des expériences courir, sur la base des précédents, pour évaluer la précision du processus de détection antimodèle. Nous avons analysé manuellement chacun des ontologies dans notre jeu et attribué à chacun d'occurrence antimodèle des trois valeurs suivantes: - TI (True Incohérence): les participe d'occurrence AntiModèle dans le insatisfiabilité des classes ou l'erreur de modélisation. - UI (Information inconnue Incohérence): l'apparition antimodèle peut être lié à la capacité unsatisfi- des classes ou des erreurs de modélisation, mais l'évaluateur n'est pas certain. - FI (Incohérence Faux): l'apparition antimodèle ne participe pas à la capacité unsatisfi- des classes ou des erreurs de modélisation. 4.2 Résultats: L'HUILE détection La configuration de l'huile est composé de 3 axiomes atomiques. Nous avons présenté 3 formules mais plus formules sont possibles, en fonction du style de mise en œuvre du développeur de l'ontologie. Voir l'ontologie site web pour les formules 8 antimodèle AntiModèle plus d'huile. Pour ces formules, on imagine qu'une définition de classe peut être composée de deux parties conjonctions. Nous avons défini 84 requêtes SPARQL. Les résultats présentés dans le tableau 1 pour la détection de l'anti-modèle sont méthode nb de résultats nb de TI nb d'UI nb de nb de FI de l'onto manuel 8 3 SP 2 2 0 0 2 SP + R 2 2 1 0 2 SP_Trans 2 2 0 0 2 + R SP_Trans 72 6 66 0 2 TAB. 1 - HUILE de détection antimodèle. inattendu. On remarque que l'axiome atomique disjoints n'a pas été détecté car il est par 8. https://sites.google.com/site/ontologyantipattern/ inférer - 267 - Détection AntiModèle raisonneur. Et en utilisant un produit de raisonneur inattendus occurrences AntiModèle. Ainsi, à ce moment-là l'une de nos méthodes de détection est bonne enought pour détecter HUILE antimodèle. Nous devons limiter notre méthode de détection au début du motif d'huile sans l'axiome disjoints. 5 Conclusion et travaux futurs Dans cet article, nous avons montré comment antimodèle d'huile peut être détectée en utilisant différentes ods qui sont basés méth- sur l'utilisation de requêtes SPARQL, raisonneurs OWL et outils de transformation. Dans de nombreux cas, ces méthodes de détection AntiModèle sont très sensibles au style de mise en œuvre du développeur de l'ontologie. De plus raisonneurs ne peuvent pas être utilisés en raison du mauvais temps de réponse et des résultats inattendus. Notre travail futur se concentrera sur le perfectionnement des méthodes que nous avons proposées dans le présent document pour améliorer les résultats de détection. Nous allons également essayer de détecter de nouveaux anti-patterns. Références Corcho, O., C. Roussey, L. M. Vilches Blázquez, et I. Pérez (2009). OWL basée sur des modèles ontologie directives de débogage. Dans l'atelier sur les modèles Ontologie (WOP 2009), à la 8e colocalisé International Semantic Web Conference (ISWC-2009)., Les ceedings de CEUR atelier, pp. 68-82. Horridge, M., B. Parsia et U. Sattler (2008). justifications laconiques et précises dans OWL. Dans Actes de ISWC, pp. 323-338. Iannone, L., A. L. recteur et R. Stevens (2009). L'intégration des modèles de connaissances en OWL. Dans Actes de l'ESWC, pp. 218-232. Kalyanpur, A., B. Parsia, E. Sirin, et J. Hendler (2005). Débogage cours insatisfiables dans ontologies OWL. Journal of Web Sémantique 3 (4), 268-293. Presutti, V., A. Gangemi, S. David, G. de Cea, M. Suárez-Figueroa, E. Montiel-Ponsoda, et M. Poveda (2008). NeOn livrable d2. 5.1. une bibliothèque de modèles de conception de l'ontologie: solutions réutilisables pour collaborativ e conception d'ontologies en réseau. Projet NeOn. http: // www. projet neon-. org. Sirin, E. et B. Parsia (2007). SPARQL-DL: SPARQL pour OWL-DL. En 3ème OWL: Expériences et directions Atelier (OWLED2007). Les CV antipatrons de conception d'ontologies des structures abstraites are Qui des reflètent Problèmes de modelisation. ILS à des évincés peuvent Incohérences Mener Logiques, de per- formances des Mauvaises d'inférences Moteurs, des formalisations OU des connaissances heuristiques du inadéquates domaine. Il Est important de Fait les antipatrons Détecter, versez les ontologies Corriger. Dans l'article de nous this methods de proposons quatre saisons d'antipatrons à détection partir de requests SPARQL. Pour EVALUER nos methods, nos tested NOUS Avons requests SPARQL sur un ensemble d'ontologies REELLES. - 268 -"
440,Revue des Nouvelles Technologies de l'Information,EGC,2012,Biological event extraction using SVM and composite kernel function,"With an overwhelming of experimental and computational results inmolecular biology, there is an increasing interest to provide tools that will automaticallyextract structured biological information recorded in freely availabletext. Extraction of named entities such as protein, gene or disease names andof simple relations of these entities, such as statements of protein-protein interactionshas gained certain success, and now the new focus research has beenmoving to higher level of information extraction such as co-reference resolutionand event extraction. It is precisely the last of these tasks which will be focusedin this paper. The biological event template allows detailed representations ofcomplex natural language statements, which is specified by a trigger and argumentslabeled by semantic roles.In this paper, we have developed a biological event extraction approach whichuses Support Vector Machines (SVM) and a suitable composite kernel functionto identify triggers and to assign the corresponding arguments. Also, we makeuse of a number of features based on both syntactic and contextual informationwhich where automatically learned from the training data.We implemented our event extraction system using the state-of-the-art of NLPtools. We achieved competitive results compared to the BioNLP'09 Shared taskbenchmark.","Maha Amami, Aymen Elkhlifi, Rim Faiz",http://editions-rnti.fr/render_pdf.php?p1&p=1001150,http://editions-rnti.fr/render_pdf.php?p=1001150,en,"extraction d'événement biologique en utilisant la fonction SVM et du noyau composite Maha Amami *, ** Elkhlifi Aymen, Rim Faiz *** * LARODEC, ISG de Tunis, 2000, Le Bardo, Tunisie MahaAmami@isg.rnu.tn, ** Lalic, Université Paris -Sorbonne, 28, rue Serpente, 75006 Paris, France Aymen.Elkhlifi@paris4.sorbonne.fr *** LARODEC, IHEC de Carthage, 2016, Carthage Présidence, Tunisie Rim.Faiz@ihec.rnu.tn Résumé. Avec une écrasante majorité des résultats expérimentaux et de calcul en biologie moléculaire, il y a un intérêt croissant pour fournir des outils qui l'information biologique automatique extrait matiquement structuré enregistré dans le texte librement disponible. Extraction d'entités nommées telles que les noms protéines, gènes ou la maladie et des relations simples de ces entités, telles que les déclarations des actions inter-protéine-protéine a gagné un certain succès, et maintenant la nouvelle recherche de mise au point a été en mouvement à un niveau supérieur d'extraction d'information telles que la résolution co-référence et d'extraction d'événement. C'est précisément la dernière de ces tâches qui seront axées dans le présent document. Le modèle d'événement biologique permet des représentations détaillées des instructions en langage naturel complexe, qui est spécifié par un déclencheur et des arguments marqués par des rôles sémantiques. Dans cet article, nous avons développé une approche d'extraction d'événement biologique qui utilise des machines à vecteurs de support (SVM de) et une fonction de noyau composite approprié pour identifier les déclencheurs et d'affecter les arguments correspondants. Nous utilisons également un certain nombre de fonctionnalités basées sur des informations à la fois syntaxique et contextuelle qui a appris où automatiquement à partir des données de formation. Nous avons mis notre système d'extraction d'événements en utilisant l'état de l'art des outils de la PNL. Nous avons obtenu des résultats compétitifs par rapport à l'indice de référence de la tâche partagée BioNLP'09. 1 Introduction La dernière décennie a connu une croissance explosive dans la quantité de données biologiques TIONNELLES expérimentales et computa-. Cette croissance est accompagnée d'une augmentation du nombre de textes biologiques discutant des résultats. La base de données MEDLINE 1 contient en 2010 plus de 20 millions cules arti-, et la base de données est actuellement en croissance à un taux de plus de 10% chaque année (Ananiadou et al., 2006). La disponibilité des énormes ressources textuelles offre aux scientifiques la possibilité 1. http://www.ncbi.nlm.nih.gov - 101 - extraction d'événement biologique en utilisant la fonction SVM et du noyau composite pour rechercher des corrélations ou des associations telles que les interactions protéine-protéine , et le gène-maladie AS- ciations. Cependant, la récupération et le traitement de cette information est difficile en raison de l'absence de structure formelle dans la langue naturelle dans ces documents (Mooney et Bunescu 2005, Elkhlifi et Faiz, 2009). Utilisation de requêtes de mots clés qui extraient un grand nombre de documents pertinents, les scientifiques peuvent naviguer à travers des liens entre la base de données du génome et des documents de référence. Pour extraire les connaissances requises des documents récupérés, ils doivent identifier les informations pertinentes. Un tel traitement manuel prend du temps et répétitif, en raison de la taille de bibli- phie et la base de données mise à jour continue. A partir de la base de données Medline, la requête ciblée « Bacillus subtilis et la transcription », qui est revenu 2209 résumés en 2002, 3727 d'entre eux récupère aujourd'hui. En conséquence, il y a eu un intérêt accru pour l'application des techniques d'extraction d'informa- tions à la construction de la base de données de soutien et de trouver intelligemment des connaissances dans les documents. Des recherches antérieures dans l'extraction de l'information biologique ont notamment porté sur la tâche de reconnaître les entités nommées dans les textes, tels que les protéines, gènes, médicaments ou noms maladie (Rindflesch et al., 2000) et sur l'extraction des relations simples de ces entités, comme des déclarations d'interactions protéine-protéine (Blaschke et al., 1999). Récemment, le centre de recherche a été en mouvement au niveau de l'extraction d'informations plus telles que la résolution co-référence (McCarthy et al., 1996) et l'extraction des événements (Kim et al., 2009). C'est précisément la dernière de ces tâches w UEL se concentrera dans cet article. En fait, la tâche d'extraction d'événement biologique a une large gamme d'applications allant de l'aide et l'annotation des voies à la population automatique et l'enrichissement des bases de données et ontologies. En ce qui concerne l'extraction des événements de tâche articles de presse, Elkhlifi et Faiz (2007, 2009, 2010b) propose une approche d'apprentissage de la machine à des événements d'extrait basé sur une ontologie. En outre, ils proposent un algorithme efficace pour annoter ces événements cinque ordre de complexité temporelle N5 (Elkhlifi et Faiz, 2010a). L'extraction de l'événement à partir de textes biologiques est une tâche non négligeable, bien que la maturation des technologies de base. Il doit reconnaître les diverses formes de surface dans le texte qui décrivent le même processus biologique (événement de classe de déclenchement) et d'identifier les entités biologiques sont impliqués (arguments d'événement). Le reste du papier est organisé comme suit: Dans la section 2, nous techniques actuelles adoptées pour l'extraction de l'événement à partir de textes biologiques. Dans la section 3, nous vous proposons notre événement extraction AP- proach avec Support Vector Machines (SVM de) et la fonction du noyau composite. Dans la section 4, nous décrivons la mise en œuvre de notre système d'extraction d'événements et les résultats expérimentaux évaluant notre approche. 2 travaux connexes sur les méthodes d'extraction d'événements biologiques L'extraction d'événement biologique fait référence à la tâche de détection des tapées, des événements liés de texte et l'attribution des protéines comme arguments, à l'aide des outils de base pour l'analyse de texte biologique et des ressources annotées manuellement tels que et BioInfer et corpora GENIA ( Kim et al., 2008). Un gabarit d'extraction d'événement biologique est définie par un déclencheur et arguments (Kim et al., 2009). Les rôles sémantiques sont assignés à ces arguments. Par exemple, dans la phrase « Monocyte attache par P-sélectine régule monocyte chimio- tactique protéine-1 et de la sécrétion du facteur alpha de nécrose tumorale. », L'événement est caractérisé par un déclencheur d'événement verbe « régule » qui est une classe d'événement de régulation, et les arguments d'événement sont « P- - 102 - M. Amami et al. sélectine », « protéine chimiotactique monocytaire-1 » et « facteur de nécrose tumorale-alpha ». Le premier argu- ment « P-sélectine » est marqué par le rôle sémantique « agent » ou « cause ». Les arguments « monocyte protéine-1 chimiotactique » et « facteur de nécrose tumorale-alpha » agit en tant que thème. Par conséquent, la plupart des approches d'extraction d'événements sont pipelines de trois sous-tâches principales (Björne et al, 2008;.. Vlachos et al, 2009;. Cohen et al, 2009). 1. Pré-traitement: il fournit des jetons, des balises de point de vente et des analyseurs de dépendance comme une entrée au détecteur d'événement. 2. Détection de déclenchement de l'événement: il faut l'affectation de chaque jeton à une classe d'événements. 3. Détection de l'argument de l'événement: il consiste à trouver tous les participants à un événement et l'attribution du rôle fonctionnel à chacun des participants déterminés à un événement. Plusieurs approches sont développées pour les sous-tâches d'extraction d'événements biologiques en utilisant des techniques telles que complète l'analyse syntaxique (Vlachos et al., 2009), basée sur des modèles (Buyko et al., 2009), l'apprentissage de ma- chine (Björne et al., 2009) et l'ontologie des techniques menées (Cohen et al., 2009). Les techniques les plus couramment utilisées dans les approches d'extraction d'événements sont les modèles match- ing, qui met en œuvre un ensemble de règles définies manuellement mis au point par des experts ou appris automatiquement à partir des données de formation; et l'apprentissage de la machine, qui exploite diverses caractéristiques d'ex- événements des voies. Les approches à base de patterns tentent d'utiliser des informations contextuelles pour trouver des événements biologiques. Ils recherchent habituellement certains mots qui se produisent près des entités nommées ou utilisation du discours (POS), la syntaxe et de l'information sémantique. Par conséquent, les modèles peuvent être écrits en utilisant des dictionnaires, l'analyse syntaxique à base de préposition et ainsi de suite. Un exemple de méthode basée sur des modèles est le travail par Buyko et al. (2009) qui em- ploy un certain nombre de dictionnaires aux déclencheurs d'événements d'extrait contenant, déclencheur discriminante pour une classe d'événements (par exemple, déclencheur « phosphorylent » pour la classe d'événements « Phosphorylation ») et d'événements pas complètement discriminante déclencheur avec des chaînes communes (in- suite ext disambiguators) pour aider à l'identification de la classe d'événements. Une autre technique de correspondance de motif est l'analyse syntaxique préposition, les plaques d'extraction d'événements sont remplis avec du matériel analysable entourant prépositions telles que « par » et « de » qui sont souvent des chaînes cue des rôles à thème ou de cause. La phrase « l'apoptose induite par le suppresseur de tumeur p53 » contient préposition « par » lesquels on en utilisant des arbres parse et modèles codés à la main, « suppresseur de tumeur p53 » comme argument de la cause, « l'apoptose » comme argument de thème et « induit » comme verbe événement déclencheur. Les approches basée sur des modèles présentent une grande précision mais leur rappel est faible parce que beaucoup des relations dans le texte sont non découvertes par des motifs gauche codés. méthodes d'apprentissage de la machine à l'extraction d'événements biologiques ont utilisé des techniques diverses telles que Support Vector Machines (SVM), modèles de Markov cachés (HMM), et K- les plus proches voisins (k-NN). Les travaux de Björne et al. (2009) applique SVM pour détecter des événements biologiques en utilisant un large éventail de caractéristiques et de réseaux sémantiques dérivées de l'analyse complète de la dépendance. Ainsi, ils représentent chaque phrase en terme de graphique où les noeuds correspondent à des protéines et déclencheurs d'événements et les arêtes correspondent aux arguments de l'événement. nœuds d'événements sont formés sur la base de jetons tion individuels prédictions, et les bords d'événements sont identifiés par la prévision pour chaque déclenchement déclencheur ou une paire d'entité déclencheur nommée si elle correspond à un argument d'événement approprié. Les caractéristiques utilisées dans le SVM comprennent les propriétés morphologiques du jeton à classer, tels que le caractère bigrammes et trigrammes, et les jetons qui en dépendent, le nombre d'entités nommées et le sac de mot de décomptes de jetons dans la phrase. Pour une classe donnée, - 103 - extraction d'événement biologique utilisant SVM et classificateur composite fonction noyau calcule le score de confiance d'un jeton appartenant à la classe. Après la détection de déclenchement d'événements, tous les bords potentiels, qui relient un noeud d'événement à un autre ou à un nœud d'entité nommée, sont classés en fonction du SVM comme thème, la cause ou la classe négative. L'ensemble des éléments de bord sont construits en combinant les attributs des jetons, des n-grammes, qui définissent l'ation Vari des indications de dépendances de 2 à 4 jetons consécutifs, les caractéristiques de noeud sémantique qui combinent les caractéristiques de jetons des deux événement terminal ou d'une entité le noeud du bord potentiel, les caractéristiques des composants indivi- indi- qui combinent un jeton ou d'un attribut de bord à la position de jeton ou de bord soit à l'intérieur ou à la fin de la trajectoire. Björne et al. (2009) traitent des textes comme des arbres de dépendance qui est la source la plus importante de fonctionnalités. Une profonde analyse sémantique et contextuelle pour connaître les fonctions sémantiques peut aborder le problème de la homonymie. De plus, ils utilisent le SVM linéaire (noyau linéaire) qui ne soit pas en mesure de saisir la tâche d'extraction d'événement spécifique similitude entre les caractéristiques de chaîne. Dans cet article, nous utilisons la technique d'apprentissage automatique pour la détection de déclenchement et des arguments sous-tâches. Nous utiliserons une méthode basée sur le noyau pour prédire des événements, à savoir la machine à vecteurs de support (de SVM). Cependant, l'un des principaux défis dans la méthode basé sur le noyau est le choix d'une fonction de noyau approprié pour un problème de classification donnée. Ainsi, nous essayons de concevoir une fonction composite du noyau approprié pour l'extraction d'événements. De plus, nous vous proposons un ensemble riche de fonctionnalités dérivées de la dépendance et l'analyse sémantique. 3 approche d'extraction des événements proposée à partir de textes biologiques Nous visons à développer une méthode nouvelle et efficace pour extraire les événements biologiques de la littérature. Notre proposition est de générer automatiquement un grand nombre de fonctionnalités, et d'utiliser un SVM avec une fonction de noyau approprié abled pour capturer la tâche détection d'événements similitude spécifique entre ces caractéristiques. L'ensemble du processus est décrit dans les sections suivantes. 3.1 Pré-traitement pour extraire les événements du texte, nous employons de nombreuses techniques de traitement du langage naturel. Nous appliquons l'état de l'art sy les tiges formées sur des corpus biologique pour le fractionnement, et tokenization Partie du discours (POS) marquage. Ensuite, nous utilisons parseurs pour analyser les relations syntaxiques entre les entités dans la phrase. Enfin, l'analyse syntaxique est complétée par un traitement sémantique; une étape qui attribue des classes sémantiques (par exemple, un gène, une protéine, un type de cellule, etc.) en utilisant des ressources sémantiques. 3.2 Détection de déclenchement La détection de déclenchement d'événements est la tâche d'identifier des mots individuels dans la phrase qui agissent comme un des mots déclencheurs d'événements et attribuer la classe d'événement correct à chacun des déclencheurs déterminés. Tout d'abord, nous filtrons jetons, qui sont une entité nommée et dont étiquette POS n'est pas un nom, un verbe ou un adjectif; et des phrases qui n'ont pas de protéines. Ensuite, on procède à l'extraction d'un ensemble de caractéristiques pour chaque déclenchement candidat en fonction à la fois du contexte dans la phrase et l'analyse syntaxique de dépendance. Nos attributs initiaux comprennent à la fois des caractéristiques similaires à celles utilisées dans (Björne et al, 2009;. Elkhlifi et Faiz, 2009) et nouveaux. Le jeu de fonctionnalités accordé est montré dans le tableau 1. - 104 - M. Amami et al. Catégorie d'entité jeton comporte Stem texte Token de l'égrappoir Porter (Porter, 1980) Lemme de la boîte à outils du langage naturel 2 Mot Token POS et POS de la protéine la plus proche présence de symbole / Lettre majuscule N-grammes (n = 2, 3) caractères Indicateur si le jeton est un mot d'arrêt Présence d'un verbe adjacent ou nom Présence dans un déclencheur gazetteer type sémantique Fréquence caractéristiques Nombre d'entités nommées dans le numéro de phrase de mots d'arrêt dans la phrase sac de nombre de mots de mots symboliques dans la phrase TF-IDF le score de mot de jeton dans l'ensemble d'apprentissage de dépendance comporte ensemble de chaînes de dépendance comporte jusqu'à une profondeur de trois chemin d'étiquette dépendance de la protéine la plus proche chemin le plus court comporte N-grammes de dépendances (n = 2, 3, 4) des N-grammes de mots ( n = 2, 3, 4) Durée du trajet le plus court présence de certains jeton le long du chemin le plus court dans un répertoire géographique de déclenchement TAB. 1 - Fonctions pour la détection de déclenchement. Après cela, les déclencheurs candidats sont classés en classes d'événements (par exemple, l'expression génique, transcription, phosphorylation) et une classe d'événement négatif à l'aide d'un classificateur d'apprentissage machine. Cependant, les techniques de classification d'apprentissage de la machine traditionnelle de mauvais résultats lorsque l'on travaille directement en raison de la haute dimensionnalité des données. Ainsi, nous utilisons la méthode basée sur le noyau SVM qui a été échelles relativement bien aux données de grande dimension. L'un des principaux défis dans la méthode basé sur le noyau est le choix d'une fonction de noyau approprié pour le problème de la classification donnée (Burges, 1998). En fait, il y a des choix standard comme une des fonctions du noyau gaussiennes ou polynomiale qui sont les options par défaut. Cependant, ils se révèlent inefficaces pour former le classificateur avec les grands ensembles de données (Hsu et al., 2010). , Ils ne sont pas appliqués aux fonctions de chaîne. Dans le travail présenté par (Björne et al., 2009), la fonction du noyau linéaire est utilisée dans la détection de déclenchement avec de grands ensembles de formation. Il calcule le produit scalaire entre des instances comme, K (X, Y) = XT · Y (1) où, <xi, yi> = 1 si xi et yi sont les mêmes et 0 sinon. 2. http://www.nltk.org/Home/ - 105 - extraction d'événement biologique en utilisant la fonction SVM et du noyau composite Cependant, lorsque nous traitons avec des fonctionnalités de chaîne, une telle similitude basée produit scalaire putation ciales est pas en mesure de saisir la tâche de détection spécifique de déclenchement similitude entre fea- tures de chaîne. Dans ce qui suit, nous élaborons une fonction composite du noyau basé sur la représentation vectorielle des caractéristiques. Nous définissons la fonction de similarité comme noyau dans SVM pour chaque type de fonction à savoir, le texte de mot, n-gramme et le chemin de dépendance. Tout d'abord, nous donnons la définition d'une matrice de similarité dans l'entrée X. La matrice de similarité S est une matrice n × n avec deux entrées pour chaque paire de vecteurs de X, S (ik, jk) = sij pour i, j ∈ {1, 2. . . , N} les indices des cas dans X. S =   s11 s12. . . . . . S1N ... .... . . sij ... SN1 SN2. . . . . . SNN   Ensuite, on définit la matrice de similarité globale basée sur le produit intérieur | MAT <..>: X l x X l 7 → R tel que, <xi | xj> MAT = lΣ k = 1 S (ik, jk) (2) où k = {texte de mot, n-gramme, chemin de dépendance}. WordNet similitude On utilise la mesure de similarité WordNet HSO (Hirst et St-Onge, 1998) qui mesure le degré de parenté sémantique entre deux lemmes définie comme suit, relHS (w1, w2) = C - PathLength- k * d (3) où C et k sont des constantes, PathLength est la longueur du chemin le plus court et d est le nombre de changements de direction dans le chemin. N-grammes fonction noyau Pour calculer la similarité entre les bigrammes et trigrammes de deux chaînes, nous utilisons le k spectre fonction du noyau (n-grammes) (Leslie et al., 2002). D'une chaîne x, un alphabet A (| A | = l), nous définissons une carte de fonction de X à Rkl par, gk (x) = (φa (x)) a∈Ak (4) où φa (x) = nombre d'occurrences d'un de x. Ainsi, la fonction du noyau k-spectre est défini comme, Kk (x, y) = <gk (x), gk (y)> (5) Dépendance noyau Notre noyau de dépendance est une modification de Kim et al. (2008) noyau à pied d'une structure de dépendance, qui est testé avec un SVM sur la tâche Lenge chal- LLL 05 pour extraire les interactions et géniques a réalisé un résultat prometteur. Nous définissons notre noyau graphique de- litispendance pour capturer l'isomorphisme entre les deux structures de graphes. Pour cela, nous résumons le nombre de marches communs caractéristiques entre deux graphes G dépendance (V, E) - 106 - M. Amami et al. et G '(V', E '). On notera que le graphique désigne les chemins de la chaîne de dépendance dirigés à la profondeur de n (n = 1, 2, 3). Dans notre travail, nous considérons la marche de longueur 1 appelé v-promenade. De plus, nous présentons un e-promenade qui commence et se termine avec un bord e. Nous générons des caractéristiques de marche lexicales, qui consistent en des mots lexicaux Lw; et marcher syntaxique dispose Sw, qui se composent de POS et les relations de dépendance. L'ensemble des caractéristiques de marche lexicale et syntaxique est noté par Fw d'un bord e. Par conséquent, notre noyau de graphe de dépendance est exprimée dans l'équation 6. K (G, G ') = Σ Σ e∈E e'∈E' Kwalk (e, e ') (6) où, Kwalk (e, e') = {1 si fw = f 'w 0 sinon. noyau linéaire caractéristiques binaires sont utilisés au sein d'un noyau linéaire (c.-à-dot produit) tel que défini dans l'équation 1. On notera que, on normalise les grains calculée en utilisant la donnée par, un modificateur de similarité de cosinus K (x, y) '= K (x, y) √ K (x, x) √ K (y, y) (7) 3,3 Argument détection d'abord, on génère des caractéristiques pour tous les chemins les plus courts de dépendance entre la gâchette prédite et l'entité appelée. Ensuite, nous définissons un calcul de similarité à base de noyau qui est capable de capturer la tâche de détection d'argument similitude spécifique entre les plus courtes caractéristiques de chemin. Chaque exemple de chemin le plus court est classé comme appartenant à l'une des classes de type d'argument (thème ou cause) ou négatif. Comme la détection de déclenchement, on définit une fonction du noyau qui dépend des valeurs de fonction de chaîne présentées dans les plus brefs vecteurs de chemin. La matrice de similarité est définie comme suit, <xi | xj> MAT = lΣ k = 1 S (ik, jk) (8) où k = {dépendant de n-gramme, chemin de dépendance, numérique, sémantique}. Nous sélectionnons pour chaque groupe de fonction d'un noyau approprié en additionnant les fonctions du noyau suivantes: - Dépendance n-gramme: nous utilisons le noyau k spectre entre les n-grammes des deux chemins les plus courts. - Chemin de dépendance: nous utilisons le noyau de dépendance décrit ci-dessus dans la détection de déclenchement pour calculer la similarité entre la dépendance des bords par rapport aux deux chemins les plus courts. - 107 - extraction d'événement biologique en utilisant SVM et fonction du noyau composite Type Fréquence Caractéristique Caractéristiques Durée du trajet le plus court entre deux entités Nombre d'entités nommées et déclencheurs d'événements par type dans la phrase N-grammes Caractéristiques N-grammes de dépendances (n = 2, 3, 4) des N-grammes de mots consécutifs Terminus jeton comporte trigger / Argument mot déclenchement / Argument Type de déclenchement argument / POS scores de confiance de jetons terminale obtenue par détection de déclenchement élément unique comporte un itinéraire de dépendance des bords par rapport au trajet le plus court Types de bords de dépendance par rapport au trajet le plus court Sem Caractéristiques antic étiquette d'annotation du plus court chemin combinaison du type spécifique du terminal jeton du chemin le plus court et leurs catégories TAB. 2 - Caractéristiques pour la détection des arguments. - numérique: la similitude cosinus est un simple noyau approprié pour calculer la similarité entre deux vecteurs numériques définis comme, K (A, B) = Σni = 1AI · Bi√Σni = 1 (Ai) 2 · √Σni = 1 (Bi) 2 (9) - sémantique: (. Tikk et al, 2010), nous utilisons le noyau Modifier la distance qui calcule la similitude entre les étiquettes d'annotation des chemins les plus courts. Notez que nos noyaux pour déclenchement et la détection des arguments sont construits à partir de ceux valides existants (par exemple, satisfaisant le théorème de Mercer) en utilisant cette règle k (x1, x2) = k1 (x1, x2) + K2 (x1, x2) ce qui signifie que la somme des deux noyaux valides résultats k1 et un k k2 noyau valide. Par conséquent, nos noyaux sont satisfaisants théorème de Mercer. 3.4 Argument groupement La sortie cible de la détection d'argument se présente sous la forme d'un châssis principal constitué d'une classe d'événements, le rôle sémantique et les participants (protéine ou de l'événement). Pour le regroupement d'arguments, nous devons trouver les meilleures combinaisons d'images d'événements qui sont détectés par le détecteur d'argument pour représenter des événements complexes (par exemple, la liaison et la réglementation). Nous construisons des modèles de classification pour la détection d'événements complexes. Tout d'abord, nous concevons les caractéristiques d'un candidat événement complexe pour la détection d'événements complexes qui limitent les types et combinaisons d'arguments d'événements définis dans l'ontologie de l'événement. Les caractéristiques contiennent trois relations (1), les relations entre les arguments, (2) les relations entre les déclencheurs et les protéines externes, (3) et les relations entre les arguments et les déclencheurs externes. Par conséquent, nous appliquons l'extracteur argument basé sur les caractéristiques comme indiqué dans le tableau 2 pour trois types de structures: chaque chemin le plus court en cas complexe, toutes les paires parmi les arguments, tous les chemins les plus courts, y compris en dehors de déclenchement d'événements d'événements, toutes les paires entre les protéines d'argument et leurs protéines les plus proches en cas de liaison. - 108 - M. Amami et al. Les premières relations sont utilisées pour éliminer les candidats qui contiennent des arguments non liés, et les deuxième et troisième rapports sont utilisés pour éliminer les candidats en trouvant les chemins les plus courts qui devraient être inclus dans les candidats et les combinaisons plus appropriés d'arguments d'événement. 4 Expérimentation Nous présentons la mise en œuvre de notre approche proposée pour résoudre la tâche d'extraction d'événements avec les tâches partagées BioNLP'09 3 ressources (Kim et al., 2009). L'ensemble des données expérimentales sont préparées sur la base du corpus de GENIA dans le contexte de la BioNLP Shared tâche. Ils se composent de documents PubMed (titre et résumé seulement) .Nous comptent plusieurs données présentées dans le tableau 3 (Kim et al., 2009). Résumé Test Training développement 800 150 260 Sentence 7499 1450 2447 Token 176146 33937 57367 événement 8597 1809 3182 TAB. 3 - (. Kim et al, 2009) Types et statistiques des ensembles de données expérimentales. La canalisation d'extraction d'événement est constitué de quatre parties principales, un pré-processeur, un détecteur de déclenchement, un détecteur d'argument et un détecteur d'événements complexes. Les documents de la formation, le développement et les données de test figurant dans le GENIA cor- pus sont segmentés et Tokenizé en utilisant le séparateur de phrase GENIA et Comparer U-tagger GENIA fourni par 4. Ensuite, nous courons l'analyseur de domaine adapté McClosky-Charniak ( Mc- Closky et Charniak, 2008). La sortie de l'analyseur est fourni dans le format bancaire standard Penn Tree- (TBP). Enfin, nous annoter la classe sémantique pour chaque terme en utilisant WordNet et le UMLS Metathesaurus. Gène, protéines, ARN, lignée cellulaire et les types cellulaires sont iden- tifiées par ABNER 5. Ensuite, le détecteur de déclenchement procéder à filtrer les éléments déclencheurs de candidats pour éliminer 70% des jetons, des fonctionnalités basées triggers méthodes d'extraction et enfin, la formation et le test - iNG déterminent la classification en utilisant le logiciel LIBSVM (Chang et Lin, 2010). Nous mettons en œuvre ces méthodes en Java utilisant Eclipse. En même, nous développons le détecteur d'arguments et des méthodes de détection des arguments com- plexes. Enfin, nos fonctions du noyau sont ajouté à la LIBSVM pour l'évaluation. Les résultats des expériences réalisées avec les données d'essai en termes de rappel, la précision et f-score (Kim et al., 2009) sont présentés dans le tableau 4. Ensuite, on obtient une valeur significative de f-score par rapport à la UTURKU système (Björne et al., 2009), le système JULIE (Buyko et al., 2009) et le système CCP-BtMG. 3. http://www-tsujii.is.su-tokyo.ac.jp/GENIA/SharedTask/ 4. http://u-compare.org/ 5. http://pages.cs.wisc.edu/ bsettles / abner / - 109 - extraction d'événement biologique par SVM et la fonction du noyau composite de rappel (%) précision (%) F-score (%) système UTURKU 46,73 58,48 51,95 système JulieLab 35,72 30,36 32,82 système CCP-BtMG 13,45 71,81 22,66 système de BioEv 50,57 64,88 56,83 TAB. 4 - Résultats expérimentaux pour l'extraction d'événement (précision / rappel / f-score). 5 Conclusion Dans la décennie précédente des travaux sur l'extraction automatique d'informations à partir de textes biologiques, les efforts ont porté notamment sur la tâche fondamentale de reconnaître les noms des entités dans le texte et sur l'extraction des relations de ces entités et, plus récemment, sur l'événement biologique extrac - tion. Dans notre travail, nous proposons une approche d'extraction d'événement à l'aide de machines support et vecteur fonction noyau composite. Nous commençons à traiter des textes en analysant les documents en langage naturel à l'aide de ressources lexicales pour obtenir des phrases, des jetons et des étiquettes de point de vente. Ensuite, les jetons sont organisés en groupes après une analyse syntaxique et sémantique a attribué un sens à ces jetons ou groupes de jetons. Dans la phase de détection de déclenchement et des arguments, on extrait des vecteurs de caractéristiques pour la formation et les tests en utilisant une modélisation SVM. Nous combinons plusieurs couches d'information syntaxique et sémantique en appliquant des noyaux distincts sur les caractéristiques. La combinaison de noyaux distincts est obtenue par l'addition des valeurs de chaque noyau pour chaque type de fonction. Afin d'évaluer notre approche, nous mettons en œuvre notre système d'extraction d'événements. On obtient un rappel autour de 50,57%, une précision d'environ 64,88% et un f-score d'environ 56,83%, pour un ensemble de résumés GENIA. Notre premier travail futur consiste à évaluer la performance de notre approche sur les articles en texte intégral GE- NIA, différents corpus tels que corpus BioInfer et en comparant nos noyaux composites aux points de référence de différents noyaux. Une autre ligne de recherche sera d'exploiter la sortie d'extraction d'événements dans les tâches d'exploration de texte telles que l'analyse de réseau d'événements, génération d'hypothèses, l'extraction de la voie et d'autres. Références Ananiadou, S., D. B. Kell, et J. ichi Tsujii (2006). Text mining et ses applications potentielles dans la biologie des systèmes. Trends in Biotechnology 14. Björne, J., F. Ginter, S. Pyysalo, J. Tsujii, et T. Salakoski (2008). extraction d'événements complexes à l'échelle PubMed. BMC Bioinformatics, 1-25. Björne, J., J. Heimonen, F. Ginter, A. Airola, T. Pahikkala, et T. Salakoski (2009). Événements biologiques y décompresser complexes avec de riches ensembles de fonctionnalités en fonction graphique. Dans Actes de l'atelier sur BioNLP: Tâche partagée, pp 10-18.. Blaschke, C., M. Andrade, C. Ouzounis et A. Valencia (1999). Extraction automatique des informations biologiques à partir du texte scientifique: interactions protéine-protéine. Dans Actes de - 110 - M. Amami et al. la septième Conférence internationale sur les systèmes intelligents pour la biologie moléculaire, pp. 60- 67. Burges, C. (1998). Un tutoriel sur Support Vector Machines pour la reconnaissance des formes. Data Mining et Knowledge Discovery. Buyko, E., E. Faessler, J. Wermter et U. Hahn (2009). extraction d'événements de graphes de dépendances rognées. Dans Actes de l'atelier sur BioNLP: Tâche partagée, pp 19-27.. Chang, C.-C. et C.-J. Lin (2010). LIBSVM: une bibliothèque pour les machines à vecteurs de support. ACM Transactions sur les systèmes intelligents et de la technologie. Cohen, K. B., K. Verspoor, H. L. Johnson, C. Roeder, P. V. Ogren, W. A. ​​B. Jr., E. White, H. Tipney, et L. Hunter (2009). extraction d'événement biologique de haute précision avec un concept de reconnaissance. Dans Actes de l'atelier sur BioNLP: Tâche partagée, pp 50-58.. Elkhlifi, A. et R. Faiz (2007). M approche d'apprentissage achine pour l'annotation automatique des événements. Dans D. Wilson et G. Sutcliffe (Eds.), FLAIRS Conférence, les Actes de la vingtième Conférence de la Société internationale d'intelligence artificielle de la recherche en Floride (AAAI Press ed.)., Pp. 362-367. Elkhlifi, A. et R. Faiz (2009). approche automatique d'annotation des événements dans des articles de presse. International Journal of Informatique et sciences de l'information (IJCIS) 7, 40-50. Elkhlifi, A. et R. Faiz (2010a). approche d'extraction de l'événement pour le Web 2.0. ACS / IEEE Conférence Inter- tional sur l'ordinateur et les applications qui - AICCSA 2010, 1-8. Elkhlifi, A. et R. Faiz (2010b). extraction d'événement écrit français basé sur ex ploration contextuelle. Dans H. W. Guesgen et R. C. Murray (Eds.), Actes du vingt et unième Conférence de la Société internationale d'intelligence artificielle de la recherche en Floride (AAAI ed.)., Pp. 180-185. Hirst, G. et D. St-Onge (1998). chaînes lexicales que la représentation de contexte pour les malapropisms de détection et de correction. WordNet: Une électronique Base de données lexicales, 305-332. Hsu, C. W., C. C. Chang et C. J. Lin (2010). Un guide pratique pour appuyer le classement vectoriel. Bioinformatics 1, 1-16. Kim, J. D., T. Ohta, S. Pyysalo, Y. Kano et J. Tsujii (2009). Vue d'ensemble des BioNLP'09 tâches partagées sur l'extraction des événements. Dans Actes de l'atelier sur BioNLP: Tâche partagée, pp 1-9.. Kim, J. D., T. Ohta, et J. Tsujii (2008). annotation corpus pour les événements de la littérature biomédicale minière. BMC Bioinformatics. Kim, S., J. Yoon, et J. Yang (2008). Kernel approche pour l'extraction de l'interaction genic. Bio-informatique 24, 118-126. Leslie, C., E. Eskin et W. S. Noble (2002). Le noyau de specrum: Un noyau de chaîne pour la classification des protéines SVM. Dans Symposium du Pacifique sur Bioinformatique 7, pp. 566-575. McCarthy, J. F., C. Brodley, J. Clouse, B. Crites, D. Mammen, E. Brown, J. Callan, A. Diwan et rian Pinnette (1996). Une approche à la résolution trainable coréférence pour l'extraction de l'information. McClosky, D. et E. Charniak (2008). Selftraining pour l'analyse biomédicale. Dans Actes de l'ACL-08: HLT, pp 101-104.. Mooney, J. R. et R. Bunescu (2005). connaissances minières du texte à l'aide des informations extrac- - 111 - extraction d'événement biologique par SVM et tion composite fonction du noyau. SIGKDD Explorations (numéro spécial sur Text Mining et langage naturel Process- ing), 3-10. Porter, M. F. (1980). Un algorithme de décapage suffixe. Programme, 130-137. Rindflesch, T., L. Tanabe, J. Weinstein, et L. Hunter (2000). Edgar: Extraction des médicaments, des gènes et des relations de la littérature biomédicale. Dans Actes du Symposium du Pacifique sur Bioinformatique, p. 517-528. Tikk, D., P. Thomas, P. Palaga, J. Hakenberg et U. Leser (2010). Une marque complète des méthodes de benchmarking noyau pour extraire les interactions proteinŰprotein de la littérature. Biologie de PLoS Com-, 1-19. Vlachos, A., P. Buttery, D. O. Seaghdha et T. Briscoe (2009). extraction d'événement biomédicale sans données de formation. Dans Actes de l'atelier sur BioNLP: Tâche partagée, pp 37-40.. Résumé ETANT l'importance des gave Résultats Scientists le domaine Dans publiés de la biologie moléculaire, il is Nécessaire de mettre en place des outils Aidant Scientists à des informations precises Extraire et structurées. L'extraction des Entités nommées les Qué Telles Protéines, les et les relations Gènes simples les interactions Que Telles Entre les Protéines, les associations Entre les et les maladies Gènes des taches Quasiment Sont résolues. Désormais, les Recherches en extraction d'information se biologique à la Sont orientées RE- solution des complexes ainsi que taches Que la Coréférence Telles et l'extraction des Evénements. Dans this article, nous nous intéressons au à l'extraction Particulier des représentations com- plex des Événements biologiques. Une représentation d'un Événement Dans un langage biologique naturel Comprend une several OU un Unités linguistiques désignant d'Événement et déclencheur des p Entités biologiques articipantes à CET Événement. Dans this article, nous presented notre approach Avons d'extraction des Événements Qui giques utiliser spécialiste en biologie les Machines à de support Vecteurs (SVM) et Une fonction site Compo noyau d'identifiant les AFIN des déclencheurs Évènements (événement déclencheur) les participants ET à CET EVENEMENT (argument event). Also, l'un des d'Avons NOUS Nombre attributes Basés sur les informations et syntaxiques contextuelles, à partir Generes des automatiquement les documents d'apprentissage. Nous Avons notre Système d'implémenté des extraction Événements en Utilisant des ou- TIL de treatment Automatique de langage naturel (TALN). Nous Avons des Obtenu par rapport Résultats au compétitifs de référence BioNLP'09 partagé la tâche. - 112 -"
452,Revue des Nouvelles Technologies de l'Information,EGC,2012,Community Detection in Social Networks with Attribute and Relationship Data,,"The Anh Dang, Emmanuel Viennet",http://editions-rnti.fr/render_pdf.php?p1&p=1001204,http://editions-rnti.fr/render_pdf.php?p=1001204,en,"Communauté de détection dans les réseaux sociaux avec attributs et les relations de données Le Anh Dang, Emmanuel Viennet * * L2TI - Institut Galilée - Université Paris-Nord 99, avenue Jean-Baptiste Clément - 93430 Villetaneuse - France {theanh.dang, emmanuel.viennet} @univ -paris13.fr 1 algorithmes de détection communautaire Nous présentons deux méthodes pour découvrir les communautés dans un graphe attribué, étant donné une mesure de similarité. Un graphique attribué est désigné par G = (V, E, X), où V est l'ensemble des noeuds, E est un ensemble de bords, X = X1, ..., Xd est l'ensemble des d attributs associés aux noeuds de V. Chaque noeud vi est associée à un vecteur d'attribut (X1i, ..., x P i). a) l'algorithme SAC1 Dans cet algorithme, nous définissons d'abord une modularité composite Q comme une extension de la fonction bien connue de la modularité de Newman (Clauset et al. (2004)) Q = Σ Σ C i, j∈C (α · 1 2m · (Gi, j - di · dj 2m) + (1- α) · SIMA (i, j)) α est le facteur de pondération, 0 ≤ α ≤ 1, SIMA (i, j) est la similitude des attributs des noeuds ( i, j). Pour trouver une optimisation approximative de Q, nous suivons une approche directement inspirée par l'algorithme Louvain (Blondel et al. (2008)). L'algorithme commence avec chaque noeud appartenant à une communauté séparée. Un nœud est alors choisi de façon aléatoire. L'algorithme essaie de déplacer ce nœud de sa communauté actuelle. Si un gain positif est trouvé, le noeud est ensuite placé à la communauté avec le gain maximum. Dans le cas contraire, il reste dans sa communauté d'origine. Cette étape est appliquée de façon répétée jusqu'à ce qu'aucune amélioration de plus est atteint. La première phase est terminée quand il n'y a pas de gain plus positif en déplaçant des noeuds. Ensuite, nous pouvons présenter une nouvelle demande de cette phase en regroupant les noeuds dans les mêmes communautés à un nouveau nœud communautaire. Pour déterminer la similitude d'attributs entre deux communautés, nous proposons deux approches. La première consiste à résumer la similitude de leurs membres, la deuxième façon consiste à fixer à la similitude de leurs centres de gravité. b) l'algorithme SAC2 Notre premier algorithme SAC1 vérifie répétitivement tous les nœuds, ce qui ToO (n2) complexité. Pour réduire le coût de calcul, nous vous proposons une autre approche qui ne fait que l'utilisation des plus proches voisins d'un nœud. Étant donné un graphe attribué: G = (V, E, X), on définit un k-plus proche graphe de voisinage (k-NN) Gk = (V, Ek) sous la forme d'un graphe dans lequel chaque noeud a exactement k bords, la connexion à son k plupart des voisins semblables à G. La mesure de similarité entre 2 noeuds i et j est défini comme S (i, j) = α · Gi, j + (1- α) · SIMA (i, j) - 515 - Détection communautaire Réseaux sociaux avec attributs et données relation où SIMA (i, j) est la fonction de similarité d'attributs, Gi, j représente le lien (i, j). Nous AP- ply la mesure S en premier lieu pour construire le graphe du plus proche voisin. L'approche naïve de la construction graphique k-NN utilise O (n2) et O (nk) l'espace. Cependant, dans la littérature, l'effort a été substantielle consacrée à accélérer le processus. Nous vous proposons un algorithme simple à deux phases: la construction d'un graphe k-NN Gk (phase 1) et de trouver des communautés structurelles Gk pour obtenir le regroupement final (phase 2). Dans la phase 2, différentes méthodes peuvent être employées pour trouver des communautés. Dans nos expériences, nous choisissons Louvain comme la méthode de détection en raison de son évolutivité. Nous avons mis k égal au degré moyen des noeuds dans le graphe G. 2 Résultats expérimentaux Nous effectuons des expériences sur plusieurs réseaux sociaux réels. Nous extrayons les communautés en utilisant différentes méthodes: regroupement basée sur les attributs, les marches aléatoires, Louvain sur le graphique non pondéré, Fast Greedy, SAC1, SAC2. Nous comparons le nombre de communautés, la taille des communautés, la structure de modularité, attributs de modularité, la densité et l'entropie du regroupement. Les résultats expérimentaux montrent que nos méthodes fournissent des communautés plus significatives que les méthodes classiques qui ne tiennent compte que des informations de relation. Références Blondel, V. D., J.-L. Guillaume, R. Lambiotte, et E. Lefebvre (2008). déploiement rapide des communautés dans les grands réseaux. Journal de la mécanique statistique: théorie et Expé- 2008 riment (10), P10008 (12pp). Clauset, A., M. E. J. Newman, et C. Moore (2004). Trouver la structure des communautés dans les très grands réseaux. Physical Review E 70, 066111. Résumé Dans cet article, nous proposons deux méthodes qui couplent la structure topologique ainsi que des informations d'attributs dans la détection des communautés. Comme les travaux futurs, nous essayons de comprendre le rôle des liens et informations sur le contenu de la formation des communautés en ligne afin d'élaborer des stratégies de découverte adaptées et pour modéliser la dynamique des réseaux. - 516 -"
456,Revue des Nouvelles Technologies de l'Information,EGC,2012,Development of a distributed recommender system using the Hadoop Framework,"Producing high quality recommendations has become a challenge inthe recent years. Indeed, the growth in the quantity of data involved in the recommendationprocess pose some scalability and effectiveness problems. Theseissues have encouraged the research of new technologies. Instead of developinga new recommender system we improve an already existing method. A distributedframework was considered based on the known quality and simplicity ofthe MapReduce project. The Hadoop Open Source project played a fundamentalrole in this research. It undoubtedly encouraged and facilitated the constructionof our application, supplying all tools needed. Our main goal in this research wasto prove that building a distributed recommender system was not only possible,but simple and productive.","Raja Chiky, Renata Ghisloti, Zakia Kazi Aoul",http://editions-rnti.fr/render_pdf.php?p1&p=1001179,http://editions-rnti.fr/render_pdf.php?p=1001179,en,"Développement d'un système recommender distribué en utilisant le framework Hadoop Raja Chiky, Renata Ghisloti, Zakia Kazi Aoul LISITE-ISEP 28 rue Notre Dame des Champs 75006 Paris firstname.lastname@isep.fr Résumé. La production de recommandations de haute qualité est devenue un défi ces dernières années. En effet, la croissance de la quantité de données impliquées dans le processus de recomman- dation pose des problèmes d'évolutivité et d'efficacité. Ces questions ont encouragé la recherche de nouvelles technologies. Au lieu de dévelop- ing un nouveau système de recommender nous améliorons une méthode déjà existante. Un cadre bué dis- était considéré en fonction de la qualité connue et la simplicité du projet MapReduce. Le projet Open Source Hadoop a joué un rôle fondamental dans cette recherche. Il sans aucun doute encouragé et facilité la construction de notre application, fournissant tous les outils nécessaires. Notre principal objectif dans cette recherche était de prouver que la construction d'un système de recommender distribué était non seulement possible, mais simple et productive. 1 Introduction La quantité d'informations dans le Web a considérablement augmenté au cours de la dernière décennie. Ce phé- nomène a favorisé l'avancée des systèmes de recommender domaine de recherche. Le but de systèmes de recommandations fournit des recommandations personnalisées. Ils aident les utilisateurs par sug- gesting objets utiles à eux, traitant généralement avec des quantités énormes de données. Amazon, par exemple, qui a intégré les systèmes recommender de personnaliser la boutique en ligne pour chaque utilisateur, a enregistré en 2003 plus de 29 millions d'utilisateurs et plusieurs millions d'articles de catalogue Linden et al. (2003). De nombreux systèmes de recommender approches ont été développées au cours des dix dernières années, mais une quantité considérable d'entre eux ont été construits et évalués avec de petits ensembles de données. En outre, le volume d'informations Web a considérablement augmenté au cours des dernières années, et pour cela, plusieurs systèmes de recommender souffrent de problèmes de performance et d'évolutivité lorsqu'ils traitent avec des ensembles de données plus importants. Notre principal objectif dans cet article est décrit une méthode pour surmonter éventuellement ces problèmes. Nous vous proposons un système recommender distribué, et nous avons l'intention de démontrer qu'il pourrait être facilement développé et présenter de bons résultats. Nous avons choisi la pente Un Lemire et Maclachlan (2005) comme algorithme recommender et nous étudions le projet Dean MapReduce et Ghemawat pour construire ce système distribué. - 275 - Développement d'un système recommender distribué en utilisant le framework Hadoop MapReduce est un cadre mis en place par Google qui supports de calcul distribué avec une grande quantité de données sur un cluster d'ordinateurs. Une implémentation open source de ce travail est disponible dans DE CADRE Apache Hadoop avait projet. Dans cet article, nous décrivons le processus d'adaptation de l'algorithme de recommender choisi de plate-forme Hadoop, puis, nous vérifions ses performances en comparant la version distribuée avec la méthode autonome. Le reste de cet article est organisé comme suit. La section 2 décrit l'état de l'art. Dans la section 3, nous présentons l'approche globale de la construction d'un système recommender distribué. La section 4 présente l'étude expérimentale et les résultats. Enfin, la section 5 conclut cet article et donne une perspective sur nos recherches actuelles et futures dans ce domaine. 2 Contexte: la pente Une pente OneLemire et Maclachlan (2005) est un moyen simple et efficace de l'algorithme de type recommender. Introduit par Daniel Lemire et Anna Maclachlan en 2005, elle implique une idée plus simple que la plupart des autres implémentations de filtrage collaboratif. Bien que ces calculent généralement la similitude entre les vecteurs des éléments en utilisant le cosinus ou les méthodes Pearson pois (1994), l'approche Slope One recommande articles aux utilisateurs en fonction de la différence moyenne dans les préférences des articles. L'idée principale de l'algorithme est de créer une relation linéaire entre les préférences des éléments tels que la relation F (x) = x + b Le nom « Slope One » vient du fait qu'ici le « x » est multiplié par « 1 ». Il calcule essentiellement la différence entre les évaluations des éléments pour chaque utilisateur (pour chaque élément que l'utilisateur a évalué). , Il crée alors une différence moyenne (diff) pour chaque paire d'éléments. Pour faire une prédiction du k article pour un utilisateur par exemple, il obtiendrait les évaluations que l'utilisateur A a donné à d'autres éléments et ajouter la différence (diff) entre chaque élément. Avec cela, nous pourrions obtenir une moyenne. Être Raí la note que l'utilisateur A a donné au point i, diff (i, j) = r la différence entre les cotes du point i et le point j, et en supposant que nous avons n éléments: La prédiction de la note que l'utilisateur A pourrait donner à k article est donnée par (rA1 + diff (k, 1)) + (+ rA2 diff (k, 2) + ... + (+ R1N diff (k, n)) n Ci-dessous, nous présentons la version pseudo-code . de l'algorithme Il peut être divisé en deux parties:. le pré-traitement et la phase de prédiction dans la phase pré-traitement, on calcule la différence entre toutes les valeurs de préférence élément-élément 1. pour chaque élément i {2. pour tous les autres éléments j { 3. pour chaque utilisateur u notation i et j {4. ajouter la différence à une diff moyenne (i, j)}}} - 276 - R. Ghisloti et al la phase de prédiction: 1. pour chaque élément i non classé. par un utilisateur u {2. pour chaque élément j {u par note 3. trouver diff (i, j) 4. ajouter cette diff note de u pour j}} 6. retour des éléments de haut prédit en termes d'exécution, le plus la phase coûteuse est celle prétraiter, qui peut être précalculée. la algorithme est très intéressante, car sa partie en ligne, la phase de prédiction, est rapide. Son temps de fonctionnement ne dépend pas du nombre d'utilisateurs, cela dépend principalement de la différence de note moyenne entre chaque paire d'éléments. Supposons que nous ayons les utilisateurs de m et n éléments, le calcul des différences moyennes pourraient prendre jusqu'à pas de temps MN2. Le stockage de la matrice de diff peut aussi être coûteux. Il peut prendre jusqu'à n (n-1) / 2 unités de stockage. 3 Approche MapReduce est un cadre mis en place par Google pour traiter des quantités de données de larges. Le cadre utilise une fonction simple idée dérivée de la carte communément appelé () et réduire () utilisés dans la programmation fonctionnelle (par exemple LISP). Il divise le principal problème en petits sous-problèmes et distribuer ces à un groupe d'ordinateurs. Il combine ensuite les réponses à ces sous-problèmes pour obtenir une réponse définitive. En premier lieu, la carte () reçoit un ensemble de paires de clé / valeur et produit une paire de clés intermédiaire / valeur en sortie. Ensuite, ces valeurs sont triées par le cadre et regroupés de manière que toutes les valeurs appartenant à la même clé sont ensemble (fonction de combinaison ()). Ainsi, en entrée pour réduire (), nous avons la même clé fournie par la carte () avec une liste des valeurs correspondantes. Ces données sont ensuite traitées par réduction () et on produit un signal de sortie final. Les données d'entrée primaire est coupé par le cadre, qui est aussi responsable de Man- vieillissement de la transition de données intermédiaire. Pour l'utilisateur, il ne reste que la tâche de définir les interfaces d'entrée / de sortie et de fournir la carte et réduire les fonctions. Sur la base de cette idée, nous avons commencé par identifier un format d'entrée pour créer notre propre MapReduce la pente d'une méthode. En tant que format, nous décidons d'utiliser un type typique d'un format de texte, avec un code d'utilisateur (identification de l'utilisateur), un ItemID (identification de l'article) et une note de cet utilisateur à cet élément. La pente de phase One peut être pré-traitement divisé en deux parties. La première est lorsque sont calculées les articles diffs pour chaque utilisateur. Dans la deuxième partie, tous les diffs pour chaque paire d'éléments sont ajoutés ensemble pour créer un différentiel moyen entre deux éléments. Notre approche est la suivante: Tout d'abord, les données d'entrée est divisé de manière indépendante afin de regrouper les évaluations des éléments à l'aide des utilisateurs en tant que clés. Ensuite, les diffs pour un utilisateur est calculé. Après cela, tous deux diffs intermédiaires serait uni pour calculer la diff moyenne pour chaque paire d'éléments. En se fondant sur cette méthode deux processus séparés MapReduce sont créés: une qui divise l'entrée en blocs indépendants et calcule le point diffs pour un utilisateur; et une autre que d'avoir les listes d'éléments diff, et calcule la overa ll diff entre deux éléments. - 277 - Mise au point d'un système de recommender distribué en utilisant le framework Hadoop Le premier présente MapReduce une carte () qui reçoit l'entrée d'origine, produit un ensemble de valeurs intermédiaires contenant chaque ligne du fichier d'entrée - avec un utilisateur, un élément et une note relatives à ces deux. Ces valeurs sont triées par le cadre et réduire () reçoit un utilisateur comme clé et une liste avec tous ses éléments et notes. Réduire () calcule les diffs entre chaque paire d'éléments pour ce certains utilisateurs et les retourner en sortie. La deuxième carte () serait une fonction d'identité, ce qui signifie que la même valeur donnée en entrée sera donnée en sortie. D'autre part, la réduire secondaire () recevrait comme la clé de la paire de l'élément <itemi, itemj> et en tant que valeurs d'une liste de tous les diffs liés à cette paire d'éléments. Il serait donc une simple question de calcul de la diff moyenne à chaque paire d'éléments, et nous aurait comme résultat une liste finale pour chaque paire d'éléments, contenant les moyennes diffs. 4 L'évaluation expérimentale Pour nos essais expérimentaux l'ensemble de données utilisé est celui fourni par MovieLens mov (2003). MovieLens est un système de recommender Web où les films de taux d'utilisateurs et reçoivent des recommandations. , Il fournit actuellement trois paquets de jeux de données: Le poing avec 100.000 notes pour 1682 films par 943 utilisateurs, le second avec 1 million d'évaluations pour 3.900 films par 6040 utilisateurs et le troisième avec le paquet de 10 millions d'évaluations pour 10681 films par 71567 utilisateurs . Ces cotes représentent les notes donnant par les utilisateurs de MovieLens aux films, variant de 1 (ne pas aimer un film tout) à 5 (vraiment aimer le film). Les fichiers contiennent des informations dans le UserId, ItemId, format Note. Dans cet article, nous utilisons le 1 million et 10 millions de paquets de notation. Pour assurer l'exactitude des demandes a été utilisé des données d'essai forgé; un petit groupe d'utilisateurs et note pour permettre une comparaison claire et simple des résultats. L'ensemble de données contient 5 utilisateurs, 10 articles et 28 notes. Les deux machines maître et esclave où les expériences ont eu lieu eu les mêmes spécifications du matériel et ont les caractéristiques suivantes: processeur Intel 2,66 GHZ, de 1.7GB ORY mem- et 100 Go de disque. Il fonctionne sur Ubuntu Linux 5.0. Nous utilisons un maître et deux esclaves dans le cas d'expérimentation entièrement distribuée. La première étape de cette procédure expérimentale était l'exécution de chaque approche SlopeOne avec chaque ensemble de données. Ensuite, tous les résultats de sortie ont été comparés à vérifier dans les résultats correspondants. Enfin, le temps d'exécution des approches ont été comparées et analysées. Les tests ont été réalisés en deux phases, une pour chaque ensemble de données. Dans chaque phase, on compare le temps d'exécution en millisecondes entre chaque pente Une approche (autonome, pseudo- distribuée et entièrement distribué) et, bien sûr, l'uniformité du résultat final. La première expérience a été avec les petites données forgées. La sortie de ces trois approches ont été comparées à notre résultat à la main et correcte. Tous les trois ont présenté la sortie correcte, ce qui prouve l'exactitude des algorithmes. Pour générer un temps d'exécution finale, chaque approche a été exécutée trois fois de suite, et le résultat présenté sur la figure 1 (a) sont la moyenne de ces trois exécutions. La deuxième phase expérimentale a testé l'ensemble de données 1 million MovieLens pour encore une fois les trois approches. L'exécution, comme ci-dessus, est une moyenne de trois exécutions simples et les resuls est donnée dans la figure 1 (b). Enfin, la dernière phase a testé l'ensemble de données de 10 millions de MovieLens. L'opération autonome n'a pas été en mesure de traiter cette quantité de données, présentant une exception java qui est java.lang.OutOfMemoryError. Les deux autres approches ont réussi à terminer la tâche et - 278 - R. Ghisloti et al. ! Temps (ms)! Temps (ms)! Temps (ms) (a) (c) (b) FIG. 1 - temps de Exectution pour les trois ensembles de données présente le temps d'exécution moyen représenté sur la figure 1 (c). De l'analyse des résultats de la figure 1, il est possible à noter que seulement à partir d'un certain point, il devient raisonnable d'utiliser Hadoop. Comme chaque cadre qui fournit la couche d'abstraction, l'exécution du cadre a un coût initial. En testant le premier et le deuxième ensemble de données, il est visible que le « poids » de l'exécution Hadoop était plus grand que les avantages qu'elle pourrait apporter. Par conséquent, la durée de fonctionnement de la méthode autonome était plus petite que les deux autres approches même pensé ces avaient plus de threads Java ou nœuds informatiques de travail. Dans le troisième scénario, nous pouvons voir l'approche entièrement distribuée surmonte celui distribué pseudo-. Une raison possible pour le pseudo faire mieux dans les autres scénarios est le coût impliqué dans l'établissement d'un cluster distribué. Avec l'augmentation de la quantité de données, les avantages d'un cluster distribué fait la différence, et ont fourni une performance plus rapide. Notre principal objectif dans cette recherche était de vérifier si l'approche distribuée vraiment apporté des avan- tages au recommender. Vérification des résultats finaux, il est clair que pour un petit ensemble de données telles que égal ou inférieur à 1 million d'évaluations méthode autonome est une meilleure approche. Cependant, il a été prouvé que lorsqu'il s'agit de plus grandes quantités de données, le framework Hadoop peut être une solution réalisable. Lorsque le volume de données croît, entièrement distribué a une meilleure performances indiquées que les deux autres méthodes. La méthode autonome n'a pas été en mesure d'effectuer du tout dans le troisième scénario et ne pouvait donc pas donner un résultat. 5 Conclusion Les systèmes sont confrontés à un défi Recommender important lorsqu'ils traitent avec une énorme quantité de données. Dans cet article, notre objectif principal est l'aide pour résoudre ce problème en décrivant un développe- ment facile d'un système de recommender distribué. Cela a été possible en utilisant des outils puissants tels que l'open source Hadoop, MapReduce et la mise en œuvre slopeone qui est un type efficace de l'algorithme de collaboration recommender. La simplicité de la pente a été ajoutée à la structure forte offerte par Hadoop a permis de compléter notre tâche. Nos résultats ont montré qu'en effet, un cadre distribué peut donner de bons résultats, et, espérons-le, ont encouragé l'intérêt dans ce domaine de la recherche. Notre travail futur sera d'analyser d'autres types d'algorithmes de recommandation d'étudier la possibilité de les rendre efficaces sur de grands ensembles de données en utilisant Hadoop ou tout autre avalaibe DE CADRE - 279 - Développement d'un système recommender distribué en utilisant le framework Hadoop fonctionne Nicolae et al. (2010). Nous envisageons également d'envisager une distribution générique des algorithmes de recommandation qui ne nécessite aucun effort dans la réécriture du code. Références Apache Hadoop, http://hadoop.apache.org/. (1994). GroupLens: Une architecture ouverte pour le filtrage collaboratif de Netnews. ACM. (En 2003). MovieLens ensemble de données, http://www.grouplens.org/data/. Dean, J. et S. Ghemawat. MapReduce: simplifié de traitement des données sur les grands clusters. pp. 137-150. Lemire, D. et A. Maclachlan (2005). Une pente en ligne pour Prédicteurs Note basée sur laboration oration Filtrage. Dans Proceedings of SIAM (Data Mining SDM'05). Linden, G., B. Smith et J. York (2003). recommandations Amazon.com: Point-to-point col- filtrage laborative. Volume 7, pp. 76-80. Nicolae, B., D. Moise, G. Antoniu, L. Bougé, et M. Dorier (2010). BlobSeer: Apporter à haut débit en lourd Concurrency à Hadoop Carte-reduce Applications. Dans 24 parallèle IEEE International et distribué Symposium traitement (IPDPS 2010), Atlanta, États-Unis. IEEE et ACM. La CV de l'information d'un Dañs LeWeb bureaux AUGMENTE Dix Dernières Années. Ce phénomène a la progression de favorisé la recherche Dans le domaine des Systèmes de recommendation. l'intention de Ontario CÉS D'Aider les en fournissant des Utilisateurs suggestions. Utiles Nous proposons papier d'Dans CE un algorithme de UTILISER recommendation et de favoriser existant sa montée en charge.Nous le ACDE verser utilisons cadre de développement Hadoop Qui propose du Paradigme juin MapReduce Implémentation Pour la répartition des Traitements. Notre principale ob jectif Dans papier is this de la construction Que prouver d'un Système de recommendation is non Distribué possible only Mais Qu'elle est simple, et also Bénéfique. - 280 -"
457,Revue des Nouvelles Technologies de l'Information,EGC,2012,Evaluating Bayesian Networks by Sampling with Simplified Assumptions,"The most common fitness evaluation for Bayesian networks in the presence of data is the Cooper-Herskovitz criterion. This technique involves massive amounts of data and, therefore, expansive computations. We propose a cheaper alternative evaluation method using simplified ssumptions which produces evaluations that are strongly correlated with the Cooper-Herskovitz criterion.","Saaid Baraty, Dan A. Simovici",http://editions-rnti.fr/render_pdf.php?p1&p=1001135,http://editions-rnti.fr/render_pdf.php?p=1001135,en,"Évaluer les réseaux bayésiens par échantillonnage avec des hypothèses simplifiées Saaid Baraty *, Dan A. Simovici * * Université du Massachusetts de Boston Computer Science Department, Boston, Massachusetts 02125 e-mail {sbaraty, DSim} @ cs.umb.edu, Résumé. La plupart des évaluations de remise en forme commune pour les réseaux bayésiens en présence de données est le critère Cooper Herskovitz. Cette technique implique d'énormes quantités de données et, par conséquent, les calculs expansives. Nous vous proposons une méthode d'évaluation alternative moins coûteuse en utilisant des hypothèses simplifiées des évaluations de duces pro- qui sont fortement corrélés avec la Cooper-Herskovitz cri- tère. 1 Introduction Nous étudions le problème de la construction d'un réseau bayésien pour un nomène composite nomène U ​​= {u1, u2,. . . , Un} où ui pour 1 ≤ i ≤ n sont discrètes variables aléatoires représentant l'affectation de l'état des attributs de U. Pour ce faire, nous partons d'une multiset de données D = {t1, t2,. . . , Tm} où un tuple n-aire ti est une instance de l'événement U. Nous appelons cette multiset comme ensemble de données de preuve (ensemble de données pour faire court). Un certain nombre d'hypothèses sont nécessaires pour obtenir une mesure pour l'évaluation de l'aptitude d'une structure de réseau bayésien (BNS) pour un ensemble de données d'apprentissage. hypothèses fortes font la evaluationmoremanageable. D'autre part, le modèle obtenu selon les hypothèses les plus faibles est mieux en mesure d'être conforme à la véritable distribution sous-jacente du problème. Soit G = (U, E) un graphe orienté acyclique ayant U comme ensemble de sommets et E que l'ensemble de ses bords, qui capture les dépendances probabilistes directes entre ces variables. Soit Θ soit l'ensemble de paramètres qui permet de quantifier la distribution de probabilité conjointe de U tel que spécifié par G. On note l'ensemble des affectations possibles d'une variable aléatoire ui par Dom (ui) = {U1i,. . . , Urii}. La notion de domaine peut être étendu à des ensembles de variables V à l'aide de produit cartésien. Si l'ensemble des nœuds parents de ui est PARG (ui), puis Dom (PARG (ui) = {U1i,..., U qii}. L'ensemble des non-descendants de ui, PNIA (ui) est l'ensemble des tous les noeuds U excluent ui et tous ses descendants Quand il est clair dans le contexte que nous laissons tomber l'indice G. la paire B = (G, Θ) les satisfait la condition de Markov local si PB (ui | e (ui)). = PB (ui | par (ui)) pour 1 ≤ i ≤ n, où PB est la distribution de probabilité de U défini par B. le modèle B est un réseau bayésien si elle satisfait à la condition de Markov locale par la règle de la chaîne, on a: PB. (u1, u2,, un...) = Πni = 1 PB (ui | Par (ui)) Par conséquent, si nous laissons θijk = P (ui = Uki | Par (ui) = U ji). et θij · = (θij1,, θijri...) pour 1 ≤ i ≤ n, 1 ≤ k ≤ ri et 1 ≤ j ≤ qi, puis la distribution de probabilité conjointe sur U est spécifiée par Θ = {θij · | 1 ≤ i ≤ n et 1 ≤ j ≤ qi} -. 11 - évaluation des réseaux bayésiens par échantillonnage 2 A base postéro-Score avec un ensemble réduit Hypothèses Cooper et Herskovitz a introduit le probabil ity P (G | D) en tant que mesure d'évaluer l'aptitude du G comme un modèle probabiliste de D. Puisque P (D) est constante à travers différents réseaux, nous pouvons travailler avec P (G, D). Soit ΩG l'espace des distributions toute probabilité thetav pour la structure G. Alors, P (G, D) = ∫ ΩG (Θ) P (D | Θ, G) f (Θ | G) P (G) dO. (1) Rappelons que Θ est un ensemble de distributions θij · = (θij1,, θij (ri-1), 1 -... Σri-1 k = 1 θijk) pour tout i et j. Les vecteurs θij · pour tout (i, j) ∈ [1..n] × [1..qi] doivent satisfaire Σri-1 k = 1 θijk ≤ 1 et θijk ≥ 0 pour tout k. En outre, Θ lui-même, la collecte de ces variables vecteurs aléatoires, peut être considéré comme une variable aléatoire. P (D | Θ, G) est la fonction de probabilité conditionnelle de données donné (G, Θ), f (Θ | G) est la fonction de densité conditionnelle de structure donnée Θ G, et P (G) est la fonction de probabilité a priori de Structure G. Pour évaluer ce nombre d'une intégrale d'hypothèses ont été introduites par Cooper et Herskovits (1993). L'indépendance des données suppose tuples de D sont indépendants compte tenu de la structure du réseau. L'hypothèse d'indépendance locale et mondiale (LGI) exige que θij · est conditionnellement indépendante de θi'j '· pour tous (i, j) = 6 (i', j ') compte tenu de la structure. Sur la base de l'hypothèse de LGI, Ω (Θ), l'espace des collections possibles Θ peut être écrit comme ΩG (Θ) = {i = 1 nn qiΠ j = 1 (θij1,..., Θij (ri-1) ) ∈ R ri-1 | ri-1Σ k = 1 θijk ≤ 1 et θij1,. . . , Θij (ri-1) ≥ 0} et on a f (Θ | G) = Πni = 1 Πqi j = 1 g (θij · | G) en raison de l'hypothèse de LGI. Cooper et Herskovits (1993) remplacer f par le produit ci-dessus dans l'égalité (1). En outre, ils prennent la g de distribution (θij · | G) pour chaque i et j est uniforme. Nous appelons cette hypothèse comme deuxième probabilité uniforme de commande (SOUPE). Heckerman et al. (1995) introduisent le BDE métrique qui est une mesure basée-postérieur similaire à CH métrique. Ils utilisent l'hypothèse de LGI et trois autres hypothèses: le second ordre de probabilité Dirichlet (de PDSO) (suggéré mais non utilisé dans Cooper et Herskovits (1993)), la modularité des paramètres et l'hypothèse échantillon multinomial (MS). PDSO est la généralisation de hypothèse SOUPE qui stipule que P (θij · | G) suit une distribution Dirichlet pour tous i et j. L'hypothèse de l'échantillon multinomial affirme que si nous définissons l'ensemble ordonné = {t1 Dl,. . . , Tl-1} puis, P (tl [ui] = uki | [... U1,, ui-1]... Tl = (UV11,, u vi-1 i-1), Dl, (G, Θ)) = θijk, t [V] désigne la restriction de V ⊆ U sur tuple t ∈ D et nous avons l'état Assign- ment PARG (ui) = U ji compatible avec tl [u1,. . . , Ui-1] = (u v1 1,..., U vi-1 i-1) et θijk ∈ Θ. Plus tard, l'hypothèse de PDSO a été remplacé par deux autres hypothèses, lence de la probabilité et la possibilité la structure, ce qui implique la prise en charge de PDSO. Notez que toutes les fonctions de ity proba- g (θij · | G) suit une distribution Dirichlet qui nécessite des paramètres ri. Ainsi, pour chaque G BNE nous devons spécifier Σn i = 1 paramètres ITRI, ce qui rend cette pratique d'approche. Pour surmonter cette difficulté Heckerman et al. (1995) codé la connais- sances avant en un seul réseau bayésien dénommé (un réseau antérieur) Bpr = (GPR, Θpr). Ensuite, ils ont mis le Dirichlet paramètre correspondant à la composante distribution de probabilité θijk à αijk = N '· PBPR (Ui = uki, ParGpr (ui) = U ji), oùN' est un paramètre donné d'utilisateur auquel ils - 12 - S. Baraty et DA Simovici considèrent comme étant une taille équivalente de l'échantillon. Le choix d'une des valeurs de N 'et la collection Θpr sans observer de données est arbitraire. Nous utilisons l'échantillonnage qui nous permettent de laisser des données façonner la répartition de la probabilité a posteriori sur des vecteurs θij ·. Dans l'évaluation de la P avant (G) Cooper et Herskovits (1993) suppose une distribution a priori uniforme. Ceci et d'autres hypothèses sont basées sur des paramètres qui doivent être spécifiés arbitrairement. L'échantillonnage permet d'utiliser les données en tant que substitut pour des hypothèses fortes ou de la connaissance de domaine dans la détermination des paramètres de la deuxième répartition de probabilité de l'ordre et de la probabilité a priori P (G). Laissez-S1 et S2 deux échantillons de D. disjoints Nous évaluons P (G | S1, S2) comme une mesure de remise en forme de la structure BN G. Puisque P (S1, S2) ne dépend pas de la spécifique bNs nous pouvons laisser tomber et à la place de calcul P (G, S1, S2). Notez que la règle de la chaîne P (G, S1, S2) = P (S1 | G, S2) · P (G | S2) · P (S2). Si nous prélevons régulièrement dans différentes structures, alors P (S2) est constante et peut être supprimée. Par conséquent, nous adoptons P (S1 | G, S2) · P (G | S2) en tant que mesure relative de l'aptitude des structures pour un ensemble de données D. Si on répète le processus d'échantillonnage, nous pouvons étendre notre mesure à (kΠ q = 1 P (S2q-1 | G, S2q) · P (G | S2q)) 1 k, où S1, S2,. . . , S2k sont des échantillons FROMD où S2q-1 ∩ S2q = ∅ pour chaque q. Nous nous référons à cette mesure que la validation de k-échantillon de la structure G pour les données SetD et désignons par SAMPk (G, D). Soit S = {t1,. . . , Ta} et S 'deux échantillons disjoints SIO. Le premier terme de SAMPk (G, D) peut être écrit sous la forme P (S | G, S ') = ∫ ΩG (Θ) P (S | Θ, G, S') f (Θ | G, S ') dO . (2) Soit D = (u1,..., Un) soit un ordre topologique des noeuds de G qui représente une connaissance préalable expert du domaine. Notons nS (t) le nombre d'occurrences de tuple t en S et laisser γijk (S) = || {t ∈ S | t [{ui}] = uki ∧ t [Par (ui)] = U ji} || et γij · (S) = k = 1 Σri γijk (S). Étant donné que les attributs de D sont discrètes, on a P (S | B) = aΠ l = 1 P (tl | Sl, Θ, G) = aΠ l = 1 nn i = 1 P (ui = tl [ui] | Ui = tl [Ui], Sl, θ, G) = aΠ l = 1 nn i = 1 j = 1 qiΠ riΠ r = 1 θ λlijr ijr, où la première égalité est par la règle de la chaîne et Sl = (t1,..., tl-1), la seconde égalité est en supposant hypothèse MS et Ui = (u1,..., ui-1) et λlijr = 1 si tl [ui] = uri ∈ Dom (ui ) et tl [Parg (ui)] = U ji ∈ Dom (Parg (ui)) et λlijr = 0 sinon. Depuis oa l = 1 λlijr = γijr (S), nous avons P (S | Θ, G) = i = 1 nn qiΠ j = 1 riΠ r = 1 θ γijr (S) ijr. (3) Ensuite, P (S | Θ, G, S ') = P (S ∪ S' | Θ, G) P (S '| Θ, G) = Πni = 1 Πqi j = 1 r Πri = 1 θ γijr (S∪S ') ijr Πni = 1 j = 1 Πqi Πri r = 1 θ γijr (S') ijr = nn i = 1 j = 1 qiΠ riΠ r = 1 θ γijr (S) ijr. (4) Pour le second terme de droite de l'égalité (2) on a f (Θ | S ', G) = P (S' | Θ, G) f (Θ | G) ∫ ΩG (Θ) P ( S '| Θ, g) f (Θ | g) dO (5) - 13 - Évaluation des réseaux bayésiens par échantillonnage On suppose theSOUP hypothèse et fixé chaque g (θij · | g) = (ri-1) !. La probabilité postérieure de Θ est conditionnée par G en présence de l'échantillon S ', comme le montre l'égalité (2). Cette approche est différente de celle utilisée dans Cooper et Herskovits (1993) où l'hypothèse de SOUPE a été appliquée directement sans intervention de données échantillon. Ensuite, nous avons ∫ ΩG (Θ) P (S '| Θ, G) f (Θ | G) dO = i = 1 nn qiΠ j = 1 ((ri - 1) · Πri r = 1 γijr! (S ') (de γij · (S!) + ri - 1)!), de l'égalité (3) et SOUPE, LGI, et d'un résultat de Jeffreys et Jeffreys (1988) (voir pages 468-470 de cette référence ). Ainsi, à partir des égalités précédentes et de (3) et (5) on a, f (Θ | S ', G) = nn i = 1 qiΠ j = 1 Γ (γij · (S') + ri) ri Π r = 1 θ γijr (S ') ijr Γ (γijr (S') + 1), (6) où Γ est la fonction d'Euler. La combinaison égalités (2), (4) et (6) on obtient P (S | G, S ') = nn i = 1 qiΠ j = 1 Γ (γij · (S') + ri) Γ (γij · (S ∪ S ') + ri) · riΠ r = 1 Γ (γijr (S ∪ S') + 1) Γ (γijr (S ') + 1), Pour rapprocher la quantité P (G | S) nous utilisons une légère variation d'une mesure appelée la distorsion de distribution introduite dans Baraty et Simovici (2009). nous voulons évaluer l'indépendance conditionnelle capturé par condition de Markov locale selon les données, ce qui est ici, nous voulons évaluer dans quelle mesure les conditions fs (Ui | e (ui)) = fs (ui | Par (ui)) détient pour 1 ≤ i ≤ n, où fS est la fonction de fréquence par rapport à l'échantillon S ⊆ D. pour y parvenir, on mesure la divergence de l'ensemble de distributions de probabilité fS (Ui | e (ui) = U) de l'ensemble de distributions de probabilité fs (ui | par (ui) = U [par (ui)]) pour tout i et U ∈ Dom (e (ui)). Définition 2.1 La divergence de Markov locale de la structure de fourche au niveau du noeud ui de G en fonction de l'échantillon S, notée LMDGS (ui), est le nombre Σ U fS (nd (ui) = U) · KL [fS (ui | e ( ui) = U), fs (ui | Par (ui) = U [Par (ui)])], où la somme s'étend sur tout U ∈ Dom (e (ui)). Ici KL [p, q] est la divergence Kullbach-Leibler entre les distributions de probabilité p = (p1,..., Pn) et q = (q1,..., Qn). Laissez-HS (π u) l'entropie de Shannon de l'ensemble S partitionné en fonction des valeurs de u, et que HS (π u | πW) l'entropie de Shannon conditionnelle de l'ensemble S partitionnées en fonction des valeurs de u, conditionnée par la partition de S conformément à l'affectation de l'ensemble des attributesW (voir Baraty et Simovici (2009)). Théorème 2.2 Pour 1 ≤ i ≤ n nous avons LMDGS (ui) = HS (πui | πPar (ui)) - HS (πui | πnd (ui)). Théorème 2.3 G S LMD (ui) = 0 si et seulement si fs (ui | e (ui)) = fs (ui | Par (ui)). Le théorème 2.2 implique que 0 ≤ LMDGS (ui) ≤ HS (πui). D'après le théorème 2.3 plus la valeur de GS DMT (ui) est, plus est la structure de la fourche au niveau du noeud ui pour satisfaire à la condition de Markov locale selon S. Par conséquent, la condition de Markov est plus proche d'être satisfaite selon l'échantillon S. On un autre côté, le plus près LMD GS (ui) est toHS (π ui) la plus divergente les deux - 14 - S. Baraty et DA Simovici distributions de probabilité fS (ui | e (ui) = U) un d fs (ui | Par (ui) = U [Par (ui)]) sont pour chaque U ∈ Dom (e (ui)). Lorsque LMDGS (ui) = HS (πui), nous haveHS (πui | πPar (ui)) = HS (πui) et HS (π ui | πnd (ui)) = 0. Cela signifie que l'ensemble Par (ui) a aucune capacité de prédiction du tout au niveau du noeud ui et l'ensemble e (ui) a une capacité de parfaite sur predication ui. Laissez BNE (U) l'ensemble de toutes les structures possibles sur bayésienne ensemble d'attributs U. P Define (G | S) P (G | S) = Σni = 1 (HS (π ui) - LMDGS (ui)) Σ G'∈BNS (U) Σni = 1 (HS (πui) - LMDG'S (ui)). En utilisant les évaluations précédentes, SAMPk (G, D) peut être écrit (kΠ q = 1 P (S2q-1 | G, S2q) · P (G | S2q)) 1 k =   kΠ q = 1 Σ ns = 1 (HS2q (π us) - LMDGS2q (us)) Σ G'∈BNS (U) Σns = 1 (HS2q (π us) - LMDG'S2q (us)) · qiΠ j = 1 Γ (γij · (S2q) + ri) Γ (γij · (S2q-1 ∪ S2q) + ri) riΠ r = 1 Γ (γijr (S2q-1 ∪ S2q) + 1) Γ (γijr (S2q) + 1)) 1 k. Si on échantillonne systématiquement les données à travers des structures différentes, nous pouvons laisser tomber les entités constantes par rapport à la BNE G et en supposant P S2q G = Σns = 1 (HS2q (π nous) - LMDGS2q (us)) nous avons mis, SAMPk (G, D) = (kΠ q = 1 P G S2q nn i = 1 qiΠ j = 1 Γ (γij · (S2q) + ri) Γ (γij · (S2q-1 ∪ S2q) + ri) riΠ r = 1 Γ (γijr (S2q-1 ∪ S2q) + 1) Γ (γijr (S2q) + 1)) 1 k. 3 Résultats expérimentaux et conclusions Nous avons mené des expériences sur trois structures bien connues GAM, GCAR et DRI pour les domaines d'alarme, Voiture Diagnosis2 et Cancer napolitain avec 37, 18 et 5 nœuds respec- tivement. Pour les deux premières structures, nous avons généré au hasard les tables de probabilités correspondantes, ΘAM et ΘCAR. Ensuite, en fonction des distributions de probabilité introduites par (GAM, ΘAM) et (GCAR, ΘCAR) nous avons produit des ensembles de données de tailles 80000 et 100000 respectivement. Pour la DRI, nous avons utilisé l'ensemble de données correspondant dans la littérature avec 7565 sans valeurs manquantes. Pour chaque ensemble de données, nous avons généré au hasard un certain nombre de structures de différentes complexités. Le nombre de bords de ces structures varie de 1 à 10, 12 - 108 et 12-330 pour NC, CAR et ensembles de données AM respectivement. Les figures 1 (a), 1 (b) et 1 (c) montrent des corrélations très fortes entre le score de CH et le score SAMP pour diverses valeurs de k. La mesure dérivée est moins cher à calculer, car il fonctionne avec des échantillons beaucoup plus petits que l'ensemble des données. Nous avons introduit une mesure basée sur la probabilité a posteriori pour mesurer l'aptitude d'une structure de réseau bayésien à partir des données. La conclusion de ce travail est que notre notation à base est une alternative-échantillonnage viable et beaucoup moins cher à la note CH. Le fait que nous utilisons l'échantillonnage pour réduire l'ensemble des hypothèses et nous obtenons une très forte corrélation entre les deux mesures confirme que la SOUPE et la distribution uniforme sur P (G) sont des hypothèses sûres et ne faussent pas la recherche. - 15 - Évaluation des réseaux bayésiens par échantillonnage (a) Les données d'alarme (b) Données de voitures Diagnosis2 (c) données sur le cancer napolitain (d) diagramme de comparaison de temps FIG. 1 - Corrélations entre log (SAMPk (G, D)) et log (CH) et le temps en ms nécessaires pour ouvrir une session de calcul (SAMP1) (G, CAR) et marque CH Références Baraty, S. et D. A. Simovici (2009). Use_edge dans les structures de réseau bayésien. Dans Actes de la 8e Conférence Data Mining d'Australie, Melbourne, pp. 193-201. Cooper, G. F. et E. Herskovits (1993). Procédé bayésien pour l'induction de réseaux de données probabilistes. Rapport technique KSL-91-02, l'Université de Stanford, laboratoire Knowledge System. Heckerman, D., D. Geiger, et D. M. Chickering (1995). Apprentissage des réseaux bayésiens: La combinaison des connaissances et des données statistiques. InMachine apprentissage, pp. 197-243. Jeffreys, H. et B. S. Jeffreys (1988). Dirichlet Intégrales. Cambridge, Royaume-Uni: Cambridge Uni- versité Press. L'évaluation qualitative résumé la, plus des Connue Bayesiens en réseaux de présence is the Données Cooper Herskovitz critère. Technique des This implique de Quantités massives Données Fait, par par conséquent, des Nombreux Calculs. Nous proposons d'Une méthode, plus évaluation des suppositions Efficace Simplifiées Utilisant et Qui produit des EVALUATIONS FORTEMENT corrélées Avec le Cooper-Herskovitz critère. - 16 -"
468,Revue des Nouvelles Technologies de l'Information,EGC,2012,Human Detection by a Small Autonomous Mobile Robot,"Nous proposons une méthode utilisant les histogrammes de gradientorienté (HOG) et les séparateurs à vaste marge (SVM) pour la détection de personnesà partir d'images prises depuis un petit robot mobile autonome. Les travauxantérieurs réalisés dans le domaine de la détection d'êtres humains à partird'images ne peuvent pas être employés pour ce type d'application car ils supposentque les images sont prises à partir d'une position élevée (au moins lahauteur d'un petit enfant) alors que la taille de notre robot n'est que de 15cm.Nous employons à la fois les HOG et les SVM car cette combinaison de méthodesest reconnue comme étant celle ayant le plus de succès pour la détectionde personnes. Pour traiter une grande variété de formes humaines, principalementen raison de la distance existant entre les personnes et le robot, nous avonsdéveloppé une nouvelleméthode de prédiction à deux étapes utilisant deux typesde classificateurs SVM qui reposent sur une estimation de la distance. L'estimationest basée sur une proportion de pixels de couleur de peau dans l'image, cequi nous permet de clairement séparer notre problème de la détection de corpsentier et de celle de corps partiel. Les essais réalisés dans un bureau ont montrédes résultats prometteurs de notre méthode avec une valeur de F de 0,93.","Kouhei Takemoto, Shigeru Takano, Einoshin Suzuki",http://editions-rnti.fr/render_pdf.php?p1&p=1001172,http://editions-rnti.fr/render_pdf.php?p=1001172,en,"Détection humaine par un petit robot autonome mobile Kouhei Takemoto *, Shigeru Takano **, *** Einoshin Suzuki * Graduate School of Systems Sciences de la vie, l'Université de Kyushu 819-0395 Fukuoka, Japon 3sl11012m@sls.kyushu-u.ac.jp * * Département d'informatique, ISEE, Université Kyushu 819-0395 Fukuoka, Japon takano@inf.kyushu-u.ac.jp *** Département d'informatique, ISEE, Université de Kyushu 819-0395 Fukuoka, Japon suzuki@inf.kyushu-u .ac.jp CV. Nous proposons Une méthode Utilisant les histogrammes de gradient orienté (HOG) et les Séparateurs à vaste marge (SVM) Pour la détection de per- Sonnes à partir de: D'Prises d'images DEPUIS un petit robot autonome mobile. Les tradi- vaux in the antérieurs réalisés domaine de la Êtres d'détection à partir Humains d'images ne pas Être employees PEUVENT verser le type d'auto CE demande SUP- NIT les images Qué posent Prises à partir Sont d'Elevée position juin ( la fille Moins d'un petit hauteur enfant) Alors Que la taille de notre robot de 15cm de Ne est Qué. Nous employons à la Fois les HOG et les voitures MVB de ME- this thodes combinaison is being Comme association reconnue le plus de Celle de Ayant succès for Detection de personnes. Pour grande Une Traiter Variété de formes humaines, principale- ment en raison de la distance de Entre les personnes existant et le robot, nous Avons Une nouvelle méthode Développé de deux à ÉTAPES prédiction deux types de Utilisant classificateurs SVM Qui Sur une estimation reposent de la distance de . L'esti- tion is Sur une proportion basée de pixels de couleur de l'image Dans peau, Ce Qui nous Përmet de reVu SEPARER notre de la Problème de corps ENTIER détection et de corps de Celle partiel. Les essais réalisés Dans bureau un des Montré en Ontario de notre Résultats prometteurs Avec Une méthode de F de Valeur 0,93. 1 Introduction Les humains de détection est une compétence fondamentale qui doit être possédé par un robot (mobile) Opéra- ting dans un environnement peuplé (Schulz et al., 2003;. Shiomi et al, 2005; Nakahara et Yamane, 2005). Extraction et gestion des connaissances utilisées pour une telle compétence est une tâche difficile et ding rewar- pour les chercheurs en matière d'apprentissage de la machine et l'exploration de données. Les méthodes existantes suivent souvent les humains (Schulz et al., 2003; Shiomi et al., 2005), qui est une compétence au-delà de la détection humaine, mais reposent sur des capteurs coûteux tels que le télémètre laser (Schulz et al., 2003; Shiomi et al., 2005). détection humaine en utilisant uniquement un capteur d'image, à savoir, une caméra, existe (Nakahara et crinière Ya, 2005), mais doit détecter le visage d'un être humain. Plus important encore, chacun des existants - 233 - Détection humaine par un petit robot autonome mobile Fig. 1 - Notre robot (à gauche), une image prise par le robot (au milieu), sa représentation HOG (à droite) fonctionne suppose que le ro (Schulz et al, 2003;; Shiomi et al, 2005 Nakahara et Yamane, 2005.). - bot un humanoïde au moins aussi grand que un enfant humain, à savoir, 135cm, qui interdit leur utilisation sur un petit robot mobile autonome,. robot Un tel est prometteur pour devenir très populaire auprès des consommateurs en général en raison de sa petite taille et à faible coût, une fois qu'il est équipé de compétences fondamentales pour fonctionner dans un environnement peuplé. Cependant, la détection humaine pour un si petit robot est difficile, car il faut des images à partir d'une position faible, ce qui entraîne dans une variété de formes de l'homme à détecter. Il convient également de noter que l'utilisation de capteurs coûteux doit être évité de maintenir le coût du bas du robot. Détection humaine d'une image ou une séquence vidéo a connu un progrès remarquable (Mohan et al, 2001;. Dalal et Triggs, 2005, Dalal et al., 2006; Zhu et al., 2006; Iwahori et al, 2010;. Lu . et al, 2009, Pang et al, 2011).. Un grand nombre d'entre eux utilisent des bases de données d'image de l'homme prises à partir d'un appareil photo soit placé sur une position haute ou prise par un autre humain (Mohan et al, 2001;. Dalal et Triggs, 2005, Dalal et al., 2006;. Zhu et al, 2006 ;. Iwahori et al, 2010, Pang et al, 2011).. Lu et al. (2009) utilise des séquences vidéo de jeux de hokey sur glace et le football jeux qui sont filmés à une position haute. Iwahori et al. (2010) utilise également des images prises à partir d'une caméra placée au plafond. Apparemment, aucun d'entre eux peut être utilisé pour un petit robot. La plupart des travaux ci-dessus utilisent Histogrammes des Dégradés Oriented (HOG), qui sont des descripteurs de fonction utilisés dans la détection d'objet (Dalal et Triggs, 2005). HOG a été utilisé avec succès dans la détection humaine (Dalal et Triggs, 2005. Dalal et al., 2006; Iwahori et al, 2010;. Lu et al, 2009;. Pang et al, 2011). Pang et al. (2011) indique que la combinaison de HOG et Support Vector Machine (SVM de) (Vapnik, 1992) est le plus grand succès algorithme de détection humain. Dans cet article, nous appliquons cette combinaison, à savoir HOG + SVM, à la détection humaine par un petit, peu coûteux, un robot mobile autonome à partir d'un capteur d'image. Pour faire face à une variété de formes humaines principalement en raison de la distance qui les sépare, nous vous proposons également une méthode de prédiction en deux étapes qui utilise deux types de classificateurs SVM. 2 problème et notre proposition 2.1 Détection humaine par un petit robot mobile Figure 1 montre gauche du robot mobile utilisé dans ce travail, qui est de 20 cm de largeur, 20 cm de longueur et de la hauteur de 15 cm (Takano et Suzuki, 2011;. Boubou et al, 2011 ). Le robot est équipé de deux caméras USB, six capteurs tactiles, huit capteurs IR, une LED, et deux organes de roulement reliés à deux roues. Le robot comporte un processeur 1GHz, une unité de mémoire de 1 Go et une carte SDHC 16GB. Le robot se déplace vers l'avant et vers l'arrière, et tourne à gauche et à droite. Il naviguera nome - 234 - K. Takemoto et al. avec peu de collisions nimité à des obstacles en utilisant les capteurs IR et les capteurs tactiles. Nous utilisons la caméra supérieure, qui prend des images de 240 × 320 pixels dans ce travail. Le robot de la première navigue dans un bureau et prend une image toutes les secondes environ. Il prend en images au total n, qui sont stockés dans la carte SD, puis transférées à un PC. Le concepteur d'étiquettes chaque image si elle contient un être humain ou non. SVM (Vapnik, 1992) est utilisé pour apprendre une ou plusieurs classificateurs des images. Un classificateur binaire M, qui prend une instance e générée à partir d'une image en entrée p, est généré à partir du classificateur (s), et ensuite transféré dans l'unité de mémoire. Enfin, le robot se déplace vers l'avant dans un bureau et prend à nouveau une image à peu près toutes les secondes. Pour chaque p image, il prédit si elle contient une base humaine sur M et enregistre le résultat, qui est oui ou non, dans sa mémoire. Comme première étape, nous supposons qu'une image contient au plus un être humain. Le problème est difficile en raison des variations dans la pose, la forme du corps, l'apparence, les vêtements, l'éclairage et l'encombrement d'arrière-plan. caméras ou arrière-plans en mouvement, il est encore plus difficile (Dalal et al., 2006). Nous définissons également un problème simplifié, qui remplace la phase de prédiction par le robot avec une validation croisée de M sur le PC. Les résultats des deux problèmes sont évalués en utilisant le rappel, la précision et la valeur F. 2.2 Notre solution Une solution naïve, simple d'obtenir M pour le problème est d'utiliser le classificateur appris par SVM comme M. Précisément parlant, chaque image est convertie en niveaux de gris et les caractéristiques HOG sont extraites pour générer une instance de formation comme décrit plus loin. Ce procédé présente une grande variété des formes du corps parce qu'un homme regarde très différent quand il / elle se tient loin ou à proximité du robot. De toute évidence, la distance h entre le robot et l'humain doit être estimé et pris en considération. Nous avons conçu une solution simple basée sur le rapport r% du pixel de couleur de la peau (Kato et Na- Kamura, 2005). Kato et Nakamura (2005) indique que des plages de couleur de peau de 0 à 38 dans la teinte de l'espace de couleurs HSV. Soit s un seuil donné par le concepteur. Nous avons divisé les images de formation dans les r> s et les r ≤ s, qui correspondent à des images avec l'homme debout loin et à proximité, respectively.We appliquer SVM aux images Withe humains debout loin et près ot obtenir classificateurs Mf et Mc respectivement. Comme résultat, M = Mf si r> s et M = Mc autrement. Nous montrons la algorith m qui génère un vecteur caractéristique x avec HOG partir d'une image en niveaux de gris de taille L × pixels H, où I (x, y) représente la luminosité du pixel (x, y). Elle a d'abord créer HL / c2 histogrammes pour les cellules avec le quadruple de la boucle, où tranformAngle2Bin (θ (u, v)) renvoie ⌊θ (u, v) / 20⌋ si θ (u, v) ≤ ⌊ correspondant 180◦ ou (θ (u, v) - 180) / 20⌋ si θ (u, v)> 180◦. Il génère alors x avec le reste de l'algorithme, où addFeature premières (x, H (x0, y0),..., H (x0 + 2c, y0 + 2c)) normalise 9 histogrammes de sorte qu'ils forment ensemble une distribution de probabilité et détermine alors le i + 9j + k-ième valeur de x en tant que valeur normalisée kième du bac de l'histogramme H j-ième à (x0, y0),. . . , H (x0 + 2c, y0 + 2c). - 235 - Détection humaine par un robot de petite autonome mobile algorithme 1 La génération d'un vecteur de caractéristique avec HOG INPUT: l'image en niveaux de gris (L × pixels H) I (x, y) OUTPUT: vecteur de caractéristique x pour x0 = 0 à L - c STEP c faire pour y0 = 0 à H - c étape c faire pour u = x0 à C - 1 ETAPE 1 faire pour v = y0 A c - 1 ETAPE 1 faire si 0 ≤ u ± 1 ≤ L et 0 ≤ v ± 1 ≤ H puis fx (u, v) = I (u + 1, v) - I (u - 1, v). fy (u, v) = I (u, v + 1) - I (u, v + 1). m (u, v) = √ fx (x, y) 2 + fy (x, y) 2 θ (u, v) = fy fx tan-1 (x, y) (x, y) Ajouter m (u, v) vers le bac tranformAngle2Bin (θ (u, v)) de l'histogramme H (x0, y0) end if end à bout pour bout pour bout pour i = 1 pour x0 = 0 à l - 3c Étape 3C faire pour y0 = 0 H - 3c Étape 3C do addFeature (x, H (x0, y0), H (x0 + 2c, y0 + 2c)...) i = i + 81 extrémité de fin pour le retour x 0,5 0,6 0,7 0,8 0,9 1 3 4 5 6 7 F _ VA LU es 0,6 0,7 0,8 0,5 0,9 1 3 4 5 6 7 P _ VA LU es 0,6 0,7 0,8 0,5 0,9 1 3 4 5 6 7 R _ VA LU es FIG. 2 - Les résultats des expériences variées s - 236 - K. Takemoto et al. 3 L'évaluation expérimentale 3.1 Expériences Toutes les expériences ont été réalisées dans le bureau avec trois sujets debout près de 2 m, que l'ensemble du corps gradient est pas pleinement pris autrement. Nous utilisons SVMlight (Joachims, 1998) avec le réglage par défaut sans utiliser la fonction du noyau. Le PC est équipé d'un processeur Intel cadencé à 2,6 GHz de base avec 8 Go de mémoire. Nous utilisons la validation croisée 10 fois avec les expériences sur PC. L'ensemble des données de formation a été générée à partir de 100 images avec les humains et 100 images sans l'homme et c = 20. Tout d'abord, nous avons testé notre proposition sur PC par différentes s entre 3% et 7% à l'étape 1%. La figure 2 montre les résultats d'expériences et on voit que s = 5 donne le meilleur résultat en termes de F Valut, à savoir, F = 0,93. Nous avons constaté que s = 5 correspond à environ h = 1m, au cours de laquelle l'image contient l'ensemble du corps humain, des pieds à la tête. La meilleure performance est réalisée probablement en raison de la séparation nette des deux cas: la détection du corps entier et la détection partielle du corps. D'autre part, nous avons testé la méthode naïve avec un SVM sur PC La précision, le rappel et la précision étaient 0,85, 0,91 et 0,89, ce qui justifie notre proposition d'utiliser deux classificateurs SVM. Enfin, nous avons testé notre proposition à bord et la précision, le rappel et la précision étaient 0.58, 0.59 et 0.56, respectivement. Comme la raison de cette baisse drastique, nous avons constaté que les images sont souvent classés à tort comme positif, principalement en raison des objets qui ont la même couleur à la peau et ne sont pas présents dans les images de formation, par exemple, un carton. Comme les résultats sur PC sont très encourageants, nous pensons y compris des images dans la formation, ainsi que l'utilisation de près misses résoudrait le problème. Acquittement Une partie de cette recherche a été financée par le Programme de coopération stratégique internationale FUn- ded par l'Agence japonaise pour la science et la technologie (JST). Nous sommes reconnaissants à Fabrice Muhlenbach et Philippe Lenca pour leur aide par écrit notre résumé en français. Boubou Références, S., A. Kouno, et E. Suzuki (2011). Mise en œuvre Camshift sur un robot mobile pour personne Suivi et Poursuite. Dans Proc. Onzième Conférence internationale IEEE sur les ateliers (Data Mining ICDMW 2011). (Accepté pour publication). Dalal, N. et B. Triggs (2005 ). Histogramme de gradient orienté pour HumanDetection. puter Incom- Vision et Pattern Recognition 2005. CVPR 2005. IEEE Computer Society Confé- rence sur, Volume 1, pp. 886-893. Dalal, N., B. Triggs, et C. Schmid (2006). Détection humain à l'aide de flux Oriented Histogrammes et apparence. InComputer Vision-ECCV 2006, LNCS 3952, pp. 428-441. Springer. Iwahori, Y., Y. Yamauchi, H. Fujiyoshi, et T. Kanade (2010). Les gens détection fondées sur les co-occurrence de l'apparence et spatio-temporelle Caractéristiques. Institut national des opérations informatiques sur les progrès en informatique (7), 33-42. - 237 - Détection humaine par un petit robot autonome mobile Joachims, T. (1998). Catégorisation texte avec Support Vector Machines: apprentissage avec de nombreuses fonctionnalités pertinentes. InMachine apprentissage: CELV-98, LNCS 1398, pages 137-142.. Springer. Kato, Y. et O. Nakamura (2005). Extraction de haute précision des visages en tenant compte de la personne avec des lunettes. IEEJ Transactions sur l'électronique, l'information et les systèmes 125, 1018- 1023. Lu, W., K. Okuma, et J. Little (2009). Suivi et reconnaissance Actions de plusieurs joueurs de hockey à l'aide du filtre à particules Dopé. Image et Informatique 27 (1-2), 189-205. Mohan, A., C. Papageorgiou, et T. Poggio (2001). Exemple-Based détection d'objet en images par des composants. IEEE Trans. Motif Anal. Mach. Intell. 23 (4), 349-361. Nakahara, T. et T. Yamane (2005). Méthode de détection humaine pour AutonomousMobile Robots. MEW Rapport technique 53 (2), 81-85. (en japonais). Pang, Y., Y. Yuan, X. Li, et J. Pan (2011). Efficace HOG détection humaine. Signal traite- ment 91 (4), 773-781. Schulz, D., W. Burgard, D. Fox, et A. Cremers (2003). Les gens de suivi avec des robots mobiles en utilisant des filtres Association conjointe probabilistes de données à base d'échantillons. La Revue internationale de la recherche robotique 22 (2), 99. Shiomi, M., T. Miyashita, et H. Ishiguro (2005). Multicapteur Based Human Tracking être- haviors Markov Chain Monte Carlo Algorithmes pour Robots Communication active. Journal de la Société robotique du Japon 23 (2), 220-228. (en japonais). Takano, S. et E. Suzuki (2011). Nouvelle détection d'objets pour bord Robot Vision par levage complexes Transforms Wavelet. Dans Proc. Onzième Conférence internationale IEEE sur les ateliers (Data Mining ICDMW 2011). (Accepté pour publication). Vapnik, V. (1992). Principes de risque Minimisation pour l'apprentissage théorique. Les progrès réalisés dans les systèmes de traitement de l'information neural 4, 831-838. Zhu, Q., M. Yeh, K. Cheng, et S. Avidan (2006). Rapide de détection humaine utilisant une cascade de Histogrammes de Oriented Dégradés. Dans Vision par ordinateur et reconnaissance, 2006 IEEE Computer Society Conférence, Volume 2, pp. 1491-1498. Résumé Nous proposons une méthode de détection humaine en utilisant HOG et SVM d'une image par un petit robot mobile autonome. Les travaux existants pour la détection humaine à partir d'images ne peuvent pas être utilisées pour notre but, car ils supposent que les images sont prises à partir d'une position haute, au moins à la hauteur d'un petit enfant humain, alors que notre robot est de 15 cm de hauteur. La combinaison de HOG et SVM est connu comme le plus grand succès méthode de détection humaine que nous adoptons. Pour faire face à une grande variété de formes humaines principalement en raison de la distance qui les sépare, nous avons conçu une méthode de prédiction en deux étapes qui utilise deux types de classificateurs SVM sur la base d'une estimation de la distance. L'estimation est basée sur le rapport du pixel de couleur de peau dans l'image, ce qui nous permet de séparer clairement notre problème dans la détection du corps entier et la détection partielle du corps. Des expériences dans un bureau ont montré des résultats prometteurs de notre méthode avec la valeur F 0,93. - 238 -"
473,Revue des Nouvelles Technologies de l'Information,EGC,2012,Mining Genetic Interactions in Genome-Wide Association Study,"Advanced biotechnologies have rendered feasible high-throughput data collecting in human and other model organisms. The availability of such data holds promise for dissecting complex biological processes. Making sense of the flood of biological data poses great statistical and computational challenges. I will discuss the problem of mining gene-gene interactions in high-throughput genetic data. Finding genetic interactions is an important biological problem since many common diseases are caused by joint effects of genes. Previously, it was considered intractable to find genetic interactions in the whole-genome scale due to the enormous search space. The problem was commonly addressed using heuristics which do not guarantee the optimality of the solution. I will show that by utilizing the upper bound of the test statistic and effectively indexing the data, we can dramatically prune the search space and reduce computational burden. Moreover, our algorithms guarantee to find the optimal solution. In addition to handling specific statistical tests, our algorithms can be applied to a wide range of study types by utilizing convexity, a common property of many commonly used statistics.",Wei Wang    ,http://editions-rnti.fr/render_pdf.php?p1&p=1001128,http://editions-rnti.fr/render_pdf.php?p=1001128,en,"Mines Interactions génétique du génome à l'échelle Study Association Wei Wang * * Université de Caroline du Nord weiwang@cs.unc.edu, http://www.cs.unc.edu/ Weiwang / Résumé biotechnologies avancées ont rendu à haut débit possible collecte de données chez l'homme hu- et d'autres organismes modèles. La disponibilité de ces données est prometteuse pour disséquer les processus biologiques complexes. Donner un sens du flot de données biologiques pose une grande statis- tique et les défis informatiques. Je vais examiner le problème des interactions gène-gène extraction des données génétiques à haut débit. Trouver des interactions génétiques est un problème biologique important, car de nombreuses communes sont dis- Soulage causés par les effets conjugués des gènes. Auparavant, il a été jugé intraitable de trouver des interactions génétiques dans l'échelle du génome entier en raison de l'espace de recherche énorme. Le pro- blème est souvent adressé à l'aide heuristiques qui ne garantissent pas l'optimalité de la solution. Je vais montrer que, en utilisant la limite supérieure de la statistique de test et efficacement in- Dexing les données, nous pouvons considérablement élaguer l'espace de recherche et de réduire la charge de calcul. De plus, nos algorithmes garantissent de trouver la solution optimale. En plus de traiter les tests statistiques spéci- fiques, nos algorithmes peuvent être appliqués à un large éventail de types d'études en utilisant convexité, une propriété commune de nombreuses statistiques couramment utilisées. Bibliographie Wei Wang est professeur au Département des sciences informatiques et membre du Centre des sciences Caroline Genomic à l'Université de Caroline du Nord à Chapel Hill. Elle a obtenu un diplôme MS de l'Université d'État de New York à Binghamton en 1995 et un diplôme de doctorat en informatique de l'Université de Californie à Los Angeles en 1999. Elle a été membre du personnel de recherche au TJ Watson d'IBM Research Center entre 1999 et 2002. intérêts de recherche du Dr Wang comprennent l'exploration de données, la bio-informatique et des bases de données. Elle a déposé sept brevets et a publié une monographie et plus d'une centaine de documents de recherche dans des revues internationales et les grands comptes rendus de conférences à comité de lecture. Dr Wang a reçu le prix d'excellence IBM Invention en 2000 et 2001. Elle a reçu un prix de développement junior Faculté UNC en 2003 et un Faculty Award NSF Early Career Development (carrière) en 2005. Elle a été nommée Microsoft Research New Fellow Faculté en 2005. Elle a récemment reçu le prix 2007 Phillip et Ruth Hettleman pour artistique - 1 - et à l'UNC Scholarly Achievement. Dr Wang est rédacteur en chef adjoint des opérations IEEE sur les connaissances et ingénierie des données et ACM Transactions sur la découverte de connaissances dans les données, et un membre du comité de rédaction de la Revue internationale des mines de données et de bio-informatique. Elle est membre des comités de programme de conférences internationales prestigieuses telles que SIGMOD, SIGKDD, VLDB, CIED, EDBT, ACM CIKM, IEEE ICDM et SSDBM. - 2 -"
475,Revue des Nouvelles Technologies de l'Information,EGC,2012,PLS path modeling and regularized generalized canonical correlation analysis for multi-block data analysis,"Regularized generalized canonical correlation analysis (RGCCA) is a generalization of regularizedcanonical correlation analysis to three or more sets of variables. It constitutes a generalframework for many multi-block data analysis methods. It combines the power of multi-blockdata analysis methods (maximization of well identified criteria) and the flexibility of PLS pathmodeling (the researcher decides which blocks are connected and which are not). Searchingfor a fixed point of the stationary equations related to RGCCA, a new monotone convergentalgorithm, very similar to the PLS algorithm proposed by Herman Wold, is obtained. Finally,a practical example is discussed.",Michel Tenenhaus,http://editions-rnti.fr/render_pdf.php?p1&p=1001133,http://editions-rnti.fr/render_pdf.php?p=1001133,en,"PLS modélisation et régularisé analyse de corrélation canonique généralisée chemin pour l'analyse de données multi-blocs Michel Tenenhaus * * HEC Paris tenenhaus@hec.fr, http://www.hec.edu/Faculty-and-Research/Faculty/TENENHAUS/ Résumé généralisée régularisée analyse de corrélation canonique (RGCCA) est une généralisation de enre- analyse de corrélation canonique ularized à trois ou plusieurs ensembles de variables. Il constitue un cadre général pour de nombreuses méthodes d'analyse des données multi-blocs. Il combine la puissance des méthodes d'analyse de données multi-blocs (maximisation des critères bien identifiés) et la flexibilité de la modélisation du chemin PLS (le chercheur décide quels blocs sont connectés et qui ne sont pas). Recherche d'un point fixe des équations stationnaires liées à RGCCA, un nouvel algorithme convergent voix monotone, très similaire à l'algorithme PLS proposé par Herman Wold, est obtenu. Enfin, un exemple pratique est discuté. Bibliographie Michel Tenenhaus est professeur émérite de statistique à HEC Paris. Ses principales recherches portent sur l'analyse de données à variables multiples: les méthodes de codage optimal pour les variables catégoriques, régression PLS, l'approche PLS et régularisée analyse de corrélation canonique généralisée. Il a publié de nombreux articles dans des revues scientifiques et trois livres: Méthodes statistiques en Gestion (Dunod, 1994), La régression PLS: Théorie et applications (Technip 1998) et Statistics: methods verser DÉCRIRE, tentatives de viol et prévoir (Dunod, 2007). Michel Tenenhaus est également consultant pour les entreprises industrielles. Il a été président de PLS'99 à Jouy-en-Josas et PLS'09 à Beijing, et co-président du PLS'01 de colloques suivants à Anacapri, PLS'03 à Lisbonne et à Barcelone PLS'05. - 9 -"
479,Revue des Nouvelles Technologies de l'Information,EGC,2012,Relational Learning from Spatial Data: Retrospect and Prospect,"Learning from spatial data is characterized by two main features. First, spatial objects have a locational property which implicitly defines several spatial relationships (topological, directional, distancebased) between objects. Second, attributes of spatially related units tend to be statistically correlated. These two features argue against the assumption of the independent generation of data samples (i.i.d. assumption) underlying classic machine learning algorithms, and motivate the application of relational learning algorithms, whose inferences are based on both instance properties and relations between data. This relational learning approach to spatial domains has already been investigated in the last decade, and important accomplishments in this direction have already been performed. In this talk, we retrospectively survey major achievements on relational learning from spatial data and we report open problems which still challenges researchers and prospectively suggest important topics for incorporation into a research agenda.",Donato Malerba,http://editions-rnti.fr/render_pdf.php?p1&p=1001131,http://editions-rnti.fr/render_pdf.php?p=1001131,en,"Apprentissage Relational à partir des données spatiales: Rétrospective et perspectives Donato Malerba * * Université de Bari malerba@di.uniba.it, http://www.di.uniba.it/~malerba/ Résumé d'apprentissage à partir des données spatiales se caractérise par deux caractéristiques principales . Tout d'abord, les objets spatiaux ont une propriété qui définit implicitement localisation plusieurs relations spatiales (topologique, direc- tionnel, distancebased) entre les objets. En second lieu, les attributs des unités liées spatialement ont tendance à être statistiquement corrélés. Ces deux caractéristiques plaident contre l'hypothèse du sous-jacent des algorithmes d'apprentissage machine à classique génération indépendante d'échantillons de données (hypothèse de i.i.d.), et de motiver l'application d'algorithmes d'apprentissage relationnels, dont des conclusions sont basées sur les propriétés de l'instance et les relations entre les données. Cette approche d'apprentissage relationnel aux domaines tielles spa- a déjà été étudiée dans la dernière décennie, et les réalisations importantes dans ce sens ont déjà été réalisées. Dans cet exposé, nous examinons rétrospectivement les réalisations majeures sur l'apprentissage relationnel à partir des données spatiales et nous signaler des problèmes ouverts qui a encore des défis et des chercheurs suggèrent de façon prospective des sujets importants pour l'incorporation dans un programme de recherche re. Bibliographie Donato Malerba est professeur titulaire au Département d'informatique, Université de Bari, où il enseigne dans les cours des « algorithmes et structures de données », « Advanced Systems Base de données » et « bases de connaissances et d'exploration de données ». En 1992, il était spécialiste assistant à l'Institut des sciences informatiques, Université de Californie, Irvine. Son activité de recherche porte principalement sur l'apprentissage de la machine et l'exploration de données, en particulier les méthodes de numériques symbolique pour l'inférence productive in-, arbres de classification et modèles, (multi) l'exploration de données relationnelle, l'extraction de données spatiales, web mining, et leurs applications à traitement de documents intelligents et l'interprétation de données cartographiques numériques. Il a publié plus de 150 articles dans des revues internationales et actes de conférence. Il était au conseil d'administration de l'action européenne coordonnée FP6- 021321 « KDUbiq - Découverte des connaissances dans les environnements ubiquitaire » (Décembre 2005 - mai 2008) et au conseil d'administration du projet européen IST-2001-33086 « KDNet - Découverte européenne du savoir réseau d'excellence »(2002 - 2004). Il a participé à plusieurs projets européens et nationaux. Il était responsable de l'unité de Bari dans le projet IST-Pean euro-1999-10536 SPIN (Mining Spatial sur les données d'intérêt public) et dans deux - 5 - projets MIUR Cofin (années 1999-2001, 2001-2003). Il est responsable d'une unité de recherche du projet stratégique PS121 « des installations de télécommunication et réseaux de capteurs sans fil dans la gestion des urgences » financé par la région des Pouilles. Il a reçu le Prix IBM Faculty pour l'année 2004. Il a été dans le conseil d'administration de l'Association italienne pour ficielle Intelligence ar- (AI * IA) de Septembre 2001 à Septembre 2005. Il a siégé au comité de programme d'un grand nombre international conférences et ateliers d'apprentissage de la machine et l'exploration de données, coprésidée sept ateliers internationaux / nationaux et ont agi comme directeur invité de six numéros spéciaux de revues internationales (thèmes: apprentissage machine en vision par ordinateur, extraction des données officielles, l'extraction de données visuelles, spatio- l'extraction de données temporelles, l'intelligence artificielle, l'exploration de données multi-relationnelle). Il a été coprésident du programme de la 18e Conférence internationale sur les applications industrielles et d'ingénierie de l'intelligence artificielle et les systèmes experts (IEA- AIE'05), Bari, Juin 2005 et du 16e Symposium international sur les méthodologies de systèmes intelligents (ISMIS'06 ), Bari, Septembre 2006. Il est un programme co-président de la confé- rence ECML / PKDD qui se tiendra à Athènes, en Grèce, à côté Septembre 2011. Il est dans le comité de rédaction de machine Learning Journal, Journal des systèmes d'information intelligents et international Journal of Data Mining, la modélisation et la gestion. - 6 -"
484,Revue des Nouvelles Technologies de l'Information,EGC,2012,Solving Problems with Visual Analytics: Challenges and Applications,"Never before in history data is generated and collected at such high volumes as it is today. As the volumes of data available to business people, scientists, and the public increase,their effective use becomes more challenging. Keeping up to date with the flood of data,using standard tools for data analysis and exploration, is fraught with difficulty. The field ofvisual analytics seeks to provide people with better and more effective ways to understandand analyze large datasets, while also enabling them to act upon their findings immediately. Visual analytics integrates the analytic capabilities of the computer and the abilities of the human analyst, allowing novel discoveries and empowering individuals to take control of the analytical process. Visual analytics enables unexpected and hidden insights, which may lead to beneficial and profitable innovation. The talk presents the challenges of visual analytics and exemplifies them with application examples, illustrating the exiting potential of current visual analysis techniques.",Daniel Keim,http://editions-rnti.fr/render_pdf.php?p1&p=1001130,http://editions-rnti.fr/render_pdf.php?p=1001130,en,"Résolution des problèmes avec l'analyse visuelle: défis et applications Daniel Keim * * Université de Constance Daniel.Keim@uni-konstanz.de, http://infovis.uni-konstanz.de/~keim/ Résumé Jamais auparavant dans l'histoire des données est généré et recueillis à ces volumes car il est day.As To- les volumes de données disponibles pour les gens d'affaires, les scientifiques et l'augmentation publique, leur utilisation efficace devient plus difficile. Rester à jour avec le flot de données, en utilisant des outils stan- dard pour l'analyse des données et l'exploration, se heurte à des difficultés. Le champ ofvisual ana- lytiques cherche à fournir aux gens des moyens meilleurs et plus efficaces pour analyser understandand grands ensembles de données, tout en leur permettant d'agir sur leurs résultats immédiatement. Visuels analyt- ics intègre les capacités analytiques de l'ordinateur et de les capacités de l'analyste humain, ce qui permet de nouvelles découvertes et de permettre aux particuliers de prendre le contrôle du pro- cessus d'analyse. L'analyse visuelle permet approfondissements inattendus et cachés, ce qui peut conduire à l'innovation bénéfique et rentable. Le discours présente les défis de l'analyse visuelle et les fies avec des exemples adoptée notamment l'application, ce qui illustre la sortie potentielle des techniques d'analyse visuelle en cours. Bibliographie Daniel A. Keim est professeur titulaire (chaire de traitement de l'information) au département informatique de l'Université de Constance. Après avoir été professeur adjoint à l'Université de Munich, et plus tard à l'Martin- Luther-Universität Halle, il a été chercheur senior chez AT & T Shannon Research Labs, Etats-Unis. La publication du professeur Keim ont eu un impact dans l'exploration de données, la visualisation de l'information et l'analyse visuelle. Il est rédacteur en chef de TKDE et le Journal visualisation de l'information et fait partie de la InfoVis IEEE, IEEE VAST et IEEE / EG EUROVIS conférences annuelles des comités de pilotage. - 3 -"
488,Revue des Nouvelles Technologies de l'Information,EGC,2012,Topological Decomposition and Heuristics for High Speed Clustering of Complex Networks,"With the exponential growth in the size of data and networks, developmentof new and fast techniques to analyze and explore these networks isbecoming a necessity. Moreover the emergence of scale free and small worldproperties in real world networks has stimulated lots of activity in the field ofnetwork analysis and data mining. Clustering remains a fundamental techniqueto explore and organize these networks. A challenging problem is to find a clusteringalgorithm that works well in terms of clustering quality and is efficient interms of time complexity.In this paper, we propose a fast clustering algorithm which combines someheuristics with a Topological Decomposition to obtain a clustering. The algorithmwhich we call Topological Decomposition and Heuristics for Clustering(TDHC) is highly efficient in terms of asymptotic time complexity as comparedto other existing algorithms in the literature. We also introduce a number ofHeuristics to complement the clustering algorithm which increases the speed ofthe clustering process maintaining the high quality of clustering. We show theeffectiveness of the proposed clustering method on different real world data setsand compare its results with well known clustering algorithms.","Faraz Zaidi, Guy Melançon",http://editions-rnti.fr/render_pdf.php?p1&p=1001147,http://editions-rnti.fr/render_pdf.php?p=1001147,en,"Topologiques et décomposition Heuristique pour la haute vitesse Clustering de réseaux complexes Faraz Zaidi *, Guy Melançon ** * Institut Karachi d'économie et de la technologie (KIET) Korangi Creek, Karachi, 75190, Pakistan faraz@pafkiet.edu.pk ** CNRS UMR 5800 LaBRI & INRIA Bordeaux - Sud Ouest 351, cours de la Libération, 33405 Talence cedex, FRANCE Résumé guy.melancon@labri.fr. Avec la croissance exponentielle de la taille des données et des réseaux, dé- veloppement des techniques nouvelles et rapides pour analyser et explorer ces réseaux devient une nécessité. De plus, l'émergence de propriétés échelle du monde libre et petits dans les réseaux du monde réel a stimulé beaucoup d'activité dans le domaine de l'analyse du réseau et l'exploration de données. Clustering reste une technique fondamentale pour explorer et organiser ces réseaux. Un problème défi est de trouver un algorithme de classification qui fonctionne bien en termes de qualité de clustering et est efficace en termes de complexité de temps. Dans cet article, nous vous proposons un algorithme de clustering rapide qui combine des heuristiques avec une décomposition topologiques pour obtenir un regroupement. Le algo- rithme que nous appelons topologiques et décomposition Heuristique pour Clustering (TDHC) est très efficace en termes de complexité temporelle asymptotique par rapport à d'autres algorithmes existants dans la littérature. Nous présentons également un certain nombre d'heuristiques pour compléter l'algorithme de clustering qui augmente la vitesse du processus de regroupement en maintenant la haute qualité de regroupement. Nous montrons l'efficacité de la méthode de classification proposée sur les différents ensembles de données du monde réel et comparer ses résultats avec des algorithmes de clustering bien connus. 1 Introduction La plupart des systèmes du monde réel prennent la forme de réseaux où un ensemble de noeuds et des arêtes peut être utilisé pour représenter ces réseaux. Les exemples incluent les réseaux sociaux, les réseaux métaboliques, web alimentaire, les réseaux de transport (Newman (2003)). Clustering reste une technique importante vers une meilleure exploration et de l'organisation de ces réseaux. En termes de réseaux représentant des données réelles du monde, un cluster peut être défini comme un groupe de noeuds qui sont semblables ou connectés dans un certain sens prédéfini et différent des noeuds appartenant aux autres groupes (Schaeffer (2007)). La détection des grappes a une large gamme d'applications dans divers domaines. Par exemple, dans les réseaux sociaux, le regroupement pourrait nous conduire vers une meilleure compréhension des interactions qui ont lieu entre les personnes ou pour les réseaux biologiques, une application utile de regroupement est dans l'identification de biomarqueurs dans un réseau d'interaction protéine-protéine. - 83 - Décomposition topologiques pour grande vitesse clustering Différentes mesures ont été étudiées pour classer ces réseaux. Deux de ces classifications ont gagné beaucoup d'intérêt lorsque les réseaux présentent petit monde (Watts et Strogatz (1998)) et l'échelle libre (Barabási et Albert (1999)) dispose. Ces caractéristiques rendent les réseaux complexes et le problème cluster difficile. Voici quelques exemples de réseaux qui sont à la fois libre à grande échelle et petit monde en même temps sont le réseau de l'auteur (Newman (2001)) et The Movie Network Acteur (Watts et Strogatz (1998)). Une autre question importante qui doit être pris en compte lorsque le développement d'algorithmes de regroupement de ces réseaux est le temps complexité de la taille croissante de ces réseaux, il est BE- presque impossible d'utiliser des algorithmes de clustering lents. Il existe des algorithmes dans la littérature résoudre le problème de clustering pour les vastes réseaux complexes, mais un compromis existe tween Clustering Précision BE- et le temps de complexité. Il est donc évident que des algorithmes plus rapides sont nécessaires pour obtenir le regroupement à grande vitesse, ainsi que une grande précision pour gérer les grands réseaux. La motivation de ce travail vient du fait que la distribution de niveau de noeud de réseaux du monde réel ne sont pas des nœuds aléatoires, plutôt différents ont des degrés divers. Spécialement avec la pré- sence d'un comportement sans échelle, beaucoup de nœuds ont tendance à avoir quelques connexions alors que quelques noeuds dominent la connectivité réseau avec un grand nombre de connexions. Ces réseaux forment une seule composante connexe, mais une analyse attentive suggère que les noeuds ayant un degré élevé, jouent un rôle important dans le maintien l'ensemble du réseau connecté. En utilisant une décom- position des réseaux topologiques en fonction du degré, nous proposons un nouvel algorithme de clustering qui est très efficace en termes de complexité de temps et performe aussi bien que les algorithmes existants en termes de de clustering la qualité du regroupement produit. Nous présentons également des heuristiques à grande vitesse qui aident à réduire la taille du réseau dans le temps de doublure en termes de nombre de nœuds. Tout au long de cet article, nous utilisons le terme de réseau pour faire référence à un graphique simple, et non orienté non pondérée représentée par Me G. Nous représentons le nombre de noeuds par n et le nombre d'arêtes par m. Le reste du papier est organisé comme suit: La section 2 porte un certain nombre d'algorithmes de clustering présents dans la littérature. Dans la section 3, nous expliquons les détails de la décomposition topologique. Nous avons ensuite introduit l'algorithme de TDHC dans la section 4. Dans la section 5, nous présentons des ensembles de données du monde réel utilisées pour l'expérimentation. Nous comparons les résultats de l'algorithme de TDHC avec des algorithmes existants dans la section 6, pour finalement conclure à la section 7. 2 travaux connexes De nombreuses approches différentes ont été proposées pour découvrir les clusters dans les réseaux complexes. Par exemple, Girvan et Newman (Girvan et Newman (2002)) bord utilisé intermédiarité pour produire un algorithme de classification hiérarchique de division. L'idée de base est d'identifier les bords intra du cluster par rapport aux bords inter-munitions. Les bords se trouvant entre les groupes auront une place centrale de BE- tweenness plus élevé par rapport aux bords d'un cluster. L'algorithme de clustering supprime les bords avec une grande place centrale betweenness pour identifier les clusters et recalcule la place centrale betweenness. L'algorithme donne de bons résultats dans la détection des clusters, mais souffre d'une grande complexité du temps. La pire complexité temporelle de cas est donnée par O (M2N). Bien que pratiquement l'algorithme tourne plus vite que son pire des cas, mais il a encore une complexité de temps en raison de la lation cal- de centralité intermédiarité à chaque itération puisque dans chaque itération, le nombre total de noeuds sont divisés par un certain facteur avant de recalculer la centralité betweenness . Wu et al. (Wu et al. (2004)) introduisons une structure à plusieurs mailles pour regrouper les grands réseaux. L'algorithme de clustering utilise centralité Betweenness et le degré de noeud pour identifier un ensemble de - 84 - noeuds représentatifs et Zaidi Melançon. Tous les autres nœuds sont assignés les noeuds représentatifs les plus proches pour obtenir des grappes. Le processus d'agglomération est répétée pour obtenir une classification hiérarchique qu'ils appellent à plusieurs niveaux de maillage. A chaque niveau, l'utilisateur choisit un facteur de ramification qui détermine le nombre de grappes pour ce niveau. Ce nombre pourrait ne pas représenter le nombre réel de clusters dans l'ensemble de données telles qu'elles sont déterminées par l'utilisateur sans l'utilisation d'une mesure heuristique ou statistique. La complexité globale de l'algorithme est donnée par O (M2N). Boccaletti et al. (Boccaletti et al. (2007)) proposent une méthode de classification basée sur les propriétés de dispersion de synchronisation des oscillateurs de phase. A partir d'un état entièrement synchronisé du réseau, un changement dynamique dans les poids des interactions qui conservent des informations sur la distribution de betweenness original, donne une classification hiérarchique progressive qui détecte pleinement les communautés denses. Etant donné que le calcul initial de betweenness prend O (n2), les échelles de l'algorithme quadratique comme le nombre de noeuds augmente. Newman (Newman (2004)) présente un algorithme plus rapide regroupement hiérarchique agglomératif qui est basé sur une fonction de qualité appelée modularité Q. L'algorithme se joint à plusieurs reprises des communautés par paires, en choisissant à chaque étape, la jointure qui entraîne le plus grand in- pli de Q. la complexité temporelle de l'algorithme est donnée par O ((m + n) n) échelles quadratiquement en termes de nombre o f noeuds dans le graphe. Une classe importante d'algorithmes de regroupement appelé algorithmes Spectral Clustering ont suscité un intérêt considérable (Spielman et Teng (1996)). Le plus grand avantage de ces algorithmes est qu'ils sont capables de détecter des clusters sans une forme spécifique par rapport aux algorithmes classiques tels que k-means. En outre, ils sont bien adaptés pour les réseaux de grande taille ainsi. Mais ces algorithmes sont adaptés que des ensembles de données où les graphiques de similarité sont rares (Luxburg (2007)). Pour les graphiques ayant des propriétés libres d'échelle, où quelques noeuds sont reliés à un bon nombre de nœuds, les résultats sous forme de graphiques de similarité non rares. Un exemple du type de graphiques que nous avons avec petit monde et les propriétés libres d'échelle est illustré à la figure 1 (a). Le graphique est aménagé à l'aide d'une force dirigée algorithme (Hachul et Jünger (2005)). Ces algorithmes sont bien connus pour mettre des nœuds plus dense, plus proches les uns des autres et peu connectés noeuds distants les uns aux autres. De la figure, il est tout à fait clair que l'algorithme ne parvient pas à le faire avec petit monde et l'échelle des graphiques libres en raison de la présence de noeuds de degré élevé. Un autre retour de tirage des algorithmes de classification spectrale est que les résultats dépendent fortement du choix des paramètres initiaux et différents paramètres peut entraîner des changements importants dans le regroupement (Luxburg (2007)). Sélectionner- ing paramètres corrects, l'utilisateur doit être bien au courant sur les données et les grappes à générer qui peut être problématique. modéliser un réseau Wu et Huberman (Wu et Huberman (2004)) sous la forme d'un circuit électrique et l'algorithme de regroupement est basé sur la notion de chutes de tension sur les réseaux. L'idée est que chaque bord est considéré comme une résistance entre deux noeuds. En résolvant les équations de Kirchhoff (Alexander et Sadiku (2008)) la valeur de tension peut être obtenue pour chaque nœud. En utilisant cette valeur de tension, la communauté du nœud peut être déterminée. Bien que la durée totale de fonctionnement de l'algorithme est O (m + n), mais l'algorithme doit être répété un certain nombre de fois pour obtenir une certaine précision. Un autre algorithme qui donne de bons résultats en termes de temps d'exécution est basé sur une méthode heuristique qui optimise la modularité (Blondel et al. (2008)). L'algorithme n'utilise pas la modularité normalisée qui est considérée comme un défaut (Fortunato et Barthélemy (2007)). Des algorithmes efficaces pour les réseaux de clusters avec seulement de petites propriétés du monde ont été proposés comme (Auber et al (2003). van Ham et van Wijk (2004)). Ces systèmes fonctionnent bien - 85 - Décomposition topologiques pour Clustering haute vitesse si la topologie du réseau suit les petites propriétés du monde, mais ne parviennent pas à effectuer dans la pré- sence de propriétés libres d'échelle. Cela est dû au fait que dans un réseau sans échelle, quelques nœuds dominent les connexions ensemble des réseaux et rend difficile d'identifier les grappes. 3 topologiques décomposition des graphiques Dans cette section, nous décrivons une méthode introduite plus tôt par les auteurs (Zaidi et Melançon (2010)) pour détecter la présence de noeuds connectés à forte densité dans un réseau relativement rapide. La méthode est basée sur une technique de décomposition qui exploite le fait que des noeuds ayant un degré élevé sont responsables de maintenir les réseaux de grande taille, comme une seule composante connexe. Pour décomposer le réseau en plusieurs composantes, Maxd-Degree induite sous-graphes (Maxd-DIS) sont réalisés où Jmax-DIS est un sous-graphe induit construit par Ering consi- que les noeuds ayant un degré au plus d dans le graphique G. Mathématiquement pour un graphe G (V, E) où V est un ensemble de noeuds et E est un ensemble d'arêtes, la Maxd-DIS est défini comme G '(V', E ') de telle sorte que V' ⊆ V et E '⊆ E et ∀u ∈ V ', degG (u) ≤ d, où d peut avoir des valeurs comprises entre 0 et le degré de noeud maximal possible pour un réseau. On construit pour d = Maxd-DIS {0, 1, · · ·, MaxDeg} pour obtenir un ensemble de graphiques (G0, G1, · · ·, GMaxDeg). Construction d'un Maxd-DIS peut être réalisé en O (n) et si elle est répétée pour toutes les valeurs possibles de niveau de noeud, le procédé peut être réalisé en O (n * MaxDeg) temps où MaxDeg est le degré maximum possible d'un noeud dans le graphique G. Prenons l'exemple du réseau d'Auteur représenté sur la figure. 1. L'ensemble du réseau est représentée sur la Fig. 1 (a), alors que la figure 1. ( b) représente une petite partie étant porté où les noeuds encerclés représentent les noeuds connectés à forte densité ou plus précisément cliques. Fig. 1 (c) et (d) montrent des parties de la Max3-DIS et Max5-DIS dessinées à l'aide d'un algorithme dirigé de force (Hachul et Jünger (2005)). Dans ces deux chiffres, il est assez facile de détecter visuellement les cliques ou les noeuds connectés à forte densité. L'inspiration de l'algorithme de clustering vient de cette visualisation. Fig. 1 (c) et (d) montrent clairement que les noeuds sont déconnectés en l'absence de noeuds de degré élevé et ces composants déconnectés peuvent être facilement identifiés comme des sous-graphes. Nous soutenons que ces sous-graphes, le problème des clusters trouver peut être simplifiée en tant que problème de comptage de nombre d'arêtes et le nombre de noeuds dans un composant connecté. Calculer une composante connexe est un problème qui peut être résolu en O (n + m) temps. De même, le comptage des noeuds et des arêtes peut aussi tourner dans le temps linéaire. En gardant à l'esprit que n et m peut être très faible en fonction de la valeur de d choisie, cette étape va assez vite alors le pire des cas, car il n'y a qu'un nombre limité de noeuds et des arêtes dans Maxd-DIS par rapport au graphique complet G . 4 Méthode proposée Clustering: TDHC de la décomposition topologique, l'idée de construire un algorithme de regroupement est assez intuitif. Etant donné que dans ces sous-graphes, nous pouvons identifier l'ensemble des noeuds qui sont connectés à forte densité à l'autre dans le temps rapide, ils peuvent être regroupés pour former des amas. Ainsi, un algorithme de classification hiérarchique peut être construit qui calcule la Maxd-DIS pour faire varier les valeurs de d et les groupes à forte densité de noeuds connectés. La notion de la définition est-densité traitée plus loin dans cette section. Le nombre d'itérations ne dépendent pas de n ou m de G mais d'un facteur de degré maximum d'un noeud peut avoir dans G. En plus de la détection de noeuds connectés à forte densité par l'intermédiaire - 86 - Zaidi et Melançon FIG. 1 - Co-Authorship réseau (a) Tout le réseau (b) Mise au point sur une petite portion (c) Une partie de Max3-DIS (d) Une partie de Max5-DIS Maxd-DIS, nous introduisons également plusieurs heuristiques qui permettent d'optimiser les performances du algorithme en cluster. Notez que les heuristiques améliorent que la vitesse de convergence de l'algorithme à un cluster unique, et l'algorithme de base peuvent être exécutées sans utiliser ces heuristiques. Toutes ces étapes sont très efficaces en termes de complexité de temps et sont discutés ci-dessous. Lavabo à l'aide de K-Sink Fonctionnement: On définit l'opération K-Sink comme suit: Les noeuds ayant un degré 1 dans un réseau donne à penser qu'ils ne sont connectés à un seul noeud. Nous fusionnons les noeuds de 1 degré dans leurs voisins créant un nouveau nœud pour chaque fusion. Les nœuds de 1 degré fusionné dans les voisins sont appelés les Plombs. Les noeuds dans lequel les noeuds de 1 degré sont appelés se fondre les dolines. Cette opération est justifiée parce qu'un noeud 1 degré ne peut pas être mis en cluster avec un autre nœud comme il est simplement connecté à un seul nœud. Nous appelons cette opération, une opération 1-évier et il est illustré sur la Fig. 2 (a) lorsque le noeud 2 est le platine et le noeud 1 est le doline. Si deux noeuds ont un degré 1 et sont reliés les uns aux autres, ce qui signifie qu'ils sont déconnectés du reste du réseau et dans ce cas, soit du noeud peut être choisi pour être le platine et l'autre comme la doline. De même on définit une opération 2-Sink, considérons deux noeuds, le noeud d'exemple 2 et le noeud 3 à la fois, ayant un degré 2 (Fig. 2 (a)). Ils sont reliés les uns aux autres et à un autre noeud par exemple du noeud 1, avec un degré plus élevé, les noeuds 2 et 3 peuvent être sinked dans le noeud 1 car ils ne sont reliées l'une à l'autre ou noeud 1. Cette opération est illustrée à la Fig. 2 (a) et nous appelons cette opération 2-Sink comme type A. Tout comme dans le cas de 1-Sink, si nous trouvons un ensemble de nœuds chaque degré ayant exactement égale à 2 et reliés les uns aux autres, ce qui signifie qu'ils ne sont pas Conne DECT au reste du graphe, dans ce cas, un nœud quelconque peut être choisie pour être la doline et les deux autres noeuds à la platine. Un autre type d'opération 2-Sink, type B, est quand un noeud de degré 2, - 87 - Décomposition topologiques pour Clustering haute vitesse Fig. 2 - (a) l'opération K-Sink illustré avec une évier et les opérations 2-évier. (B) l'opération de serrage, où les noeuds 1 et 2 get déconnectée en laissant les autres noeuds connectés à forte densité. est reliée à deux autres noeuds de degré supérieur à 2. Indépendamment du fait que ces deux noeuds de haut niveau sont reliés les uns aux autres ou non, le noeud de degré deux ne peut être regroupé avec l'un de ces deux noeuds. Ce que nous faisons est simplement mis le nœud de deux degrés avec le voisin ayant un plus haut degré et de créer un avantage entre ce groupe et l'autre voisin. Pour la mise en oeuvre de l'algorithme, nous utilisons seulement 1 évier et opérations 2 évier AL- bien que l'idée peut être généralisée aux nœuds de puits jusqu'à une constante K. Les deux 1-évier et opérations 2-évier peut être effectuée en temps O (n). Mais une mise en œuvre généralisée au fonctionnement des entreprises in- K-Sink ne sera reste plus linéaire et puisque notre objectif est de maintenir la complexité temporelle limitée par une fonction linéaire ou aussi proche que possible d'une fonction linéaire, nous évitons d'utiliser une opération généralisée K-Sink . L'ordre dans lequel ces K-Sink opérations ont été formés per- est important, où le premier nous effectuons une opération 1-Sink, suivi d'un type A et type B Opérations 2-évier. Ensuite, l'opération de type A 2-Sink est répété finalement suivie d'une opération 1-évier. Rememeber, dans la plupart des réseaux du monde réel, la distribution des degrés de nœud est pas aléatoire, mais est exponentielle. Et il y a beaucoup de noeuds avec un faible degré de nœud et à seulement quelques noeuds avec un haut degré de nœud. L'idée de base l'opération K-Sink est desinged pour regrouper ces nœuds rapidement, étant que si un nœud est connecté à un seul nœud, il doit être mis en cluster immédiatement avec ce nœud. Nous répétons que cette heuristique est tout à fait logique et n'affecte pas la qualité du regroupement, à moins que les clusters singleton sont autorisés à générer, ce qui pourrait ne pas être intéressant pour un expert du domaine à analyser. Degré maximum Induced sous-graphe: L'étape suivante de l'algorithme est de créer un Maxd- DIS avec une petite valeur de d. En raison de cette faible valeur, le réseau peut se briser en plusieurs composants déconnectés les uns des autres comme représenté sur la Fig. 1 (c) et (d). Serrage: Déconnexion Connecté nœuds Librement: Après avoir obtenu le Maxd-DIS, nous effectuons une opération que nous appelons serrage. Nous examinons les noeuds ayant une degré dans cette sous-graphe et l'on enlève simplement les bords de connexion 1 degré noeuds du sous-graphe induit comme représenté sur la Fig. 2 (b). Ce processus nous aide à faire les composants connectés trouvés dans le sous-graphe plus dense. De plus, comme il est certain que les 1 noeuds degré dans les Jmax-DIS font réellement appartiennent au cluster du nœud avec lequel il est connecté, cette étape permet de garantir que les noeuds sont uniquement affectés à des clusters auxquels ils appartiennent. L'étape peut être facilement effectuée en temps O (n) où n peut avoir des petites valeurs par rapport à l'ensemble du graphe G. Calcul des composantes connexes: Une fois que nous avons la Maxd-DIS, on calcule tous les composants connectés dans le sous-graphe. Nous utilisons un algorithme de recherche première largeur (BFS) - 88 - Zaidi et Melançon à partir d'un nœud et itérer ses voisins pour trouver le composant connecté, il appartient. Une fois que nous avons identifié les noeuds connectés au noeud de départ, nous remettons en marche le BFS à partir d'un noeud qui n'a pas encore été visités. Les pistes de l'algorithme en O (n + m) temps. Le regroupement des composants plus dense: La dernière étape consiste à regrouper les composants connectés qui sont connectés les uns aux autres à forte densité. Nous expliquons comment évaluer si le Ponent ciales est assez dense plus loin dans cette section. Une fois que nous avons trouvé les composantes com- dans le sous-graphe, nous regroupons ces nœuds dans le graphique G. connectés dense Nous remplaçons ce groupe de noeuds avec un seul nœud G. bords multiples reliant cette nouvelle cl uster noeud à d'autres noeuds sont supprimés pour faire en sorte que le graphique reste simple. Nous considérons que des composants de taille supérieure à 2 noeuds à être regroupés ensemble. Clustering Algorithm: Maintenant que nous avons expliqué toutes les étapes nécessaires, la décomposition ical Topolog- et heuristiques pour Clustering (TDHC) est présenté comme algorithme 1. L'algorithme commence par le calcul d'un MaxD2-DIS pour rechercher des triangles représentant trois nœuds et reliés les uns aux autres. Pour les noeuds ayant un degré 1, ils se sinked à l'étape 1-évier et donc on n'a pas besoin d'exécuter l'algorithme de MaxD1-DIS. On notera que dans l'algorithme, lorsque l'étape est réalisée sur G, la taille de G en termes de nombre de noeuds diminue en tant que noeuds à l'intérieur de G sont regroupés à des groupes de formulaires. Algorithme 1 TDHC algorithme d'entrée G (V, E) d ← 2 incrément ← 1 tandis que Number_of_Nodes (G)> 1 do K-évier (G) G '= Create_Maxd-DIS (G) de serrage (G') Calculate_Connected_Component (G ') Group_Densely_Connected_Component (G ') d ← d + incrément fin tandis que toutes les étapes de traitement ont une complexité en temps linéaire comme indiqué dans les sections précédentes. Le nombre d'itérations nécessaires pour converger vers une solution ne dépend plus du nombre de nœuds ni les bords, mais le degré maximal d'un nœud peut avoir. De plus, comme dans l'algorithme donné, nous avons choisi un incrément de 1 à chaque itération, dans ce cas, l'algorithme exécute la plupart du temps d. Le choix de la valeur de l'incrément variable dépend de l'utilisateur, qui peut être augmentée en fonction de la façon dont les résultats varient en fonction de cette valeur. Un moyen de valeur d'incrément moins élevé nombre d'itérations, mais les risques dans les composants moins denses trouvés. À l'heure actuelle, nous avons maintenu la valeur d'incrément à 1, mais nous avons l'intention d'expérimenter avec ce paramètre dans l'avenir pour étudier la variation de la qualité des grappes produits. La complexité du temps de cas moyen de l'algorithme complet peut être exprimé AsO (d * (m + n)) où d est le degré maximal d'un noeud dans le graphique G. Une observation importante de l'algorithme de regroupement est qu'il utilise à la fois le Divisive que ainsi que Ascendante approches graphiques du cluster. La partie vient de division du fait que nous construisons degré sous-graphes induits - 89 - Décomposition topologiques pour grande vitesse Clustering et la partie est représentée agglomératif lorsque nous nœuds de cluster pendant le fonctionnement K-Sink et le regroupement des composants plus dense. Aplatissant les clusters: La classification hiérarchique ainsi produit peut avoir plusieurs pôles avec 2 ou 3 noeuds en raison de l'opération K-Sink expliqué précédemment. Nous analysons simplement récursive par différents groupes pour supprimer ces grappes de petite taille et de les fusionner en groupes de plus grande taille. Pour produire un regroupement partitionnel (à plat), en utilisant le même algorithme, tout ce que nous devons faire est de remplacer la condition dans l'algorithme où nous voulons converger vers un seul nœud par le nombre de groupes que nous voulons obtenir dans le réseau. Une fois que nous arrivons à ce numéro, nous pouvons aplatir la hiérarchie pour obtenir un regroupement partitionnel. Nous avons utilisé cette même approche pour comparer les résultats de l'algorithme de TDHC avec les autres algorithmes de regroupement. Densité Fonction: Il existe plusieurs définitions de la façon de calculer la densité d'un graphique (Melançon (2006)). Par souci de simplicité, nous utilisons le noeud de rapport de bordure (n / m) pour désigner la densité du graphe. Melançon (2006)) fait valoir que la densité d'un graphique varie en fonction du domaine d'application donnant des exemples du monde réel. Pour l'algorithme de regroupement proposé, nous utilisons une fonction de densité pour déterminer à quel point un ensemble de noeuds est connecté à l'autre. Sur la base des arguments et des exemples fournis dans (Melançon (2006)), nous soutenons que nous ne pouvons pas avoir un ensemble de valeurs de densité générique comme seuil pour décider si un ensemble de noeuds est assez connecté ou non. En outre, la question de savoir si un ensemble de noeuds sont connectés assez bien pour être en cluster, dépend non seulement de la densité de l'ensemble graphique, mais sur la structure sous-jacente du réseau ainsi. Pour résoudre ce problème, nous PROP Ose une fonction de densité flottante à-dire, nous proposons un ensemble de fonctions à partir de valeurs de densité élevée à des fonctions progressivement moins denses. L'idée est d'essayer de trouver des communautés très denses en premier lieu, pour toutes les valeurs possibles de la Maxd-DIS, puis remplacer la fonction de densité avec une fonction moins dense. Nous commençons par chercher le nombre maximum d'arêtes possibles pour un ensemble de noeuds et finissent par se retrouver à la recherche du nombre minimal d'arêtes possibles pour un ensemble de noeuds à connecter. Nous cluster un ensemble de noeuds si le nombre d'arêtes m est égal à: m = n (n-1) / 2 m ≥ n (n-1) * 0,9 / 2 m ≥ n (n-1) * 0,6 / 2 m ≥ n (n-1) * 0,4 / 2 m ≥ (1,5 * n) - 0,5 m ≥ n l'ensemble des équations représentent une diminution progressive de la densité de noeuds bord requis pour un groupe de noeuds à considérer comme suffisamment dense pour être regroupés. Bien que l'idée en utilisant l'équation flottante peut effectuer le nombre d'itérations nécessaires pour regrouper l'ensemble des données, mais il nous assure que les grappes trouvées seraient denses. Ceci est le seul paramètre de contrôle qui est requis par l'algorithme proposé et varie d'un ensemble de données à l'autre. La complexité globale de l'algorithme reste le même que le nombre de gammes d'équations d'une valeur constante de 2 à 6. 5 ensembles de données Expérimentation: Le premier ensemble de données est le réseau copaternité de scientifiques travaillant sur la théorie des réseaux et des expériences ( Newman (2006)). Le deuxième ensemble de données est un ensemble de données de mappage de réseau qui se compose de trajets à partir d'un hôte de test vers d'autres réseaux sur l'Internet contenant ING rout- et des informations d'accessibilité (www.opte.org). Depuis le Divisive Clustering algorithme a une complexité de temps, nous considérons qu'un sous-ensemble des données réelles avec 1049 nœuds et - 90 - Zaidi et Melançon 1319 bords. Le troisième ensemble de données est un réseau d'interactions de protéine utilisée par (Gavin (2002)). Les données sont disponibles sur le site Web (http://dip.doe-mbi.ucla.edu/dip) et contient 1246 nœuds et 3142 arêtes. noeuds déconnectés (80 nœuds) ont été retirés à partir des données. Le choix de ces ensembles de données sont basées sur les critères que tous ces réseaux appartiennent à une classification différente des réseaux tels que décrits dans la littérature (Newman (2003)). Le réseau auteur représente un réseau social de la collaboration, le réseau Internet représente un réseau technologique et le réseau de protéines représente un réseau biologique. Tous ces réseaux ont une exponentielle (pas nécessairement suivant une loi de puissance) de distribution de degré. Le coef ficients Clustering du réseau auteur est 0,74 et la longueur du trajet moyen est 6,04, celle du réseau Internet est de 0,005 et 6,42, et enfin pour le réseau de protéines est de 0,23 et 4,89 respectivement. Clustering algorithmes: à se regrouper ces ensembles de données, nous utilisons deux connu regroupement algorithmes de l'algorithme Bissectrice K-Means (. Steinbach et al (2000)) et l'algorithme de clustering Divisive basé sur le bord Centralité (Girvan et Newman (2002)) . Le choix de ces algorithmes est basée sur les critères que ces algorithmes ne tentent pas d'optimiser ou d'influencer l'algorithme de classification en fonction de la densité ou une autre mesure de qualité de cluster par rapport à d'autres algorithmes présents dans la littérature comme (Newman (2004 )). En outre, ils sont connus pour bien performer pour un certain nombre d'ensembles de données du monde réel (Girvan et Newman (2002)). Nous utilisons également la force Clustering algorithme proposé par (Auber et al. (2003)). L'algorithme a été montré que de bons résultats pour l'identification des composants connectés à forte densité que les clusters. Cluster d'évaluation Metrics: Pour évaluer la qualité de la mise en grappes produits, nous utilisons les mesures suivantes. La modularité (Q) (Newman et Girvan (2004)) (Q métrique) est une métrique qui mesure la fraction des arêtes dans le réseau que des bords de connexion au sein de la collectivité, moins la valeur attendue de la même quantité dans un réseau avec la même communauté divisions, mais les connexions aléatoires entre les sommets. Si le nombre d'intérieur communautaire arêtes est pas mieux que aléatoire, nous obtenons Q = 0. Les valeurs approchant Q = 1, est le maximum, indique la structure communautaire forte. La seconde métrique utilisée par Auber et al. (Auber et al. (2003)) est appelée MQ métrique. Il se compose de deux facteurs où le premier terme contribue au poids positif représenté par la valeur moyenne de densité de bord à l'intérieur de chaque grappe. Le second terme contribue en tant que poids négatif et représente la valeur moyenne de densité de bord entre les grappes. Enfin, la densité relative (RD) (Mihail et al. (2002)) d'un groupe calcule le rapport de la densité de bord à l'intérieur d'une grappe à la somme des densités de bord à l'intérieur et l'extérieur de ce groupe. La RD finale est la somme moyenne des ces différentes densités relatives pour tous les groupes. 6 Résultats et discussion Comme le montrent les sections précédentes du temps de cas moyen de l'algorithme de complexité est O (d * (m + n)), mais en réalité, les pistes de l'algorithme beaucoup plus rapide que son cas en moyenne. Ceci est parce que les progrès de l'algorithme, les noeuds sont regroupés en grappes et la taille du réseau devient plus petit. Nous comparons les résultats de l'algorithme de clustering avec TDHC (Girvan et Newman (2002), Newman (2004), et al Auber (2003).) Au tableau 1. A partir des différentes valeurs, il est tout à fait clair que l'algorithme de TDHC performe aussi bien que les autres algorithmes de regroupement. Bien que l'utilisation de la métrique RD, ses performances ne sont pas aussi bon que l'autre groupement algorithmes de. Ces différences mettent en évidence le comportement des diverses mesures d'évaluation des clusters présents dans la littérature. Néanmoins, compte tenu de la complexité temporelle de TDHC par rapport à la - 91 - Décomposition topologiques pour grande vitesse Clustering Auteur Internet protéine algorithme MQ Q RD MQ Q RD MQ Q RD Div. Clus. 0,77 0,63 0,32 0,53 0,79 0,69 0,31 0,63 0,49 Bis. K-Means 0,77 0,63 0,41 0,42 0,59 0,58 0,41 0,33 0,31 Force 0,26 0,23 0,50 0,83 0,35 0,55 0,52 0,16 0,29 0,82 0,42 TDHC 0,42 0,55 0,85 0,49 0,38 0,44 0,23 TAB. 1 - Résultats de Divisive Clustering basé sur la distribution Edge (Div Clus..), Bissectrice K-Means (Bis K-Means.) Et des algorithmes de clustering force avec l'algorithme de TDHC. FIGUE. 3 - linéaire Durée de l'algorithme de TDHC avec l'augmentation de la taille du graphique. d'autres algorithmes, les résultats empiriques montrent de TDHC que l'algorithme donne de bons résultats sur des ensembles de données dif- férents. Nous ne prétendons pas que notre algorithme produit de meilleurs résultats de qualité pour différents types de réseaux et techniques d'évaluation du cluster, mais nous montrons que notre algorithme fonctionne aussi bien que d'autres algorithmes. La contribution majeure de l'algorithme est la faible complexité temporelle asymptotique qui nous permet d'exécuter l'algorithme pour les réseaux de grande taille. La figure 3 montre le temps d'exécution de l'algorithme TDHC pour les graphes de taille croissante en termes de nombre de nœuds. Les graphiques ont été produits en utilisant le modèle de génération de réseau artificiel pour petit monde et graphiques libres d'échelle en utilisant le modèle de Klemm et Eguiluz (2002). L'analyse de l'algorithme, nous essayons d'exploiter deux caractéristiques importantes des réseaux, la distribution des degrés et le coefficient de clustering. La décomposition topologiques utilise le fait que les réseaux du monde réel ne sont pas une distribution uniforme de degré, ainsi la décomposition contribue à briser le réseau en plusieurs composants. Et d'autre part, les réseaux ayant un haut coefficient de classification représentent la présence de noeuds connectés à forte densité du réseau, qui peuvent être regroupées pour former des grappes. L'idée de la fonction flottante de densité fonctionne bien pour les réseaux qui n'ont pas haut coefficient de clustering (Voir réseau Internet) que nous essayons de nœuds de groupe qui sont connectés moins denses. Les résultats montrent que l'algorithme fonctionne bien pour les différents types de réseaux. 7 Conclusion et futures recherches Dans cet article, nous avons utilisé heuristiques et une technique basée sur la position topologique du réseau décom- pour développer un algorithme de clustering à haute vitesse. La faible asymptotique - 92 - Zaidi et complexité temporelle de l'algorithme Melançon ouvre de nouveaux horizons au domaine de l'analyse du réseau et le regroupement. Comme le montrent les résultats, les exécute algorithme proposé, ainsi que d'autres algorithmes existants en termes de précision, mais en grande partie sur les pose en termes de complexité de temps. De cette étude, il y a beaucoup de questions qui doivent être examinées plus en détail et présente de nouvelles opportunités stimulantes de recherche. Par exemple l'opération K-Sink comme un utilitaire important de réduire la complexité des réseaux libres d'échelle et les regroupant basée sur cette opération uniquement. Les Jmax-DIS comme une décomposition importante de petits réseaux mondiaux pour le regroupement. Nous avons l'intention d'effectuer une étude approfondie en utilisant la position topologique présentée et décom- attendre à trouver de nouveaux résultats intéressants. Références Alexander, C. et M. Sadiku (2008). Principes de base des circuits électriques. McGraw-Hill. Auber, D., Y. Chiricota, F. Jourdan et G. Melancon (2003). visualisation multi-échelles des petits réseaux mondiaux. En InfoVis '03: Actes du Symposium IEEE sur la visualisation de l'information, pp 75-81.. Barabási, A. L. et R. Albert (1999). Emergence de mise à l'échelle dans les réseaux aléatoires. ence fiques 286 (5439), 509-512. Blondel, V. D., J.-L. Guillaume, R. Lambiotte, et E. Lefebvre (2008). déploiement rapide des communautés dans les grands réseaux. J. Stat. Mech. 2008 (10), P10008 +. Boccaletti, S., M. Ivanchenko, V. Latora, A. Pluchino et A. Rapisarda (2007). La détection de modularité complexe des réseaux par le regroupement dynamique. Physical Review E 75. Fortunato, S. et M. Barthélemy (2007). limite de résolution dans la détection de la communauté. Ings Proceed- de l'Académie nationale des sciences 104 (1), 36-41. Gavin (2002). organisation fonctionnelle du protéome de levure par une analyse systématique des complexes de protéines. Nature 415 (6868), 141-147. Girvan, M. et M. E. J. Newman (2002). Structure communautaire dans les réseaux sociaux et biologiques. Proc. Natl. Acad. Sci. USA 99, 8271-8276. Hachul, S. et M. Jünger (2005). Dessin grands graphes avec un algorithme multi-niveaux sur le terrain-potentiel. Graphique Dessin, 285-295. Klemm, K. et V. M. Eguiluz (2002). De plus en plus des réseaux sans échelle avec un petit comportement mondial. Physical Review E 65, 057102. Luxburg, U. (2007). Tutoriel sur la classification spectrale. Statistiques et informatique 17 (4), 395-416. Melançon, G. (2006). Juste combien denses graphiques denses dans le monde réel ?: une note méthodologique. Dans BELIV '06: Proc. de l'atelier AVI 2006 sur le temps et les erreurs au-delà, pp. 1-7. Mihail, M., C. Gkantsidis, A. Saberi et E. Zegura (2002). Sur la sémantique des topologies Internet, gitcc0207. Rapport technique, College of Comp., Institute of Georgia Tech., États-Unis. Newman, M. E. (2001). réseaux de collaboration scientifique. je. la construction du réseau et les résultats fonda- mentaux. Phys Rev E Stat Nonlin matière molle Phys 64 (1 Pt 2). Newman, M. E. et M. Girvan (2004). La recherche et l'évaluation de la structure des communautés dans les travaux Net-. Phys Rev E Stat Nonlin matière molle Phys 69 (2 Pt 2). - 93 - pour Décomposition topologiques haute vitesse Clustering Newman, M. E. J. (2003). Structure et fonction des réseaux complexes. SIAM Review 45, 167. Newman, M. E. J. (2004). algorithme rapide pour la détection de la structure des communautés dans les réseaux. Physical Review E 69, 066133. Newman, M. E. J. (2006). Trouver la structure des communautés dans les réseaux utilisant les matrices de vecteurs propres. Physical Review E (statistiques, non-linéaire, et physique de la matière souple) 74 (3). Schaeffer, S. E. (2007). Graphique en cluster. Computer Science Review 1 (1), 27-64. Spielman, D. A. et S.-H. Teng (1996). travaux de partitionnement spectraux: graphes planaires et maillages éléments finis. Dans Au Symposium IEEE sur les fondations de l'informatique, pp. 96-105. Steinbach, M., G. Karypis et V. Kumar (2000). Une comparaison des des techniques de classification des documents. Rapport technique, Département des sciences informatiques et en génie, Univ. du Minnesota. van Ham, F. et J. van Wijk (2004). La visualisation interactive des petits graphiques du monde. En InfoVis 2004. Symposium IEEE sur la visualisation de l'information, pp. 199-206. Watts, D. J. et S. H. Strogatz (1998). Les dynamiques collectives des réseaux « petits monde ». La nature 393, 440-442. Wu, A. Y., M. Garland, et J. Han (2004). Exploitation minière réseaux sans échelle en utilisant géodésique cluster- ment. Dans KDD '04: Proc. de SIGKDD, pp. 719-724. Wu, F. et B. A. Huberman (2004). Trouver les communautés dans le temps linéaire: Une approche de la physique. Le Journal européen pour la physique B 38, 331-338. publication informelle. Zaidi, F. et G. Melançon (2010). Identification de la présence des communautés dans les réseaux complexes par décomposition et topologiques des composants Densités. En EGC 2010, Extraction et Gestion de Connaissance, volume E-19, RNTI. 163-174. Avec l'résumé de la Exponentiel Accroissement des taille et des réseaux Données, il DEVIENT né- cessaire de develop des techniques et nouvelles d'analyse et rapides d'exploration de bureaux réseaux. De plus, l'Émergence de petit monde du Propriétés et Graphes sans les réseaux Dans échelle du monde un réel STIMULE l'activité grandement Dans le domaine de l'analyse de l'exploitation d'et réseau des Données. Le Regroupement technique Fondamentale Une demeure verser ex plorer et organisateur bureaux réseaux. La Consiste à Difficulté Trouver un algorithme de regroupement Qui bien en Fonctionne Termes de qualité de et Qui Regroupement en Soit Termes de Efficace de temps Complexité. Dans this article, nous proposons un algorithme de regroupement rapide Qui combinent cer- Taine heuristiques with a verser obtain Décomposition un topologique regroupement. L'al gorithme nous appelons Que et Heuristiques Décomposition topologique verser Regroupement (TDHC) est tres en Annoter de Efficace Complexité de temps asymptotique aux Autres algorithmes comparé Dans la litérature existant. Nous introduisons un also Heuristiques D'Nombre de postes verser l'Agenda item algorithme de la accroit Qui Regroupement du Processus de vitesse en regroupement la haute qualité Maintenant du regroupement. Nous montrons de la l 'efficacité de méthode sur proposed Regroupement de Données Différentes séries du monde et nous comparons réel SES Résultats Avec des algorithmes de bien connus regroupement. - 94 -"
495,Revue des Nouvelles Technologies de l'Information,EGC,2012,User Evaluation: Why?,"Research in information visualisation has changed significantly in the past two decades.Once it was sufficient to simply design and implement an impressive visualisation system.Today editors and reviewers expect papers to present not only a novel system, but empiricalevidence of its worth. Why has this change come about, and what impact has it had on thoseworking in this area? This talk will discuss how a field dominated by algorithms and toolsbecame infected by human participants, and why this is a positive development in a maturingresearch discipline.",Helen Purchase,http://editions-rnti.fr/render_pdf.php?p1&p=1001132,http://editions-rnti.fr/render_pdf.php?p=1001132,en,"Évaluation de l'utilisateur: Pourquoi? Helen Achat * * Université de Glasgow hcp@dcs.gla.ac.uk, http://www.dcs.gla.ac.uk/~hcp/ Résumé de la recherche en matière de visualisation de l'information a changé au cours des deux dernières décennies. Une fois qu'il suffisait simplement concevoir et mettre en œuvre un système de visualisation impressionnante. Aujourd'hui, les rédacteurs et les réviseurs attendent des documents à présenter non seulement un nouveau système, mais des preuves empiriques de sa valeur. Pourquoi ce changement se, et quel impact at-il eu sur les personnes qui travaillent dans ce domaine? Cette conférence expliquera comment un domaine dominé par des algorithmes et des outils ont été infectés par les participants humains, et pourquoi il en est un développement positif dans une discipline de recherche de maturation. Bibliographie Dr Helen Achat est maître de conférences à l'École des sciences informatiques à l'Université de Glasgow. Elle a travaillé dans le domaine des études empiriques de la mise en page du graphique pour plusieurs années, et a également des intérêts de recherche dans l'esthétique visuelle, conception empirique basée sur les tâches, l'apprentissage collaboratif dans l'enseignement supérieur, et des outils d'esquisse pour la conception. Elle est en train d'écrire un livre sur les méthodes empiriques pour la recherche HCI. - 7 -"
522,Revue des Nouvelles Technologies de l'Information,EGC,2011,Closed-set-based Discovery of Representative Association Rules Revisited,"The output of an association rule miner is often huge in practice. This is why several concise lossless representations have been proposed, such as the “essential” or “representative” rules. We revisit the algorithm given by Kryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS 2189, 350–359) for mining representative rules. We show that its output is sometimes incomplete, due to an oversight in its mathematical validation, and we propose an alternative complete generator that works within only slightly larger running times.","José L Balcazar , Cristina Tîrnauca",http://editions-rnti.fr/render_pdf.php?p1&p=1001032,http://editions-rnti.fr/render_pdf.php?p=1001032,en,"Découverte à base ensemble fermé de l'Association Représentant Règles Revisited José L Balcázar, Cristina Tîrnăucă Departamento de Matemáticas, Estadística y Computación Universidad de Cantabria, Santander, Espagne {joseluis.balcazar, cristina.tirnauca}@unican.es~~V~~singular~~3rd Résumé. La sortie d'un mineur de règle d'association est souvent énorme dans la pratique. Voilà pourquoi plusieurs représentations de concises ont été proposées, comme le « essentiel » ou « représentatives » des règles. Nous revisitons l'algorithme donné par Kryszkiewicz (Int. Symp. L'analyse intelligente des données 2001, Springer-Verlag LNCS 2189, 350-359) des règles de représentation minière. Nous montrons que sa sortie est incomplète par- fois, en raison d'une erreur dans la validation mathématique, et nous vous proposons un générateur complet alternatif qui fonctionne dans les temps en cours d'exécution légèrement plus grandes. 1 Introduction Association minière de la règle est parmi les plus populaires des outils conceptuels dans le domaine des mines de données. Nous sommes intéressés par le processus de découverte et de représenter régularités entre les séries d'articles en grandes quantités de données transactionnelles à grande échelle. Syntaxiquement, la représentation de la règle d'association a la forme d'une implication, X → Y; Cependant, alors que dans la logique d'une telle expression est vraie si et seulement si Y détient chaque fois que X fait, une règle d'association est une implication partielle, dans le sens où il suffit que Y détient la plupart du temps X ne. Doter règles d'association avec une sémantique définitive, nous devons nous précise comment cette intuition de « la plupart du temps » est formalisée. Il y a beaucoup de propositions pour cette tion formalisation. L'une des mesures fréquemment utilisées d'intensité de ce genre d'implication partielle est sa confiance: le rapport entre le nombre de transactions dans lesquelles X et Y sont vus ensemble et le nombre de transactions qui contiennent X. Dans la plupart des cas d'application, l'espace de recherche est en outre limité aux règles d'association qui répondent à un critère de soutien minimal, donc ing évitant la génération de règles à partir d'éléments qui apparaissent très rarement ensemble dans l'ensemble de données (définitions formelles de soutien et de confiance sont donnés dans la section 2.1). De nombreux mineurs de règles d'association existe, Apriori (voir Agrawal et al. (1996)) étant l'un des plus discuté et utilisé. Le problème majeur partagé par tous les algorithmes d'extraction est que, dans la pratique, même pour des seuils de soutien raisonnables et confiance, la sortie est souvent énorme. Par conséquent, plusieurs représentations de concises de l'ensemble des règles d'association ont été proposées. Ces représentations sont basées sur différentes notions de « redondance ». Dans l'un de ceux-ci, une règle est redondante s'il est possible de calculer exactement la confiance et le soutien d'autres informations telles que les documents confidentiels et supports d'autres règles d'information (voir Association fermée ensemble basée sur la découverte du représentant Règles Revisited Luxenburger (1991) ; Kryszkiewicz (2002), Hamrouni et al (2008)). c'est un établissement très exigeant. Nous nous installons pour une version plus faible proposée dans plusieurs ouvrages; de manière informelle, dans cette version, une règle est redondante par rapport à un autre si sa confiance et de soutien sont toujours plus, dans un ensemble de données. Pour éviter cette redondance, exactement une notion a été identifiée dans plusieurs sources, à savoir les règles représentatives (définitions précises et des références sont données ci-dessous). Nous nous concentrons dans cet article sur les principaux résultats de Kryszkiewicz (2001), où un algorithme supposément plus rapide pour construire des règles de représentation est donnée, et montrer par un exemple que cet algorithme n'est pas garanti toujours sortie toutes les règles de représentation, car il est basé sur une propriété qui ne tient pas en général; à savoir la caractérisation des ensembles fermés fréquents qui admettent une décomposition en règles représentatives manque certains de ces ensembles. Nous vous proposons une alternative, la caractérisation complète, qui nous conduit à la proposition d'un premier algorithme alternatif qui est garanti à toutes les règles de sortie représentatives: nous précalculer, pour chaque ensemble fermé, certains paramètres qui dépendent des seuils de confiance et de soutien, et ensuite utiliser la nouvelle caractérisation mentionnée ci-dessus pour générer toutes les règles représentatives. Par rapport à l'algorithme précédent, potentiellement incomplètes Kryszkiewicz (2001), cet algorithme, garanti pour être complet, a un inconvénient majeur: en Kryszkiewicz (2001), les paramètres locaux internes ne dépendent que du seuil de soutien, mais dans notre algorithme ces paramètres dépendent aussi de la confiance. Par conséquent, chaque fois qu'un nouveau seuil de confiance est introduit par l'utilisateur, l'algorithme doit refaire tous les calculs. Ainsi, nous fournissons un second algorithme, composé de deux parties: la première est une phase de pré-traitement, dépendant uniquement de support, dans lequel une subdivision de l'intervalle (0, 1] est associé à chaque jeu d'éléments fermé, et la seconde partie utilise cette partition pour déterminer, pour une valeur donnée du seuil de confiance, qui sont les ensembles qui peuvent générer des règles de représentation. Il y a quelques différences subtiles entre l'une des définitions habituelles de la règle d'association (celle que nous d'emploi) et la un en Kryszkiewicz (2001). tout d'abord, nous interdisons d'avoir des règles avec antécédent vide (clairement, tous ont confiance égale à l'appui normalisé du conséquent). de plus, nous ne demandons pas les inégalités d'être stricte en imposant une donnée le soutien et le seuil de confiance. Ceci est juste un petit détail qui est à portée de main lorsque l'utilisateur est intéressé à obtenir l'ensemble des règles représentatives de la confiance 1. Toutefois, nous avons soigneusement réglé tous nos argumentations de telle sorte que ces différences ne sont pas pertinentes; Par exemple, nous avons choisi un contre qui invalide la propriété 9 de Kryszkiewicz (2001) indépendamment de celle des deux définitions est utilisé. L'article est structuré comme suit. Dans la section 2, nous présentons les notions de base et nota- tions qui seront utilisés dans le papier et une partie du contenu de Kryszkiewicz (2001); et nous montrons que l'algorithme fourni il n'y a pas garanti de toujours fournir l'ensemble des règles de représentation. Dans la section 3, nous définissons de nouveaux paramètres et de discuter de leur utilité pour générer l'ensemble des règles de représentation, fournissant également des algorithmes efficaces pour cette tâche. L'article 4 contient une comparaison de notre approche avec celle Kryszkiewicz (2001) sur certains jeux de données. Conclusions et d'autres sujets de recherche sont présentés dans la section 5. 2 Préliminaires Un ensemble donné d'éléments disponibles U SUPPOSANT; des sous-ensembles de celui-ci sont appelés itemsets. Nous noterons par itemsets lettres majuscules de la fin de l'alphabet, et la juxtaposition d'utilisation pour désigner l'union, comme dans XY. Le signe d'inclusion tel que dans X ⊂ Y désigne sous-ensemble, alors impropre J. L. et C. Balcázar inclusion Tîrnăucă est notée X ⊆ Y. Pour une donnée ensemble de données D, comprenant des opérations de n, dont chacun est un jeu d'éléments marqués avec un identificateur de transaction unique, nous définissons le support sup (X) d'un jeu d'éléments X comme étant le rapport entre la cardinalité de l'ensemble de transactions qui contiennent X et le nombre total de transactions n. Un jeu d'éléments X est appelé fréquemment si son support est supérieur ou égal à un seuil τ ∈ (0 défini par l'utilisateur, 1]. Etant donné un ensemble X ⊆ U, la fermeture X de X est l'ensemble maximal (par rapport à l'ensemble inclusion) y ⊆ U tel que X ⊆ y et sup (X) = sup (y). Il est facile de voir que X est définie de manière unique. on dit qu'un ensemble X ⊆ U est fermé si X = X. opérateurs de fermeture sont caractérisé par les trois propriétés de monotonicité X ⊆ X, la puissance idem- X = X, et extensivité, X ⊆ Y si X ⊆ Y. les intersections d'ensembles fermés sont fermés. un générateur minimal est un ensemble X pour lequel tous les sous-ensembles propres ont fermetures différent de la fermeture de X (qui est, X est un générateur minimal si et seulement si sup (Y)> sup (X) pour tous les Y ⊂ X). On note Fτ = {X ⊆ U sup (X) ≥ τ} l'ensemble de tous itemsets fréquents. de plus, le FC τ = {X ∈ Fτ X = X} représente l'ensemble de tous les ensembles fermés fréquents et FGτ = {X ∈ Fτ ∀Y ⊂ X, sup ( Y)> sup (X)} est la ensemble de tous les fréquents générateurs minimaux. Notez que le FC τ constitue une représentation sans perte concise des itemsets fréquents, car connaissant le soutien de tous les ensembles de FC τ est suffisant pour récupérer le soutien de tous les ensembles de Fτ. Exemple 1 Soit D l'ensemble de données représentées dans le tableau 1 où l'univers U d'attributs est {a, b, c, d, e, f} et examiner τ = 0,15. Il est clair que tous les sous-ensembles de U sont fréquentes, FC τ = {∅, a, b, c, ab, ac, ad, bc, abcde, abcdef} et FGτ = {∅, a, b, c, d, e, f , ab, ac, bc, bd, cd, abc} (nous abusons la notation et ensembles désignons par la juxtaposition de leurs éléments constitutifs). LANGUETTE. 1 - Dataset D abcdef 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 2.1 Règles et règles représentant de l'Association Comme X Fτ, deux définitions, avec des noms plus longs, sont introduits dans Kryszkiewicz (2001): mxsτ (X) = max ({sup (Z) | Z ∈ FC τ, Z ⊃ X} ∪ {0}), mnsτ (X) = min ({sup (Y) | Y ∈ FGτ, Y ⊂ X} ∪ {∞}). C'est, mxsτ (X) représente le soutien maximal de tous surensembles fermés fréquents propres de X et mnsτ (X) est le support minimum de générateurs minimaux qui sont des sous-ensembles appropriés de X. Extra 0 et ∞ sont ajoutés pour sont définis faire en sorte que mxsτ (X) et mnsτ (X), même pour les cas où X n'a ​​pas surensembles appropriés qui sont fréquents et fermés, ou quand il ne sous-ensembles ne pas avoir appropriés que sont générateurs minimaux. Il est facile de vérifier que mxsτ (X) ≤ sup (X) ≤ mnsτ (X). De plus, il peut être démontré que: fermé ensemble fondé sur la découverte de l'Association représentant les règles Revisited Proposition 1 (Kryszkiewicz (2001)) Soit X ∈ Fτ. Alors X ∈ FC τ ssi sup (X)> mxsτ (X), et X ∈ FGτ sup IFF (X) <mnsτ (X). Les types de règles d'association considérées dans ce travail sont les implications de la forme X → Y, où X, Y ⊆ U, Y = ∅ et X ∩ Y = ∅. En Kryszkiewicz (2001), les règles avec X = ∅ sont autorisés dis-, mais nous ne leur permettent que dans la pratique, ces règles jouent souvent un rôle utile lié à erings la recou-, décrit ci-dessous. La confiance de X → Y est conf (X → Y) = sup (XY) / sup (X) et son support est sup (X → Y) = sup (XY). Le problème de l'Association minière règles pour générer des con- siste toutes les règles qui répondent aux critères seuils minimaux de soutien et de confiance. Laissez ARτ, γ = {X → Y sup (X → Y) ≥ τ, CONF (X → Y) ≥ γ}. Depuis l'ensemble des règles d'association est tout à fait grand dans les applications du monde réel, un certain nombre de formalisations de la notion de redondance entre les règles d'association ont été mis en place (voir Aggarwal et Yu (2001); Balcázar (2010); Cristofor et Simovici (2002 ); Kryszkiewicz (1998b), Luxenburger (1991). Pasquier et al (2005), Phan-Luong (2001), Zaki (2004), l'enquête Kryszkiewicz (2002) et l'article 6 de Ceglar et Roddick (2006)) . Dans une approche commune, l'ensemble de couvercle C (X → Y) d'une règle X → Y est défini par C (X → Y) = {X → Y X ⊆ X et XY ⊇ X Y} . De telles règles X → Y sont redondants par rapport à X → Y dans le sens suivant: Proposition 2 (Kryszkiewicz (1998b), Aggarwal et Yu (2001)) Soit r: X → Y et r: X → Y des règles d'association. Ensuite, r ∈ C (r) implique sup (r) ≥ sup (r) et conf (r) ≥ conf (r). En fait, cette implication est une caractérisation complète, qui est, si X → Y a toujours au moins la même confiance et au moins le même support que X → Y il doit appartenir à l'ensemble de la couverture (voir Balcázar (2010) ). Éviter de tels licenciements conduit à l'ensemble RRτ, γ des règles d'association représentatives. Une règle r dans ARτ, γ est dit être représentative, ou parfois indispensable, si elle ne figure pas dans l'ensemble de couverture d'une autre règle dans ARτ, γ. RRτ, γ = {r ∈ ARτ, γ ∈ ∀r ARτ, γ (r ∈ C (r) ⇒ r = r)}. Sous des noms différents, cette notion a été proposé et étudié dans plusieurs sources, par exemple Aggarwal et Yu (2001); Kryszkiewicz (1998b); Phan-Luong (2001). Proposition 3 (Kryszkiewicz (1998a, b)) Les propriétés suivantes sont satisfaites: • RRτ, γ = {X → Y ∈ ARτ, γ ¬∃X → Y ∈ ARτ, γ, (X = X, XY ⊂ X Y) ou (X ⊃ X, XY = X Y)} • si X → Z \ X avec X ⊂ Z est en RRτ, puis γ ∈ Z FC τ et X ∈ FGτ. Par conséquent , Tout algorithme qui vise à la découverte de toutes les règles représentatives devraient con- Sider règles que de la forme X → Z \ X avec X ⊂ Z, Z ∈ FC τ et X ∈ FGτ. De toute évidence, tous les ensembles de FC τ peuvent être décomposées en manière telle, et il faut regarder que dans ceux qui le font. Exemple 2 Considérons l'ensemble de données dans l'exemple 1. L'annonce de jeu est à la fois fréquente et fermé, mais aucune des règles a → d, d → un ou ∅ → annonce représentatifs étant donné les seuils τ = 0,15 et γ = 0,33: a → d est dans l'ensemble de la couverture d'un → bd, d → un est dans l'ensemble de couverture d → ab et ∅ → ad est dans l'ensemble de la couverture de ∅ → abd. En outre, il est facile de vérifier que, à γ = 0,4, on peut obtenir des règles représentatives exactement sur les ensembles fermés suivants: ab, ac, ad, bc, ABCDE, et abcdef. JL Balcázar et C. Tîrnăucă Ainsi, si l'on désigne par RI τ, γ l'ensemble de tous itemsets fermés fréquents dont au moins une règle représentative peut se former, une approche possible de l'exploitation minière de la règle représentative est de synthétiser d'abord l'ensemble RI τ, γ, puis, pour chaque élément Z dans RI τ, γ, pour trouver un sous-ensemble non vide de telle sorte que X X → Z \ X est représentatif. C'est précisément l'idée derrière l'algorithme GenRR dans Kryszkiewicz (2001). Le problème est que la caractérisation de l'ensemble RI τ, γ donné par la proposition 9 du même document (à la page 355) est incorrect, peut-être en laissant quelques-uns des jeux qui peuvent conduire à des règles représentatives. A savoir, il est indiqué que RI T pour, γ = {X ∈ FC τ sup (X)> γ * mnsτ (X) ≥ mxsτ (X)}; l'inclusion de droite à gauche tient en effet, mais l'égalité ne tient pas en général, comme on peut le voir sur la suivante contre-. Exemple 3 Considérons le jeu d'éléments X = abcde dans l'exemple 1, et supposons τ = 0,15 et γ = 0,4. Vérifions que ABCDE ∈ RI τ, γ \ {X ∈ FC τ sup (X)> γ * mnsτ (X) ≥ mxsτ (X)}. De toute évidence, la règle b → acde est en ARτ, γ, ayant support 6/2 et 0,5 confiance. De plus, en étendant la droite ou de déplacer l'élément b à la droite, nous obtenons que les règles b → ACDEF, ∅ → ABCDE et ∅ → abcdef de confiance 1/4, 2/6 et 1/6, respectivement. Par conséquent, nous pouvons conclure que b ∈ → ACDE RRτ, γ. D'autre part, mxsτ (X) = 6/1 et mnsτ (X) = 6/2, de sorte que γ * mnsτ (X) = 0,8 / 6 est strictement inférieure à mxsτ (X). Dans ce cas, l'algorithme GenRR ne fonctionne pas correctement car il ne liste pas la règle b → ACDE comme représentant. Une alternative est donnée en contre-la preuve du lemme 1 ci-dessous. 3 Bounds que l'aide Caractériser règles représentatives L'objectif de la taille de jeux qui ne donnent pas de règles représentatives, en ne gardant que RI τ, γ, ne peut pas être atteint en utilisant les limites données, comme nous l'avons vu que cet ensemble comprend tous les X FC τ avec sup (X) ≥ γ * mnsτ (X)> mxsτ (X), mais peut également inclure d'autres ensembles fermés fréquents X qui ne satisfait pas la condition γ * mnsτ (X)> mxsτ (X). Nous considérons deux alternatives. 3.1 fermés au lieu de générateurs minimal pour X fermé, mnsτ (X) est presque la même chose que le soutien minimal parmi tous les sous-ensembles propres de X, ou encore parmi tous les sous-ensembles fermés appropriés de X; toutes ces notions coïncident lorsque X est son propre générateur minimal, sinon ils ne diffèrent que du fait des générateurs minimaux de X. Par conséquent, il est logique d'essayer et d'exclure les générateurs minimaux de X de la considération. De cette façon, nous obtenons un autre paramètre, bmnsτ (X) = min ({sup (Y) | Y ∈ FC τ, Y ⊂ X} ∪ {∞}). La valeur de bmnsτ est plus petite que jamais mnsτ comme nous le verrons bientôt. Ainsi, il y aura plus de jeux qui répondent à la condition γ * bmnsτ (X)> mxsτ (X). Proposition 4 Les propriétés suivantes. • bmnsτ (X) = min ({sup (Y) | Y ∈ FGτ, Y ⊂ X} ∪ {∞}), • mnsτ (X) ≤ bmnsτ (X), • si X ∈ FC τ ∩ FGτ puis mnsτ ( X) = bmnsτ (X), à base de jeu fermé Découverte de la preuve représentant règles de l'Association revisité. Nous omettons la preuve des deux premières demandes parce qu'ils sont simples. Alors, laissez-X un ensemble fermé fréquent qui est aussi un générateur minimal. Si X = ∅, alors mnsτ (X) = bmnsτ (X) = ∞. Dans le cas contraire, que Y ∈ FGτ est telle que Y ⊂ X et mnsτ (X) = sup (Y). De toute évidence, Y ∈ T pour FC et Y ⊆ X = X. Etant donné que X ∈ FGτ et Y ⊂ X, sup (Y)> sup (X) et donc sup (Y)> sup (X), et donc Y ⊂ X. Nous obtenons sup (Y) ≥ bmnsτ (X) et mnsτ (X) ≥ bmnsτ (X). La combinaison avec le fait que mnsτ (X) ≤ bmnsτ (X) tient toujours, nous concluons que mnsτ (X) = bmnsτ (X). Malheureusement, le nouveau paramètre peut laisser encore quelques ensembles en RI τ, γ. Lemme 1 RI τ, γ ⊆ {X ∈ FC τ sup (X)> γ * bmnsτ (X) ≥ mxsτ (X)}. Preuve. Soit u = {a, b, c} et D le jeu de données contenant les 13 opérations suivantes: t1 = · · · = t8 = abc, t9 = ab, t10 = t11 t12 = = a, t13 = b; supposer τ = 0,07 et γ = 0,7. On peut vérifier que, même si ab ∈ RI τ, γ (depuis a → b ∈ RRτ, γ), les deux bmnsτ (ab) = 10/13 et mnsτ (ab) = 10/13; mais γ * mnsτ (ab) = γ * bmnsτ (ab) = 7/13 <8/13 = mxsτ (ab). Les prochains spectacles de construction qu'en utilisant bmnsτ au lieu de mnsτ on peut même laisser quelques ensembles en RI τ, γ qui n'aurait pas été laissé autrement. Lemme 2 RI τ, γ ∩ {X ∈ FC τ sup (X)> γ * mnsτ (X) ≥ mxsτ (X)} ⊆ {X ∈ FC τ sup (X)> γ * bmnsτ (X ) ≥ mxsτ (X)}. Preuve. Soit u = {a, b, c, d, e} et D un ensemble de données contenant des 35 opérations: t1 = t2 = abcde, t3 = t4 = t5 = abcd, T6 · · · = t20 = a et t21 = · · · t35 = b. Choix τ = 0,05 et γ = 0,75. Notez que ab de cd ∈ RRτ, γ et donc ABCD ∈ RI τ, γ. Maintenant, mnsτ (ABCD) = 5/35, bmnsτ (ABCD) = 20/35, sup (ABCD) = 5/35 et mxsτ (ABCD) = 2/35. Bien que γ * mnsτ (abcd) = 3,5 / 0,1 = 35 appartient à l'intervalle [2/35, 5/35), γ * bmnsτ (abcd) = 15/35 ne fonctionne pas. 3.2 Les générateurs minimaux de Borné support Afin de donner une caractérisation complète pour l'ensemble RI τ, γ, laissez d'abord introduire la notation suivante: pour un ensemble X FC τ, laisser mxgsτ, γ (X) le soutien maximal de ceux générateurs minimaux qui sont inclus dans X et ne sont pas plus fréquentes que sup (X) / γ: mxgsτ, γ (X) = max (sup {(Y) Y ∈ FGτ, Y ⊂ X, γ * sup (Y) ≤ sup (X)} ∪ {0}). Notez que mxgsτ, γ (X) est soit 0, soit elle est supérieure ou égale à sup (X). Nous montrons deux propositions qui expliquent comment nous pouvons utiliser cette valeur pour calculer l'ensemble RI τ, γ et comment trouver, étant donné X ∈ RI τ, γ, un X0 sous-ensemble ⊂ X tel que X0 → X \ X0 ∈ RRτ, γ. Proposition 5 RI T pour, γ = {X ∈ FC τ γ * mxgsτ, γ (X)> mxsτ (X)}. Preuve. Soit X un ensemble arbitraire RI τ, γ et prendre X0 dans FGτ tels que X0 → X \ X0 ∈ RRτ, γ et X0 ⊂ X. Nous avons, d'une part, conf (X0 → X \ X0) ≥ γ, et d'autre part, conf (X0 → Z \ X0) <γ pour tout Z ∈ FC τ avec Z ⊃ X. Autrement dit, sup (X) ≥ γ * sup (X0)> sup (Z) pour tout z ∈ Z avec τ FC ⊃ X. De la première inégalité, on en déduit que X0 remplit toutes les conditions afin de prendre en compte pour le calcul de mxgsτ, γ (X), et par conséquent, mxgsτ, γ (X) ≥ sup (X0). De la seconde, nous obtenons γ * sup (X0)> mxsτ (X). Nous concluons que γ * mxgsτ, γ (X)> mxsτ (X). JL Balcázar et C. Tîrnăucă algorithme 1 RR Générateur 1: Entrée: seuil τ support, seuil de confiance γ 2: Fτ = {X ⊆ U sup (X) ≥ τ} 3: FC τ = {X ∈ X Fτ = X} 4: FGτ = {X ∈ Fτ ∀Y ⊂ X, sup (Y)> sup (X)} 5: pour tout X ∈ FGτ do 6: mnsτ (X) = min ({sup (Y) Y ∈ FGτ, Y ⊂ X} ∪ {∞}) 7: fin de 8: RI τ, γ = ∅ 9: pour tout X ∈ FC τ \ {∅} faire 10: mxsτ (X) = max ({ sup (Z) Z ∈ FC τ, Z ⊃ X} ∪ {0}) 11: mxgsτ, γ (X) = max (sup {(Y) Y ∈ FGτ, Y ⊂ X, γ * sup ( Y) ≤ sup (X)} ∪ {0}) 12: si γ * mxgsτ, γ (X)> mxsτ (X) puis 13: ajouter X RI τ, γ 14: end if 15: fin de 16: pour tout X ∈ RI τ, γ faire 17: c1 = mxsτ (X) / γ 18: c2 = sup (X) / γ 19: Ant = {X0 ∈ FGτ X0 ⊂ X, c1 <sup (X0) ≤ c2 <mnsτ (X0)} 20: pour tout X0 ∈ Ant do 21: X0 de sortie → X \ X0 22: fin de 23: fin de Inversement, soit X ∈ FC τ être tel que γ * mxgsτ, γ (X)> mxsτ (X). De toute évidence, mxgsτ, γ (X) ne peut pas être 0 (puisque mxsτ (X) ≥ 0), de sorte que Y ∈ {FGτ Y ⊂ X, γ * sup (Y) ≤ sup (X)} est non vide. Prendre X0 ∈ FGτ à un ensemble de support maximal qui satisfait X0 ⊂ X et γ * sup (X0) ≤ sup (X). Par conséquent, mxgsτ, γ (X) = sup (X0). Depuis sup (X0 → X \ X0) = sup (X) ≥ τ et conf (X0 → X \ X0) = sup (X) sup (X0) ≥ γ on en déduit que X0 → X \ X0 ∈ ARτ, γ. On notera que pour toute Z ⊃ X, conf (X0 → Z \ X0) = sup (Z) sup (X0) ≤ mxsτ (X) sup (X0) = MXS τ (X) mxgsτ, γ (x) <γ. De plus, pour tout X 0 ⊂ X0, sup (X 0)> sup (X0) (depuis X0 ∈ FGτ) et γ * sup (X 0)> sup (X) (en raison du choix que nous avons fait pour X0). C'est pourquoi CONF (X 0 → X \ X 0) = sup (X) sup (X0) <γ. Nous concluons que X0 → X \ X0 ∈ RRτ, γ et X ∈ RI τ, γ. Proposition 6 Soit X ∈ RI τ, γ, c1 = mxsτ (X) / γ, c2 = sup (X) / γ et X0 ⊂ X. Ensuite X0 → X \ X0 ∈ RRτ, γ si et seulement si c1 <sup (X0) ≤ c2 <mnsτ (X0). Preuve. Considérons X ∈ RI τ, γ et X0 ⊂ X. De toute évidence, X0 → X \ X0 ∈ RRτ, γ si et seulement si la règle X0 → X \ X0 est ARτ, γ et ne pas appartenir à l'ensemble de la couverture de toute autre règle ARτ, γ. Cela équivaut à: sup (X) ≥ τ, sup (X) sup (X0) ≥ γ, sup (X) sup (X0) <γ 0 pour tout X ⊂ X et sup (Z) sup (X0 ) <γ pour tous les Z ⊃ X qui satisfont sup (Z) ≥ τ. Maintenant, il est facile de voir que: fermé ensemble fondé sur la découverte du représentant de règles d'association revisité • sup (X) ≥ τ tient toujours parce que X ∈ FC τ, • sup (X) sup (X0) ≥ γ ⇔ sup (X0 ) ≤ c2, • ∀X 0 ⊂ X0: sup (X) sup (X0) <γ ⇔ sup (X) mnsτ (X0) <γ ⇔ c2 <mnsτ (X0), • ∀z ⊃ X: de Z ∈ Fτ sup (Z) de sup (X0) <γ ⇔ mxsτ (X) sup (X0) <γ ⇔ c1 <sup (X0), qui conclut la preuve. Exemple 4 En considérant à nouveau l'exemple 1, simple suffit arithmétiques pour vérifier que Proposi- tion 5 identifie exactement les fermés à partir de laquelle des règles de représentation suivent comme dans l'exemple 2; De même, la proposition 6 peut être illustré par la règle représentant b → acde de l'exemple 3, qui est obtenu à partir de abcde (pour laquelle en effet 0,4 * 6/5> 6/1 selon la proposition 5) en utilisant c1 = 2,5 / 6 et c2 = 6/5, comme c1 <c2 ≤ 4/6 <6/6. L'exactitude de l'algorithme 1 suit trivialement de la prop 5 et Proposition 6. 3.3 Un algorithme pour différents seuils de confiance à l'inconvénient de l'algorithme 1, par rapport à celui de Kryszkiewicz (2001), est que, pour un X donné dans FC τ, mxgsτ, γ (X) dépend du seuil de confiance, et par conséquent, il ne peut être réutilisé une fois γ a changé, alors que les deux mxsτ (X) et mnsτ (X) peut être calculé qu'une seule fois pour une valeur donnée de τ puis utilisé pour différentes valeurs de confiance . D'autre part, celui-ci est garanti de ne pas perdre des règles de représentation, alors que celui de Kryszkiewicz (2001) risque de donner la production incomplète, comme dans notre contre-dessus. Algorithme 2 RR Generator - la phase de prétraitement 1: Entrée: seuil de support τ 2: Fτ = {X ⊆ U sup (X) ≥ τ} 3: FC τ = {X ∈ Fτ X = X} 4: FGτ = {X ∈ Fτ ∀Y ⊂ X, sup (Y)> sup (X)} 5: pour tout X ∈ FGτ do 6: mnsτ (X) = min ({sup (Y) Y ∈ FGτ, Y ⊂ X} ∪ {∞}) 7: fin de 8: pour tout X ∈ FC τ \ {∅} do 9: mxsτ (X) = max (sup {(Z) Z ∈ FC τ, Z ⊃ X} ∪ {0}) 10: n [X] = | {Y ∈ FGτ Y ⊂ X} | 11: let {Y1,. . . , Yn [X]} l'ensemble {Y ∈ FGτ Y ⊂ X} dans l'ordre décroissant de support 12: pour tout i ∈ {1,. . . , N [X]} faire 13: yi [X] = sup (Yi) 14: pi [X] = sup (X) / yi [X] 15: fin de 16: p0 [X] = 0 17: fin de JL Balcázar et C. Tîrnăucă au lieu de calculer mxgsτ, γ (X) pour chaque γ, on peut trouver les points individuels de l'intervalle (0, 1] où mxgsτ, γ (X) modifie sa valeur. en effet, compte tenu de X ∈ FC τ \ {∅}, soit {Y1,..., Yn [X]} l'ensemble {Y ∈ FGτ Y ⊂ X} dans l'ordre décroissant de soutien. Il est facile de voir que mxgsτ, γ ( X) =    sup (Y1), si γ ≤ sup (X) sup (Y1) sup (Yi + 1), si γ ∈ sup (X) sup (Yi), sup (X) sup (Yi + 1), i ∈ {1,, n [X] -... 1 0}, si γ> sup (X) sup (Yn [X]) maintenant, chaque fois qu'une nouvelle valeur de la. seuil de confiance γ est donnée, on peut déterminer si un ensemble fermé fréquent X est RI τ, γ en récupérant simplement l'intervalle (pi [X], pi + 1 [X]] avec i ∈ {0,..., n [X] - 1} où γ appartient (rappelons que dans ce mxgsτ cas, γ (X) = yi + 1 [X]) et de vérifier ensuite si l'inégalité γ * yi + 1 [X]> mxsτ (X) ho lds. Notez que si aucun i existe (ce qui est, à chaque fois que γ a une valeur strictement supérieure à pn [X] [X]), mxgsτ, γ (X) prend la valeur 0, ce qui rend γ * mxgsτ, γ (X ) inférieure ou égale à mxsτ (X). Ces notions sont mises en oeuvre dans les algorithmes 2 et 3. Algorithme 3 RR Générateur - deuxième phase 1: Entrée: seuil τ support, seuil de confiance y 2: RI τ, γ = ∅ 3 : Pour tout X ∈ FC τ \ {∅} faire 4: si ∃i ∈ {0,. . . , N [X] - 1} tel que γ ∈ (pi [X], pi + 1 [X]] puis 5: si γ * yi + 1 [X]> mxsτ (X) puis 6: Ajouter X RI τ , y 7: end if 8: end if 9: fin de 10: pour tout X ∈ RI τ, γ faire 11: c1 = mxsτ (X) / γ 12: c2 = sup (X) / γ 13: Ant = { X0 ∈ FGτ X0 ⊂ X, c1 <sup (X0) ≤ c2 <mnsτ (X0)} 14: pour tout X0 ∈ Ant do 15: X0 de sortie → X \ X0 16: fin de 17: fin de 4 Comparaison empirique nous avons vu que l'on peut trouver des exemples de jouets de jeux de données dans lequel la sortie de l'algorithme Kryszkiewicz (2001) est incomplète nous avons testé l'algorithme sur deux ensembles de données du monde réel:. un ensemble de données de panier de marché typique, tiré de l'atelier de data mining Clémentine (2005), et la partie de jeu de formation de l'ensemble de données de recensement UCI adultes des États-Unis, voir Asuncion et Newman (2007) Nous avons mis en place trois algorithmes différents: un pour l'heuristique incomplète donnée dans Kryszkiewicz (2001), l'un pour la première heuristique. proposé par nous dans lequel mnsτ est remplacé à base-ensemble fermé Découverte du représentant de l'Association Règles Revisited bmnsτ (également incomplète), et qui génère l'ensemble complet de règles représentatives tel que décrit par l'algorithme 1. Afin d'obtenir des résultats comparables, toutes les règles permettent avec antécédent vide et utilisent la même définition des ensembles fréquents et association règles comme indiqué dans nos Préliminaires. Le premier ensemble de données à l'étude se compose de 1000 transactions de plus de 15 attributs, 11 d'entre eux reflétant le type de produit que le client aurait pu acheter (fruitveg, freshmeat, produits laitiers, cannedveg, cannedmeat, frozenmeal, bière, vin, liqueur douce, poissons, confiserie) et 4 autres données par le sexe et le statut de la propriété du client (homme, femme, propriétaire, donotownhome). Le tableau 2 montre le nombre de règles représentatifs obtenus pour différents supports et confi- seuils ance (troisième colonne), ainsi que la cardinalité de l'ensemble de sortie lorsque bmnsτ ou mnsτ est utilisé (les quatrième et cinquième colonnes, respectivement). On peut constater que, bien que pour un soutien plus élevé des seuils de la sortie des algorithmes est, la plupart du temps, identiques (rappelons que la sortie de l'algorithme Kryszkiewicz (2001) est toujours un sous-ensemble de l'ensemble des règles représen- tant), abaisser les seuils montre plus grandes différences. A titre de comparaison, la colonne de droite indique le nombre de règles au sens de la norme Agrawal et al. (1996). LANGUETTE. 2 - panier de jeu de données (nombre de règles) τ γ RR RR avec bmnsτ RR avec mnsτ standard 0,7 41 33 33 67 0,05 0,8 17 16 16 36 0,9 15 15 15 15 0,7 12 10 10 21 0,10 0,8 5 5 5 12 0,9 4 4 4 4 0,7 6 6 6 16 0,15 0,8 2 2 2 2 0,9 0 0 0 0 a titre d'exemple, dans le cas des seuils pour le soutien et la confiance sont 0,10 et 0,70, respectivement, il y a un total de 12 règles de représentation, parmi lesquels deux sont perdus lors de l'utilisation mns ou bmns (indiqués en caractères gras): [c: 0,70, s: 0,14] bière mâle frozenmeal⇒, [c: 0,72, s: 0,15] mâle frozenmeal⇒ cannedveg, [c: 0,86, s: 0,12] confiserie wine⇒ femelle, [c: 0,70, s: 0,14] mâle frozenmeal bière cannedveg⇒, [c: 0,82, s: 0,14] bière frozenmeal⇒ cannedveg mâle, [c: 0,84, s: 0,14] bière cannedveg⇒ frozenmeal mâle, [c: 0,71, s: 0,14] mâle beer⇒ cannedveg frozenmeal, [c: 0,81, s: 0,14] cannedveg frozenmeal⇒ bière mâle, [c: 0,73, s: 0,10] mâle fish⇒ donotownhome, [c: 0,89, s : 0,12] poisson fruitveg⇒ donotownhome, [c: 0,70, s: 0,10] donotownhome beer⇒ mâle, [c: 0,70, s: 0,11] donotownho moi frozenmeal⇒ mâle Dataset adulte est une version transactionnelle de la partie du jeu de la formation de l'ensemble de données de recensement UCI adultes des États-Unis, voir Asuncion et Newman (2007); il se compose de 32561 transactions sur 269 articles. Notez que dans ce cas, il existe des différences significatives entre la sortie de l'algorithme Kryszkiewicz (2001) et l'ensemble des règles de représentation (tableau 3). Par exemple, des seuils de soutien et de confiance de 0,05 et 0,8, respectivement, plus de la moitié des règles sont perdus. Nous avons mené des expériences sur un processeur Intel Core 2CPU 6300 @ machine à 1.86GHz avec 2 Go de RAM fonctionnant sous Microsoft Windows XP Professionnel. Le temps d'exécution o f trois J. L. et C. Balcázar Tîrnăucă TAB. 3 - Adult ensemble de données de recensement des États-Unis (numéro de règles) τ γ RR RR avec bmnsτ RR avec mnsτ standard 0,6 872 383 383 3443 0,05 0,7 781 425 425 2926 0,8 851 640 640 2426 0,6 326 124 124 1284 0,10 0,7 274 162 162 1083 0,8 345 270 270 923 algorithmes étaient entre 15 et 47 millisecondes dans le cas du marché et ensemble de données baignait entre 62 et 1203 millisecondes pour l'ensemble de données adultes. L'algorithme qui génère correctement toutes les règles de représentation est légèrement plus lent que les deux autres, mais, lors de nos tests, la différence était plutôt hors de propos depuis le temps nécessaire pour imprimer les résultats à l'écran (un dispositif plus lent que le CPU) domine toujours le processus. Il faut noter que la quantité de règles représentatives peut diminuer à la baisse des seuils de confiance ou de soutien. Ce phénomène a été observé et expliqué précédemment, voir Balcázar (2010), et est causé par des règles puissantes d'une confiance donnée, disons 0,8, qui sont filtrés à des seuils plus élevés, ce qui laisse donc beaucoup d'autres règles en tant que représentant, mais que la force tous ceux-ci sur les règles de représentation qu'ils deviennent redondantes lorsque le seuil de confiance passe en dessous de 0,8 et permet la règle puissante. 5 Perspectives comme les sujets de recherche futurs, nous souhaitons étendre la caractérisation donnée dans la proposition 5 de tous les itemsets fermés qui peuvent être décomposées en règles représentatives à la notion de redondance forte introduite dans Balcázar (2010), à savoir la redondance à base de fermeture. Additionnalité allié, un fait déconcertant que nous envisageons d'étudier plus est que, dans la plupart des ensembles de données réelles, nous avons exécuté nos algorithmes, notre première alternative du paragraphe 3.1, aussi incomplète, donne la même quantité de règles représentatives que le algorithme incomplet d'origine; cela peut indiquer que mieux comprendre les ensembles de règles obtenues par ces algorithmes incomplets pourraient être utiles. Références Aggarwal, C. C. et P. S. Yu (2001). Une nouvelle approche de génération en ligne de règles d'association. IEEE Transactions sur les connaissances et l'ingénierie des données 13 (4), 527-540. Agrawal, R., H. Mannila, R. Srikant, H. Toivonen et A. I. Verkamo (1996). découverte rapide des règles d'association. Progrès de la connaissance Découverte et exploration de données, p. 307-328. AAAI / MIT Press. Asuncion, A. et D. Newman (2007). référentiel apprentissage automatique UCI. Balcázar, J. L. (2010). Redondance, les systèmes de déduction, et les bases de taille minimale pour les règles de asso- ciation. Méthodes logiques in Computer Science 6 (2: 3), 1-33. Découverte à base ensemble fermé de l'Association représentant les règles Revisited Ceglar, A. et J. F. Roddick (2006). Association minière. ACM Comput. Surv. 38 (2). Clémentine (2005). Guide de l'utilisateur Clementine 10.0 de bureau. Cristofor, L. et D. A. Simovici (2002). Génération d'une couverture informative des règles d'association. Dans Proc. de la Conférence internationale IEEE 2002 sur les mines de données (CISM), pp. 597-600. IEEE Computer Society. Hamrouni, T., S. Ben Yahia et E. Mephu Nguifo (2008). Succinctes générateurs minimaux: fondements théoriques et applications. Int. J. Trouvé. Comput. Sci. 19 (2), 271-296. Kryszkiewicz, M. (1998a). découverte rapide des règles d'association représentatives. Dans L. Polkowski et A. Skowron (Eds.), Proc. de la 1ère Conférence internationale sur les ensembles bruts et les tendances actuelles en informatique (RSCTC), volume 1424 de notes de cours en intelligence artificielle, pp. 214-221. Springer-Verlag. Kryszkiewicz, M. (1998b). règles d'association représentatives. Dans X. Wu, K. Ramamohanarao et K. B. Korb (Eds.), Proc. de la 2e Conférence Asie-Pacifique sur la découverte de connaissances et d'exploration de données (PAKDD), volume 1394 de Lecture Notes in Intelligence artificielle, pp. 198-209. Springer-Verlag. Kryszkiewicz, M. (2001). ensemble fermé découverte basée sur des règles d'association représentatives. Dans F. Hoffmann, D. J. main, N. M. Adams, D. H. Fisher et G. Guimarães (Eds.), Proc. du 4e Symposium international sur l'analyse intelligente des données (IDA), volume 2189 de Lecture Notes in Computer Science, pp. 350-359. Springer- Verlag. Kryszkiewicz, M. (2002). représentations concises des règles d'association. Dans D. J. main, N. M. Adams, et R. J. Bolton (Eds.), Proc. de l'atelier sur le FSE Exploratoire Motif De- tection et découverte, volume 2447 de Lecture Notes in Computer Science, pp. 92-109. Springer-Verlag. Luxenburger, M. (1991). Dans un Implications partielles Contexte. Mathématiques et Sciences Humaines 29, 35-55. Pasquier, N., R. Taouil, Y. Bastide, G. Stumme et L. Lakhal (2005). Génération d'une représentation condensée des règles d'association. J. Intell. Inf. Syst. 24 (1), 29-60. Phan-Luong, V. (2001). La base représentative des règles d'association. Dans N. Cercone, T. Y. Lin et X. Wu (Eds.), CISM, p. 639-640. IEEE Computer Society. Zaki, M. J. (2004). Exploitation minière règles d'association non redondants. Données Min. Knowl. Décou. 9 (3), 223-248. La sortie d'CV un de mineur d'association rules is in the Énorme Souvent pratique. C'est several Pourquoi sans représentations concises perte were proposées, les rules Que Telles « Essentielles » ou « » REpRésEntants. Nous reviendrons sur l'Kryszkie- par Donné Algorithme wicz (Int. Symp. L'analyse intelligente des données 2001, Springer-Verlag LNCS 2189, 350-359) pour L'extraction des rêgles représentants. Nous montrons sa production Que is incom- plète Parfois, à cause de d'à la Une manque de preuve mathématique de validité this algorithme, et nous proposons un de Générateur complet avec remplacement les Presqu'île temps d'éxécution Mêmes."
524,Revue des Nouvelles Technologies de l'Information,EGC,2011,Complex Information Processing,"It is commonplace nowadays to claim that information is everywhere and that, as a result, finding the right information (mathematically : according to a set of criteria optimizing a specific goal) is very difficult. Defence applications have to cope with similar problems : communication networks, surveillance and information systems transmit and generate significant amounts of complex information which cannot be processed with low level algorithms. The challenge is to build high-level processing units (which demand a lot of computing power) so as process video streams and communication packets with little possibility of a false alarm as automatically as possible. Methods for processing, aligning, merging low-level and high-level information (from syntactic to semantic information) extracted from still images, videos, speech, text and the Internet are being considered. The framework includes theoretical approaches, algorithms as well as evaluation methods. Topics of interest are data fusion, learning techniques, data mining, HCI, even Artificial Intelligence. Defence applications are numerous, from scene understanding to weak signal detection.",Jacques Blanc-Talon,http://editions-rnti.fr/render_pdf.php?p1&p=1000922,http://editions-rnti.fr/render_pdf.php?p=1000922,en,"Traitement de l'information complexe Jacques Blanc-Talon Responsable du Domaine Scientifique ""de l'information Ingénierie et Robotique"" DGA / DS / MRIS France jacques.blanc-talon@dga.defense.gouv.fr Résumé Il est courant de nos jours de prétendre que l'information est partout et que, en tant que Sult re, trouver la bonne information (mathématiquement: selon un ensemble de critères d'un objectif de l'optimisation spécifique) est très difficile. Les applications militaires doivent faire face à des problèmes similaires: réseaux de communication, la surveillance et les systèmes d'information de transmission et de générer des quantités signifi- catives d'informations complexes qui ne peuvent être traités avec des algorithmes de bas niveau. Le défi consiste à construire des unités de traitement de haut niveau (qui exigent beaucoup de wer po- calcul) afin que les flux vidéo de processus et des paquets de communication avec peu de possibilité d'une fausse alarme automatique que possible. Les méthodes de traitement, l'alignement, la fusion de bas niveau et des informations de haut niveau (de syntaxe à l'information sémantique) extraites des images fixes, des vidéos, de la parole, le texte et l'Internet sont à l'étude. Le cadre comprend des approches théo- rique, des algorithmes ainsi que des méthodes d'évaluation. Les sujets d'intérêt sont la fusion de données, les techniques d'apprentissage, l'exploration de données, HCl, même l'intelligence artificielle. Les applications militaires sont nombreux, de la scène à la compréhension de la détection du signal faible. Bibliographie Jacques Blanc-Talon est le chef du « génie de l'information et de la robotique » Domaine scientifique à l'Office de recherches avancées et de l'innovation (SIRG) à la DGA (Direction Générale de l'Armement). Il est le président de traitement du signal chapitre IEEE France, membre du GdR ISIS et au Conseil du SEE SI2D Club (voir http: //www.viadeo.com). Il est le co-fondateur et Président de la Conférence des concepts avancés pour les systèmes de vision intelligente (ACIVS): http: //acivs.org."
528,Revue des Nouvelles Technologies de l'Information,EGC,2011,Data stream summarization by on-line histograms clustering,,"Antonio Balzanella, Lidia Rivoli, Rosanna Verde",http://editions-rnti.fr/render_pdf.php?p1&p=1000977,http://editions-rnti.fr/render_pdf.php?p=1000977,en,"flux de données par summarization en ligne Antonio Balzanella regroupement histogrammes *, Lidia Rivoli ** Rosanna Verde * * Université de Naples - Via del Setificio 15 - San Leucio - 81100 Caserta - Italie antonio.balzanella@gmail.com, ** Université de Naples Federico II - Complesso Universitario di Monte S. Angelo Napoli lidia.rivoli@unina.it Au cours des dernières années, un grand nombre de champs génère des flux applicatifs de données en continu, potentiellement sans bornes. L'analyse de ce type de données est limitée par l'impossibilité de stocker l'ensemble des données et par la nécessité de fournir les résultats le plus rapidement possible afin de soutenir les décisions. Lorsque nous avons affaire à des données très en constante évolution, un défi important est de découvrir des résumés en mesure de mettre en évidence les principaux concepts qui caractérisent le phénomène analysé. Dans ce contexte, nous présentons une stratégie efficace qui fournit, en sortie, un ensemble de his- tograms pour résumer les principaux concepts émergents dans un flux de données en constante évolution. Un flux de données Y = {(y1, t1), (y2, t2),. . . (Y∞, t∞)} est un ensemble de réels obser- vations commandés d'une valeur sur une grille de temps discret T = {t1, ..., t2, ... t∞} ∈ <. De Y, il est possible d'obtenir un lot de données Qi = {(yl, tl),. . . (Yj, tj),. . . (Yn, tn)} avec i ∈ =, où = est l'ensemble de toutes les bornes des sous-ensembles ordonnés de telle sorte que Y Qi Qi ⋂ + 1 = ∅. La taille de Qi est N = N- l. Nous pouvons synthétiser les données par un histogramme comme suit. Soit S = [y; y] soit le support d'un lot de données Qi. Les observations de Qi sont divisées en un ensemble d'intervalles contigus (bins) {I1i,. . . , Iki,. . . , Iki} où Iki = [yki; yki) et ⋃k k = 1 Ik = [y; y]. Pour chaque intervalle Iki nous associons la FKI de fréquence relative, qui est le nombre d'éléments de Qi dans [yki; yki) normalisée à N. la construction Histogramme nécessite la définition de la taille et le nombre d'intervalles. Dans cet article, nous faisons référence à Histogrammes équi-profondeur où la gamme des valeurs observées est vided di- en K intervalles de sorte que chaque intervalle comprennent les mêmes éléments de numéros. Le but de cet article est de détecter un ensemble de résumés G = {g1,. . . , Gz,. . . , GZ} qui représente le Salut associé aux histogrammes lots de données Qi. La stratégie que nous introduisons pour atteindre cet objectif, est fait par une étape en ligne et sur une étape hors ligne. Le premier, permet d'obtenir un ensemble de synopsis du courant, celui-ci, commence à partir des résultats de l'étape en ligne pour produire le dernier ensemble de résumés G. Il est une variante de l'algorithme de CluStream dans (Aggarwal et al. , 2003). En particulier, les regards étape sur la ligne pour la synthèse des lots non-chevauchement des données au moyen d'un ensemble de taille C >> Z de structures spécifiques appelés des micro-amas. A magasins micro-cluster un gc prototype, le nombre de nc. Attribués histogrammes Chaque fois qu'un nouveau lot de données q'i est disponible et l'histogramme associé H 'i est érigé, la distance entre H' i et le prototype gc, ∀c = 1,. . . , C de chaque micro-grappe est calculé. Si la distance à la plus proche prototype est inférieure à une valeur de seuil fixe, e, flux de données par synthèse histogrammes en ligne regroupement H 'i est alloué à elle et les statistiques des micro-amas sont actualisées. Si aucun prototype est à une distance inférieure à e, un nouveau micro-cluster est généré avoir H 'i comme prototype et nc = 1. Afin de comparer les histogrammes et de calculer le prototype de chaque micro-cluster, nous devons introduire une fonction de distance approprié. Nous vous proposons d'utiliser la distance Wasserstein comme indiqué Verde et Irpino (2007). Soient F et G deux distributions et F-1 et G-1 leurs fonctions quantile. Il est possible de définir le Mallow de (Mallow (1972)) dans la distance L2 dérivée de la Wasserstein métrique comme suit: dM (F, G): = √√√√√ 1∫ 0 (F-1 (t) -G- 1 (t)) 2 dt (1) Selon Verde et Irpino (2007), étant donné que chaque intervalle de l'histogramme peut être ex- pressé en fonction des centres et des rayons c + r (2T-1) pour 0 ≤ t ≤ 1, la ( 1) peut être calculé beaucoup plus facile. De plus, l'utilisation de Wasserstein métrique permet de trouver le prototype de chaque groupe micro sous forme d'histogramme qui est barycentrique par rapport aux éléments du cluster. Ceci est obtenu comme la moyenne des centres et des rayons de chaque intervalle des histogrammes du cluster. Enfin, à partir des, il est possible de micro-agrégats mis à jour, en ligne pour découvrir la dernière série de résumés G par une procédure de classification sur les micro-amas qui est une variante de l'algorithme K-means. La sortie sera la dernière série de résumés G = {g1,. . . , Gz,. . . , GZ}. Références Aggarwal, C. C., J. Han, J. Wang et P. S. Yu (2003). Un cadre pour le regroupement des flux de données en constante évolution. En VLDB « 2003: Compte rendu de la conférence internationale du 29 sur des bases de données très volumineux, pp 81-92.. VLDB de dotation. Vert, R. et A. Irpino (2007). regroupement dynamique des données de l'histogramme: en utilisant la bonne méthode. Dans à: Contributions choisies dans l'analyse des données et la classification, pp 123-134.. Springer. Résumé Ces dernières années une large gamme d'applications génère des flux de données potentiellement sans bornes. Lorsque nous avons affaire à des données hautement évolutifs, des résumés en mesure de mettre en évidence les principaux concepts du phénomène contrôlé sont nécessaires. Dans cet article, nous présentons une nouvelle stratégie capable de résumer le flux de données à travers un ensemble d'histogrammes. Il est une procédure de regroupement où les prototypes des clusters sont correctement détectés histogrammes."
534,Revue des Nouvelles Technologies de l'Information,EGC,2011,Early Classification on Temporal Sequences,"Early classification of temporal sequences has applications in, for example, health informatics, intrusion detection, anomaly detection, and scientific and engineering sequence data monitoring. In early classification, instead of optimizing accuracy, our goal is to produce classification as early as possible provided that the accuracy meets some expectation. In this talk, I will advocate early classification as an exciting and challenging research problem, which has not been systematically studied in the literature. I will discuss several interesting formulations of the problem, which provide complimentary features possibly desirable in different application scenarios. I will also review some of our recent progress on this aspect.",Jian Pei,http://editions-rnti.fr/render_pdf.php?p1&p=1000918,http://editions-rnti.fr/render_pdf.php?p=1000918,en,"Début Classification sur Temporal Pei Jian Sequences Simon Fraser University, Canada jpei@cs.sfu.ca http://www.cs.sfu.ca/~jpei/ Résumé classification précoce des séquences temporelles des applications dans, par exemple, de la santé infor- matiques, détection d'intrusion, de détection des anomalies, et la séquence scientifique et technique de surveillance de données. Dans la classification précoce, au lieu d'optimiser la précision, notre objectif est de produire le classement le plus tôt possible à condition que la précision répond à une certaine attente. Dans cet exposé, je défendrai le classement au début comme un problème de recherche passionnant et stimulant, qui n'a pas été étudié de façon systématique dans la littérature. Je discuterai plusieurs lations formula- intéressants du problème, qui offrent des fonctionnalités complémentaires éventuellement souhaitables dans différents scénarios d'application. Je vais passer en revue également certains de nos récents progrès sur cet aspect. Bibliographie Jian Pei est actuellement professeur agrégé de sciences informatiques et le recteur associé Di- (Relations de recherche et de l'industrie) à l'École des sciences informatiques, Université Simon Fraser, Canada. Ses intérêts de recherche peuvent être résumés comme le développement de techniques d'analyse des données cace efficaces et effi- pour les applications intensives nouvelles données. En particulier, il est actuellement intéressé par diverses techniques de l'exploration de données, recherche sur le Web, la recherche d'information, la relogements de données, le traitement analytique en ligne et les systèmes de bases de données, ainsi que leurs applications dans les réseaux sociaux, la santé-informatique, les entreprises et la bio-informatique. Ses recherches ont été largement porté par SUP- les organismes de financement du gouvernement et des partenaires de l'industrie. Il a publié dans des revues et prolifiquement conférences avec comité de lecture, a régulièrement servi dans les comités d'organisation et les comités de programme de nombreuses conférences internationales et des ateliers, et a été critique pour les revues académiques dans grands ses champs. Il est rédacteur en chef adjoint de plusieurs revues telles que les transactions ACM sur la découverte des connaissances à partir de données (TKDD) et IEEE Transactions de connaissances et d'ingénierie de données (TKDE). Il est membre senior de l'Association for Computing Machinery (ACM) et l'Institut des ingénieurs électriciens et électroniciens (IEEE). Il est le lauréat du British Columbia Innovation Council 2005 Prix des jeunes innovateurs, du CRSNG 2008 Discovery Accelerator Award suppléments (100 prix traversent tout le pays, 10 en science informatique), un prix IBM Faculty (2006), une KDD meilleure application Paper Award (2008), un PAKDD le plus influent Paper Award (2009), et un Outstanding Paper Award IEEE (2007)."
557,Revue des Nouvelles Technologies de l'Information,EGC,2011,"Mobility, Data Mining and Privacy: Mining Human Movement Patterns from Trajectory Data","The technologies of mobile communications and ubiquitous computing pervade our society, and wireless networks sense the movement of people and vehicles, generating large volumes of mobility data, such as mobile phone call records and GPS tracks. This is a scenario of great opportunities and risks : on one side, mining this data can produce useful knowledge, supporting sustainable mobility and intelligent transportation systems ; on the other side, individual privacy is at risk, as the mobility data contain sensitive personal information. A new multidisciplinary research area is emerging at this crossroads of mobility, data mining, and privacy. The talk assesses this research frontier from a data mining perspective, and illustrates the results of a European-wide research project called GeoPKDD, Geographic Privacy-Aware Knowledge Discovery and Delivery. GeoPKDD has created an integrated platform named MATLAS for complex analysis of mobility data, which combines spatio-temporal querying capabilities with data mining, visual analytics and semantic technologies, thus providing a full support for the Mobility Knowledge Discovery process. In this talk, we focus on the key data mining models : trajectory patterns and trajectory clustering, and illustrate the analytical power of our system in unvealing the complexity of urban mobility in a large metropolitan area by means of a large scale experiment, based on a massive real life GPS dataset, obtained from 17,000 vehicles with on-board GPS receivers, tracked during one week of ordinary mobile activity in the urban area of the city of Milan, Italy.",Fosca Giannotti,http://editions-rnti.fr/render_pdf.php?p1&p=1000920,http://editions-rnti.fr/render_pdf.php?p=1000920,en,"Mobilité, Data Mining et vie privée: Exploitation minière modèles de mouvement humain Trajectoire données Fosca Giannotti IIIST « Alessandro Faedo » Italie fosca.giannotti@isti.cnr.it~~V~~plural~~3rd Résumé Les technologies de communications mobiles et de l'informatique omniprésente répande dans nos soi-ciété, et les réseaux sans fil sens la circulation des personnes et des véhicules, grandes Lumes vo- de données de mobilité, comme les dossiers d'appels de téléphonie mobile et les pistes GPS. Ce scénario de grandes opportunités et risques: d'un côté, l'exploitation minière, ces données peuvent produire des connaissances utiles, soutenir la mobilité durable et les systèmes de transport intelligents; de l'autre côté, la vie privée indivi- duelles est à risque, comme les données de mobilité contiennent des informations personnelles sensibles. Un nouveau domaine de recherche pluridisciplinaire est en train d'émerger à ce carrefour de la mobilité, l'exploration de données et la vie privée. Le discours évalue cette frontière de recherche du point de vue de l'exploration de données et illustre les résultats d'un projet de recherche à l'échelle européenne appelée GeoPKDD, Confidentialité-Aware géographique Découverte de connaissances et de livraison. GeoPKDD a créé une plate-forme intégrée nommée M- ATLAS pour l'analyse complexe des données de mobilité qui combine spatio-temporelle avec l'interrogation CA- pacités data mining, l'analyse visuelle et des technologies sémantiques, offrant ainsi un support complet pour le processus de découverte du savoir Mobilité. Dans cet exposé, nous nous concentrons sur les modèles d'exploration de données clés: les modèles de trajectoire et le regroupement trajectoire et illustrons la puissance analytique de notre système unvealing la complexité de la mobilité urbaine dans une grande région métropolitaine au moyen d'une expérience à grande échelle, basée sur une réel massif ensemble de données GPS de vie, obtenue à partir de 17.000 véhicules équipés de récepteurs GPS embarqués, chenillés au cours d'une semaine d'activité mobile ordinaire dans la zone urbaine de la ville de Milan, en Italie. Bibliographie Fosca Giannotti est chercheur principal à la science de l'information et de la technologie Insti- tute du Conseil national de recherches à Pise, en Italie. Ses recherches actuelles comprennent des langages de requêtes d'extraction de données, l'environnement de soutien à la découverte de connaissances, web-mining, le raisonnement spatio-temporelle, l'exploration de données spatio-temporelles, et la vie privée en préservant l'exploration de données. Elle a été le coordinateur de divers projets de recherche européens et nationaux et est actuellement le coordinateur du projet FP6-IST GeoPKDD: géographique Confidentialité-aware connaissances Dis- Covery et livraison. Elle est membre du comité de pilotage de la coordination européenne FP7 action MODAP: Mobilité, l'exploitation minière et la confidentialité des données. Elle a enseigné des cours sur les bases de données et l'exploration de données dans les universités en Italie et à l'étranger. Elle est l'auteur de plus d'un Hon- publications Dred et a servi au sein du comité scientifique de diverses conférences dans le domaine de la mobilité, Data Mining et confidentialité Programmation Logique, bases de données et Data Mining. En 2004, elle a co-présidé la confé- rence européenne sur l'apprentissage machine et découverte des connaissances dans Bases de données ECML / PKDD 2004 et en 2008, elle a été le co-président du programme de ICDM 2008, l'IEEE Int. Conf. sur Data Mining. Fosca Giannotti codirige le tory Discovery Lab Connaissance Pisa KDD et Data Mining Labora- (http: //www-kdd.isti.cnr.it) - une initiative de recherche conjointe de l'Université de Pise et l'Institut des sciences de l'information et de la technologie du Conseil national de recherche italien, fon- dée en 1995, l'un des premiers groupes de recherche européens spécifiquement destiné aux mines de données et de découverte de connaissances. Publications de Fosca Giannotti de serveur bibliographique DBLP: http: //www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giannotti: Fosca.html"
570,Revue des Nouvelles Technologies de l'Information,EGC,2011,Parameter-free association rule mining with yacaree,,José L Balcazar ,http://editions-rnti.fr/render_pdf.php?p1&p=1000952,http://editions-rnti.fr/render_pdf.php?p=1000952,en,"règle d'association sans paramètre minière avec yacaree José L Balcázar Departamento de Matemáticas, Estadística y Computación Universidad de Cantabria, Santander, Espagne 1 Introduction Association minière de la règle est l'un des aspects les plus centraux de l'exploration de données. Il existe de nombreuses implémentations de mineurs d'association: Borgelt (2003); Witten et Frank (2005); Zaki et Hsiao (2005). Les problèmes rencontrés en association minière de règle sont de deux ordres. Tout d'abord, la quantité de itemsets candidats pouvant conduire à des règles croît de façon exponentielle avec l'univers souvent déjà grand nombre d'éléments. L'introduction d'un paramètre de support est une avancée décisive qui a permis la conception des mineurs fixés fréquents et efficaces pour le calcul des règles d'association dans les grands ensembles de données: il y a, l'exploration est limitée à ces jeux d'éléments qui apparaissent « assez souvent », comme des sous-ensembles des transactions , qui est, leur fréquence relative dépasse un certain rapport des transactions (Agrawal et al. (1996)). Ensuite, le deuxième problème est que, souvent, l'ensemble des règles à condition que la production est énorme, surtout si l'on considère que son but est d'être lu par un humain. De nombreuses études de notions de redondance qui existent de limiter la sortie aux règles « » selon non redondants plusieurs notions existantes de redondance (par exemple Zaki (2004); Kryszkiewicz (2001)); mais même en prenant en compte les redondances, les résultats sont, dans bien des cas, peu satisfaisant: des seuils élevés d'implication laissent de nombreuses règles intéressantes, alors que les bas laissent passer beaucoup trop de règles à inspecter manuellement. De nombreuses mesures de qualité alternatives existent des règles d'association: Geng et Hamilton (2006); Lenca et al. (2008); Tan et al. (2004). Notre système yacaree (Yet Another Association Règle Expérimentation basée fermeture vironnement En-) traite des ensembles de données transactionnelles, chaque transaction étant un jeu d'éléments, et obtient des règles X → Y non redondants, où X et Y peuvent être itemsets disjoints arbitraires; pourtant, il ne nécessite pas l'utilisateur de sélectionner la valeur de tout paramètre de seuil. Comme dans la plupart als actuels proposi-, nous exploitons seulement itemsets fermés fréquents, et appliquer des filtres de redondance connus. Notre algorithme d'exploration de fermeture actuelle est une variante simplifiée de CHARM (voir Zaki et Hsiao (2005)), plutôt proche d'une recherche en profondeur d'abord. Comme dans certains des associateurs de Weka, nous exploitons les fermetures afin de diminuer le soutien; voir Witten et Frank (2005). Cependant, notre algorithme est très différent: il ne nécessite aucun paramètre « delta » pour une baisse de seuil de soutien par étapes comme « Apriori » weka fait, et ne concerne pas le soutien à la confiance que « Apriori prédictive » fait (Scheffer (2005), également présent dans Weka ). , Nous auto-ajustons place le soutien interne effectif lié sur la base des limites technologiques: il commence à un niveau presque trivial, et se développe, le cas échéant, comme monitorization du processus minier révèle que la consommation de mémoire dépasse les seuils internes. Les fermetures sont transmises à un algorithme « frontière » qui calcule la structure du réseau, et les règles sont extraites non redondants. règle d'association sans paramètre minière avec yacaree En second lieu, nous mettons en œuvre un processus de qualité de l'implication nouvelle. Notre option est la suivante: nous imposons un seuil de confiance très doux qui reste fixe, en laissant de grandes quantités de règles passent; mais nous contrôlons le nombre de règles à fournir à l'utilisateur via un parent, plutôt que soluté AB-, la confiance liée, mesurée par le paramètre appelé « boost de confiance basée fermeture » (Balcázar (2010)), liée à la fois à l'ascenseur et le « rapport de soutien » (introduit en Kryszkiewicz (2001), sans lui donner un nom, comme l'une des propriétés utilisées pour accélérer un calcul de la base représentative des règles d'association); ces connexions sont cruciales pour notre système: nous utilisons l'approximation de l'augmentation de la confiance fournie par le rapport de soutien pour pousser la contrainte de renforcer la confiance dans le processus d'extraction, et nous utilisons l'ascenseur, appliqué à c particulier Ases où l'on peut prouver un lien fort avec le coup de pouce, pour se régler automatiquement le seuil de suralimentation. Tous nos composants sont reliés entre eux par l'évaluation paresseuse en utilisant les installations de pro- grammation fonctionnels de Python: fermetures, bords de diagramme de Hasse, prédécesseurs d'une fermeture, et les règles sont obtenues à partir itérateurs. Les bouclages et les règles candidats sont soit mis au rebut, si nous pouvons garantir que les ajustements de seuil futurs ne seront jamais les récupérer; ou transformés, si elles obéissent aux seuils; ou maintenus séparément en attente, si elles échouent les seuils actuels, mais pourraient leur obéir après ajustements ultérieurs. Ainsi, la charge de la mémoire est contrôlée en maintenant uniquement les éléments d'information qui peuvent être pertinentes pour les valeurs de seuil actuelles ou futures. Le résultat est un système préliminaire fonctionnel, où beaucoup de place reste pour améliorer l'efficacité et algorithmiques, ce qui montre qu'il est possible de trouver des règles d'association intéressantes d'une manière totalement autonome: l'utilisateur sélectionne simplement un ensemble de données et lance le processus, qui prend juste un à cinq minutes dans de nombreux jeux de données faciles, et jusqu'à dix à vingt minutes sur un ordinateur portable moderne pour quelques difficiles, des ensembles de données très denses. La sortie est un ensemble de règles qui, dans la plupart des cas, est raisonnablement faible et montre des associations indépendantes et sensibles. La capture d'écran présentée à la figure 1 montre l'interface simple (bouton « Exécuter » est désactivé que le système a été il suffit d'exécuter) et les deux fichiers texte généré: le journal, où l'on voit que le processus a pris un peu plus de cinq minutes, et le début du fichier contenant les règles trouvées. La console et le journal indiquent les ajustements auto-du support; aucun ajustement n'a été effectué sur le seuil d'amplification, comme assez des règles de haute boost a été trouvé pour sa valeur initiale. Voir http://sourceforge.net/projects/yacaree/ pour plus d'informations. Références Agrawal, R., H. Mannila, R. Srikant, H. Toivonen et A. I. Verkamo (1996). découverte rapide des règles d'association. Progrès de la connaissance Découverte et exploration de données, p. 307-328. AAAI / MIT Press. Balcázar, J. L. (2010). renforcer la confiance basée fermeture dans les règles d'association. Dans Atelier sur les applications de modèle Analyse, Volume 11, pp. 74-80. JMLR Atelier et Actes de la Conférence. Borgelt, C. (2003). implémentations efficaces de apriori et eclat. Dans B. Goethals et M. J. Zaki (Eds.), FIMI, Volume 90 de CEUR rendu de l'atelier. CEUR-WS.org. Geng, L. et H. J. Hamilton (2006). mesures d'intérêt pour l'exploration de données: une enquête. ACM Comput. Surv. 38 (3). Kryszkiewicz, M. (2001). ensemble fermé découverte basée sur des règles d'association représentatives. Dans F. Hoffmann, D. J. main, N. M. Adams, D. H. Fisher et G. Guimarães (Eds.), Proc. de J. L. Balcázar FIG. 1 - Une capture d'écran de yacaree avec les règles et la production de fichiers journaux 4ème Symposium international sur l'analyse intelligente des données (IDA), volume 2189 de Lecture Notes in Computer Science, pp 350-359.. Springer-Verlag. Lenca, P., P. Meyer, B. Vaillant et S. Lallich (2008). Lors de la sélection des mesures pour Interestingness règles d'association: l'utilisateur description orientée et critères multiples aide à la décision. Revue européenne de recherche opérationnelle 184 (2), 610-626. Scheffer, T. (2005). Association de trouver des règles que le soutien du commerce de manière optimale contre la confiance. L'analyse intelligente des données 9, 293-313. Tan, P.-N., V. Kumar et J. Srivastava (2004). Sélection de la bonne mesure interestingness pour les modèles d'association. Inf. Syst. 29 (4), 293-313. Witten, I. H. et E. Frank (2005). Data Mining: Machine pratiques Outils et techniques d'apprentissage (2ED). Morgan Kaufmann. Zaki, M. J. (2004). Exploitation minière règles d'association non redondants. Données Min. Knowl. Décou. 9 (3), 223-248. Zaki, M. J. et C.-J. Hsiao (2005). Des algorithmes efficaces pour l'extraction et itemsets fermés leur structure en treillis. IEEE Transactions sur les connaissances et l'ingénierie des données 17 (4), 462-478. Nous décrivons notre CV yacaree programme que les mines règles d'association non redondants, en quantités Nable habituellement reaso-, mais it d OES ne requièrent pas l'utilisateur de choisir une valeur pour un paramètre. Au lieu de cela, il fixe des valeurs pour certains paramètres et exécute un auto-réglage par l'ensemble de données-des deux principales: le soutien et une variante de confiance relative étudiée récemment par l'auteur."
571,Revue des Nouvelles Technologies de l'Information,EGC,2011,Point of View Based Clustering of Socio-Semantic Networks,,"Juan David Cruz, Cécile Bothorel, François Poulet",http://editions-rnti.fr/render_pdf.php?p1&p=1000973,http://editions-rnti.fr/render_pdf.php?p=1000973,en,"EGCPaperPoV_shortversion.dvi Point de vue basé Clustering du développement socio-sémantique Réseaux Juan David Cruz !, Cécile Bothorel !, François Poulet !! ! Département LUSSI - Télécom - Bretagne {juan.cruzgomez, cecile.bothorel}@telecom-bretagne.eu, http://www.telecom-bretagne.eu/ !! Université de Rennes 1 - IRISA francois.poulet@irisa.fr http://www.irisa.fr/texmex/ 1 introduction les réseaux socio-sémantiques contiennent deux types d'informations, les navires sociaux et de l'information RELATION liés aux acteurs tels que les profils thématiques. Afin d'analyser ce réseau Augmentée de différents points de vue, nous vous proposons d'influencer le processus de détection des communautés avec des informations sémantiques, le processus de regroupement est divisé en deux phases. Dans le premier, le point de vue est regroupée en utilisant l'auto - Cartes d'organisation (__gVirt_NP_NN_NNPS<__ SOM) (Kohonen (1997)) pour obtenir des groupes en fonction de la similitude des caractéristiques des noeuds et en changeant les poids du graphe en fonction des groupes sémantiques trouvé. Puis, dans la deuxième phase, un algorithme classique de détection communautaire est utilisé, en fonction de la topologie du réseau. 2 Utilisation du point de vue d'influencer l'Clustering Un réseau social peut être représenté comme un graphG non dirigé (V, E) où V est l'ensemble non vide de sommets représentant les acteurs est l'ensemble etE des arêtes représentant les relations entre les . Etant donné un graphG (V, E), C = {C1, C2,. . . , Ck} est une partition de l'ensemble V de k dans disjoints non vide Ci. FV Laissez l'ensemble des caractéristiques sémantiques des acteurs du réseau social, qui peuvent être représentés par une matrice de taille | V |! | FV |. Soit F! V « P (FV) \ FV, où P (A) est le powerset de l'ensemble A, un ensemble de caractéristiques non vide à utiliser pour définir le point de vue PoV. Pour chaque sommet vi » V nous attribuons un vecteur binaire! i = vi! ! F V de taille | F! V | = F. Si le sommet i a la caractéristique p, 1 # p # f de F! V, alors! I, p = 1 ou 0, sinon. Un point de vue est défini comme l'ensemble de toutes les instances dérivées de la FV set: PoVF! V = | V |! i = 1 i (1) Point de vue basé Clustering des réseaux socio-sémantique 1. Phase 1: Clustering sémantique utilisant SOM nous regroupons les noeuds selon les rités de leurs simi- fonctions. Pour la formation du réseau SOM, chaque instance! I est utilisé comme modèle d'entrée. Le résultat de l'algorithme SOM est une CSOM de partition des noeuds affectés aux neurones. Pour chaque paire de voisins sommets vi, vj, $ i% = j ""V, le poids de l'arête e (vi, vj) est modifiée en fonction de la distance euclidienne des instances PoV correspondant à chaque noeud par: wij = 1 + ""(1 et d (Nij)) #ij (2) où"" '1 est une valeur constante, d (Nij) est la distance entre les neurones i et j, et #ij = 1 si vi et vj appartiennent à la même partition dans CSOM, #ij = 0 sinon 2. phase 2:.. structurelle Clustering et détection communautaire Cet algorithme utilise le modularityQ, qui est proposé par Newman et Girvan (2004) comme mesure de la qualité une fois que les poids sont modifiés selon l'équation 2, un partition CSOM ""FU est com- puted en utilisant l'algorithme rapide dépliage proposé par Blondel et al. (2008), un algorithme de topologie classique pour trouver des communautés dans les graphiques. Ce nouveau CSOM ""FU partition contient l'ensemble final des communautés, qui a à la fois les informations sémantiques et les informations structurelles. 3 Conclusion et travaux futurs Les informations contenues dans un réseau socio-sémantique, tels que Twitter, est liée à certaines caractéristiques du acteurs. la méthode proposée permet d'analyser les informations Twitter de différents points de vue comme les communautés de jeux olympiques sur les commentaires géolocalisés. Affectation des poids dérivés des résultats du regroupement sémantique aux bords, l'information sémantique est inclus dans le processus de détection communautaire et la deux types de données sont fusionnées pour trouver et visualiser un réseau social d'un point de vue sélectionné. pour les travaux futurs, nous allons également poursuivre l'étude de l'influence de la po int de vue dans le processus de détection communautaire, y compris la définition des points de vue depuis les bords du graphique. De plus, nous prévoyons de travailler à l'élaboration d'un algorithme de visualisation pour les réseaux sociaux hiérarchiques. Références Blondel, V. D., J.-L. Guillaume, R. Lambiotte, et E. Lefebvre (2008). déploiement rapide des communautés dans les grands réseaux. Journal de la mécanique statistique: théorie et Expé- 2008 riment (10), P10008 (12pp). Kohonen, T. (1997). Autoorganisables Maps. Springer. Newman, M. E. J. et M. Girvan (2004). La recherche et l'évaluation de la structure des communautés dans les travaux Net-. Physical Review. E, Nonliner statistique et matière molle Physics 69 (2), 026.113."
577,Revue des Nouvelles Technologies de l'Information,EGC,2011,Reasoning about the learning process,"Data Mining is faced with new challenges. In emerging applications (like financial data, traffic TCP/IP, sensor networks, etc) data continuously flow eventually at high speed. The processes generating data evolve over time, and the concepts we are learning change. In this talk we present a one-pass classification algorithm able to detect and react to changes. We present a framework that identify contexts using drift detection, characterize contexts using meta-learning, and select the most appropriate base model for the incoming data using unlabeled examples. Evolving data requires that learning algorithms must be able to monitor the learning process and the ability of predictive self-diagnosis. A significant and useful characteristic is diagnostics - not only after failure has occurred, but also predictive (before failure). These aspects require monitoring the evolution of the learning process, taking into account the available resources, and the ability of reasoning and learning about it.",João Gama,http://editions-rnti.fr/render_pdf.php?p1&p=1000921,http://editions-rnti.fr/render_pdf.php?p=1000921,en,"Raisonnant sur le processus d'apprentissage João Gama LIACC, Université de Porto Portugal jgama@fep.up.pt http://www.liaad.up.pt/~jgama/ Résumé Data Mining est confronté à de nouveaux défis. Dans les applications émergentes (comme les données financières, les réseaux de capteurs trafic TCP / IP, etc.) flux de données en continu éventuellement à grande vitesse. Les processus de génération de données évoluent au fil du temps, et les concepts dont nous apprennent le changement. Dans cet exposé, nous présentons un algorithme de classification d'une passe capable de détecter et de réagir aux changements. Nous présentons un cadre permettant d'identifier les contextes en utilisant la détection de dérive, des contextes Caractériser en utilisant des méta-apprentissage et choisir les modèles de base approprié pour les données entrantes en utilisant des exemples unla- Beled. les données en pleine évolution exige que les algorithmes d'apprentissage doivent être en mesure de suivre le processus d'apprentissage et la capacité d'auto-diagnostic prédictif. Une carac- téristique importante et utile est le diagnostic - non seulement après l'échec a eu lieu, mais aussi prédictive (avant l'échec). Ces aspects nécessitent une surveillance de l'évolution du processus d'apprentissage, en tenant compte des ressources disponibles et la capacité de raisonnement et d'apprentissage à ce sujet. Bibliographie João Gama est chercheur à LIAAD, le Laboratoire d'intelligence artificielle et aide à la décision de l'Université de Porto, en travaillant au sein du groupe d'apprentissage machine. Son intérêt principal de recherche est en apprentissage à partir des flux de données. Il a publié plusieurs articles dans la détection des changements, des arbres de décision apprentissage de flux de données, clustering hiérarchique des flux, etc. éditeur de numéros spéciaux sur les flux de données dans l'analyse intelligente des données, J. Universal Computer Sciences, et nouvelle génération Computing. Il coprésident de la piste sur les flux de données dans Symposium ACM en informatique appliquée, une série d'ateliers sur la découverte des connaissances dans les flux de données, et la découverte des connaissances à partir de données de capteurs avec SIGKDD. Il est l'auteur d'un livre récent sur la découverte des connaissances à partir des flux de données."
585,Revue des Nouvelles Technologies de l'Information,EGC,2011,Towards a DistributedWeb Search Engine,"In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (230 millions at the end of 2009) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability ; in spite of network latency and scattered data. In this talk we present the main challenges behind the design of a distributed Web retrieval system and our research in all the components of a search engine : crawling, indexing, and query processing.",Ricardo Baeza-Yates,http://editions-rnti.fr/render_pdf.php?p1&p=1000919,http://editions-rnti.fr/render_pdf.php?p=1000919,en,"Vers une recherche Web Distributed Engine Ricardo Baeza-Yates Yahoo! Recherche de Barcelone Espagne ricardo.baeza@barcelonamedia.org http://www.dcc.uchile.cl/~rbaeza/ Résumé Dans l'océan des données Web, les moteurs de recherche Web sont le principal moyen d'accès aux contenus. Comme les données sont de l'ordre de pétaoctets, les moteurs de recherche actuels sont très grands systèmes centralisés basés sur des clusters répliquées. Les données Web, cependant, évoluent toujours. Le nombre de sites Web continue de croître rapidement (230 millions à la fin de 2009) et il y a actuellement plus de 20 milliards de pages indexées. D'autre part, les internautes sont au-dessus d'un milliard et des centaines de millions de requêtes sont émises chaque jour. Dans un avenir proche, les systèmes centralisés sont susceptibles de devenir moins efficace contre une telle charge requête de données, ce qui suggère la nécessité des moteurs de recherche entièrement distribués. Ces moteurs doivent maintenir des réponses de haute qualité, le temps de réponse rapide, un débit élevé de requêtes, de haute disponibilité et l'évolutivité; en dépit de la latence du réseau et des données éparses. Dans cet exposé, nous présentons les principaux défis derrière la conception d'un système de recherche Web distribué et nos recherches dans tous les composants d'un moteur de recherche: exploration, l'indexation et le traitement des requêtes. Bibliographie Ricardo Baeza-Yates est vice-président de Yahoo! La recherche pour l'Europe, Moyen-Orient et en latin Ame- rica, conduisant les laboratoires à Barcelone, en Espagne et Santiago du Chili, ainsi que la supervision du laboratoire récent à Haïfa, en Israël. Jusqu'en 2005, il a été le directeur du Centre de recherche sur le Web au Département des sciences informatiques de l'École d'ingénierie de l'Université du Chili; et ICREA Professeur au Département de Technologie de l'Univ. Pompeu Fabra à Barcelone, en Espagne. Il est co-auteur du livre best-seller Modern Information Retrieval, publiée en 1999 par Addison-Wesley avec une deuxième édition à venir en 2010, ainsi que co-auteur de la 2e édition du Manuel des algorithmes et structures de données, Addison -Wesley, 1991; et co-éditeur de la recherche d'information: algorithmes et structures de données, Prentice-Hall, 1992, parmi plus de 200 autres publications. Il a reçu l'Organisation des États américains prix pour les jeunes chercheurs en sciences exactes (1993) et avec deux collègues brésiliens ont obtenu le prix de PAQ COM- pour le meilleur article de recherche CS Brésil (1997). En 2003, il a été le premier informaticien pour être élu à l'Académie des sciences du Chili. En 2007, il a reçu la médaille Graham pour l'innovation dans l'informatique, donnée par l'Université de Waterloo aux ex-anciens hangar distingui-. En 2009, il a reçu la distinction latino-américaine pour sa contribution à CS dans la région et est devenu un ACM Fellow."
603,Revue des Nouvelles Technologies de l'Information,EGC,2010,Action Rules and Meta-actions,,Zbigniew W. Ras,http://editions-rnti.fr/render_pdf.php?p1&p=1001261,http://editions-rnti.fr/render_pdf.php?p=1001261,en,"articles assemblage.pdf Règles d'action et de méta-actions Zbigniew W. Ras Université de Caroline du Nord Charlotte, Caroline du Nord (Etats-Unis). ras@uncc.edu http://coitweb.uncc.edu/~ ras / règles d'action décrivent des transitions possibles d'objets dans un système de décision S d'un état à l'autre par rapport à l'attribut de décision. attributs de classification en S sont divisées en stable et flexible. Les méta-actions sont définies comme des actions qui déclenchent des changements d'attributs flexibles par S soit directement, soit indirectement, en raison de corrélations entre certaines caractéristiques du système. Dans la zone médicale, en prenant un médicament est un exemple d'une méta action étant donné que certains résultats des tests (valeurs des attributs S) pour un patient donné se sont changés. Laboratoire et des examens radiologiques sont des exemples d'attributs de classification qui sont flexibles. Les premières recherches sur la découverte de la règle d'action adopter une approche basée sur des règles et il a fallu l'extraction de règles de classification d'un système de décision avant de construire une règle d'action. Les plus récents algorithmes suivent une approche basée sur les objets et ils extraient des règles d'action directement à partir d'un système de décision. Dans cette présentation, nous montrerons comment les méta-actions peuvent être utilisées pour identifier les règles d'action intéressantes et les règles d'action de moindre coût. RNTI-E-19- 21 -"
613,Revue des Nouvelles Technologies de l'Information,EGC,2010,Applying Markov Logic to Document Annotation and Citation Deduplication,"Structured learning approaches are able to take into account the relationalstructure of data, thus promising an enhancement over non-relationalapproaches. In this paper we explore two document-related tasks in relationaldomains setting, the annotation of semi-structured documents and the citationdeduplication. For both tasks, we report results of comparing relational learningapproach namely Markov logic, to non-relational one namely Support VectorMachines (SVM). We discover that increased complexity due to the relationalsetting is difficult to manage in large scale cases, where non-relational modelsmight perform better. Moreover, our experiments show that in Markov logic,the contribution of its probabilistic component decreases in large scale domains,and it tends to act like First-order logic (FOL).","Jean Baptiste Faddoul, Boris Chidlovskii",http://editions-rnti.fr/render_pdf.php?p1&p=1001329,http://editions-rnti.fr/render_pdf.php?p=1001329,en,"articles assemblage.pdf Application de Markov logique au document Annotation et Citation Déduplication Jean Baptiste Faddoul *, Boris Chidlovskii * * Xerox Research Centre Europe 6, chemin de Maupertuis 38240 Meylan-FRANCE Résumé. approches d'apprentissage structurés sont capables de prendre en compte la structure relationnelle des données, promettant ainsi une amélioration par rapport aux approches non relationnelles. Dans cet article, nous examinons deux tâches liées aux documents dans les domaines paramètre relationnel, l'annotation de documents semi-structurés et la citation Déduplication. Pour les deux tâches, nous présentons les résultats de la comparaison approche d'apprentissage relationnel à savoir la logique de Markov, à une non-relationnelle à savoir Support Vector Machines (SVM). Nous découvrons que la complexité accrue en raison de la mise en relationnel est difficile à gérer dans les grands cas à grande échelle, où les modèles non-relationnels pourraient mieux performer. De plus, nos expériences montrent que, dans la logique de Markov, la contribution de sa composante probabiliste diminue dans les grands domaines d'échelle, et il tend à agir comme la logique du premier ordre (FOL). 1 Introduction Une grande majorité des modèles existants d'apprentissage de la machine peut être considérée comme non relationnelle. Ils représentent chaque objet comme un point isolé dans un espace, et ils apprennent des modèles de prédiction en utilisant les caractéristiques de chaque objet. Une nouvelle tendance dans l'apprentissage statistique est représentée par des modèles relationnels qui tiennent compte de la structure relationnelle des données. Les relations entre les objets sont fréquents dans les cas du monde réel, et de les prendre en compte peuvent offrir une amélioration de la performance potentielle par rapport aux modèles non relationnelles. D'autre part, les ensembles de données du monde réel sont grandes dans le nombre d'objets, les dimensions de caractéristiques et même les numéros de relation. Donc, du point de vue de l'évolutivité, plus simples modèles non relationnelles pourraient évoluer mieux et pourraient surpasser complexes. Dans cet article, nous étudions les modèles relationnels et non relationnels sur deux tâches différentes, l'annotation de documents semi-structurés et la citation Déduplication. Dans nos expériences, nous utilisons comme le meilleur SVM état de l'art modèle non-relationnel. A l'approche des modèles relationnels, nous avons appliqué une logique de Markov (introduite dans Domingos et Richardson (2007)), qui est un apprentissage statistique Relationnel (SRL) qui combine FOL et réseaux de Markov. la logique de Markov a été choisi pour sa capacité à représenter des relations plus complexes que les modèles précé- ously utilisés et un certain nombre de résultats importants présentés lors des tests sur des domaines différents (Culotta et McCallum (2006) et Kok et Yih (2009)). RNTI-E-19- 435 - Application de Markov logique au document Annotation et Citation Déduplication 1.1 Annotation des documents semi-structurés annotation de documents peut être considérée comme une classification1task collective (nous appelons les détails à Chakrabarti et al (1998) et Neville et Jensen. (2003)). Les objets à classer sont des fragments d'un document, tels que les jetons, les lignes et les paragraphes, alors que les classes sont des étiquettes sémantiques de ces fragments. Chaque objet est décrit par un ensemble de caractéristiques et une classe représente son label tic seman- (titre, auteur, référence, etc.). En outre, diverses relations entre deux objets quelconques peuvent être identifiés, comme next_token (), same_paragraph (), etc. 1.2 Citation Déduplication Citation Déduplication est le problème de la détermination des enregistrements dans une base de données faisant référence à la même entité monde réel (Monge et Elkan ( 1996)). Chaque enregistrement dans la base de données est composée de plusieurs champs. Et des informations obtenues à partir de la mise en correspondance d'enregistrement peut se propager à la correspondance des champs et vice versa. Nous avons introduit la différence entre les modèles relationnels et non relationnels et décrit deux tâches dans un cadre de domaines relationnel. Dans la section suivante, nous présentons une logique de Markov. Dans la section 3, nous présentons les résultats de la comparaison entre les modèles logiques de Markov et SVM sur les deux tâches. L'article 4 traite des problèmes d'évolutivité et de la section 5 conclut cet article. 2 logique Markov Markov est un modèle logique d'apprentissage relationnel structuré qui com bine FOL et probabilistes Bilistic modèles graphiques. Il a été introduit par Domingos et Richardson (2007). En FOL, un KB est un ensemble de formules qui peuvent être considérées comme un ensemble de contraintes sur l'ensemble des mondes possibles. Si un monde possible viole une formule, il a une probabilité nulle. L'idée de base dans la logique de Markov est d'adoucir ces contraintes: quand un monde possible viole une formule dans le KB, il est moins probable, mais pas impossible. Les moins formules un monde viole, plus il est probable. Chaque formule a un poids associé qui reflète la façon dont contrainte forte une est: plus le poids, plus la différence de probabilité de journal entre un monde qui répond à la formule et qui n'a pas, d'autres choses étant égales par ailleurs. Un ensemble de formules de la logique de Markov Markov est appelé un réseau logique (MLN). MLN définissent la distribution de probabilité sur mondes 2, où chaque état du réseau de Markov ML, C représente un monde possible. 3 Paramètres Expérience et résultats 3.1 Annotation Tâche Pour la tâche d'annotation, nous avons utilisé la collection BizCard, qui est une collection de cartes de visite numérisées avec différentes mises en page. Chaque carte est segmenté en blocs et des lignes, chaque ligne est segmenté en jetons (chacun a une annotation sémantique) et des séparateurs qui n'ont pas l'annotation (-., ,, etc.). Chaque jeton est annotées avec l'un des 17 classes, comme l'adresse, nom, email, affiliation, etc. La collection contient 106 cartes de visite avec une moyenne de classes d'objets 1Les sont pas indépendants, compte tenu des observations (les caractéristiques). 2A monde possible est une mission de vérité pour tous les échouages ​​de tous les prédicats dans la Base de connaissances (KB). RNTI-E-19 - 436 - J.-B. Faddoul et B. Chidlovskii 30 jetons à annoter dans chaque carte. Chaque jeton est décrit avec 135 caractéristiques qui ont été définies par un expert, les caractéristiques sont classées en trois groupes: caractéristiques de contenu Jeton (par exemple le nombre de chiffres dans un jeton), les caractéristiques d'attributs Jeton (egeg, le type de police) et les caractéristiques de contenu ligne (par exemple le confinement d'un nom de pays dans une certaine ligne) 3.1.1 Modèles nous avons comparé une approche non-relationnelle à savoir SVM avec un relationnel à savoir logique Markov: SVM: nous déployons le paquet libsvm par Chang et Lin (2001). Nous apprenons des modèles de SVM avec le noyau linéaire, parce que la formation avec différents types de rapports de noyaux que le linéaire montre les meilleures performances dans des domaines de grande dimension. Comme caractéristiques, nous avons utilisé toutes les 135 caractéristiques extraites de l'ensemble de données. A l'étape de pré-traitement, nous RESCALE caractéristiques de valeur float finis dans l'intervalle [0,1]. De plus, les caractéristiques qualitatives (comme la couleur) ont été cartographiés dans un ensemble de fonctions booléennes (isBlue, isGreen, etc.). logique de Markov: Pour la formation des modèles logiques de Markov nous déployons le logiciel Alchemy 3. Comme la logique de Markov adapte la syntaxe de FOL, ce qui représente la tâche d'annotation exige une définition de KB. Les prédicats d'un tel KB ont les motifs suivants: • Feat (x, v): x jeton a une valeur v pour la fonction Feat. Le domaine de x est l'ensemble des jetons dans le jeu de la formation au cours de la formation, et dans les tests ensemble au cours des essais. Le domaine de v est l'ensemble des valeurs possibles pour la fonction Feat. Le Feat caractéristique peut être une caractéristique locale dans les x jeton lui-même, ou une caractéristique dans un autre jeton x 'être dans une certaine relation avec x. Il y a 135 caractéristiques prédicats de ce type. • Classe (x, c): x jeton a la classe c. Le domaine de c est l'ensemble de toutes les annotations possibles. • Rel (x, x '): une relation Rel existe entre x et x'. Pour nos expériences, les pred- icat Rel pourrait être l'un des six prédicats, dont chacun est défini sur une paire de jetons: Gauche Frère LB (x1, x2), deuxième à gauche frère 2LB (x1, x2), Droite Frère RB (x1, x2), deuxième à droite Frère 2RB (x1, x2), la ligne suivante NL (x1, x2) et ligne précédente PL (x1, x2). Nous courons plusieurs tests de MLN pour cette tâche. Dans chaque essai, nous avons utilisé différents ensembles de caractéristiques et relations. Deux modèles de formules ont été utilisées: 1. formules non consanguine: Feat (x, v) ⇒ classe (x, c), e à modélise le problème de la classification en fonction ne dispose que sur (sans relations). 2. Formules relationnelles: Classe (x1, c1) ∧ Rel (x1, x2) ⇒ classe (x2, c2), qui modélise les classes basées sur les relations entre les jetons et leurs classes. 3.1.2 Résultats des rapports Tableau 1 résultats des expériences MLN sur la collection BizCard. Dans tous les essais, nous utilisons la validation croisée 5 fois. Dans chaque essai un ensemble différent de relations ont été utilisées avec 3http: //alchemy.cs.washington.edu/ RNTI-E-19- 437 - Application de Markov logique au document Annotation et Citation Déduplication toutes les 135 fonctions. Un grand nombre d'essais ont été fait, les personnes les plus accuracy4 sont présentés dans le tableau 1. Le cas de référence sans relations utilisées donne 66,42% de précision. formules relationnelles améliorer la précision lors de l'utilisation des petits de profondeur, avec une profondeur supérieure (2-RB, 2-LB), nous observons la perte de précision. Relations avec précision aucune 66,42 LB RB 72,85 70,78 LB + RB + LB 67,82 2-LB 66,87 NL + PL 66,56 TAB. 1 - Résultats de précision sur BizCard en utilisant différents paramètres de la logique de Markov. Le tableau 2 compare les valeurs de précision et le temps de fonctionnement en mode validation croisée 5 fois pour les deux SVM et le MLN qui présente les meilleurs résultats. Comme on peut le voir MLN de SVM dans la précision et la durée. 3.2 Citation Déduplication Tâche 3.2.1 CORA CORA Data Set est une collection de 1879 citations dans différents documents de recherche informatique Sciences 5. Il contient des citations à 168 document de recherche différent, donc il y a une moyenne de 11 citations au même article. Chaque citation est segmenté en champs (auteur, titre, éditeur, année, etc.). 3.2.2 Modèles Comme dans le cas de la carte d'affaires, nous avons comparé un SVM avec un classificateur MLN: MVB: nous avons utilisé libsvm par Chang et Lin (2001) pour mettre en œuvre un classificateur binaire basé sur la distance Levenshtein 6 comme suit. Pour chaque paire de citations on calcule la distance Levenshtein entre chaque type de champ (auteur, année, titre, etc.). On obtient alors un vecteur de distance pour le pourcentage 4Le d'exemples correctement classés 5Disponible à http://www.cs.umass.edu/~mccallum/data/cora-refs.tar.gz 6Minimum nombre d'opérations nécessaires pour transformer une chaîne dans la autre, où une opération est une insertion, la suppression ou la substitution d'un seul caractère Précision Temps de marche SVM 78.42 0,3 heures 135 Caractéristiques MLN 135 Feat. 72,85 + LB 5,1 heures par rapport TAB. 2 - SVM et MLN Précision et Durée BizCard. RNTI-E-19 - 438 - J.-B. Faddoul et B. Chidlovskii chaque paire de citations. Ce vecteur de distance est un vecteur dans l'espace de la classe 1 SVM si les deux citations sont les mêmes, et la classe 0 sinon. La tâche se transforme en une tâche de classification binaire. logique de Markov: Nous citerons les résultats par Singla et Domingos (2006). Le KB utilisé contient trois types de prédicats (on parle de la description détaillée de la base de connaissances de leur papier): 1. prédicats de classe sont les prédicats qui doivent être prédits: Auteur (b, a), Titre (b, t), V enue (b, c). 2. prédicats de preuve sont les prédicats observées: HasWordAuthor (a, w), HasWordT itre (t, w), HasWordV enue (v, w). 3. prédicats match sont des prédicats d'égalité entre les champs (ils doivent être associés à des axiomes d'égalité): SameAuthor (A1, A2), SameBib (b1, b2), SameTitle (T1, T2), SAMEV IEU (v1, v2). Les formules dans le KB modélisent les relations en connectant prédicats de preuve et de classe avec prédicats match. ASC Durée SVM 97.88 0,2 heures MLN 98.01 4,2 heures TAB. 3 - ASC et exécution de temps sur CORA. 3.2.3 Résultats Le tableau 3 compare MLN et SVM. Nous avons utilisé AUC (aire sous la courbe de précision rappel) pour pouvoir comparer nos résultats avec Domingos et les meilleurs résultats de Singla. Durée montré dans la table est obtenue en exécutant une validation croisée de 5 fois sur les données. Nous voyons que la CUA pour MLN et SVM est comparable, mais avec une différence significative dans le temps en cours d'exécution. 4 Discussion Comme nos évaluations montrent, MLN n'échelle pas aussi bien que des modèles plus simples comme SVM. Cela est susceptible de se produire lors de l'utilisation complic relations ATED ou ensembles de données très importants, ce qui donne dans un très grand Markov générés réseaux. En fait, FOL est un MLN avec des valeurs infinies de poids, parce que dans FOL chaque formule est une contrainte infiniment dur, donc un monde ne peut pas violer une formule (prouvée dans Domingos et Richardson (2007)). Dans nos expériences, la tendance MLN d'avoir de très grandes valeurs de poids et ainsi agir comme FOL était évident que nous avons utilisé des modèles complexes avec de grands ensembles de données à grande échelle. Le tableau 4 compare la différence entre les valeurs de poids moyen pour la collecte BizCard quand on change la taille du domaine des constantes. Le rôle de la logique dans MLN est juste au niveau de la représentation, afin d'obtenir un petit ularity Gran- dans la représentation des connaissances. Considérant que, au niveau d'inférence, l'inférence logique est attrayante, mais elle est remplacée par inférence probabiliste. Comme expériences montre, cette capacité de représenta- tion fournie par FOL, complique le graphique rendant l'évolutivité une tâche plus difficile. RNTI-E-19- 439 - Application de Markov logique de document d'annotation et Citation Deduplication Fraction de corpus utilisé poids moyen de 10% 1,03 50% 50,3% 100 807,01 TAB. 4 - Les poids moyens sur différents sous-ensembles de BizCard. 5 Conclusion logique de Markov est un modèle d'apprentissage de la structure capable de modéliser des relations complexes entre les objets pour mieux attraper la complexité des données du monde réel. Nos expériences confirment que la modélisation des relations d'objet et de les utiliser dans l'apprentissage peut améliorer les performances des tâches de documents pertinents. Néanmoins, sa performance reste modeste par rapport aux meilleurs modèles non relationnelles. Un autre inconvénient est son lac d'évolutivité. inconvénient d'un tel est dû à la très grande taille des réseaux de Markov générés. Une solution possible au problème est de combiner des modèles relationnels et non relationnelles, de manière à ce que les modèles non relationnelles formés avec des caractéristiques de l'objet alimenteront des modèles relationnels formés avec ces prédictions et que les relations. Références Chakrabarti, S., B. Dom et P. Indyk (1998). Amélioration de la catégorisation des liens hypertexte en utilisant hyper-. Chang, C.-C. et C.-J. Lin (2001). LIBSVM: une bibliothèque pour les machines à vecteurs de support. Logiciel disponible à http://www.csie.ntu.edu.tw/ cjlin / libsvm. Culotta, A. et A. McCallum (2006). logique pratique markoviens contenant quantificateurs du premier ordre avec une application à l'incertitude d'identité. En CHSLP '06: Actes de l'atelier sur les problèmes et de calcul ardus conjoint Inference dans le discours et le traitement du langage, Morristown, NJ, USA, pp 41-48.. Association de linguistique informatique. Domingos, P. et M. Richardson (2007). logique de Markov: Un Cadre Unifié pour l'apprentissage statistique Relationnel. Dans L. Getoor et B. Taskar (Eds.), Introduction à la statistique Relational d'apprentissage, pp. 339-371. MIT Press. Kok, S. et W.-T. Yih (2009). Extraction information produit des recettes de courrier électronique en utilisant la logique de markov. Dans Actes de la sixième Conférence sur Email et Anti-Spam. Monge, A. et C. Elkan (1996). Le problème de la correspondance des champs: algorithmes et applications. Dans aux instances de la deuxième Conférence internationale sur la découverte de connaissances et d'exploration de données, p. 267-270. Neville, J. et D. Jensen (2003). Classement collective avec les réseaux de dépendance relationnelle. Journal of Research Machine Learning 8, 2007. Singla, P. et P. Domingos (2006). résolution Entité avec la logique de markov. Dans En ICDM, pp. 572-582. IEEE Computer Society Press. RNTI-E-19 - 440 -"
641,Revue des Nouvelles Technologies de l'Information,EGC,2010,Density estimation on data streams : an application to Change Detection,"In recent years, the amount of data to process has increased in manyapplication areas such as network monitoring, web click and sensor data analysis. Data stream mining answers to the challenge of massive data processing, this paradigm allows for treating pieces of data on the fly and overcoming data storage. The detection of changes in a data stream distribution is an important issue. This article proposes a new schema of change detection :i) the summarization of the input data stream by a set of micro-clusters;ii) the estimate of the data stream distribution exploiting micro-clusters;iii) the estimate of the divergence between the current estimated distribution and a reference distribution;iv) diagnostic step through the contribution of each predictive variable to the overall divergence between both distributions.Our schema of change detection is applied and evaluated on artificial data streams.","Marie-Luce Picard, Benoît Grossin, Alexis Bondu",http://editions-rnti.fr/render_pdf.php?p1&p=1001297,http://editions-rnti.fr/render_pdf.php?p=1001297,en,"articles assemblage.pdf estimation de densité sur les flux de données: une application de détection de changement de Alexis Bondu *, Benoît Grossin *, Marie-Luce Picard * * EDF R & D ICAME / SOAD, 1 avenue du Général de Gaulle, 92140 Clamart. firstname.name@edf.fr Résumé. Ces dernières années, la quantité de données à traiter a augmenté dans de nombreux domaines d'application tels que la surveillance du réseau, cliquez sur Web et la yse de données du capteur. réponses minières de flux de données au défi du traitement de données massives, ce paradigme permet de traiter des morceaux de données à la volée et de surmonter le stockage des données. La détection de changements dans une distribution de flux de données est une question importante. Cet article propose un nouveau schéma de détection de changement: i) le risation Résumés d'du flux de données d'entrée par un ensemble de micro-agrégats; ii) l'estimation de la distribution de flux de données exploitant des micro-amas; iii) l'estimation de la convergence entre la distribution di- et une répartition de référence de courant estimé; iv) l'étape de diagnostic grâce à la contribution de chaque variable prédictive à l'écart global entre les deux distributions. Notre schéma de détection de changement est appliqué et évalué sur les flux de données artificielles. 1 Introduction Ces dernières années, la quantité de données à traiter a augmenté dans de nombreux domaines d'application tels que les flux réseau, cliquez sur Web et d'analyse de données capteur. l'extraction de flux de données indique les algorithmes qui traitent tuples 1 à la volée: quand ils sont émis, sans les stocker. Le traitement des tuples doit être aussi rapide que possible, ce qui permet de gérer les flux de données à haut débit. Un problème important dans le traitement des flux de données détecte des changements dans la distribution sous-jacent qui est généré par tuples. La conception des systèmes de détection de changement qui sont d'ordre général, évolutive et statistiquement pertinente est un grand défi. Un changement dans la distribution sous-jacente peut être interprétée en différentes façons: i) le phénomène a servi ob- dérive naturellement en raison d'un changement dans un contexte caché (Widmer et Kubat, 1996) qui n'est pas donnée explicitement par facteur prédictif; ii) un changement anormal a lieu dans le système observé. Distinguer les deux cas est une question très difficile qui nécessite une expertise sur l'application. Dans cet article, nous supposons un expert, qui connaît bien le flux de données observées, peut se prononcer sur l'interprétation des changements détectés. Une vue d'ensemble des principales approches de détection de changement est donnée par A. Dries (Dries et Rückert, 2009): Détection des changements dans la distribution des tuples peut être considérée comme un test d'hypothèse statistique qui implique deux échantillons de tuples multidimensionnels. De tels problèmes sont étudiés dans la littérature statistique. Les tests de Wald-Wolfowitz et Smirnov a été généralisée 1. Le terme « tuple » fait référence à une donnée qui est émise à partir du flux d'entrée. RNTI-E-19- 229 - Estimation de densité sur flux de données à des ensembles de données multidimensionnelles dans (Friedman et Rafsky, 2006). Par la suite, des approches fondées sur des analyses plus proches voisins (Hall, 2002) ou la distance entre les estimations de densité (Anderson et al., 1994) ont été mis au point. Plus récemment, les statistiques fondées sur un écart moyen maximal pour les noyaux universels sont devenus populaires (Gretton et al., 2006). Une gamme de travaux statistiques sur la détection de changement brusque ont été fait (Basseville et Nikiforov, 1993, Desobry et Davy, 2003). Dans cet article, un nouveau schéma de détection de changement est proposé (voir la figure 1). Ce schéma est composé de quatre étapes successives. Dans la section 2 du flux de données d'entrée est résumée par un algorithme de micro-clusters. Cette première étape est nécessaire en raison du taux élevé de flux de données d'entrée, dans la pratique toutes les lignes ne peuvent pas être traitées en temps réel. L'algorithme « Denstream » (Feng Cao et al., 2006) a la capacité de résumer les zones denses de l'espace d'entrée et d'oublier les anciens tuples par une pondération en fonction du temps. Nous vous proposons un moyen simple de régler cet algorithme en termes de durées. Dans la section 3 une nouvelle variante de la fenêtre de Parzen est proposé et il est utilisé pour estimer la distribution sous-jacente du flux de données. Cet estimateur de densité exploite le résumé du flux de données d'entrée au lieu de tuples. Cette étape est répétée périodiquement avec un taux inférieur à celui de l'émission de tuples à partir du flux de données. L'article 4 montre comment la distance entre la répartition actuelle estimée et une distribution de référence peut être évaluée par la divergence de Kullback-Leibler. Cette mesure permet d'envoyer une alarme à l'expert lorsque les deux distributions sont significativement différentes. La dernière étape de notre schéma consiste en un diagnostic qui est donnée à l'expert pour l'aider à comprendre les causes de l'anomalie détectée. La contribution de chaque variable à la distance globale entre les deux distributions est évaluée en raison d'un nouveau critère proposé. Synopsis de données d'entrée courant basée sur l'estimation de détection de DiagnosticEvents synopsis Densité Densité à base Clustering (DenStream) Addapted Parzen fenêtre estimateur divergence de Kullback-Leibler contribution de chaque variable de l'étape de divergence 1 (voir la section 2) étape 2 (voir la section 3) l'étape 3 (voir la section 4) l'étape 4 (voir la section 4) FIG. 1 - schéma global pour la détection de changement dans la distribution de flux de données d'entrée. Enfin, notre approche est appliquée et évaluée sur deux flux de données artificielle dans la section 5. applications industrielles possibles de nos travaux de schéma et futurs sont discutés dans la section 6. 2 Summarization du flux de données d'entrée Dans le paradigme de flux de données, tuples émis ne peuvent pas être exhaustive stockées et traitées en raison du taux élevé de flux d'entrée. Cette section présente la récapitulation de flux de données d'entrée qui est une étape préliminaire dans le traitement de détection de changement. Notre approche exploite l'algorithme « Denstream » (. Feng Cao et al, 2006) pour résumer le flux de données: une pondération en fonction du temps est appliquée sur un ensemble de micro-clusters. RNTI-E-19 - 230 - A. Bondu et al 2.1 flux de données pondérées sont progressivement Tuples émis à partir du flux de données et sont pondérées en fonction de leur âge. Plus précisément, tuples (xi) dénotées par sont définis dans Rk et sont caractérisés par le vecteur {x1i, X2i ... xki}. Chaque tuple xi est émise à l'instant (Tcurrent - αi) avec αi indiquant l'âge de xi. A Tcurrent de l'heure chaque tuple est pondérée par wi = 2-λ.αi, où λ est un paramètre d'affaiblissement appartenant à l'intervalle] 0, + ∞]. Plus la valeur de λ, plus l'importance des données historiques par rapport aux données les plus récentes. Dans cet article, N représente le nombre total de tuples émis à Tcurrent. Que WN soit le poids total du flux de données à l'instant où Tcurrent de N tuples ont été émis. Nous avons WN = ΣN i = 1 2 -λ.αi et WN + 1 = ΣN i = 1 -λ 2 (αi + At) + 1, avec At correspondant au temps écoulé entre l'émission des deux dernières tuples . Sous l'hypothèse que le taux de flux de données est constante, le poids total est défini comme récursivement WN + 1 = 1 + 2 × λ.Δt WN. Cette série « géométrique arithmétique » converge 2 à limN → + ∞ WN = 11-2-λΔt. 2.2 Les micro-amas Un ensemble d'objectifs micro-amas de résumer le flux de données d'entrée des informations de maintien sur la distribution de densité. Ce synopsis est maintenu en mémoire à tout moment. Le « jème » de MCJ de cluster micro (cj, rj, wj) est défini par: i) un wj de poids qui correspond à la somme des poids des tuples appartenant au cluster (désignés par x1j, x2j ... xnjj) avec wj = Σnj i = 1 2 -λαij; ii) la cj centrale qui est un vecteur correspondant au barycentre pondéré d'exemples avec cj = 1 wj Σnj i = 1 wijxij; iii) le rj rayon qui est un vecteur qui correspond à l'écart type pondéré avec rj = 1wj √Σnj i = 1 .d wij (xij, cj) 2 avec la distance euclidienne notée d (). L ' « âge » de tuples augmente quand une nouvelle ligne est émise, comme αi ← αi + Dt, le temps écoulé entre deux tuple est considéré comme At constant. Le nouveau tuple est affecté au micro-cluster plus proche. L'ensemble de micro-agrégats est maintenu en raison d'un iter Procédé ative. Les poids des micro-amas sont maintenus par les étapes successives suivantes: 1. le vieillissement de tous les micro-agrégats tels que w (1) j ← wj 0,2-λΔt ∀j ∈ [1, C]; 2. L'augmentation du poids de la micro-cluster j * où le tuple émis est affecté tel que w (2) j * w ← (1) j + 1. Deux amas caractéristiques sont nécessaires pour maintenir le centre et le rayon des micro-amas (Zhang et al., 1996). Soit CF 1j [respectivement CF 2 j] est un vecteur k-dimensionnel de mémorisation, pour chaque variable, la somme pondérée des coordonnées [respectivement la somme des coordonnées au carré] des exemples appartenant à la micro-cluster « jème »: CF 1j = Σ nj i = 1 wijxij [respectivement CF 2 j = i = 1 Σnj wijx 2 ij]. cj et rj sont maintenus comme suit: cj = CF 1 et rj = wj √ | CF 2J | wj - (| CF 1j | wj) 2 2.3 L'approche Denstream Les poignées d'approche Denstream deux types de micro-cluster correspondant à différentes fonctions. L'ensemble de ""potentiel micro-clusters"", désignés par mcp, résume 2. significative in- Dans ce cas, la condition | 2-λΔt | ≤ 1 est toujours satisfaite. RNTI-E-19- 231 - Estimation de densité sur la formation de flux de données à partir du flux de données. Les micro-grappes dépassant un poids minimal sont considérés comme représentant des informations importantes. L'ensemble de « aberrantes-micro-clusters », désignés par AGC, consiste en un tampon de conservation des informations négligeable du flux de données. L'intuition est la suivante: une légère micro-cluster (sous le poids minimal) peut se développer si la densité distri- bution du flux de données est en train de changer. L'objectif est de conserver des informations insignifiantes au début détecter de nouvelles zones denses dans le flux de données. Deux contraintes sont appliquées sur les micro-groupes: i) les micro-amas dont le poids diminue en dessous d'un poids minimum (notée μ) sont supprimés; ii) un nouveau tuple est fusionné dans son micro-groupe le plus proche si son rayon de mise à jour r * j est inférieur à un écart-type maximum (notée). Ces contraintes assurent les micro-clusters représentent des zones denses de l'espace Rk où tuples sont apparus récemment. Une stratégie est mise en œuvre par la taille de l'algorithme « Denstream ». Cette stratégie vise à réglementer de l'espace mémoire nécessaire pour stocker les deux ensembles de micro-clusters et mco. Mcp Comment nous paramètres tune en termes de durées: Nous supposons notre système de détection de changement ap- proche est exploitée par un expert qui connaît bien les phénomènes intégrés dans le flux de données. L'algorithme « Denstream » implique plusieurs paramètres (λ, u et) qui peuvent être difficiles à régler par l'expert. Dans ce paragraphe, l'examen se fait sur la façon de régler les paramètres d'une manière compréhensible. L'expert connaît la durée de validité de tuples émis et il est capable de mettre en place une période de demi-vie 3 (notée ΔHalfLivet). Le paramètre d'affaiblissement peut être déterminé dans un second temps tel que λ = - log2 (12) / Δ t HalfLive. Nous démontrons que le paramètre μ qui représente le poids minimal de grappes est délimitée comme suit: 2-λΔ ClusMin t 1 - 2-λΔt> μ ≥ 1 au 2 janvier-λΔClusMaxt Soit ΔClusMaxt soit le laps de temps au-delà de l'arrivée d'un nouveau tuple dans un p-micro-cluster qui n'est pas reasonable de garder le statut « potentiel ». Pour tout p-micro-cluster, nous avons wj .2-λΔ ClusMax t + 1 <μ et wj <μ. A la fin, on obtient μ> 1 / (1 - 2 λΔ ClusMax t). Que ΔClusMint soit la durée minimum de temps qui doit être maintenu un non mis à jour p-micro-cluster dans le synopsis. Pour tout p-micro-cluster, nous avons wj .2-λΔ ClusMin t> μ. Le poids d'un p-micro-cluster est inférieure ou égale au poids total du flux de données, donc nous avons ClusMin W.2-t λΔ> μ. A la fin on obtient (2-λΔ ClusMin t) / (1 - 2-λΔt)> μ. Dans cet article, nous adoptons le même choix que dans (Feng Cao et al., 2006) où les auteurs définissent la période de taille T comme le minimum de ΔClusMaxt. Nous considérons que T et Δ t HalfLive sont donnés par l'expert. Dans ces conditions μ peut être exprimée comme suit: μ = 1 1 - 2 - T ΔHalfLivet Un nouveau micro-cluster est créé lorsque l'écart-type maximal est re dans le plus proche faisait mal micro-cluster d'un tuple émis. Intuitivement, la valeur des influences du nombre de micro-agrégats potentiels qui sont maintenues dans la mémoire. L'accord est une question de l'écart parce que l'ensemble standard du flux de données d'entrée est inconnue dans le cas général. Dans cet article, nous supposons que l'écart-type global à être connu par l'expert et est ajustée en proportion de l'écart-type global. 3. La valeur de wi est périodiquement divisé par 2. RNTI-E-19 - 232 - A. Bondu et al Notations: • l'ensemble des MCP-micro-cluster potentiel; • mco l'ensemble des micro-cluster-aberrant; • u le poids minimum d'un micro-cluster potentiel; • l'écart-type maximum de un micro-cluster potentiel; • T la période d'élagage. Répétez Obtenez le point suivant xi + 1 à partir du flux de données. / * Procédure de fusion * / Essayez de fusion xi + 1 à son plus proche p-micro-cluster, noté mcp (cp, rp, wp). Soit r * p le nouveau rayon de mcp. Si r * p ≤ puis fusionner xi + 1 dans mcp, et mettre à jour cp, rp, wp. Essayez de fusion autre xi + 1 à son plus proche o-micro-cluster, noté mco (co, ro, wo). Soit r * o être le nouveau rayon de mco. Si r * o ≤ puis fusionner xi + 1 dans mco, et mise à jour co, ro, wo. Si wo> μ puis retirez mco du tampon des valeurs aberrantes et de créer un nouveau p-Microcluster par mco. end Si d'autre Créer une nouvelle o-micro-cluster par xi + 1 et l'insérer dans la mémoire tampon des valeurs aberrantes. end if end Si / * Procédure d'élagage * / Si la taille Période T est écoulée alors pour chaque p-micro-cluster mcp (cp, rp, wp) faire Si wp <μ puis Supprimer fin mcp Si end Pour Pour chaque o-micro -cluster mco (co, ro, wo) faire Si wo <2-λ (à + T) -1 2 λT -1 puis Supprimer fin de mco Si end pour fin Si jusqu'à ce que le flux de données existe algorithme 1: synthèse de flux de données par approche Denstream RNTI-E-19- 233 - estimation de densité sur les données flux 3 estimation de densité exploitant la synopsis Cette section montre comment le synopsis du flux de données d'entrée est exploitée pour estimer la densité des données. On modifie l'estimateur de densité de fenêtre de Parzen (Parzen, 1962) pour exploiter les micro-agrégats à la place de tuples. Le paragraphe 3.1 présente la fenêtre de Parzen classique dans le cas du noyau gaussien, au paragraphe 3.2 cet estimateur de densité est adaptée pour les micro-amas. 3.1 Parzen de Windows Parmi les large gamme de modèles capables d'estimer la densité de données à partir d'un ensemble de tuples, fenêtre Parzen muni d'un noyau gaussienne (Shawe-Taylor et Cristianini, 2004) a l'avantage de nécessiter quelques paramètres. L'équation 1 correspond à la « sortie » de ce modèle prédictif qui est une estimation de la probabilité d'observer le tuple x ∈ Rk. K (x - xi) est une fonction noyau évaluer la proximité entre les tuples x et xi, cette fonction est additionnée sur tous les tuples émis. P (x) = 1 N NΣ i = 1 K (x - xi) (1) Dans la pratique, la fonction de noyau doit être spécifié. L'équation 2 correspond à la « sortie » d'une fenêtre de Parzen pourvu d'un noyau gaussien 4. Dans ce cas, la fenêtre de Parzen implique un seul paramètre qui est σ: écart type du noyau Gaussien. K (x - xi) = 1 (σ √ 2π) k EXP- d (x, xi) 22.σ2 (2) La figure 2 illustre l'estimation de P (x) par un estimateur de fenêtres de Parzen. gaussiennes sont positionnés sur chaque tuple, à côté, ils sont additionnés et normalisés. Dans ce cas, chaque tuple contribue à l'estimation de P (x). x P (x) estimation densité de la fenêtre de Parzen Contribution de chaque exemple de formation des exemples de formation figure. 2 - Estimation de la distribution des flux de données en raison d'une fenêtre de Parzen. 4. Nous considérons l'écart-type du noyau gaussienne est constante sur toutes les dimensions de l'espace d'entrée Rk. RNTI-E-19 - 234 - A. Bondu et al 3.2 Notre modifiées fenêtres Parzen Dans ce paragraphe, l'estimateur de densité de fenêtre de Parzen est adaptée pour exploiter l'ensemble des micro-amas po- tentiel au lieu de tuples. La distribution de P (x) est approchée par l'équation 3: P * (x) = 1 CW CΣ j = 1 2π ωj√ (δ2 + R2J) k EXP- d (x, cj) 2 2 (δ2 + R2J) (3) • W représente le poids total du flux de données; • C représente le nombre de micro-agrégats potentiels résumant le flux de données; • ωj représente le poids du micro groupe jème; • cj représente le barycentre des points pondérés appartenant au micro groupe jème; • rj désigne l'écart-type des points pondérés appartenant à la jème micro grappe; • δ représente un paramètre planéité joue le même rôle que σ dans l'équation 2. Chaque tuple observée est censé être le plus probable d'un ensemble de tuples qui inobservée est normalement distribué avec un écart-type égal à δ. Dans cette hypothèse, la loi de la variance totale donne la variance du micro-cluster potentiel « jeme » comme la somme de la variance intra-δ2 et la R2J entre-variance. Dans l'équation 3 noyaux gaussiens sont positionnés sur le centre de chaque micro-cluster potentiel. Ensuite gaussiennes sont additionnés et normalisés en ce qui concerne le nombre de micro-clusters potentiels et le poids total du flux de données. L'écart-type ot exemples de formation x P (x) la position du centre et la valeur des exemples de formation de poids estimation de la densité de la fenêtre de Parzen Micro Cluster FIG. 3 - Influence du poids des micro-amas sur l'estimation de la densité La figure 3 illustre l'estimation de P (x) par nos fenêtres Parzen modifiés. Sur cette figure, l'ensemble des tuples est divisé en deux micro-agrégats potentiels dont le rayon est symbolisé par une ligne pleine horizontale et les poids sont symbolisés par une ligne pointillée verticale. D'une part, l'RNTI-E-19- 235 - Estimation de densité sur une estimation de flux de données de P (x) est moins précis que sur la figure 2 en raison de la perte de chaque emplacement de tuple. D'autre part, cette estimation prend en compte le poids de chaque groupe. L'estimation de la distribution de P (x) change au fil du temps en raison du vieillissement des micro-amas. Si aucun changement se produit dans la distribution sous-jacente, les tuples dont une diminution du poids sont remplacés par de nouveaux émis: dans ce cas, l'estimation de P (x) ne changeront pas. Dans le cas contraire, les tuples non remplacés dans un micro-cluster engendrer une diminution de son poids: alors un changement de l'estimation de P (x) est observée. 4 Changement de détection et de diagnostic On suppose qu'une anomalie qui se produit dans les résultats de flux de données d'entrée dans un changement de dis- tribution de P (x). Une distribution de référence est mis en place après une période d'apprentissage sans anomalies à détecter. L'expert examine le flux de données d'entrée pour assurer aucune anomalie n'a eu lieu au cours de cette période. Ensuite, l'estimation actuelle de la distribution de P (x) est comparée à la distribution de référence en raison de la divergence de Kullback-Leibler (Hershey et Olsen, 2007) montre l'équation 4. La divergence Kullback-Leibler a des propriétés statistiques intéressantes, en parti- paramètres LAR de constatation d'un modèle statistique maximisant la probabilité est analogue à la recherche de paramètres réduisant au minimum la divergence (Eguchi et Copas, 2006). La divergence de Kullback-Leibler généralise des tests statistiques classiques comme le t-test et le χ2: i) le test t est équivalente à la divergence de Kullback-Leigler entre deux distributions normales; ii) la fonction χ2 est le premier terme dans le développement de Taylor de la divergence de Kullback-Leigler. Dans notre schéma de détection de changement, une alarme est envoyée à l'expert lorsque la divergence entre les distributions et Préf P * atteint un seuil fixe. KL <Pref (x) ‖P * (x)> = - ∫ Rk Pref (x) log Pref (x) * P (x) dx (4) Le diagnostic est requis par l'expert afin de donner une réponse appropriée à l'alarme. Les objectifs de la phase de diagnostic à l'évaluation de la contribution de chaque variable de l'écart entre Pref et P *. Ainsi, l'expert est informé que facteur prédictif sont impliqués dans le changement détecté. La contribution des variables est évaluée par l'équation 5. Soit KLiminus soit la divergence Kullback-Leigler évalué dans un (k - 1) après l'exclusion espace de dimension de la variable « ième ». Lorsque la contribution de la variable « LTH » est évaluée, KLlminus est comparée à la somme des KLminus sur tout les variables, la contribution est normalisée. Contrib (l) = (rk i = 1 i KL moins) - KLlminusΣk i = 1 i KL moins (5) la contribution de chacun des objectifs variables à aider l'expert de se prononcer sur l'interprétation de la variation détectée. Dans la pratique, l'expert peut être autorisé à mettre à jour la distribution de référence avec la distribution actuelle si le changement détecté n'est pas une anomalie. Cette mise à jour constitue un moyen possible de prendre en compte la dérive naturelle du phénomène observé. RNTI-E-19 - 236 - A. Bondu et al 5 Expériences Dans cette section, notre schéma de détection de changement est appliqué sur deux flux de données d'artificiel. L'objectif est d'évaluer la capacité de notre schéma pour détecter deux types de changements différents: i) un changement dans la moyenne d'une distribution normale; i) un changement de l'écart type d'une distribution normale. 5.1 Protocole expérimental Les deux flux de données artificielle considérés partagent la même structure temporelle. Chaque seconde, un tuple est tiré d'une distribution sous-jacente qui change au fil du temps. La figure 4 montre comment la distribution sous-jacente évolue. Les 2000 premiers tuples sont émis à partir de la « distribution initiale » qui représente l'opération habituelle. En ce moment, la distribution de référence est mis en place: notre schéma de détection de changement commence. Entre 4000 et 6000 secondes, la distribution sous de couchage se déplace progressivement à partir de la « initial » pour la distribution « modifié ». Ensuite, 2000 tuples sont émis de la distribution « modifiée ». Entre 8000 et 10000 secondes, la distribution sous-jacente progressivement de retour à son état « initial ». Enfin, 2000 tuples sont émis par la distribution « initiale ». Répartition des données flux 0 4000 8000 Temps (s) 12000 Distribution initiale de référence mis en place la distribution modifiée Fig. 4 - Structure temporelle des deux flux de données artificielles. Dans nos expériences tuples sont définies dans R2. Les distributions « initial » et « modifié » sont définis dans le Tableau 1 pour les deux flux de données artificielles. Ces distributions normales sont désignées par N (m, v), où m est un vecteur à deux dimensions correspondant à la moyenne et v est la matrice de covariance. distribution initiale distribution modifiée flux de données 1: modification moyenne N « 0 0, 1 0 0 1« N « 4 8, 1 0 0 1« flux de données 2: changement de l'écart type N « 0 0, 1 0 0 1« N « 0 0 4 0 0 9« TAB. 1 - Définition des distributions « initial » et « modifiés » pour les deux flux de données artificielles. Notre schéma de détection de changement implique plusieurs paramètres qui doivent être fixés avant les expériences. L'algorithme « Denstream » qui résument le flux de données d'entrée (voir la section 2.3) est paramétrée par = 0,1, ΔHalfLivet = 300s et T = 1000s. Le paramètre de planéité de notre estimateur de densité (voir section 3.2) est fixé par δ = 1. RNTI-E-19- 237 - Estimation de densité sur flux de données 5.2 Résultats La figure 5 présente les résultats de nos expériences, le graphique gauche [respectivement le droit tableau] indique la détection d'un changement dans la moyenne [respectivement de l'écart type] de la distribution sous-jacente (décrite dans le tableau 1). Sur les deux cartes, les correspond à axe horizontal au temps et commence quand est Répartissez la distribution « de référence » vers le haut (à t = 2000). Les correspond à axe vertical à l'écart entre la « référence » et les distributions « courant ». La contribution de chaque variable à la divergence est également symbolisée par des couleurs. FIGUE. 5 - détection des changements dans la distribution des deux flux de données artificiels. Le premier flux de données artificielle implique un changement dans la moyenne d'une distribution normale (graphique de gauche sur la figure 5). Dans ce cas, le changement qui se produit lorsque t ∈ [4000, 6000] est détecté tôt, en effet l'augmentation de la divergence significative une fois que t = 4500. Entre 6000 et 8000 secondes, l'augmentation de la divergence à son maximum (KL = 25) et les contributions bien estimer le mouvement de la distribution sous-jacente des deux dimensions. Le retour à la distribution initiale sous-jacente (t ∈ [8000, 10000]) est dete CTED relativement tard. La divergence maintient des valeurs élevées jusqu'à ce que t = 9000 et diminue fortement après. Ce comportement peut être expliqué soit le laps de temps nécessaire pour supprimer les micro-clusters potentiels inutiles dans le résumé du flux de données d'entrée. Le second flux de données artificiel implique un changement dans l'écart type d'une distribution normale (graphique de droite sur la figure 5). Dans ce cas, la détection de changement est moins nette que pré viously: i) la divergence varie fortement au fil du temps et ne se stabilisent pas; ii) la divergence atteint une faible valeur maximale (KL = 0,6). Cependant, la première modification de la dis- tribution sous-jacent est détecté: la divergence augmente à partir de t = 4800 à t = 6000. Entre 6000 et 8000 secondes, la divergence atteint sa valeur maximale qui est compatible avec la structure du flux de données d'entrée. Au cours de cette période, la contribution de la deuxième variable a tendance à être plus importante que la première variable. Enfin, le retour à la distribution initiale sous-jacente est détectée dans le temps. Ces expériences montrent l'intérêt de notre approche pour la détection de la dérive progressive de la distribution sous-jacente. D'autres tests concluants ont été effectués sur les changements brusques. Dans ce cas, un temps de latence très court est observée en raison de la durée de temps nécessaire pour créer de nouveaux micro-clusters potentiels. On remarque le réglage du paramètre d'affaiblissement λ est sensible et soulève le dilemme entre la réduction de la latence et d'assurer la détection de signification statistique de l'estimation de la distribution. Réglage du paramètre λ pourrait être moins sensible dans la pratique, si le rythme des changements est connu à l'avance par l'expert. RNTI-E-19 - 238 - A. Bondu et al 6 Conclusion et perspectives Cet article propose un nouveau schéma de détection de changement dans la distribution sous-jacente d'un flux de données. Notre approche se compose de quatre étapes successives. Tout d'abord, le flux de données d'entrée est résumée par un ensemble de micro-agrégats du fait de l'algorithme « Denstream » (Feng Cao et al., 2006), ainsi des flux de données à vitesse élevée peut être traitée. L'algorithme « Denstream » a la capacité de résumer les zones denses de l'espace d'entrée et d'oublier les anciens tuples par une pondération en fonction de temps. Nous vous proposons un moyen simple de régler les paramètres de cet algorithme en termes de durées. La deuxième étape consiste en une estimation de la distribution sous-jacente exploitant le résumé du flux de données: une nouvelle variante de l'estimateur de fenêtre de Parzen (Parzen, 1962) est proposé. Ensuite, la dérive de la distribution actuelle estimée est évaluée par rapport à une distribution de référence: la divergence Kullback-Leibler est exploitée (Hershey et Olsen, 2007). A la fin, un diagnostic est donné par un nouveau critère qui évalue la contribution de chaque variable de la distance totale entre les deux distributions. Dans la pratique, cette dernière étape pourrait être utile de comprendre les causes d'une anomalie détectée et d'y répondre d'une manière appropriée. Depuis notre schéma de détection de changement implique un estimateur de densité, la probabilité de chaque tuple émise peut être estimée par la fenêtre Parzen actuelle. Cette information devrait être exploitée pour détecter rapidement les changements brusques dans la distribution sous-jacente, en supposant qu'un changement provoque l'émission d'une séquence improbable de tuples. Dans ce cas, la principale difficulté est de gérer la dépendance temporelle des tuples émis, les travaux futurs étudieront ce point. Un autre aspect sur lequel nous travaillons est la quantification théorique des informations perdues en utilisant des micro-clusters au lieu de tuples, lorsque la distribution du flux de données est estimée. Les poignées algorithme « Denstream » la variance de chaque micro-cluster comme une seule valeur scalaire, cela représente une perte d'information importante. Par exemple, la matrice de covariance de tuples émis pourrait être maintenue en ligne pour chaque micro-cluster. Dans les œuvres FUTUR, nous allons étudier le maintien en ligne de la matrice de covariance et des moments statistiques plus élevés, et nous wil l utiliser ces nouveaux éléments d'information pour estimer plus précisément la répartition des tuples. Notre schéma de détection de changement a été favorablement évaluée sur deux flux de données artificielle. Dans nos travaux futurs, d'autres expériences vont évaluer l'influence de plus en plus la dimension de l'espace d'entrée sur la capacité de notre schéma pour détecter les changements. Enfin, notre schéma sera appliqué sur les flux de données réelles. En particulier, nous visons à améliorer la maintenance préventive dans les centrales électriques grâce à la détection d'événements inhabituels. De manière plus générale, notre schéma de détection de changement pourrait être exploitée dans de nombreux domaines d'applications. Par exemple, la NASA a lancé un vaste programme de recherche en gestion de la santé intégrée des véhicules dont le but est de détecter automatiquement, diagnostiquer, prédire et atténuer les effets indésirables pendant le vol d'un aéronef (Srivastava, 2009). La détection précoce des anomalies sur les flux de données de capteur représente un réel intérêt pour la communauté scientifique. Références Anderson, N. H., P. Hall et D. M. Titterington (1994). Les statistiques de test sur deux échantillons pour écarts entre mesurables ING deux fonctions de densité de probabilité à plusieurs variables en utilisant des estimations de densité à base de noyau. Journal of multivariée Analyse 50 (1), 41-54. Basseville, M. et I. V. Nikiforov (1993). La détection de changements abrupts: tion théorie et Applica-. Prentice Hall. RNTI-E-19- 239 - Estimation de densité sur flux de données Desobry, F. et M. Davy (2003). Support Vector-Based détection en ligne des changements abrupts. Dans Proc. IEEE ICASSP, Hong Kong, pp. 872-875. Dries, A. et U. Rückert (2009). Adaptive Concept de détection de dérive. Dans Conférence SIAM sur Data Mining, p. 233-244. Eguchi, S. et J. Copas (2006). L'interprétation divergence Kullback-Leibler avec le lemme Pearson Neyman. Journal of Multivariate Analysis 97 (9), 2034-2040. Feng Cao, F., M. Ester, W. Qian et Zhou A. (2006). à base de densité regroupement sur un flux de données avec le bruit evolv- ment. Dans Conférence SIAM sur Data Mining, p. 328-339. Friedman, J. et L. Rafsky (2006). généralisations multivariées des tests sur deux échantillons Wald-Wolfowitz et Smirnov. Annales de statistique 7 (4), 697-717. Gretton, A., K. M. Borgwardt, M. J. Rasch, B. Schölkopf et S. A. J. (2006). Une méthode du noyau pour les deux-Sample-problème. En NIPS, pp. 513-520. MIT Press. Hall, P. (2002). Permutation tests pour l'égalité des distributions en milieu de grande dimension. Biometrika 89 (2), 359-374. Hershey, J. R. et P. A. Olsen (2007). Approximation les Kullback Leibler tween gaussiennes modèles Be- Mélange. En ICASSP: IEEE Conférence internationale sur l'acoustique, Speech and Signal Processing, Volume 4, pp 317-320... Parzen, E. (1962). Sur l'estimation d'une fonction et d'un mode de densité de probabilité. Annales de la statistique mathématique 33, 1065-1076. Shawe-Taylor, J. et N. Cristianini (2004). Méthodes du noyau pour l'analyse de modèle. La presse de l'Universite de Cambridge. Srivastava, A. (2009). L'extraction de données de la NASA: de la théorie à des applications. Dans Proc. KDD, Paris, pp. 7-8. Widmer, G. et M. Kubat (1996). L'apprentissage en présence de Concept Drift et caché Contextes. Machine Learning 23’ (1), 69-101. Zhang, T., R. Ramakrishan, et M. Livny (1996). BIRCH: Un clustering efficace des données Méthode de très grandes bases de données. Sigmod Rec. 25 (2), 103-114. Résumé CÉS Dernieres, La QUANTITE Années de Donnees Ë Ë Traiter de la DANS Augmentée considérablement les applications Nombreuses. La fouille de flux de Données au défi des Répond Mas- Sives par Données des Traitements à la volée Qui de capacity Une requièrent reasonable stockage. Detection of in the density Changements de probabilité d'un flux is Une question importantes. article propose un nouveau this schéma de Detection of Qui se Compose changement de Suivantes Quatre ÉTAPES: i) le résumé du flux ensemble par un de micro-clusters; ii) L'estimation La densité de probabilité de flux GRÂCE aux DU micro-grappes; iii) l'estimation de la divergence between Estimée à l'density instant et juin courant de density référence; iv) un diagnos la contribution tic estimant de variables each descriptif à la divergence separé les Qui globale deux densi- Tés. Notre schéma de détection de changement is FINALEMENT et Evalue sur Appliqué deux flux de Données artificiels. RNTI-E-19 - 240 -"
642,Revue des Nouvelles Technologies de l'Information,EGC,2010,Detecting Anomalies in Data Streams using Statecharts,"The environment around us is progressively equipped withvarious sensors, producing data continuously. The applications usingthese data face many challenges, such as data stream integration over anattribute (such as time) and knowledge extraction from raw data. In thispaper we propose one approach to face those two challenges. First, datastreams integration is performed using statecharts which represents aresume of data produced by the corresponding data producer. Second,we detect anomalous events over temporal relations among statecharts.We describe our approach in a demonstration scenario, that is using avisual tool called Patternator","Vasile-Marian Scuturici, Dan-Mircea Suciu, Romain Vuillemot, Aris Ouksel, Lionel Brunie",http://editions-rnti.fr/render_pdf.php?p1&p=1001394,http://editions-rnti.fr/render_pdf.php?p=1001394,en,"articles assemblage.pdf détection d'anomalies dans les flux de données à l'aide Statecharts Vasile-Marian Scuturici1, Dan-Mircea Suciu2, Romain Vuillemot1, Aris Ouksel3, Lionel Brunie1 1 INSA de Lyon 2 Babes-Bolyai, Cluj-Napoca 3 Northwestern University, Illinois Résumé. L'environnement qui nous entoure est progressivement équipé de différents capteurs, produisant des données en continu. Les applications utilisant ces données font face à de nombreux défis, comme l'intégration des flux de données sur un attribut (comme le temps) et l'extraction de connaissances à partir des données brutes. Dans cet article, nous proposons une approche pour faire face à ces deux défis. intégration d'abord, les flux de données est effectuée en utilisant les diagrammes d'états qui représente un résumé des données produites par le producteur de données correspondant. En second lieu, nous détecter des événements anormaux sur les relations temporelles entre statecharts. Nous décrivons notre approche dans un scénario de démonstration, qui utilise un outil visuel appelé modélisateur. 1 Introduction L'espace physique qui nous entoure est progressivement équipé de capteurs différents, nous donnant une vue numérique / projection du monde réel. Ces capteurs produisent des flux de données brutes, telles que des valeurs de température, des images et des lectures de badge. Pourtant, il reste difficile pour l'homme de percevoir des informations intéressantes à partir de ces données brutes qui augmentent en fréquence et précision. Dans cette démonstration, nous vous proposons un mécanisme pour décrire le comportement des flux de données basé sur statecharts. Nous utilisons l'attribut temporel associé à un flux afin de détecter les relations entre statecharts correspondant à des sources de données non homogènes. Les événements non titulaires de ces relations sont considérées comme des anomalies. 2 Statecharts modélisation datasources Dans notre vision, une source de données est une abstraction pour une fontaine / tissu d'entités de données dans l'environnement. Des exemples de sources de données sont les valeurs lues par un capteur de température ou les événements produits par un lecteur de badge. Nous associons les entités produit par une source de données avec la source de données elle-même. Ces entités partagent la même structure, associée à la source de données. Le temps joue un rôle dans la description d'une source de données DS. A chaque (discret) instant t, DS est dans un état défini par l'entité produit à l'instant t. Les entités produites par une source de données, associée à un horodatage, forment un flux de données. Une source de données DS à un temps t est une séquence d'entités de données horodatées partageant la même structure (en-tête). Chaque entité de données a des valeurs du produit cartésien nAAAR ... 21, où R désigne l'ensemble des nombres réels positifs (le temps RNTI-E-19- 635 - Détection des anomalies dans les flux de données en utilisant le domaine Statecharts ) et Ai correspond au domaine de datasource attributs. Nous considérons le cas particulier où chaque domaine Ai est catégorique. Nous considérons une partition {S1, S2, ..., Sn} de l'espace A1 × A2 × ... × An. Chaque ensemble sera appelé Si un état de la source de données DS. Les États S1, S2, ..., Sn forme le diagramme d'états associé à la source de données DS. Chaque entité produite par DS va générer une transition dans le diagramme d'états associés. Si la dernière entité produite est appelée EA et la nouvelle entité produit est eb, la transition ajoutée relie les deux états correspondants contenant ces entités. 2.1 Les relations entre statecharts Le problème est l'intégration des sources de données décrites par différentes structures. Les seules informations utiles pour relier ces sources de données est l'horodatage associé à chaque entité produit. Nous utilisons l'horodatage pour trouver définir une relation d'inclusion entre les deux états. DS1.S1 désigne l'état S1 du diagramme d'états correspondant à la DS1 de source de données. DS1.S1 est inclus dans le DS2.S2 de l'Etat si, pour chaque fois que DS1 est en S1, DS2 est dans l'état S2: Pour une quantité prédéfinie de données que nous détectons les relations entre les Etats composant les statecharts correspondant à des sources de données étudiées. Tous les nouveaux événements non respect de ces relations sont qualifi ed comme des anomalies. 3 Prototype et scénario de démonstration Nos objectifs de scénario de démonstration à la détection d'anomalies dans les flux de données en continu. Les cours d'eau ont des structures différentes, et l'horodatage est l'information unique utilisé pour statecharts lien ces flux reprenant. En comparant à d'autres approches que nous utilisons une intégration de métadonnées (statecharts) construit automatiquement sur les flux de données. Un autre avantage de cette approche: les statecharts ont une expression visuelle puissante, utile pour les applications de surveillance en temps réel. Le prototype proposé lit les données brutes provenant de différentes sources de données sur HTTP (Scuturici, 2009). Il peut également charger des données horodatées à partir d'une base de données relationnelle. Patternator trouve automatiquement les relations entre statecharts et suggère que des événements anormaux les événements ne respectant pas ces relations. Comme ensemble de données, nous avons utilisé un sous-ensemble de l'ensemble de données présenté dans (VAST, 2009). Le sous-ensemble contient un mois de badges lecteurs traces et communications réseau informatique dans un bâtiment. L'objectif est de détecter les comportements des utilisateurs suspects, et que la vérité du terrain est connu, nous pouvons évaluer nos résultats et de les comparer à d'autres résultats publiés. Références Varun Chandola, Arindam Banerjee, Vipin Kumar (2009). Détection des anomalies: Une enquête. ACM Comput. Surv. 41 (3). Scuturici, M. (2009) API Dataspace. Rapport technique. LIRIS. Symposium IEEE sur Visual Science Analytics et de la technologie - VAST (2009), Atlantic City, New Jersey. RNTI-E-19 - 636 -"
658,Revue des Nouvelles Technologies de l'Information,EGC,2010,Identifying the Presence of Communities in Complex Networks Through Topological Decomposition and Component Densities,"The exponential growth of data in various fields such as Social Networksand Internet has stimulated lots of activity in the field of network analysisand data mining. Identifying Communities remains a fundamental technique toexplore and organize these networks. Few metrics are widely used to discoverthe presence of communities in a network. We argue that these metrics do nottruly reflect the presence of communities by presenting counter examples. Thisis because these metrics concentrate on local cohesiveness among nodes wherethe goal is to judge whether two nodes belong to the same community or viseversa. Thus loosing the overall perspective of the presence of communities in theentire network. In this paper, we propose a new metric to identify the presenceof communities in real world networks. This metric is based on the topologicaldecomposition of networks taking into account two important ingredients of realworld networks, the degree distribution and the density of nodes. We show theeffectiveness of the proposed metric by testing it on various real world data sets","Faraz Zaidi, Guy Melançon",http://editions-rnti.fr/render_pdf.php?p1&p=1001286,http://editions-rnti.fr/render_pdf.php?p=1001286,en,"articles assemblage.pdf identification de la présence des communautés dans les réseaux complexes par décomposition et topologiques des composants Densités Faraz Zaidi *, Guy Melançon * * CNRS UMR 5800 LaBRI & INRIA Bordeaux - Sud Ouest 351, cours de la Libération 33405 Talence cedex, FRANCE {Faraz. Zaidi, guy.melancon}@labri.fr~~V~~singular~~3rd Résumé. La croissance exponentielle des données dans divers domaines tels que Net- sociale et des œuvres Internet a stimulé beaucoup d'activité dans le domaine de l'analyse du réseau et l'exploration de données. Identifier les communautés reste une technique fondamentale pour explorer et organiser ces réseaux. Peu de mesures sont largement utilisées pour découvrir la présence de communautés dans un réseau. Nous soutenons que ces mesures ne reflètent pas vraiment la présence des communautés en présentant des contre-exemples. En effet, ces mesures se concentrent sur la cohésion locale entre les nœuds dont le but est de déterminer si deux nœuds appartiennent à la même communauté ou vice versa. Ainsi perdre la perspective globale de la présence des communautés dans l'ensemble du réseau. Dans cet article, nous proposons une nouvelle mesure d'identifier la présence de communautés dans les réseaux du monde réel. Cette mesure est basée sur la décomposition topologique des réseaux prenant en compte deux éléments importants des réseaux du monde réel, la distribution de degré et la densité des noeuds. Nous montrons l'efficacité de la mesure en le testant sur divers ensembles de données du monde réel proposé. 1 Introduction La plupart des systèmes du monde réel prennent la forme de réseaux où un ensemble de noeuds et des arêtes peut être utilisé pour représenter ces réseaux. Les exemples incluent les réseaux sociaux, les réseaux métaboliques, world wide web, web alimentaire, les réseaux de transport. la détection communautaire reste une technique importante pour organiser et comprendre ces réseaux complexes (Girvan et Newman (2002)). En gros, nous aimons définir une communauté en décomposition d'un ensemble d'entités en « groupes naturels ». La détection des communautés a un large éventail d'applications dans divers domaines. Par exemple, dans les réseaux sociaux, la détection communautaire pourrait nous conduire vers une meilleure compréhension de la façon dont les gens collaborent entre eux ou dans un réseau de transport, une communauté peut représenter des villes ou des pays bien connectés par des moyens de transport. D'une manière générale, la recherche dans le domaine de l'analyse du réseau peut être divisé en deux catégories. Tout d'abord, en développant des mesures qui peuvent nous aider à analyser et détecter les structures communautaires et, deuxièmement, l'élaboration de procédures algorithmiques pour trouver et regrouper les communautés présentes dans les réseaux. Dans cet article, nous nous concentrons sur les différentes mesures proposées pour la communauté RNTI-E-19- 163 - Identification de la présence des communautés par décomposition et Densités figure. 1 - Deux réseaux ayant le même nombre de noeuds et d'arêtes (a) des noeuds Entourée représentent quatre structures complètes communautaires déconnectés les uns des autres et le plus dense à l'intérieur et (b) représente une composante connexe unique avec des noeuds voisins partage. détection. La motivation de ce travail vient d'une simple mais importante question: Y at-il une mesure qui peut me garantir la présence des communautés dans un réseau? Pour répondre à cette question, nous avons besoin d'une définition formelle d'une communauté. Jusqu'à présent, nous avons évité d'utiliser le regroupement à long terme qui est probablement un formalisme plus générique de la communauté à long terme. Ceci est dû à un coefficient de clustering appelé métrique qui pourrait induire en erreur, nous allons discuter de cette mesure plus loin dans Sec. 2. Sociologues utiliser la communauté à long terme (Coleman (1964)) par rapport au domaine minier statistique et de données où les gens utilisent les clusters (terme Tryon (1939)) pour désigner le même concept. Ainsi, une communauté ou un groupe peut être définie comme un groupe d'éléments ayant les propriétés telles que décrites par Wasserman et Faust (1994): • Réciprocité: Les membres du groupe se choisissent à inclure dans le groupe. Dans un sens théorique graph-, ce qui signifie qu'ils sont adjacents. • Compacité: les membres du groupe un re bien accessible pour l'autre, mais pas nécessairement adjacents. Graphique-théoriquement, les éléments du même groupe ont de courtes distances. • Densité: Les membres du groupe ont beaucoup de contacts entre eux. En ce qui concerne la théorie des graphes, ce sont les membres du groupe ont un grand quartier à l'intérieur du groupe. • Séparation: Les membres du groupe ont plus de contacts au sein du groupe qu'à l'extérieur. Sur la base de ces concepts, une structure communautaire parfaite dans un réseau serait représenté par des cliques déconnectées comme le montre la figure. 1 (a). Les composantes connexes de la Fig. 1 (a) satisfont à toutes les propriétés décrites ci-dessus comme étant une structure de communauté parfaite. Notez que les deux graphiques a exactement le même nombre de noeuds et les arêtes. Au meilleur de notre connaissance, il n'y a pas de mesure qui tente d'identifier la présence de communautés dans un réseau en analysant le graphique sur l'ensemble dans une perspective globale. Il y a des paramètres comme le clustering et l'indice Coefficient Jaccard (voir la section 2 pour plus de détails) qui déterminent la cohésion locale d'un ensemble de noeuds, à savoir qu'ils se concentrent sur le voisinage immédiat des nœuds, mais ils ne parviennent pas à saisir la pré- sence des communautés dans l'ensemble comme l'a soutenu par différents chercheurs (Brandes et Erlebach (2005), Girvan et Newman (2002)). Fig. 1 (b) est un exemple qui illustre ce phénomène où plusieurs nœuds partagent les voisins communs, mais il est difficile d'identifier les communautés dans un tel réseau. Dans cet article, nous visons à développer une mesure qui nous permet d'identifier ces composants denses comme le montre la figure. 1 (a). Nous utilisons deux ingrédients importants de réseaux du monde réel; le noeud RNTI-E-19 - 164 - degré Zaidi et Melançon pour décomposer le réseau en plusieurs sous-graphes et de la densité de noeuds pour identifier la présence d'une communauté. Être en mesure d'identifier les communautés dans un réseau a plusieurs applications du monde réel, telles que le regroupement des entités similaires en groupes peuvent nous aider à comprendre et modéliser le comportement des systèmes du monde réel. Une fois que nous identifions la présence des communautés dans un réseau, éventuellement un algorithme peut être construit pour regrouper ces communautés. Mais dans cet article, nous nous limiterons à proposer uniquement une mesure où une étude détaillée d'une éventuelle procédure de regroupement et de comparer ses résultats avec d'autres algorithmes de regroupement reste hors de portée. Un énorme avantage du temps métrique est qu'il peut être calculé dans presque linéaire proposé rendant applicable sur de grands ensembles de données de taille par rapport aux différents indices qui ont une très grande complexité de l'exécution. De plus, la décomposition topologique intègre implicitement la connais- sances des degrés de nœud qui nous permet d'identifier la présence de communautés en ce qui concerne le degré de nœuds. Nous décrivons brièvement les différents paramètres présents dans la littérature liée au problème de la dé- tection de la communauté dans la section 2. La section 3 présente les différents ensembles de données utilisés pour l'expérimentation. Dans la section 4, nous présentons les détails mathématiques de la mesure proposée. L'article 5 est consacré à l'analyse de différents ensembles de données par rapport à la métrique suivie obser- vations intéressantes proposées dérivées de la métrique à l'article 6. Enfin, nous présentons les conclusions et les orientations futures de la recherche proposée. 2 travaux connexes Différentes mesures existent dans la littérature pour étudier le problème de la détection de la communauté. De façon générale classer ces mesures, on peut dire que certains paramètres sont à savoir locale calculée soit sur des noeuds ou des arêtes par rapport à des mesures qui sont calculées sur l'ensemble du graphique. Un exemple d'une métrique locale serait le degré d'un noeud d'entrée-sortie. Notre objectif est de trouver une mesure qui est calculé pour le graphique entier et ne se concentre pas sur les noeuds individuels et les arêtes. D'autre part, des exemples de mesures globales populaires incluent Betweenness Centralité et le nœud Excentricité (Brandes et Erlebach (2005)). Au meilleur de notre connaissance, aucun des indicateurs globaux sont conçus pour identifier la présence de structures communautaires. Ci-dessous, nous passons en revue quelques mesures qui abordent le problème de la détection de la communauté locale. L'un des plus w rès utilisé dans l'analyse métrique de réseau est le pro- coefficient d'agrégation posé par Watts et Strogatz (1998). Cette mesure peut être souvent trompeur en raison de son nom que cette mesure ne garantit pas la présence des communautés dans un réseau. Par exemple, considérons les deux réseaux représentés sur la figure. 1. Le calcul du coefficient de clustering moyen de réseau (a) nous donne une valeur de 0,88 et celle du réseau (b), une valeur de 0,61. Ces deux valeurs ne font pas Flect re la structure sous-jacente du réseau où la figure. 1 (a) a intuitivement distincte structure communautaire par rapport à la figure. 1 (b). Cette mesure ainsi que la longueur du trajet moyen ont été utilisés par Watts et Strogatz (1998) pour classer les réseaux comme étant de petits réseaux mondiaux ayant « six degrés de séparation » principe (Milgram (1967)). Une autre mesure populaire est l'indice Jaccard introduit par Jaccard (1901) également connu sous le coefficient de similarité Jaccard. Cette mesure est utilisée pour mesurer la similitude des deux éléments basés sur le voisinage commun. Plus précisément, l'indice se penche sur le nombre de voisins communs des deux éléments et il se compare à la taille de tous les voisins des deux éléments. Un bord est attribué une grande valeur de similarité si elles partagent beaucoup de voisins. Pour en revenir à notre exemple à la figure 1 (a), si l'on considère les arêtes reliant la clique avec trois RNTI-E-19- 165 - Identification de la présence des communautés à travers des noeuds de décomposition et Densités seulement, tous ses bords sont attribués une valeur 0,33 par rapport aux bords de la clique de cinq noeuds qui sont affectés d'une valeur de 0,6. Une faible valeur de bord peut indiquer que l'un bord ne fait pas partie d'une communauté qui, dans ce cas, est en contradiction avec la définition d'une structure communautaire, nous avons présenté plus tôt, comme le bord avec 0,33 valeur est en fait partie d'une communauté. Plusieurs autres mesures ont été proposées dans la littérature où et Melançon Sallaberry (2008) offrent une bonne étude comparative des différentes mesures locales pour le problème de la détection de la communauté. L'indice de Jaccard se démarque clairement comme l'archétype métrique pour trouver les communautés dans les réseaux basés sur la notion d'une triade. Les lecteurs sont recommandés (et Melançon Sallaberry (2008)) pour plus de détails. Nous aimerions faire référence à une autre mesure qui n'est pas utilisé pour la détection de la communauté, plutôt de classer les réseaux et a gagné beaucoup de popularité dans le domaine de l'analyse du réseau. Les classifie de distribution métriques, degré réseaux comme étant libre à grande échelle si la distribution de degré suit une loi de puissance. En d'autres termes, cela signifie qu'il ya quelques noeuds qui ont un nombre très élevé de connexions et beaucoup de noeuds ont seulement quelques connexions. Ces réseaux ont de nombreuses propriétés intéressantes (Barabasi et Albert (1999)) et nous utilisons ces connaissances pour trouver l'inspiration pour notre mesure où les détails sont décrits dans la section 4. 3 ensembles de données Nous utilisons plusieurs ensembles de données du monde réel pour l'expérimentation. Co-auteur du réseau comprend le réseau de collaboration de scientifiques affichant prépublications sur les archives de la matière condensée à www.arxiv.org entre 1995-1999, tel que compilé par Newman (2001). Le réseau contient des noeuds 1670 et 47600 bords. Movie Database est un graphique-acteur acteur où deux acteurs sont reliés les uns aux autres si elles ont agi dans un film ensemble. Il contient un graphique de la distance accessible à partir de 5 l'actrice Sharon Stone de films réalisés jusqu'à l'année 1999 (voir Archambault et al. (2007) pour plus de détails). Ce réseau contient 7640 nœuds et 27600 bords env. Réseau de transport aérien est un graphique-aéroport de l'aéroport où les bords représentent un vol direct d'un aéroport à l'autre. Ce réseau a attiré beaucoup de chercheurs du domaine de la géographie et de transport (voir Rozenblat et al. (2008) pour plus de détails). Ce réseau contient 1540 nœuds et 16500 bords. Internet Tomographie Network est une collection de chemins de routage d'un hôte de test à d'autres réseaux sur Internet. La base de données contient des informations de routage et joignabilité, et est accessible au public de le site Web du projet OPTE (http://opte.org/). Ce réseau contient 35800 noeuds et 42400 bords. Football américain contient le réseau de jeux de football américain entre les collèges Division IA au cours de la saison régulière automne 2000, tel que compilé par Girvan et Newman (2002). Les équipes sont représentées par des noeuds et des arêtes représentent un jeu entre les deux équipes. Ce réseau comprend des noeuds 115 et 616 des bords. Affirmer davantage l'efficacité de notre métrique, pour chacun de ces réseaux du monde réel, nous générons des réseaux artificiels de tailles égales en utilisant deux modèles graphiques connus, les réseaux aléatoires (Erdos et Renyi (1960)) et le petit réseau mondial (Watts et Strogatz (1998)). Nous nous attendons à trouver des structures communautaires dans les petits réseaux mondiaux où en l'absence de communautés des réseaux aléatoires. RNTI-E-19 - 166 - Zaidi et Melançon FIG. 2 - Une vue partielle de Max12-DIS du réseau copaternité où composantes déconnectées peuvent être facilement identifiés pour la haute densité. 4 topologiques Décomposition pour la distribution bord Comme indiqué précédemment, dans cet article, nous présentons une nouvelle mesure d'identifier la présence de communautés. L'objectif est évidemment de trouver des nœuds plus dense qui peuvent être identifiés en tant que communautés. Notre inspiration vient du fait que, dans de véritables réseaux mondiaux, le degré distri- bution est pas au hasard, mais plutôt suit un schéma où les différents noeuds ont des degrés divers (Barabasi et Albert (1999)). En présence de noeuds ayant un degré élevé, il est difficile d'identifier les composants denses dans un réseau (Zaidi et al. (2009)). Ainsi, comme première étape, on introduit une décomposition en fonction de la topologie du réseau. En conséquence, les sauts de réseau en plusieurs composants déconnectés les uns des autres comme représenté sur la Fig.2. On peut alors calculer les densités de chaque composant connecté à identifier la présence de composants denses dans l'ensemble du réseau. Nous proposons donc une approche en trois étapes, la décomposition topologique, l'identification des composants connectés et le calcul des densités de composants. Toutes ces étapes peuvent être effectuées en temps linéaire en fonction du nombre de bords dans le pire des cas, compte tenu de ce que le degré maximal dans un graphe est délimitée par un faible facteur de constante. Topologiques décomposition: Nous introduisons l'idée de Maxd-degré Induced sous-graphe (Maxd- DIS) où Jmax-DIS est un sous-graphe induit construit en ne considérant que les nœuds hav- ing degré au plus d dans G. Mathématiquement pour un graphe G (V, E) où V est un ensemble de noeuds et E est un ensemble d'arêtes, la Maxd-DIS est défini comme G '(V', E ') de telle sorte que V' ∈ V et E '∈ E et ∀u ∈ V' , degG (u) ≤ d, où d peut avoir des valeurs de 0 à degré de noeud maximal possible pour un réseau. De même, nous pouvons construire l'esprit-DIS où DEGG (u) ≥ d. A travers le papier, nous utilisons le terme DIS pour désigner les deux degrés max et min induite par les sous-graphes. Permet de considérer un exemple du réseau co-auteur en tirant une Max12-DIS. Notez qu'un noeud ayant un degré 12 dans le graphique d'origine pourrait ne pas avoir le même degré dans le sous-graphe induit. De plus, il est également évident que ces noeuds pourraient ne plus être reliés les uns aux autres comme le montre la figure 2. Par construction, lorsque l'on considère un Max-DIS, nous incluons essentiellement noeuds jusqu'à degré d seulement. Cela nous aide à comprendre comment les bords sont répartis entre les noeuds ayant un degré d - 1. À partir de la figure 2, il est évident que le Max12-DIS nous aide à identifier les composants denses jusqu'à 1 degré d- dans l'ensemble du réseau. Pensez à une partie d'un graphe où une clique de 6 nœuds existe RNTI-E-19- 167 - Identification de la présence des communautés par décomposition et Densités figure. 3 - Exemple d'Max5-DIS avant et après avoir calculé le Max5-DIS comme représenté sur la Fig.3. La clique est reliée à l'ensemble du réseau par un seul noeud. Tous les noeuds de cette clique ont un degré 5, sauf le noeud qui relie cette clique à l'ensemble du réseau, qui dans ce cas est de degré 6. Lorsque nous construisons le Max5-DIS le nœud avec le degré 6 ne sera pas inclus dans le sous-graphe induit et seuls les nœuds ha clos Ving degré 5 fera partie du Max5-DIS. Ainsi de cette façon, nous pouvons identifier les noeuds connectés dans le dense réseau. Nous soutenons que itérer sur la Maxd-DIS de petites valeurs de d au degré maximum possible dans le réseau d'origine, nous pouvons identifier la présence de composants connectés à forte densité, s'il y en a. Création de sous-graphe induit pour toutes les valeurs possibles de degrés de noeuds est limitée par le degré maximum possible dans le réseau. Comme nous allons le calcul de la métrique pour à la fois la complexité temporelle Maxd-DIS et l'esprit-DIS, de créer le sous-graphe induit est O (2 * maxd * m) où Jmax est le degré de noeud maximal et m est le nombre total des arêtes de G. Calcul des composantes connexes: L'étape suivante consiste à calculer tous les composants connectés dans le sous-graphe. Nous utilisons un algorithme de recherche première largeur (BFS) à partir d'un nœud et itérer ses voisins pour trouver le composant connecté, il appartient. Une fois que nous avons identifié les noeuds connectés au noeud de départ, nous remettons en marche le BFS à partir d'un noeud qui n'a pas encore été visités. Toutes les composantes connexes d'un graphe peuvent ainsi être calculés en O (n + m) temps où n est le nombre de noeuds et m est le nombre de bords dans G. Composant de mesure de densités Graphiques: Maintenant que nous avons décomposé le graphique et composants connectés identifiés, on calcule une mesure de quantifier la présence de composants connectés à forte densité s'il y a lieu. Nous utilisons une mesure proposée par Watts et Strogatz (1998) a appelé coefficients de regroupement locaux, mais étant donné que nous appliquons aux composants connectés individuels, par opposition à tout le graphique, nous préférons appeler les densités de composants. Nous attribuons une densité composant à chaque composante connexe individu en utilisant l'équation suivante. CDk = (ek * 2) / (nk (nk - 1)) (1) CDk représente la densité de composants (DR) de k composante connexe, ek représente le nombre de bords dans k et nk représente le nombre de noeuds en k. L'équation représente le rapport entre le nombre de bords dans le composant k au nombre maximal d'arêtes possibles dans le composant. La valeur 1 indique que le composant est une clique et que le composant est raccordé à la valeur de CD minimum possible est de 2 / nk. Notez que la vérification si un composant connecté est une clique est pas plus qu'un problème de comptage où nous pouvons identifier RNTI-E-19 - 168 - Zaidi et Melançon la présence d'une clique simplement en comptant le nombre de noeuds et le nombre d'arêtes un composant connecté. Nous tenons à souligner que nous ne nous attaquons pas au problème de la clique maximale bien connue en utilisant cette métrique qui est connu pour être NP-complet (Cook (1971)). De plus, nous essayons de ne pas trouver des cliques d'une taille fixe k qui est montré résoluble en O (nk) par Nesetril et Poljak (1985) comme notre la méthode ne guarantee pas que nous trouverons cliques de quelque taille fixe k. La méthode proposée est capable de trouver des cliques dans les temps linéaires en termes de nombre d'arêtes, quelle que soit la taille de la clique. Une fois que nous avons calculé le CD pour les composants connectés individuels dans le DIS, on calcule la densité composante pondérée pour tous les composants connectés dans un DIS. Nous représentons cette valeur par CDGd pour le degré d induit sous-graphe et est calculé par l'équation: = CDGd kmaxΣ k = 0 nk * nd CDk (2) CDGd représente la densité pondérée Composant de d-DIS. Lors du calcul du CDGd, nous excluons le poids des composants ayant seulement 1 ou 2 noeuds car il sollicite le CDGd. Nous les compter dans le nombre total de noeuds (e) présent dans le sous-graphe induit cependant. En effet, si un graphique a beaucoup de 0 nœuds de degré, sa densité globale composante sera inférieure à un graphique avec bien connectés des noeuds de degré supérieur. Le poids est associé à veiller à ce que des composants ayant plus de nœuds sont pondérés plus par rapport à des composants ayant moins de nœuds. Le CDGd peut être calculée pour différentes valeurs de d, où d peut prendre les valeurs de 0 à un degré maximum d'un noeud dans G. Les calculs de l'équation. 1 et 2 sont totalement indépen dent de la manière dont le sous-graphe a été construit et peut donc être utilisée pour calculer les densités de chacune des composantes Maxd-DIS ou l'esprit-DIS. Les valeurs de CDG Maxd-DIS (donnée par CDGMaxd) et de l'esprit-DIS (donnée par CDGMind) représente la présence ou l'absence de composants denses dans G. hautes valeurs de CDG suggèrent qu'il existe des composants connectés à forte densité dans le sous-graphe induit qui représentent éventuellement communautés G. Une autre information supplémentaire qui peut être extrait à partir des valeurs CDG est que en identifiant la valeur maximale de CDGMaxd et CDGMind, nous pouvons pointer le les sous-graphes induits dans lequel ces communautés sont présentes comme le montre la figure 2. Nous graphiques de tracé pour les valeurs CDGMaxd et CDGMind respectives pour les ensembles de données DE- transcrites dans la section 3 comme le montre la figure 6. Les valeurs sur l'axe des x représente le degré maximal de chaque réseau, qui à son tour représente le nombre de sous-graphes générés pour chaque réseau. Les valeurs sur l'axe des y sont les valeurs CDG qui peuvent être compris entre 0 et 1, où 1 représente la présence d'une structure parfaite de la communauté avec cliques. Pour chaque ensemble de données réelles du monde, nous avons également généré petit monde et les réseaux aléatoires de nombre équivalent de noeuds et des arêtes de sorte que nous pouvons comparer le comportement de la métrique avec le réseau artificiellement généré correspondant. Ces réseaux sont tirés à l'aide en ligne pour Dotted petits réseaux mondiaux et pointillées ligne pour les réseaux aléatoires. 5 Analyse différents ensembles de données en utilisant des composants de liens Densi- Graphique L'évaluation des CDGMaxd et CDGMind pour les réseaux aléatoires et les petits réseaux mondiaux pour des graphiques de différentes tailles peuvent être généralisées facilement. Pour les réseaux aléatoires, nous ne RNTI-E-19- 169 - Identification de la présence des communautés par décomposition et Densités attendre à trouver une structure communautaire et donc ces réseaux ont de faibles valeurs CDG pour tous les cas de test comme le montre la figure 6. D'autre part, nous avons les petits réseaux mondiaux qui contiennent par définition communautés, et ce bien réfléchi par les valeurs élevées CDG pour tous les réseaux générés artificiellement. Une exception est le cas où le petit graphique mondial généré est équivalent à la taille du réseau Internet. Cela est dû à la faible densité globale du graphique Internet lui-même comme les bords échelle linéairement avec le nombre de noeuds. Nous examinons d'abord co-auteur et les réseaux de cinéma. Ces deux graphes ont les mêmes valeurs CDGMaxd et CDGMind comme représenté sur la figure 6 (a) (b) (c) (d). Cela est dû au fait que ces deux réseaux suivent le petit monde et de la structure libre à l'échelle où il y a beaucoup de nœuds de bas degré et quelques noeuds dominés plus grand nombre de connexions représentant l'échelle des graphiques freeness. Ce phénomène peut être déduite de (Fig.4 (a)) où nous montrons la parcelle pour la distribution des degrés du réseau Film. De plus, les valeurs élevées CDGMaxd donnent à penser qu'il existe des communautés présentes dans les nœuds de bas degré, ainsi que dans les valeurs élevées CDGMind représentant la petite architecture mondiale des deux réseaux. FIGUE. 4 - Répartition Degree pour le film, le transport aérien et le réseau Internet. Dans le cas du réseau de co-auteur (Fig.6 (a) et (b)), les valeurs de crête de 0,4 pour CDGMaxd sont atteints lorsque les valeurs de degré sont compris entre 6 et 8. Ceci suggère que la plupart des communautés denses sont présents lorsque degrés de nœud ont une valeur maximale de 8. Ce résultat a la sémantique iques logarithmiques à, comme nous envisageons un réseau de collaboration de chercheurs et ils sont reliés les uns aux autres si elles publient un article. La plupart du temps 6 à 8 personnes apparaissent comme auteurs dans un article formant des cliques dans le réseau de collaboration. Cette information est bien représentée par les valeurs CDGMaxd. D'autre part, la CDGMind représente la collaboration des auteurs qui publient beaucoup. Étant donné que l'ensemble de données provient d'un domaine particulier de la recherche, il est un prix sur- de voir que les personnes ayant les plus hauts degrés collaborent les uns avec les autres. Movie Database suit une structure similaire à celle de la co-authorsh réseau IP. La valeur maximale pour CDGMax16 est 0,87. Dessin du Max16-DIS, nous pouvons voir clairement les différents acteurs sont reliés les uns aux autres à forte densité comme le montre Fig.5 (a). Le réseau de transport aérien est un exemple intéressant (Fig.6 (e) et (f)). Il a une structure libre à grande échelle comme on peut le voir dans la distribution des degrés du réseau dans Fig.4 (b). Ce réseau n'est pas classé comme un petit réseau mondial car il n'a pas de ((2008) Rozenblat et al.) Co-haute efficacité en cluster. En utilisant notre mesure, nous sommes toujours en mesure de trouver des nœuds plus dense que nous obtenons des valeurs élevées pour Mind-DIS. La structure Trouvé communauté est montré dans Fig.5 (b) où tous les mondes aéroports les plus fréquentés sont reliés entre eux par un vol direct. Cela rend RNTI-E-19 - 170 - Zaidi et Melançon figure. 5 - (a) Max16-DIS pour la base de données de films représentant beaucoup de composantes com- reliés à forte densité (b) Min158-DIS pour le réseau de transport aérien où toutes les grandes villes du monde sont bien reliés les uns aux autres sens que les mondes les plus aéroports importants comme New York, Paris, Londres ont tous un des vols directs à l'autre. Le réseau Internet n'est pas classé comme un petit monde tel qu'il est a co-faible regroupement efficace. En utilisant notre mesure, nous ne trouvons aucune structure communautaire ni dans le CDGMaxd ni dans le CDGMind graphiques reflétant ainsi l'exactitude de la métrique. Le degré de distribu- tion graphique internet est fig.4 (c). Enfin, le réseau de football où nous soupçonnions une absence de communautés au sein de ce réseau que les équipes pourraient vous distribuer uniformément le nombre de jeux parmi leurs oppo- qui ont tourné nents être une fausse hypothèse. Il est tout à fait clair du haut CDGMaxd et CDGMind qu'il existe des communautés présentes dans ce réseau. En effet, certaines équipes jouent plus souvent dans la même région ou en jouant avec les équipes qui ont une his- toire derrière eux. Une remarque importante en utilisant les valeurs CDGMind est que les plus hauts degrés nœuds ne forment pas une structures communautaires qui laisse supposer que les équipes jouant plus de jeux jouent pas nécessairement les uns contre les autres. 6 Déductions et observations En plus d'identifier la présence de communautés dans des réseaux différents, intéressants de proprié- tés réseaux mondiaux réels peuvent être observés à l'aide des graphiques CDGMaxd et CDGMind. Nous énumérons ci-dessous: • Les réseaux sont généralement classés comme aléatoire, Small World, échelle libre, ou les deux Small World échelle libre en même temps. En utilisant l'on peut avoir proposé métrique, plus la vue in- dans ces réseaux par comprendre comment les bords sont répartis dans des noeuds de degré faible ou élevé de ces réseaux. Les communautés peuvent exister dans les noeuds qui ont un faible degré dans le graphique (comme le cas de co-auteur et les réseaux de cinéma), ou ils peuvent exister RNTI-E-19- 171 - identification de la présence des communautés par décomposition et Densités seulement hauts degrés des nœuds de réseau (transport aérien) ou dans les noeuds ayant un degré moyen (réseau de football). • L'absence de communautés peut avoir deux conséquences, que ce soit le réseau est aléatoire ou il suit un comportement sans échelle. En cas de comportement libre à l'échelle, les réseaux se composent généralement d'étoiles comme des structures où de nombreux noeuds se connectent à un seul nœud. Un exemple typique est les informations de routage des serveurs en cas du réseau Internet. De la mesure proposée, nous sommes en mesure de trouver le même comportement dans le réseau de transport aérien où le degré de noeuds est pas très élevé. Logiquement parlant, un aéroport dans une capitale d'un petit pays jouera le rôle d'une plaque tournante où les petites villes auront un vol direct vers la capitale qui sera éventuellement relié à l'un des principaux aéroports du monde. • Une autre observation intéressante sur le réseau Internet est l'absence de structure communautaire dans les noeuds de degré élevé. Cela signifie que si deux noeuds de degré élevé doivent communiquer entre eux, ils passent à travers un noeud à faible degré. Bien que cette analyse préliminaire révèle quelques faits intéressants au sujet de la di ensembles de données fférents, une étude plus détaillée par les experts du domaine pourraient révéler plus d'informations. Nous pensons que cette analyse et toute autre information peut aider les chercheurs à trouver de meilleures façons d'entités du groupe ensemble. De plus, depuis l'analyse est très efficace en termes de complexité de temps, il est tout à fait pratique pour la recherche future où la taille des données est en croissance exponentielle. 7 Conclusions et travaux futurs Dans cet article, nous avons introduit une mesure basée sur la décomposition topologique des graphes. Nous appelons cette densité métrique, composante des graphiques. La mesure nous permet d'identifier la présence de la structure communautaire dans les réseaux complexes du monde réel. Le calcul de cette mesure prend du temps linéaire en termes de nombre d'arêtes et se révèle être très rapide lorsqu'il est appliqué à de grands ensembles de données de taille. Nous avons testé la métrique avec différents ensembles de données et de montrer l'efficacité de la mesure en comparant les résultats avec petit monde et les modèles de réseau aléatoires. Plus- sur la décomposition topologique proposée ouvre de nouvelles dimensions dans le domaine de l'exploration de données que les réseaux complexes peuvent être simplifiées en utilisant la méthode proposée. Dans le cadre des travaux futurs, nous pensons que cette mesure peut jeter les bases pour la construction d'une vitesse élevée regroupement al- gorithme pour les réseaux de grande taille. Étant donné que cette mesure est tout à fait efficace, découvrir les clusters utilisant cette mesure demeurera très efficace en termes de complexité de temps. Références Archambault, D., T. Munzner et D. Auber (2007). Grouse: basée sur les caractéristiques, l'exploration de la hiérarchie graphique dirigeable. En EUROVIS, pp. 67-74. Barabasi, A. L. et R. Albert (1999). Emergence de mise à l'échelle dans les réseaux aléatoires. ence fiques 286 (5439), 509-512. Brandes, U. et T. Erlebach (2005). Analyse de réseaux: Bases méthodologiques (Notes de cours en informatique). Springer. RNTI-E-19 - 172 - Zaidi et Melançon Coleman, J. S. (1964). Introduction à la sociologie mathématique. Collier-Macmillan, Lon- dres, Royaume-Uni. Cook, S. A. (1971). La complexité des procédures de prouver le théorème. Dans Proc. de la 3e édition annuelle ACM Symp. sur la théorie de l'informatique, pp. 151-158. Erdos, P. et A. Renyi (1960). Sur l'évolution des graphes aléatoires. Publ. Math. Inst. Hung. Acad. Science 5, 17-61. Girvan, M. et M. E. J. Newman (2002). Structure communautaire dans les réseaux sociaux et biologiques. Proc. Natl. Acad. Sci. USA 99, 8271-8276. Jaccard, P. (1901). Bulletin del la société vaudoisedes. Sciences naturelles 37, 241-272. Melançon, G. et A. Sallaberry (2008). mesures Edge pour l'analyse graphique visuelle: une étude comparative. En IV, pp. 610-615. IEEE Computer Society. Milgram, S. (1967). Le petit problème mondial. Psychologie Aujourd'hui 1, 61-67. Nesetril, J. S. et Poljak (1985). Sur la complexité du problème de la sous-graphe. Commentaire. Math. Univ. Carolinae 26, 415-419. Newman, M. E. (2001). réseaux de collaboration scientifique. je. la construction du réseau et les résultats fonda- mentaux. Phys Rev E Stat Nonlin matière molle Phys 64 (1 Pt 2). Rozenblat, C., G. Melançon et P.-Y. Koenig (2008). L'intégration continentale dans l'approche à plusieurs niveaux de transport aérien mondial (2000-2004). Réseaux et Spatial Economics. Tryon, R. C. (1939). L'analyse par grappes. Edwards Brothers, Ann Arbor, Michigan. Wasserman, S. et K. Faust (1994). Analyse des réseaux sociaux: méthodes et applications. Cambridge: Cambridge University Press. Watts, D. J. et S. H. Strogatz (1998). Les dynamiques collectives des réseaux « petits monde ». Na- ture 393, 440-442. Zaidi, F., A. Sallaberry et G. Melançon (2009). Révéler les structures communautaires cachées et identifier les ponts dans les réseaux complexes. Dans WI-IAT '09: Proceedings of the 2009 IEEE / WIC / Conférence ACM, pp 198-205.. La taille des CV Jeux de ne Cesse Données de plongeurs Dans exploser tells Que les Domaines RE- Seaux et internet sociaux. Explosion a dynamisé This les travaux de recherche sur les analyses de la Réseaux et de fouille Données. L'identification de juin Reste COMMUNAUTES clas- sique verser méthode explorateur et classificateur de tells réseaux. Pour celà, Métriques Quelques, le Souvent s memes, fréquement Sont utilisées. Nous Allons montrer, à des Grâce Contre-exemples, Que ne Métriques bureaux pas évincés peuvent mettre en Evidance l'existance de COMMUNAUTES. Celà Parcé la qu'elles comparent localement des noeuds similarité, Que l'Alors de DETERMINER is Objectif exists des Se il en COMMUNAUTES le réseau Dans Considérant sa globalité. Dans this article, nous proposons Une nouvelle identification répandrai la métrique de présence des réseaux Dans COMMUNAUTES du monde réel tirées. This is basée sur métrique Une des décomposition topologique réseaux, en compte deux Prenant acpects Importants de bureaux réseaux, la répartition des degrés Que la den- AINSI Sité des noeuds. Nous démontrerons de la l 'efficacité en l'proposed métrique candidat à de jeux de Données Nombreux de la vie courante. RNTI-E-19- 173 - identification de la présence des communautés par décomposition et Densités figure. 6 -. Composant Densités de graphiques pour 5 jeux de données {(a) (c) (e) (g) (i)} représente la CDGMaxd et {(b) (d) (f) (h) (j)} représente les valeurs CDGMind. RNTI-E-19 - 174 -"
671,Revue des Nouvelles Technologies de l'Information,EGC,2010,Mysins : Make Your Semantic INformation System,,"Anthony Ventresque, Thomas Cerqueus, Louis-Alexandre Celton, Gaëtan Hervouet, Damien Levin, Philippe Lamarre, Sylvie Cazalens",http://editions-rnti.fr/render_pdf.php?p1&p=1001382,http://editions-rnti.fr/render_pdf.php?p=1001382,en,"articles assemblage.pdf Mysins: rendre votre système sémantique INformation Anthony Ventresque *, Thomas Cerqueus **, Louis-Alexandre Celton ***, Gaëtan Hervouet ***, Damien Levin ***, Philippe Lamarre *, Sylvie Cazalens ** * INRIA & LINA, Université de Nantes prenom.nom@univ-nantes.fr, ** LINA, Université de Nantes prenom.nom@univ-nantes.fr, *** Université de Nantes prenom.nom@etu.univ-nantes.fr , 1 introduction la sémantique de l'Est, plus en plus de Dans Utilisée est comme la Domaines Différents recherche d'information (RI) et le Web sémantique. Dans le domaine de la RI, les participants DIFFERENTS interviennent: des informations et d'Fournisseurs des Utilisateurs. Dans CE Contexte, la mise en place d'un Système Semble approprié voiture Distribué il d'EVITER les Përmet Problèmes à la centralisation Liés de Données (passage à l'échelle, confidentiality, etc.) et de l'auto- nomie garantir sémantique : participant each may fils de ontologie auswählen, la Manière d'indexeur SES Données, etc. Par ailleurs, l'utilisation de la sémantique la mise en Nécessite de Meca œuvre Nismes précis. En RI, il s'agit de l'Entre utilisation Autre ontologies D'oise, du calcul de de l'et similarité indexation. L'étude de each de bureaux axes d'un effort Nécessite important de Synthèse et d'intégration. Par exemple, de mesure lorsqu'une nouvelle proposed is similarité, il en lieu Faut les Mettre d'Mécanismes Accès à l'ontologie, verser comparateur SES Résultats Pouvoir à des Autres Measures Ceux (Wu et Palmer (1994), Resnik (1995) ). LORs de l'étude d'un Processus complexe, l'indexation Comme sémantique, ACDE DEVIENT particuliérement laborieux. Pour le PA- Lierre Évident D'une manque l'architecture Générique.Nous la conception distribuée de verser Systèmes d'information sémantiques, un cadre NOUS proposons: Mysins. 2 Le cadre Mysins Mysins definit un certain Nombre de «Briques sémantiques» (c.-à-d Logicielles Unités.) Né- cessaires à la conception d'un Système de partage d'information: aux Accès Données, aux ontologies Accès, de calcul similarité, requêtage, etc. CÉS intégrables Composants et Sont Facilement au sein d'réutilisables Une demande de distribuée. L'aspect mentez la distribution de nous Semble pri- mordial Pour Deux Raisons principales a. Premièrement, Liées à CERTAINES la Tâches sémantique en PEUVENT Être contraignantes en ressources ous compétences, il Faut each Que Fait partici- pant la possibility de l'ACI sous-Traiter une tache pas capable Ne est Qu'il de Réaliser. Deuxième- ment, nous ACDE d'Aborder les Përmet Problématiques Liées à la personnalisation (each par- RNTI-E-19- 629 - Mysins: rendre votre système sémantique INformation ticipant du Système may proposer une nouvelle brique sémantique) et à l'autonomy (participant each CHOISIT lui-same les services wants Qu'il consommer / proposeur). Péchés My- émission C'est le Pourquoi PERMET Très FACILEMENT UNE sous toucheurs de mise en œuvre des services. 3 Nous Présentons les Démonstrations de Mysins Avec Possibilités deux rôles Différents. D'un côté, un quatre nisseur d'information à la possibility d'indexeur un corpus de documents en les choisissant paramêtres Qui le composent d'indexation Processus: du choix l'Accès Gérant Composant à juin ontologie, du choix du Chargé Composant de calcul sémantique similarité, etc. d'un côté Autre, un à la possibility Utilisateur de formuler une et de Requête au provider d'Demander infor- mation de documents les lui Envoyer à sa correspondants Meilleurs request. Pour sémantiquement SA REQUETE caractériser, L'UTILISATEUR may SES Propres UTILISER les services OU Composants Distants Par le propose d'information provider. Intègre la demande This ExI2D proposed par méthode Ventresque et al. (2008), l'Do not is d'Improving Objectif l'interoperability Entre l'Initiateur sémantique de et le quatre Requête nisseur d'information, par l'ajout de nouvelles d'analyse briques des requests très sémantiques Adaptées à un cadre hétérogène sémantiquement. Nous en Mettons v AINSI ALEUR le fait Que Mysins de Përmet construct de modificateur ous d'information un Système aisement sémantique. 4 Conclusion Mysins definit architecture un Générique le Développement distribuée de simplifiant tèmes TEMES D'information sémantiques. En nous Basant sur CE cadre, des com- NOUS posants proposons de de base Essentiels une application la sémantique Toute Utilisant: aux ontologies Accès, de calcul similarité, l'indexation des Données. Nous proposons des Also, plus complexes Composants utilisés Par la méthode ExI2D (extension et d'interprétation de Requête, l'image de documents, etc.). Resnik Références, P. (1995). En utilisant le contenu de l'information pour évaluer la similarité sémantique dans une taxonomie. Dans Dans Actes de la 14e Conférence internationale conjointe sur l'intelligence artificielle, pp. 448-453. Ventresque, A., S. Cazalens, P. Lamarre, et P. Valduriez (2008). L'amélioration de l'interopérabilité en utilisant l'interprétation des requêtes dans des espaces vectoriels sémantiques. Dans l'ESWC. Wu, Z. et M. Palmer (1994). sémantique verbe et la sélection lexicale. En 32e. Réunion annuelle de l'Association de linguistique informatique, pp. 133-138. RNTI-E-19 - 630 -"
672,Revue des Nouvelles Technologies de l'Information,EGC,2010,Objective Novelty of Association Rules: Measuring the Confidence Boost1,"On sait bien que la confiance des régles d'association n'est pas vraimentsatisfaisant comme mésure d'interêt. Nous proposons, au lieu de la substituerpar des autres mésures (soit, en l'employant de façon conjointe a desautres mésures), évaluer la nouveauté de chaque régle par comparaison de saconfiance par rapport á des régles plus fortes qu'on trouve au même ensemblede données. C'est á dire, on considère un seuil “relative” de confiance au lieu duseuil absolute habituel. Cette idée se précise avec la magnitude du “confidenceboost”, mésurant l'increment rélative de confiance prés des régles plus fortes.Nous prouvons que nôtre proposte peut remplacer la “confidence width” et leblockage de régles employés a des publications précedentes.",José L Balcazar ,http://editions-rnti.fr/render_pdf.php?p1&p=1001308,http://editions-rnti.fr/render_pdf.php?p=1001308,en,"articles assemblage.pdf Objectif nouveauté des règles d'association: Mesure de la confiance Boost1 José L Balcázar * * Departamento de Matemáticas, Estadística y Computación Universidad de Cantabria Santander, Espagne Résumé. On sait bien Que la confiance des régles d'association Ne est pas vrai- ment satisfaisant d'Mesuré Comme interêt. Nous proposons, au lieu de la sous-stituer par des Autres Measures (Soit, en l'employant de Façon une des Autres conjointe Measures), EVALUER la régle each de nouveauté par sa comparaison de par rapport á confiance des régles, plus qu fortes » sur au same ensemble Trouvé de Données. C'est à dire, sur un seuil Considère « relatif » au lieu de confiance du seuil absolu habituel. This se Précise idée Avec la grandeur du « coup de pouce de confiance », mésurant l'incrément de rELative des régles confiance, plus prés fortes. Nous prouvons Que Nôtre proposte la may Remplacer « confiance largeur » Et le blocage de rêgles des publications EMPLOYES une precedentes. 1 Introduction Le résultat d'un projet d'exploration de données devrait offrir un certain degré de nouveauté. HO- wever, pour étudier formellement la nouveauté des résultats Data Mining est loin d'être une tâche triviale. La nouveauté se réfère à des faits qui sont en quelque sorte inattendu, et donc une certaine attente, inférieur à celui effectivement constaté, doit exister, en raison de certains faits ou des mécanismes de prévision alternatifs. Ici, nous considérons la tâche spécifique de l'Association Règle minière, et le suivi d'une posal récente pro que, au minimum, chaque règle doit être évaluée pour la nouveauté en fonction du reste des règles minées, traitées comme mécanisme « alternatif » [4] [5]. Ces travaux proposent la largeur de confiance en tant que mesure d'une forme relative de nouveauté objective ou surprisingness de chaque règle individuelle par rapport à d'autres règles qui attente dans le même ensemble de données, et de proposer également de bloquer certaines règles au cas où ils ne rapportent pas assez de nouveauté par rapport à un itemset « bloqueur ». Notre contribution est ici une nouvelle notion de nouveauté, l'élan de confiance, comme dans sa défini- tion syntaxique à la largeur de confiance, mais très différent dans sa sémantique; son trait principal est qu'il englobe à la fois la limite de la largeur de la confiance et la capacité à détecter qu'une règle serait bloquée, de sorte que l'élan de confiance lié à la fois des incarne les limites proposées dans [5]. Les notions de largeur de confiance et de règle de blocage [5] sont similaires à la « taille » proposition de [14], en ce que l'intuition est la même; aussi notre proposition suit une voie intuitive logous ana-. Les principales différences sont que, dans les propositions dont nous discutons, une grande partie de la taille devient inutile parce que nous travaillons sur des bases de taille minimale (règles représentatives, proposées dans et dans [16] [1], [13]) et, plus important encore, que la taille dans [14] RNTI-E-19- 297 - Nouveauté Objectif via le Boost de confiance est basé sur la statistique de χ2, alors que nous examinerons la place dans les seuils de confiance qui rendraient la règle « redondant ». Nos notions sont également similaires à la notion d'amélioration, proposée dans [6] (et aussi brièvement discuté dans [14]), mais portent plusieurs différences clés avec elle. Nous noterons par itemsets lettres majuscules de la fin de l'alphabet, et utiliser pour désigner juxtaposition union, comme dans XY. Pour une donnée ensemble de données D, composé de transactions, dont chacune est un jeu d'éléments marqués avec un identificateur de transaction unique, on peut compter (le X) d'un jeu d'éléments X du support, qui est la cardinalité de l'ensemble de transactions qui contiennent X. La confiance d'une règle X → Y est C (X → Y) = s (XY) / s (X). Dans toutes les règles d'association X → Y le long de cet article, nous supposons que X ∩ Y = ∅; nous ne permettons X = ∅. 2 Objectif Mesures Nouveauté Nous proposons de mesurer la nouveauté de chaque règle par rapport au reste des résultats du même processus d'extraction de données. Parmi un certain nombre de notions de redondance entre les règles d'association, il y a une notion très précise et naturelle qui permet caractérisante bases non redondants de taille minimum (voir [3]). Lemme 1 Soit X0 → Y0 et X1 → Y1 des règles d'association. Ci-après sont équivalentes: 1. Dans chaque jeu de données D, c (X0 → Y0) ≥ c (X1 → Y1) et s (X0Y0) ≥ s (X1Y1). 2. X1 ⊆ X0 ⊆ X0Y0 ⊆ X1Y1. Lorsque ces cas détiennent, nous disons que X1 → Y1 fait X0 → Y0 redondant ou que X1 → Y1 est logiquement plus forte que X0 → Y0. Pour un seuil de confiance fixe, les règles qui atteignent, et ne sont pas licenciés par d'autres règles aussi au-dessus du seuil, forment le représentant (ou essentiel) base de règles pour ce seuil de confiance [1], [13], [16]. 2.1 Confiance Largeur Dans [4], [5], l'intuition de la redondance est poussée plus loin afin d'obtenir une perspective de la nouveauté des règles d'association: une règle irredondante dans la base est ainsi parce que sa confiance est plus élevé que le reste de la base suggère; alors, on peut se demander: « combien plus ». Cela peut être évaluée au moyen de la définition suivante. Pour toute discussion, supposons qu'un jeu de données D et un seuil de soutien τ ont été fixés: toutes nos règles sont censées atteindre cet appui sur D. Définition 1 La largeur de confiance de X → Y D est w (X → Y) = = c (X → Y) max {c (X '→ Y') || (X → Y) = (X '→ Y'), X '⊆ X, XY ⊆ X' Y '} Dans le cas où le dénominateur est nulle, la largeur de confiance est infini par convention. Pour expliquer cette notion, envisager une règle X → Y d'une confiance donnée, disons c (X → Y) = c0 ∈ [0, 1], et nous allons voir ce qui se passe comme nous le mien la base représentative à un seuil de confiance variant γ. Si c0 <γ, la règle à la main ne joue aucun rôle, étant de confiance trop faible pour le seuil. A γ = c0, la règle devient une partie de la production de toute norme RNTI-E-19 - 298 - J L Balcázar association processus d'extraction, mais il pourrait être qu'une autre règle logiquement plus forte apparaît à la même confiance. Par exemple, il se pourrait que les deux règles A → B et A → BC ont confiance c0: alors A → B est redondant et ne appartiendra à la base de cette confiance. Si aucune règle apparaît plus forte, X → Y appartiendra à la base représentative pour cette thre- Shold. Gardons diminuer le seuil. À une certaine confiance inférieure, une règle logiquement plus forte peut apparaître. Si une montre règle logiquement plus fort tôt, à un seuil de confiance γ très proche de c0, la règle X → Y est pas nouvelle: elle est similaire à celle logiquement plus forte, et cela se voit dans le fait que l'intervalle des seuils de confiance où il est une règle représentative est courte. Au contraire, une règle plus forte peut prendre longtemps à apparaître: dans ce cas, les règles seulement entail de confiance beaucoup plus faible X → Y, de sorte que le fait qu'il ne atteint la confiance c0 est nouveau dans ce sens. L'intervalle de seuils de confiance où X → Y est une règle représentative est grande. 2.2 Règles Blocking Le principal inconvénient souvent argumenté contre la confiance concerne son incapacité à détecter les corrélations néga- tives. Par exemple, pour un seuil, disons, 2/3 (soit environ 66%), envisager une règle A → repré- sentant B de confiance légèrement au-delà du seuil. Il va être fourni comme intéressant dans la production, ce qui suggère que les transactions ayant une tendance à avoir aussi B. Cependant, dans le cas où la fréquence réelle de B est assez élevé, disons, 80%, la corrélation est négative de fait, puisque B apparaît moins souvent parmi les transactions ayant une que dans l'ensemble des données. En fait, c'est la principale critique qui a été fait pour la confiance en tant que mesure de « degré de cation impli- », et a motivé un grand nombre d'alternatives; la littérature sur ces notions est, en fait, assez grand [9], [11], [12], [17]. Une bonne enquête avec beaucoup de références est [10]. Référence [5] suggère une notion de « blocage des règles ». Un exemple de motivation spécifique qui est décrit dans ce document est la suivante: extraction de règles d'association à l'appui de 5% et 100% de confiance de l'ensemble de formation d'adultes à partir de Irvine [2], 67 (sur 71) des règles de la base sont de la forme mari + « autre chose » → Homme. La raison en est que la règle mari → Homme, que nous nous attendons à la main, n'atteint pas 100% confiance: en effet, tuple 7110 comprend les éléments « mari » et « femme » (au lieu de « Homme »). Cela ouvre la porte à de nombreuses règles, intuitivement, uninformative qui agrandissent un peu le côté gauche, assez pour éviter tuple 7110 afin d'atteindre 100% confiance. D'autres exemples sont donnés dans le même journal. Pour aborder ce problème, [5] propose un système qui peut être expliqué sous une forme équivalente comme suit: Définition 2 règle donnée X → Y, avec X ∩ Y = ∅, un sous-ensemble Z ⊂ X blocs X → Y au seuil de blocage b if (de (XY) - c (z → Y) s (X)) / (c (Z → Y) s (X)) ≤ b. Cette définition compare le nombre de tuples ayant XY avec la quantité qui serait prédite à partir de la confiance de la règle Z → Y. Soit c (Z → Y) = c. Si Y est distribuée le long du support de X dans le même rapport que le long du support plus large de Z, nous nous attendons s (XY) ≈ c x s (x). Dans le cas où la différence dans le numérateur est négatif, cela signifie que s (XY) est plus faible même que ce que Z → Y suggère. Si elle est positive, mais le quotient est faible, X → Y ne fonctionne toujours pas assez apporter une grande confiance par rapport à Z → Y à considérer: il reste bloqué. Mais, si le quotient est plus grand, et cela se produit pour tous les Z, alors X → Y devient nouveau puisque sa confiance est assez élevé que suggéré par les règles de la forme Z → Y. On peut vérifier que les problèmes de l'ensemble de données des adultes évoqués précédemment sont résolus. Remarque, pour une utilisation ultérieure, que la confiance largeur liée et le blocage lié sont liés dans [5] comme suit: si la largeur limite de confiance est d, alors le blocage lié est b = d - 1. RNTI-E-19- 299 - Nouveauté Objectif via la confiance Boost 3 confiance Boost la définition de notre notion principale est très similaire à celle donnée pour la largeur de confiance; mais avec une torsion qui, même si elle est formellement petit, change sémantiquement assez loin de manière à englober la notion de blocage. Définition 3 La poussée de confiance de X → Y est un β (X → Y) = = c (X → Y) max {c (X '→ Y') || (X → Y) = (X '→ Y') , X '⊆ X, Y ⊆ X' Y '} Encore une fois, dans le cas où le dénominateur est nul, le regain de confiance est infinie par convention. Dans le cadre de notre hypothèse que X ∩ Y = ∅ et X '∩ Y' = ∅, Y ⊆ X 'Y' est en fait équivalent à Y ⊆ Y '. A correspond faible regain de confiance à une faible nouveauté. Pour voir cela, supposons que β (X → Y) est faible, dire β (X → Y) ≤ b, où b est légèrement supérieur à 1. Selon la définition, il doit y avoir une autre règle X '→ Y' , avec X '⊆ X et Y ⊆ X' Y ', telle que c (X → Y) c (X' → Y ') ≤ b, ou c (X' → Y ') ≥ c (X → Y) / b. Cette inégalité dit que la règle X '→ Y', indiquant que les transactions avec X 'Y ont tendance à avoir », a une confiance relativement élevée, pas beaucoup plus faible que celle de X → Y; de manière équivalente, la confiance des X → Y est pas beaucoup plus élevé (il pourrait être plus faible) que celle de X '→ Y'. Mais toutes les transactions ayant X ont X ', et toutes les transactions ayant X' Y 'ont Y, de sorte que la confiance trouvée pour X → Y est pas nouvelle, étant donné qu'elle ne donne pas confiance tellement supplémentaire sur une règle que les Etats comme ou bien plus, à savoir X '→ Y'. Comme un strict minimum, on ne doit pas tenir compte des règles de confiance avec boost 1 ou moins. Notez que cela résout l'objection contre la confiance que les corrélations négatives ne sont pas détectés: par exemple, si le soutien de B est de 80%, une règle A → B de confiance inférieure à celle serait en effet donner un coup de pouce de confiance ci-dessous 1. Toutes les règles filtrées sur la sortie d'un mineur d'association en raison du faible largeur sera filtré et fait de la faible poussée, si le même seuil est utilisé, parce que, de toute évidence, β (X → Y) ≤ w (X → Y). La même chose peut être montré pour bloquer, à condition que l'on tient compte de la différence dans le sens des seuils: si une règle X → Y est bloqué par itemset Z à bloquer le seuil b - 1, alors on peut montrer c (X → Y ) c (Z → y) ≤ b, et obtenir plutôt directement à partir de là que β (X → y) ≤ b. Par conséquent, limitant l'élan de confiance à b nous assure que les règles qui auraient été FILT Ered par l'une (ou les deux) des contrôles w (X → Y) ≤ b ou bloquer au seuil b-1 sera filtré et par la poussée de confiance liée. En ce sens, renforcer la confiance incarne les deux tests à faible nouveauté de [5], et avec les mêmes seuils qui y sont employés. 3.1 Le calcul de la confiance Boost: Applications Pour être pratique, nous avons besoin d'une étude plus approfondie du regain de confiance. À l'heure actuelle, il n'a pas de sens de parcourir toutes les règles alternatives à prendre en compte pour calculer la confiance maximale dans le dénominateur. Le même genre de difficulté apparaît pour la largeur de confiance et pour bloquer. Une précalcul légère permet un très efficace pour calculer la largeur [4], mais la même méthode ne semble pas fonctionner pour bloquer ou stimuler. En fait, les expériences rapportées dans [5], comme il est indiqué, le recours à une faible approximation de blocage, les valeurs réelles étant très coûteux à calculer. nous développons ici une autre approche complètement différente. Considérons l'algorithme suivant: RNTI-E-19 - 300 - entrée J L Balcázar: ensemble de données D; seuils de soutien, de confiance c, et renforcer la confiance b; règle X → Y avec C (X → Y) ≥ c, X ∩ Y = ∅ sortie: valeur booléenne indiquant si β (X → Y)> b mine D pour les règles de représentation R au seuil c / b pour chaque règle X ' → Y '∈ R tel que X' ∩ Y '= ∅, X' ⊆ X et Y ⊆ X 'Y': si ∃z ⊂ X - X 'de telle sorte que c (X → Y) ≤ b × c (X' Z → Y): Faux retour si ∃a ∈ Y '- XY tel que c (X → Y) ≤ b × c (X → AY): retour Faux sinon: Vrai retour théorème 1 soit X → Y une règle de confiance au moins c. Ensuite, cet algorithme accepte si et seulement si β (X → Y)> b. Les comparaisons sont rédigées de manière à éviter la division par zéro dans les cas de boost infini, comme s (Xay) = 0. Bien sûr, l'exploitation de R doit être fait une fois pour toutes. Faute d'espace, les descriptions de la validation empirique sont reportées à un prochain article. Parmi les autres tests, nous avons utilisé cet algorithme pour calculer le nombre de règles qui passe plutôt des seuils doux boost de confiance de 1, 1,05, 1,1, 1,15, 1,2, 1,25 et 1,3 sur trois ensembles de données très différentes, à deux thesholds différents de support, l'intervalle de 0,1% à 5%, et trois différents niveaux de confiance (70%, 80%, 90%). Le calcul entier a pris environ 30 minutes sur un ordinateur portable à faible portée. Les trois ensembles de données se composent de données du monde réel, et sont des caractéristiques très différentes: l'ensemble de la formation des adultes de l'UCI, au détail de FIMI et une forme transactionnelle de données provenant de sites européens fossiles paléontologiques. Les résultats obtenus ont été très satisfaisants. 4 Conclusions et travaux complémentaires Les insuffisances des seuils de confiance discutés au début du paragraphe 2.2 ont été souvent interprété comme une insuffisance de la notion de confiance. Nous vous proposons ici qu'une autre alternative pourrait être envisagée: peut-être ce genre d'objections ne sont pas à interpréter comme la conséquence répandue que « la confiance est inapproprate » de filtrer et de règles d'association de rang, mais « un seuil absolu sur la confiance ne convient pas » à règles de filtrage et association de rang. Notre message est qu'il pourrait être complété par des seuils de confiance relatifs qui évaluent la nouveauté de chaque règle par rapport à la confiance des règles logiquement (ou intuitivement) plus forts. L'identification de la mesure appropriée est une question de recherche claire, à laquelle nous avons contribué ici par la notion de renforcer la confiance: une définition TURELLE raisonnablement na- qui comporte à la fois de deux notions précédemment mis en avant dans le même but. Bien sûr, l'utilisation de boost de confiance ne fait pas obstacle à une combinaison avec ascenseur ou toute autre mesure de l'intensité de l'implication; dans quelle mesure ces mesures distinctes Interagir avec coup de fouet fiance de confiance, et qui obtiennent les meilleurs résultats, est l'une des nombreuses lignes ouvertes de la recherche future. Références [1] C C Aggarwal, P S Yu: Une nouvelle approche de génération en ligne des règles d'association. IEEE Transactions sur les connaissances et Données techniques, 13 (2001), 527-540. (Voir aussi ICDE'98.) RNTI-E-19- 301 - Nouveauté Objectif via le Boost de confiance [2] A Asuncion, D J Newman: UCI machine référentiel d'apprentissage. Irvine, Californie, Université de Californie, École de l'information et de l'informatique, 2007. [3] J L Balcázar: redondance, les systèmes de déduction et Bases-Taille minimum pour les règles d'asso- ciation; soumis pour publication. [4] J L Balcázar: Confiance Largeur: une mesure objective de l'Association Règle nouveauté. Atelier sur les questions de qualité, des mesures de interestingness et l'évaluation des modèles d'exploration de données QIMIE'09 à PAKDD'09. [5] J L Balcázar: Deux mesures de l'objectif de nouveauté dans l'Association minière Règle. version étendue de [4], à paraître dans LNCS (post-rendus des ateliers de PAKDD'09). [6] R Bayardo, R Agrawal, D Gunopulos: Règle contrainte basée sur l'exploitation minière dans les grandes bases de données denses. ICDE'99, 188-197. [7] C Borgelt: Les mises en œuvre efficace de Apriori et Eclat. Atelier sur le plateau item- fréquent Mining Implémentations (2003) [borgelt.net]. [8] fréquente itemset Mining Implémentations Repository [fimi.cs.helsinki.fi]. [9] G C Garriga: Stratégies statistiques pour Élagage Toutes les règles d'association sans intérêt. ECAI 2004, 430-434. [10] L Geng, H J Hamilton: mesures d'intérêt pour l'exploration de données: une enquête. ACM Comp. Surveys 38, 2006. [11] S Guillaume, F Guillet, J Philippe: « Amélioration de la découverte de règles d'association avec l'intensité de l'implication », PKDD 1998, 318-327. [12] C Hébert, B Crémilleux: une vue unifiée de l'objectif mesures d'intérêt. MLDM 2007, 533-547. [13] M Kryszkiewicz: Association représentant les règles. PAKDD'98, 198-209. [14] B Liu, W Hsu, Y Ma: l'élagage et résumant les associations de découverte. KDD'99, 125-134. [15] N Megiddo, R Srikant: A la découverte de règles d'association prédictives. KDD'98, 274-278 [16] V Phan-Luong: La base représentative pour les règles d'association. ICDM'01, 639-640. [17] P-N Tan, V Kumar, J Srivastava: Sélection de la mesure objective droit d'analyse Association. Inf. Syst. 29 (4): 293-313 (2004) [18] M Zaki: Association minière non redondant Règles. Connaissances minières et les données Dis- Covery 9 (2004), 223-248. Résumé En association minière de la règle, il est bien connu que le simple fait d'imposer un conduit de seuil de confiance absolue à certaines lacunes. De nombreuses propositions alternatives ont été proposées pour les surmonter. nous vous proposons ici, au contraire, pour compléter le processus en filtrant aussi les règles obtenues en fonction de leur nouveauté, mesurée de manière relative par rapport aux dences des règles plus strictes confi- du même ensemble de données. Notre proposition, le regain de confiance d'une règle, englobe deux précédentes notions similaires (largeur de confiance et le blocage des règles) de travaux antérieurs. RNTI-E-19 - 302 -"
674,Revue des Nouvelles Technologies de l'Information,EGC,2010,"Pattern Mining: The Past, Present, and Future","Pattern mining is one of the fundamental techniques in data mining. As one increases thecomplexity of the pattern types, from subsets, to subsequences, subtrees, and subgraphs, onediscovers potentially more informative patterns. In this talk I will offer a tour of the past andthe present research landscape in this area, and I'll conclude with some thoughts on directionsfor the future",Mohammed Zaki,http://editions-rnti.fr/render_pdf.php?p1&p=1001258,http://editions-rnti.fr/render_pdf.php?p=1001258,en,"articles assemblage.pdf Motif Exploitation minière: Passé, présent et futur Mohammed Zaki Rensselaer Polytechnic Institute de Troy, New York (Etats-Unis) zaki@cs.rpi.edu http://www.cs.rpi.edu/ zaki / index.php extraction de motifs est l'une des techniques fondamentales dans l'exploration de données. Comme on augmente la complexité des types de motifs, de sous-ensembles, à, séquences et sous-arbres, on découvre des sous-graphes de motifs potentiellement plus d'information. Dans cet exposé, je vais offrir une visite guidée du passé et le paysage actuel de la recherche dans ce domaine, et je vais conclure avec quelques réflexions sur les orientations pour l'avenir. RNTI-E-19- 17 -"
681,Revue des Nouvelles Technologies de l'Information,EGC,2010,Protein Graph Repository,"Protein Graph Repository (PGR) est i, outil bioinformatique sur le web permettant d'obtenir une nouvelle representation de protéines sous la forme de graphes d'acides aminés, une représentation plus simple et plus facile à étudier par les moyens informatiques et statistiques dédiés aux graphes. La génération des graphes est faite à partir d'un parseur appliqué sur des fichiers des protéines PDB extraits de la base Protein Data Bank et en precisant les parametres et la methode a utiliser. Les graphes generes sont ensuite enregistres dans un entrepot doté de moyens de recherche, de filtrage et de telechargement. PGR peut etre provisoirement consulte à l'adresse http://www.enode-edition.com/pgr/, il est spécialement dédié aux recherches intéressées à l'étude de données protéiques sous la forme de graphes et permettra donc de fournir des échantillons pour des travaux expérimentaux.","Wajdi Dhifli, Rabie Saidi",http://editions-rnti.fr/render_pdf.php?p1&p=1001403,http://editions-rnti.fr/render_pdf.php?p=1001403,en,
682,Revue des Nouvelles Technologies de l'Information,EGC,2010,Recent Advances in Partitioning Clustering Algorithms for Interval-Valued Data,,Francisco de Assis Tenório de Carvalho,http://editions-rnti.fr/render_pdf.php?p1&p=1001260,http://editions-rnti.fr/render_pdf.php?p=1001260,en,"articles assemblage.pdf Progrès récents dans Cloisonnement Clustering algorithmes pour Interval-Valued données Francisco de Tenório de Carvalho Assis Universidade Federal de Pernambuco Recife, Brésil. fatc@cin.ufpe.br http://www.cin.ufpe.br/~ FATC / analyse typologique ont été largement utilisés dans de nombreux domaines, y compris la reconnaissance des formes, l'exploration de données et de traitement d'image. Leur but est d'organiser un ensemble d'objets dans des groupes tels que les éléments d'un groupe donné ont un degré élevé de similitude, tandis que les éléments appartenant aux groupes de diffé- rentes ont un haut degré de dissimilarité. En particulier, le partitionnement des modèles de clustering vise à organiser un ensemble d'éléments en un certain nombre de pré-défini des clusters. Nos références algorithmes anneau cluste- sont les algorithmes de cluster dynamique partitionnement soi-disant. Ils sont des algorithmes de classification itérative de relocalisation en deux étapes impliquant la construction à chaque itération des clusters et l'identification d'un représentant approprié ou prototype (moyennes, axes factoriels, les lois de probabilité, etc.) de chaque grappe en optimisant localement un critère d'adéquation entre les grappes et leurs prototypes correspondants. Souvent, les objets à être regroupées sont représentées en tant que vecteur de caractéristiques quantitatives. Cependant, l'enregistrement des données d'une valeur d'intervalle est devenu une pratique courante dans des applications réelles et de nos jours ce genre de données est souvent utilisé pour décrire des objets. Analyse de données symboliques (de SDA) est un domaine lié à l'analyse multivariée, l'extraction de données et la reconnaissance de formes, ce qui a fourni des méthodes d'analyse de données appropriées pour la gestion des objets décrits en tant que vecteur d'intervalles. Dans cet exposé, nous passons en revue le partitionnement des algorithmes pour les données de clustering à valeur d'intervalle ayant comme référence l'algorithme de clustering dynamique. Pour chaque algorithme de classification, il est donné le critère regroupement, le meilleur prototype de chaque groupe, la meilleure distance associée à chaque groupe (le cas échéant), ainsi que la meilleure partition en un nombre fixe de pôles. En outre, divers outils pour la partition et l'interprétation des données groupe d'une valeur d'intervalle fournis par ces algorithmes sont également présentés. Enfin, afin de montrer l'utilité de ces algorithmes et le mérite de la partition et des outils d'interprétation du cluster, des expériences avec des données réelles d'une valeur d'intervalle-ensembles sont donnés. RNTI-E-19- 19 -"
693,Revue des Nouvelles Technologies de l'Information,EGC,2010,Self-Clustering for Identification of Customer Purchase Behaviours,"La segmentation d'une base client peut avoir différents objectifs etplusieurs segmentation peuvent être utiles pour décrire les clients ou pour s'adapteravec les stratégies commerciales d'une entreprise. Dans ce papier, nous présentonsun schéma expérimental visant à proposer un ensemble de segmentationsalternatives. Ces segmentations sont produites sur des données réelles par latransformation des données initiales, la génération et la sélection de différentessegmentations.","Guillem Lefait, Gilles Goncalves, M. Tahar Kechadi",http://editions-rnti.fr/render_pdf.php?p1&p=1001273,http://editions-rnti.fr/render_pdf.php?p=1001273,en,"articles assemblage.pdf auto-Clustering pour l'identification d'achat client Behaviors Guillem Lefait *, **, *** Gilles Goncalves **, Tahar Kechadi * * UCD, Belfield, École des sciences de l'informatique et l'informatique, Dublin 4, Irlande guillem.lefait @ ucd.ie, tahar.kechadi@ucd.ie ** Univ Lille Nord de France, F-59000 Lille, France *** UArtois, LG2IA, F-62400, Béthune, France gilles.goncalves@univ-artois.fr Résumé . La segmentation client de base d'Une may et Avoir several Différents Objectifs segmentation PEUVENT Être DÉCRIRE les verser Utiles clients òû verser s'adapter Avec les D'une stratégies Commerciales entreprise. Dans papier this, nous présenta- tonnes un à Vasant schéma expérimental proposeur d'ensemble de l'ONU alternatives de segmentations. CÉS segmentations sur des Sont produites par la REELLES Données transformation des Initiales Données, la et la génération de sélection segmentations Différentes. 1 Introduction Clustering consiste à la création de groupes, tels que les objets à l'intérieur même des groupes sont très similaires et objets de différents groupes sont très dissemblables Xu et Wunsch (2005). Clustering est utilisé pour des objectifs différents: pour explorer l'ensemble de données, pour condenser en un petit ensemble de points représentatifs ou d'organiser les données. La segmentation est la capacité de reconnaître les groupes de clients qui partagent les mêmes ou similaires, des McDonald (1996). Pas tous les clients dans un marché au sens large ont les mêmes besoins, donc la segmentation permet aux entreprises de fournir des produits ou des services spécifiques à des segments différents. L'utilisation de clusters pour fournir automatiquement une segmentation est pas récente et a été réalisée pour deux objectifs principaux: 1) pour identifier les groupes d'entités qui partagent certaines caractéristiques com- munes et 2) afin de mieux comprendre les comportements des acheteurs en identifiant des groupes homogènes d'acheteurs Punj et Stewart (1983). Cependant, il existe différents défis à relever lors de l'utilisation en cluster pour effectuer la segmentation: les données à sélectionner, le nombre de grappes pour produire et comment évaluer les résultats de clustering. Très peu de solutions ont été proposées pour évaluer la qualité de la mise du client. enquête manuelle est souvent la solution utilisée pour évaluer la pertinence des grappes Aggelis et CHRISTODOULAKIS (2005). Dans Cheng et Chen (2009), le résultat de la segmentation est as- sessed par la précision de prédire la fidélité des clients inconnus. Dans Chang et al. (2009), la prévision des ventes est utilisé sur le résultat de la segmentation. Dans cet article, nous sommes intéressés à créer, sélectionner et évaluer les résultats de clustering qui seront présentés aux experts. L'idée présentée dans ce document est basée sur la recherche systémique RNTI-E-19- 109 - Auto-Clustering pour l'identification du client Achat de clusters différents Behaviors mais pertinents. Cette approche consiste en la génération de milliers de grappes à la fois par la transformation de l'entrée et en utilisant des méthodes en cluster avec des paramètres différents. Ces grappes sont ensuite évaluées pour estimer les résultats de clustering utiles sont, et seuls les plus cohérentes et les plus différents modèles sont conservés. Enfin, l'ensemble de segments sélectionné est fourni à des experts avec des informations structurelles pour faciliter l'exploration et la comparaison des différentes segmentations. Le reste de cet article est organisé comme suit: Les composants pour créer et évaluer les résultats de clustering sont décrits dans la section 2. Les résultats expérimentaux avec la discussion sont fournis à la section 3, et la conclusion de ce document et les perspectives sont données à la section 4. 2 architecture Segmentation client Notre objectif est de produire automatiquement des clients segmentations diverses et significatives. Les composants automatiques sont décrits dans les sous-sections suivantes. 2.1 Composant d'entrée Le composant d'entrée est en charge de la transformation des données. Ce composant est composé de plusieurs méthodes de transformation des données et peut réduire ou augmenter le nombre de caractéristiques de l'original ensemble de données. De plus, toutes ces techniques de transformations peuvent être combinées pour produire des données multi-transformé. Un premier ensemble de données, DSIR, est créé pour recueillir la valeur Ancienneté (R) qui décrit le dernier récence d'achat, les valeurs de fréquence (F) et monétaire (M) qui représentent respectivement le nombre d'achats effectués et le montant d'argent passé dans la période t par un client. Une deuxième série d'ensembles de données est créée par la discrétisation des données de consommation avec la méthode symbolique d'agrégat approximation (SAX) Lin et al. (2003). Tout d'abord, le temps est discrétisé et les données sont séparées en poids des périodes où les valeurs retenues sont les valeurs moyennes de la période considérée. D'autre part, les valeurs sont également discrétisées avec un alphabet de taille α. Nous créons une dernière série de jeux de données qui enregistrent les transitions de fréquence entre les symboles identifiés par SAX. Cet ensemble de données rassemble les relations dans les fréquences d'achat ou les similitudes des taux de consommation. D'autres méthodes pour transformer les données, telles que le lissage ou les méthodes de sélection de caractéristiques pourraient également être ajoutés. 2.2 Composante Génération La composante de génération est responsable de la création des résultats de regroupement. Il est défini pour accepter plusieurs méthodes de classification avec plusieurs paramètres. Lorsque l'algorithme de clustering présente un comportement stochastique, il est répété r fois. Nous limitons le choix des algorithmes de regroupement des algorithmes qui produisent une partition difficile en raison de l'utilisation qui sera faite de cette segmentation. L'utilisation d'un résultat de classification floue peut être très utile si elle est associée à une autre technique d'apprentissage ou lorsque Ering un consi- client particulier. Cependant, il est d'une aide limitée si un expert doit explorer et analyser la partition. RNTI-E-19 - 110 - G. Lefait et al. 2.3 Sélection des composants L'objectif de la composante de sélection est de marquer les résultats de clustering pour garder les segmentations plus pertinentes uniquement. Nous avons utilisé trois approches différentes pour quantifier la qualité des grappes. Le premier estimateur, Q1, est le coefficient de détermination (R2) et mesure la proportion de la variabilité dans l'ensemble de données qui est expliquée par le modèle. Il est défini comme suit: Q1 = R2 = 1 - Σ (pi - pJ) 2Σ (pi - p) 2 (1) où p représente les données d'information et d'achat uJ le modèle de cluster associé au client i. Le deuxième estimateur, Q2, mesure la précision du modèle donné une tâche de classification. Compte tenu de la valeur de chaque RFM client (RFM Value = R * F * M), les clients ont reçu une étiquette (très faible, faible, moyenne, élevé, très élevé) à condition que le quantile ils appartiennent. Les grappes sont ensuite évaluées par la dispersion sur les étiquettes entre les grappes. Nous sélectionnons la F-mesure, la moyenne harmonique de la précision et le rappel pour mesurer l'homogénéité des grappes. La précision P (l, c) est la proportion des clients avec l'étiquette l dans le groupe c. Le retrait R (l, c) est le nombre des clients avec l'étiquette l dans le c de la grappe par rapport au nombre total de clients avec étiquette l. Le dernier estimateur de la qualité de clustering, Q3, indique la précision des prévisions de ventes en utilisant la segmentation. La prévision est réalisée avec le lissage exponentiel. Les ventes à l'instant t + 1 dépend à la fois la dernière valeur réelle et la dernière valeur lissée. Etant donné une valeur réelle vt, une valeur lissée st à l'instant t et le paramètre de lissage α, le pi de prévision + 1 La valeur à l'instant t + 1 est donné par: ft + 1 = αvt + (1 - α) r (2) La le paramètre α est sélectionné pour chaque segment de validation interne. On calcule également un estimateur global de cohérence, G, qui prend en compte la qualité et les mesures de cohérence: G = CQ1 × CQ2 × CQ3 (3) G constitué d'un bassin de mesure et de même à Williams (1999), cette architecture pourrait être étendu, comme chaque mesure reçoivent un weigth qui décrit sa contribution à la découverte de bons segments. Pour tous les clusters, les meilleurs résultats étant donné chacun des Q0 estimateur, Q1, Q2 et G sont sélectionnés et envoyés au composant suivant. 2.4 Visualisation Composant Le composant de visualisation est en charge de la représentation graphique des résultats de classification. Il doit fournir des informations à la fois sur les grappes et sur la qualité estimée. Plus- plus, il doit fournir des outils pour faciliter la comparaison entre les différents résultats de clustering. Icônes Intelligent Keogh et al. (2006) sont une technique qui cartographier la transition de fréquence entre les symboles en couleurs. Ensuite, étant donné une matrice de fréquence, une icône carré peut être dérivée pour représenter visuellement le contenu de la matrice. Rappelant que nous avons discrétisé les données avec SAX RNTI-E-19- 111 - Auto-Clustering pour l'identification d'achat client Behaviors (a) Le client 27 (b) Le client 94 figure. 1 - Icônes appliquées sur des données intelligentes d'achat des consommateurs (marque 1) et calcule la matrice de transition de fréquence (dsisf), on peut appliquer le même processus pour décrire et représenter visuellement les groupes identifiés. La figure 1 montre l'efficacité de cette représentation pour deux clients différents. L'achat et d'événements non-achat sont représentés par un carré composé de quatre pixels. Bien que l'information retenue a été divisé par 15, il permet encore la comparaison de deux consommateurs. 3 Résultats expérimentaux Des expériences ont été réalisées sur un ensemble de données obtenues à partir du défi SLDS09 1. Cet ensemble de données comprend le journal hebdomadaire d'achat de 10 000 clients de plus de 62 semaines. Les achats ont été faites pour 3 marques dans deux supermarchés différents (6 marques au total). L'objectif initial était d'identifier les spécificités de la marque et / ou supermarché. L'enquête suivante est per- formé uniquement avec le journal des achats des clients: aucune autre information est connue sur les clients ni les marques. En raison du manque d'espace, nous ne les résultats actuels et comparer avec K = 5. Résultats pour un nombre différent de groupes seront mis en ligne 2. Tout d'abord, nous présentons les résultats R2 sur les 6 marques. En moyenne, les scores R2 sont très faibles, ce qui indique que la variance très peu peut être expliquée par le modèle. , On peut noter cependant deux comportements distincts dans la population des groupes identifiés. Par exemple, les segments des marques 1, 3, 4 et 5 sont clairement séparés par le volume d'achat. Cependant, lorsque l'on considère la marque 2 et 6, on voit que les grappes semble aussi avoir des spécificités tem- poral. Cela indique que les grappes peuvent regrouper des personnes qui ont fait des achats simultanous (pic d'un segment à Noël dans la marque 6). Nous pouvons également remarqué que les marques 2 et 6 (où segments reposent en partie sur les achats simulatenous) obtenir une meilleure valeur R2 de segmentation sur DSIS, tandis que les autres (où iden- tifié segments sont séparés par des taux de consommation) se fondent sur les données de la matrice de transition dsisf. Le deuxième résultat, la valeur de classification RFM évaluée par la F-mesure sont donnés dans le tableau 1. Il est intéressant de noter que pour la marque 1, 3, 4 et 5, la segmentation qui sépare les meilleurs clients avec leurs valeurs RFM ne pas utiliser les données RFM, mais la matrice de transition dsisf. Lors de l'examen de la distribution de la classe interne, nous pouvons voir que pour la meilleure Q2 étant donné le regroupement, chacune des grappes contient un certain type de clients. Nous pouvons également noter que les segments avec les clients des valeurs extrêmes RFM (avec une très faible ou très forte valeur) sont plus cohérentes. 1Symposium et Science des Apprentissage 2009 Données, http://www.ceremade.dauphine.fr/SLDS2009 informations 2Extra sont disponibles sur http://www.emining.fr/data/consumer-behaviour/ RNTI-E-19 - 112 - G. Lefait et al. Marque 1 2 3 4 5 6 train 71,00 78,14 77,02 70,41 74,86 77,28 40,96 48,56 43,41 test 39,94 41,56 48,47 Les données dsisf DSIR dsisf dsisf dsisf DSIR TAB. 1 - Best F-mesure sur des données marquées avec K = 5 La performance des prévisions des ventes estimées par Q3 sont très faibles. La différence avec un segment et n est pas significatif, et d'ailleurs et contrairement aux deux précédents résultats, il n'y a pas de corrélation entre les bons résultats dans le tra INING ensemble et dans l'ensemble de test. Ce résultat est très décevant car Q3 peut être un indicateur très sensible de la performance de segmentation réelle. Ce résultat indique que 1) le lissage exponentiel est pas adapté pour réaliser la prévision de ces données, ou 2) la sélection du paramètre de lissage α ne soit pas effectué correctement, à savoir l'intervalle de validation devrait être fait une plus historique. Le pouvoir prédictif de la segmentation doit ensuite être évaluée au moyen d'une méthode de prévision plus fiable, telles que les méthodes basées sur les réseaux de neurones. FIGUE. 2 - Global score pour tous les segmentations effectuées sur la marque 1 Le score global (3 Eq.) Est la dernière méthode pour effectuer la sélection entre les différents groupes, par rapport à tous les estimateurs définis. La figure 2 montre la distribution mondiale de résultat pour tous les résultats de regroupement de la marque 1. Sur cette figure, la meilleure mondiale la segmentation est l'un des segmentations avec K = 6. Utilisation des icônes intelligentes, il est facile de comparer les différentes segmentations tout à fait par de- rayage l'intersection de deux segments. Cela peut aider à sélectionner le nombre approprié de grappes: lorsque l'intersection reste cohérente, il n'est pas nécessaire d'augmenter encore le nombre de grappes. 4 Conclusion Dans cet article, nous avons expérimentalement effectué une segmentation de la clientèle sur les données du journal d'achat par 1) la transformation des données, 2) la génération de modèles divers, 3) la sélection la plus adéquate RNTI-E-19- 113 - Auto-Clustering pour l'identification du client achat modèles Behaviors donné un ensemble de fonctions d'évaluation et 4) la création d'une représentation visuelle des mentations seg-. Cette segmentation a été réalisée sur un ensemble de données du monde réel et des modèles de segmentation divers et pertinents ont été choisis parmi un grand nombre de candidats de regroupement. Notre prochaine priorité est d'améliorer la vente des prévisions en utilisant une approche plus sophistiqués tels que les réseaux de neurones. Un deuxième objectif est de pouvoir itérer et favoriser la transformation des données et les méthodes de classification qui ont été à l'origine des meilleurs résultats de clustering. Références Aggelis, V. et D. CHRISTODOULAKIS (2005). regroupement des clients en utilisant une analyse RFM. Stevens Point, Wisconsin, États-Unis. WSEAS. Chang, P.-C., C.-H. Liu, et C.-Y. Fan (2009). le regroupement des données et réseau de neurones floue pour la prévision des ventes: Une étude de cas dans l'industrie de la carte de circuit imprimé. Connaissance des systèmes basés sur 22 (5), 344-355. Cheng, C.-H. et Y.-S. Chen (2009). La segmentation de la classification valeur client par la théorie du modèle RFM et rs. Expert Syst. Appl. 36 (3), 4176-4184. Jiang, T. et A. Tuzhilin (2006). L'amélioration des solutions de personnalisation par seg- optimale des mise en bases de clients. En ICDM. Keogh, E., L. Wei, X. Xi, S. Lonardi, J. Shieh et S. Sirowy (2006). icônes intelligentes: données intégrant de lite-poids extraction et de visualisation dans les systèmes d'exploitation graphiques. En ICDM. Lin, J., E. Keogh, S. Lonardi, et B. Chiu (2003). Une représentation symbolique de séries temporelles, avec des implications pour les algorithmes en continu. En DMKD, New York, NY, USA. ACM. McDonald, M. (1996). Le rôle du marketing dans la création de valeur client. Dans le marketing d'un point de vue technique (Digest n ° 1996/172), p. 1 / 1-111. Punj, G. et D. W. Stewart (1983). L'analyse typologique dans la recherche marketing: Examen et Gestions sug- pour l'application. Journal of Marketing Research 20 (2), 134-148. Williams, J. G. (1999). données évolutionnaire points chauds minière - une architecture pour explorer des découvertes intéressantes. En PAKDD, pp. 184-193. Xu, R. et D. Wunsch (2005). Enquête sur les algorithmes de clustering. Les réseaux de neurones, IEEE Transactions on 16 (3), 645-678. Résumé Segmentation, le processus de division clients en groupes, ont longtemps utilisé par les Panies com- de regrouper les clients avec les mêmes caractéristiques. Cependant, avec le nombre croissant de services disponibles, plusieurs segmentations sont maintenant tenus de décrire les gies de l'entreprise et l'ajustement des comportements des clients. Nous présentons un programme expérimental pour produire automatique ly segmentations différentes et significatives, en transformant les données initiales, générant segmentation multiple et la sélection d'un ensemble de segmentations différentes. Cet article présente des résultats expérimentaux sur des données du monde réel ensemble de 10000 clients plus de 60 semaines pour 6 produits. RNTI-E-19 - 114 -"
702,Revue des Nouvelles Technologies de l'Information,EGC,2010,Tulip: a Scalable Graph Visualization Framework,"The Graph Visualization Framework Tulip now enjoys 10 years ofuser experience, and has matured its architecture and development cycle. Originallydesigned to interactively navigate large graphs, the framework integratesstate-of-the-art software engineering concepts and good practices. It offers alarge panel of graphical representations (traditional graph drawing as well asalternate representations). Tulip is most useful in a data mining and knowledgediscovery context, allowing users to easily add their own data analysis and computingroutines through its plug-in architecture.","David Auber, Patrick Mary, Morgan Mathiaut, Jonathan Dubois, Antoine Lambert, Dan Archambault, Romain Bourqui, Bruno Pinaud, Maylis Delest, Guy Melançon",http://editions-rnti.fr/render_pdf.php?p1&p=1001374,http://editions-rnti.fr/render_pdf.php?p=1001374,en,"articles assemblage.pdf Tulip: un cadre de visualisation graphique évolutive David Auber, Patrick Mary, Morgan Mathiaut, Jonathan Dubois, Antoine Lambert, Dan Archambault, Romain Bourqui, Bruno Pinaud, Maylis Delest, Guy Melançon *, ** * UMR CNRS 5800 LaBRI, campus Université Bordeaux I @ labri.fr http://www.labri.fr ** INRIA Bordeaux Sud-Ouest, campus de l'Université Bordeaux I @ inria.fr http://www.inria.fr/bordeaux CV. Le cadre graphique de visualisation Tulip bénéficie maintenant 10 ans d'expérience utilisateur, et a mûri son cycle d'architecture et de développement. Origi- nalement conçu pour les grands graphes Navigate de manière interactive, le cadre implante des systèmes sur l'art concepts et bonnes pratiques d'ingénierie logicielle. Il offre un large panel de représentations graphiques (dessin graphique traditionnel, ainsi que des représentations alternatives). Tulip est le plus utile dans un contexte d'exploration de données et la découverte de connaissances, ce qui permet aux utilisateurs d'ajouter facilement leur propre analyse des données et des routines de com- Puting par son architecture plug-in. La plupart des efforts de recherche dans l'exploration de données et l'extraction de connaissances et de représentation ont besoin d'expérimentation et de validation. À cette fin, notre groupe développe le Tulip Graph cadre de visualisation. Tulip offre un C ++ plug-in mécanisme facilitant le développement et l'ajout de nouveaux algorithmes (statistiques de graphique informatique, dessin graphique ou regroupement graphique). Ceci est en fait une caractéristique principale de la plate-forme Tulip. Tulip outils optimisés et des structures de données efficaces, la gestion des données et le filtrage / in- mécanismes de Heritance. Son moteur de rendu OpenGL repose sur, tandis que ses restes de l'interface graphique sur la bibliothèque QT ch Trollte-. Le principal paradigme d'interaction offerte par Tulip est le calcul et la manipulation directe des hiérarchies de graphique, ce qui rend unique parmi toutes les plates-formes de visualisation graphique disponibles. Tulip peut être utilisé dans un scénario typique où l'objectif est de découvrir les tendances des données. L'exploration peut être entraîné par l'utilisateur à l'aide des plug-ins disponibles, le calcul de statistiques sur un graphique, la conception de palettes de couleurs ou d'attribuer tailles à des noeuds, par exemple. Fig. 1 illustre un nario scé- typique lors de l'exécution exploration et l'analyse des données en utilisant des tulipes. Alors que le volet de gauche donne accès à plusieurs propriétés du graphe et les paramètres de réglage, les volets de droite montre plusieurs vues sur les données: Dessin graphique de noeuds de liaison (en haut à gauche); Les coordonnées parallèles (en bas à gauche, ici représenté circulairement); des cartes d'auto-organisation et histogrammes, matrice de nuage de points (en haut droite) et la feuille tabulaire standard (en bas à droite). Tulip est incroyablement efficace à synchroniser des vues et des propriétés gardant cohérence entre les vues. L'application colormaps par histogrammes ou des noeuds de sélection par l'auto-organisation des cartes immédiatement les transferts à toutes les autres vues. RNTI-E-19- 623 - Tulip: une évolutive et cadre Adaptive graphique Visualisation figure. 1 - Exemple séance de travail en utilisant Tulip. L'une des caractéristique principale de Tulip est la possibilité pour un utilisateur d'ajouter ses propres plug-ins pour l'ensemble du cadre. 1 Le résultat d'un plug-in sera typiquement stocké dans un récipient conçu avant appelé une propriété. Propriétés apparaissent comme paradigme central Tulip, ce qui permet de modéliser le cadre et appliquer à une grande variété de domaines d'application et l'utilisation de l'application. Plug-ins peuvent compter les uns des autres grâce à des mécanismes d'invocation de la propriété. plug-in plus sophistiqué et interacteurs peut être développé au prix de faire face à l'intervalle QT. Tulip apparaît également comme un cadre unique permettant le calcul et la visualisation des algorithmes de regroupement multi-niveaux. En se fondant sur OpenGL, les hiérarchies de graphique peuvent être interactive explorés où des graphiques de niveau inférieur apparaît comme ils gagnent size.The les plus récentes avancées avec Tulip ont montré comment la programmation GPU pourrait être utilisée pour améliorer les performances lors de l'élaboration des graphiques ou lors de la conception des bords sous forme de courbes haut polynômes (Béziers) . Résumé La Plate-forme de visualisation de Graphes Tulip is de Maintenant riche d'ex- Dix Années PERIENCE, du Point de Tant vue de l'utilisation de fils, de la conception de l'architecture de fils de fils Que développement logiciel. Conçue verser de très grands Manipuler Graphes Dans un Contexte interactif, les concepts Elle Adopte et pratiques de l'état de l'art du génie et logiciel Une large palette implémente de Graphes de représentations. Destinées à des Avec Être utilisées d'- analyse de outils Données, architecture fils à l'Përmet d'integrer SES Utilisateur Propres d'analyser et Composants de dessin. 1. La méthode réelle peut actuellement être consulté sur le site Web de l'établissement Tulip. Voir l'URL www.tulip-software.org, pour accéder manuel et tutoriel, le mode d'emploi du développeur. Nous fournissons des modèles de C à partir de laquelle on peut développer son propre plug-in pour dessiner des graphiques ou calculer les statistiques graphiques de spécifiques. RNTI-E-19 - 624 -"
714,Revue des Nouvelles Technologies de l'Information,EGC,2010,Visual Sentence-Phrase-Based Document Representation for Effective and Efficient Content-Based Image Retrieval,"Having effective and efficient methods to get access to desired imagesis essential nowadays with the huge amount of digital images. This paperpresents an analogy between content-based image retrieval and text retrieval.We make this analogy from pixels to letters, patches to words, sets of patchesto phrases, and groups of sets of patches to sentences. To achieve a more accuratedocument matching, more informative features including phrases and sentencesare needed to improve these scenarios. The proposed approach is basedfirst on constructing different visual words using local patch extraction and description.After that, we study different association rules between frequent visualwords in the context of local regions in the image to construct visual phrases,which will be grouped to different sentences.","Ismail Elsayad, Jean Martinet, Thierry Urruty, Chabane Djeraba",http://editions-rnti.fr/render_pdf.php?p1&p=1001284,http://editions-rnti.fr/render_pdf.php?p=1001284,en,"articles assemblage.pdf visuels Phrase Phrase-document basé sur la représentation efficace et pour la base de contenu efficace image de récupération Ismail Elsayad, Jean Martinet, Thierry Urruty, Chabane Djeraba LIFL / CNRS-UMR 8022-Université de Lille 1, France {ismail.elsayad, jean.martinet, thierry.urruty, chabane.djeraba}@lifl.fr Résumé. Avoir des méthodes efficaces et efficaces pour obtenir l'accès aux images souhaitées est aujourd'hui essentielle avec l'énorme quantité d'images numériques. Cet article présente une analogie entre la récupération et la récupération de texte images par le contenu. Nous faisons cette analogie de pixels aux lettres, patches à des mots, des ensembles de patches à des phrases et des groupes d'ensembles de correctifs à des phrases. Pour obtenir un taux correspondant plus accu- des documents, des fonctionnalités plus d'information, y compris des phrases et tences sen- sont nécessaires pour améliorer ces scénarios. L'approche proposée est basée d'abord sur la construction de différents mots visuels utilisant l'extraction locale des correctifs et des- cription. Après cela, nous étudions différentes règles d'association entre les mots visuels fréquents dans le contexte des régions locales dans l'image pour construire es de phras- visuels, qui seront regroupés à des peines différentes. 1 Introduction Dans les systèmes de recherche (CBIR) image en fonction du contenu typique, il est toujours important de sé- lect une représentation appropriée pour les documents (Baeza-Yates et Ribeiro-Neto, 1999). En effet, la qualité de la récupération dépend de la qualité de la représentation interne du contenu des documents. Une technique populaire qui a été utilisée consiste récemment à considérer les images comme sac de mots (Grauman et al., 2005, Jurie et al., 2005, 2003 Djeraba et Sivic et al., 2005). De même à la représentation des documents en termes de mots dans le domaine de texte, les mots sac-of-modèles approchent une image comme un sac de mots visuels, qui est formé par une quantification vectorielle de descripteurs de région locale. D'une part, le sac de mots approche permet d'atteindre de bons résultats en représentant les apparences d'objets variables causées par des changements dans la pose, l'échelle, la traduction, etc. D'autre part, la faible puissance de discrimination des mots visuels conduit à une faible corrélation entre les caractéristiques de l'image et sa sémantique. Nous développons une riche et la représentation de la structure globale corsés des documents visuels en tenant compte non seulement des mots visuels, mais aussi l'introduction de deux représentations de niveau supérieur à savoir: des phrases et des phrases visuelles. Dans notre approche, nous extrayons patches d'image échelle et invariantes d'orientation locale de chaque image en utilisant SURF (Bay et al., 2008). Les correctifs sont regroupés en différents groupes pour former un vocabulaire visuel. Les images sont divisées en bandes verticales et horizontales qui définissent les régions locales où les apprenants des règles d'association sont utilisés pour découvrir des modèles de mots visuels qui co-produisent fréquemment dans ces régions. Des mots visuels différences qui ont ents règles d'association fortes dans ces régions, des expressions visuelles sont construites. Enfin, des phrases voisines qui se trouvent dans la même bande sont regroupés en tences sen-. RNTI-E-19- 157 - (. Lew et al, 2006) visuelle Phrase Phrase-document basé sur la représentation pour efficace et efficiente image Basé tente Con- récupération par rapport à l'état des techniques de l'art, nous proposons une approche basée sur mots visuels, des phrases et des phrases qui maintiennent les différentes informa- tions structurelles entre les parcelles locales et dans un ensemble de correctifs locaux qui sont situés dans des zones d'image. Cela enrichit la présentation avec plus d'informations et donne une meilleure représentation structurelle globale pour l'ensemble de l'image. Le reste de l'article est structuré comme suit. Dans la section 2, nous décrivons notre méthode pour construire des mots visuels à partir d'images et de l'exploitation minière des expressions visuelles de mots visuels qui mène nous à construire les phrases visuelles. Dans la section 3, nous vous présentons notre similitude d'image moi- ThOD basée sur les mots visuels, des phrases et des phrases visuelles visuelles. L'article 4 conclut le document. 2 image représentation Dans cette section, nous décrivons trois composantes de la chaîne de processus dans la construction de la représentation phrase-expression visuelle (voir la figure 1). FIGUE. 1 - Flux d'informations dans la représentation du document visuel 2.1 construction visuelle des mots Nous utilisons la fonction de bas niveau SURF descripteur qui décrit comment les intensités de pixels sont distribués dans un quartier dépendant à l'échelle de chaque point d'intérêt détecté par le Fast- hessois. Cette approche est similaire à EIPD (Lowe, 2004), mais les images intégrales (Viola et Jones, 2001) est utilisé conjointement avec des filtres connus sous le nom vaguelettes Haar sont utilisés afin d'augmenter la robustesse et de diminuer le temps de calcul. vaguelettes Haar sont simples filtres qui peuvent être utilisés pour trouver des gradients dans les directions x et y. L'extraction du descripteur peut être divisé en deux tâches distinctes (voir Figure 2). Tout d'abord, chaque point d'intérêt est attribué une orientation reproductible. En second lieu, un E-RNTI-19 dépend de l'échelle gagnant - 158 - I. Elsayad et al. dow est construit, dans lequel un vecteur de dimension 64 est extrait. Il est important que tous les calculs pour le descripteur sont basées sur des mesures par rapport à l'échelle détectée afin d'obtenir des résultats invariants d'échelle. mots visuels sont créés en regroupant les caractéristiques observées afin de former un vocabulaire visuel. Nous quantifions l'espace caractéristique de vecteur par chaque caractéristique Assign- ing observée au mot visuel le plus proche. 2.2 Construction Phrase Visuelle En retournant aux documents texte, une phrase peut être définie comme un groupe de mots qui fonctionnent comme une seule unité dans la syntaxe d'une phrase et ayant un sens différent pris ensemble ou séparément. Ceci est également applicable dans une image, mais dans un espace 2D. Dans notre approche, nous seg- ment l'image en différentes bandes locales à travers des colonnes et des lignes couvrant l'ensemble de l'image (voir les lignes rouges et vertes de la figure 2). Avoir une image représentée par des mots visuels, nous examinons les règles d'association (Simovic et Djeraba 2008) entre les différents mots visuels fréquents qui se produisent dans les mêmes bandes locales. Considérant que l'ensemble des tous les mots visuels (vocabulaire visuel) est W = {w1, w2 ... wk}, D est la base de données (ensemble d'images I), et T = {t1, t2 ... tn} est l'ensemble de tous différents jeux de mots visuels situés dans une même bande. En revenant à la définition des règles d'association, W représente l'ensemble des éléments et T désigne l'ensemble des transactions. Une règle d'association est une relation d'un XVy d'expression, où X et Y sont des ensembles d'éléments. Les propriétés des règles d'association de caractériser sont les suivants: - La règle XVy tient dans l'ensemble de la transaction T avec le support s si s% des transactions par T contient X et Y; - La règle XVy tient dans l'ensemble des transactions T avec confiance c si c% des transac- tions en T qui contiennent X contient également Y. Après extraction l'ensemble des transactions et de trouver les règles d'association, les règles d'association sont appelés forte si elles ont soutien et la confiance ci-dessus et confiance minsupport minconfi- respectivement. Enfin, tous les mots visuels qui sont dans la même bande et impliqués dans les règles d'association fortes formeront une expression visuelle. FIGUE. 2 - Un exemple d'une image après taches locales (carrés bleus) extraction par SURF alors il est segmentée à différentes bandes verticales et horizontales (lignes verte et rouge). RNTI-E-19- 159 - Phrase visuelle Phrase-document basé sur la représentation efficace et pour l'image de base tente efficace Con- récupération 2.3 construction de la phrase visuelle Une fois des phrases visuelles ont été construites, nous pouvons traiter à l'étape suivante pour Construc l'ing phrases visuelles en regroupant des phrases voisines qui se trouvent dans la même bande. phrases visuelles Con- de a un avantage de construction définies intrinsèque depuis une phrase visuelle peut être partagée par différents objets dans une image, et cela donne une bonne représentation des rela- tions structurelles entre les différents objets qui ne sont pas représentés par le mot ou l'expression visuelle. 3 appariement de similarité et de recherche Compte tenu de la représentation d'image proposée dans la section 2, cette section décrit comment les images sont adaptées, en estimant une valeur de similarité de la représentation 3 facettes. Le modèle espace vectoriel tradi- tionnelle (Salton et al., 1975) de la recherche d'information est adapté à notre représentation, et utilisé pour la mise en correspondance de similarité et la récupération des images. Le triplet représente chaque image dans le modèle: ddd SPW d); ,,, (,, 2,1 dnddd ppppp) ,,, (,, 2,1 dnddd wwwww ) ,,, (,, 2,1 dsddd wsssS Où dW, dP et dS sont les vecteurs pour le mot, la phrase, et facette de phrase dans les représentations d'un document respectivement. Notez que les vecteurs pour chaque niveau de mensonge de représenta- tion dans un espace séparé. dans les vecteurs ci-dessus, chaque composante représente le poids de la Nous avons utilisé le système de pondération standard tf.idf dimension correspondante., où les valeurs tf et idf sont indépendamment des estimations des mots, des phrases et des phrases. Nous avons une simple mesure de similarité qui permet d'évaluer la contribution des mots, des phrases et des phrases. la mesure de similarité entre une requête q et un document d est estimé à 1), (), () () (SSR dqdqdq SVPPRSVWWRSVdqsimilarity la récupération Status Value (RSV) de 2 vecteurs est estimé avec le cosinus. Les 3 paramètres non-négatifs, et doivent être fixés suivant les pistes d'expérimentation afin d'évaluer la contribution de chaque niveau de représentation de manière indépendante, et une combinaison de tous les niveaux de représentation. 4 expériences Dans cette section, nous décrivons un ensemble d'expériences dédiées à tester l'approche proposée. L'ensemble de données d'image utilisé pour ces expériences est un sous-ensemble d'images à partir de 1000 Caltech 101 Dataset1 (Fei-Fei et al., 2004) également répartis en 10 catégories d'objets divers. Nous avons choisi au hasard 10 images de chacune de ces catégories et les ont utilisés comme requête im- RNTI-E-19 - 160 - I. Elsayad et al. âge. Toutes les expériences ont été effectuées sur une machine Intel Xeon 3 GHz avec 3 Go de mémoire fonctionnant sous Microsoft Windows XP. Les algorithmes sont implémentés en C ++ en utilisant la bibliothèque OpenCV ver1.0. (Bradski et Kaehler, 2008). Pour mesurer l'efficacité de notre approche, on calcule le temps utilisé pour récupérer les 10 plus proches voisins de chaque image de requête. Ce temps de traitement des requêtes comprend le temps utilisé pour récupérer des images candidats de la base de données et le temps de rang eux. Le temps de traitement des requêtes moyenne varie. Elle varie de moins de 10 millisecondes à environ 160 millisecondes, en fonction du nombre de mots visuels, des phrases et des phrases dans les images de la requête. Dans l'ensemble jeu moyen d'images de la requête, il faut environ 48 millisecondes pour chaque catégorie pour obtenir les résultats de récupération. Pour évaluer l'efficacité de notre représentation à base d'expression visuelle phrase, nous essayons de récupérer des images à l'aide (i) les mots visuels seulement (sur la base de mots visuels), (ii) des expressions visuelles que (syntagmatique visuelle) et (iii ) mots visuels, phrase phrases et en même temps (visuel basé phrase-tence sen-). L'efficacité de chaque paramètre est déterminé par la précision moyenne, qui est le pourcentage des images pertinentes à partir des 10 images extraites dans chaque catégorie (voir figure 3). FIGUE. 3 - Comparaison de l'efficacité de la recherche d'images entre des algorithmes basés phrase-expression basée mot-visuel, phrase- base et visuel Si l'on compare les approches basées sur des mots-visuels et syntagmatique, l'approche de la phrase ne surclasse pas pas le mot visuel basée dans une catégorie parce que certaines images ne sont pas assez patchs locaux pour créer un ensemble de phrases qui est à lui seul une bonne repré- sentation pour l'image. D'autre part, il est évident que l'approche fondée phrase-phrase surclasse les autres parce que le mot et les approches sont intégrées dans la phrase de cette approche. 5 Conclusion Nous avons présenté une nouvelle approche pour la recherche d'images basée sur le contenu qui a proposé une chaîne de processus pour la construction de mots visuels, des phrases et des phrases. Nous avons également présenté la méthodologie trieval re qui utilise une mesure de similarité basée sur les mots visuels, des phrases et des sen- 0 50 100 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 visuelle moyenne précision mot: précision moyenne basée expression visuelle basée visuelle précision moyenne syntagmatique sentence- av er ag e pr ec est io n catégorie RNTI-e-19- 161 - visuelle phrase phrase-document basé sur la représentation pour efficace et efficace image en fonction de la tente con- récupération tences. Enfin, nos résultats expérimentaux ont montré que l'approche proposée pourrait récupérer des images de façon efficace. Dans nos travaux futurs, nous allons étudier comment mesurer la similarité d'image en appliquant différentes techniques par différents niveaux de représentation (mots, phrases et des phrases). Plus- plus, plus de travail doit être fait dans le choix et la combinaison de descripteur de fonction de bas niveau. Références Baeza-Yates, R. et B. Ribeiro-Neto (1999). Modern Information Retrieval. ACM Press. Bay, H., A. Ess, T. Tuytelaars et L. V. Gool (2008) .SURF: Accélérez fonction, vision par ordinateur et la compréhension de l'image (CVIU), Vol. 110, n ° 3, pp. 346-359. Bradski, G. et A. Kaehler (2008). Apprentissage OpenCV. Vision par ordinateur avec la bibliothèque OpenCV. Djeraba, C. (2003). Association et Content-Based récupération. IEEE Trans. Knowl. Les données Eng. 15 (1): 118-135. Fei-Fei, L., R. Fergus et P. Perona (2004). L'apprentissage des modèles visuels génératives de quelques exemples de formation: une approche bayésienne incrémentale testée sur 101 catégories d'objets. CVPR 2004, Atelier sur générative-Model Based Vision. IEEE Computer Society. Grauman, K. et T. Darrel (2005). Le noyau de match de pyramide: classement avec discriminante ensembles de caractéristiques d'image. Actes de la Conférence internationale sur l'ordinateur Vi- sion, pp.1458-1465. IEEE Computer Society, ICCV 239. Etats-Unis. Jurie, F. et B. Triggs (2005). Création d'codebooks efficaces pour la reconnaissance visuelle. Ings Proceed- de la Conférence internationale sur la vision par ordinateur. Washington, DC, États-Unis. Lowe, D.G. (2004). image distinctive comporte des points clés échelle invariant. International Journal of Computer Vision, 60 (2): 91-110. Lew, M. S., N. Sebe, C. Djeraba et R. Jain (2006). la recherche d'information multimédia basée sur le contenu: Etat de l'art et de défis. TOMCCAP 2 (1): 1-19. Burrus, C.S. R.A. Sivic, J., B. Russell, A. Efros, A. Zisserman et W. Freeman (2005). A la découverte des objets et leur emplacement dans les images. ICCV, volume 1, pages 370-377. Simovic, D., C. Djeraba (2008). outils mathématiques pour l'exploration de données. La théorie des ensembles, des ordres partiels, Combinatoire, série Advanced Information et traitement des connaissances. Springer, ISBN 978-1-84800-200-5. Salton, G., R. Wong, C. et S. Yang (1975). Un modèle vectoriel pour ING automatique Index-. Communications de l'ACM, vol. 18, n °. 11, pages 613-620. Viola, P. et M. Jones (2001). la détection d'objet rapide en utilisant une cascade boosté de fonctionnalités simples. Conférence internationale en vision par ordinateur et de reconnaissance de motif (CVPR 2001). RNTI-E-19 - 162 -"
717,Revue des Nouvelles Technologies de l'Information,EGC,2009,A Contextualization Service for a Personalized Access Model,"Personalization paradigm aims at providing users with the most rel-evant content and services according to many factors such as interest center orlocation at the querying time. All this knowledge and requirements are orga-nized into user profiles and contexts. A user profile encompasses metadata de-scribing the user whereas a context groups information about the environmentof interaction between the user and the system. An interesting problem is there-fore to identify which part of the profile is significant in a given context. Thispaper proposes a contextualization service which allows defining relationshipsbetween user preferences and contexts. Further, we propose an approach forthe automatic discovery of these mappings by analyzing user behavior extractedfrom log files.","Sofiane Abbar, Mokrane Bouzeghoub, Dimitre Kostadinov, Stéphane Lopes",http://editions-rnti.fr/render_pdf.php?p1&p=1000771,http://editions-rnti.fr/render_pdf.php?p=1000771,en,"(Actes_non_num \ 351rotes.pdf) Un service contextualisation pour un accès personnalisé Modèle ABBAR * Sofiane, Mokrane Bouzeghoub *, Dimitre KOSTADINOV **, Stéphane LOPES * * Laboratoire PRISME, Université de Versailles, 45 avenue des Etats-Unis, Versailles, 78035 prenom .lastname @ prism.uvsq.fr, ** Alcatel Lucent Bell Labs Villarceaux Route de Villejust, 91620 Nozay, France Résumé dimitre.kostadinov@alcatel-lucent.fr. Personnalisation des objectifs de paradigme à fournir aux utilisateurs le contenu de Evant plus perti- et services en fonction de nombreux facteurs tels que le centre d'intérêt ou l'emplacement au moment de l'interrogation. Toutes ces connaissances et exigences sont nized dans les profils orga- utilisateur et contextes. Un profil d'utilisateur englobe les métadonnées de- traçage à l'utilisateur que des informations de groupes de contexte de l'environnement de l'interaction entre l'utilisateur et le système. Un problème intéressant est there- avant d'identifier quelle partie du profil est significatif dans un contexte donné. Cet article propose un service de contextualisation qui permet de définir les relations entre les préférences de l'utilisateur et contextes. De plus, nous proposons une approche pour la découverte automatique de ces applications en analysant le comportement des utilisateurs extraites des fichiers journaux. 1 Présentation Personnalisation des objectifs de paradigme à l'adaptation des applications autant que possible aux préférences de l'utilisateur et au contexte de l'utilisateur. L'adaptation peut concerner plusieurs aspects, tels que la reconfiguration du système, les protocoles de communication, la sélection des sources de données, des requêtes reformulation, mise en page de données ou les utilisateurs utilisation des retours. personnalisation Les données se réfèrent à l'ensemble des techniques qui permettent offrant aux utilisateurs la contenu le plus pertinent. Il existe deux approches pour l'adaptation et la personnalisation des interactions d'application: User Centric Personnalisation et application Context-Aware. Considérant que l'un des approaches précédents peut ne pas être satisfaisante pour de nombreuses applica- tions. En effet, le même utilisateur, avec des profils différents, peut préférer écouter nouvelles pendant le petit déjeuner et d'écouter la musique Rn'B en conduisant une voiture. Sinon, le même utilisateur, à son domicile texte con-, peut avoir des domaines d'intérêt liés à ses passe-temps ou à son travail. Ainsi, ce qui permet des applications de combiner les deux approches met à profit leur capacité d'adaptation au profit des utilisateurs. Le but de cet article est de montrer, à travers la définition d'un service spécifique contex- tualization, comment un accès personnalisé modèle (PAM) peut fonctionner à la fois le profil et le contexte. Étant donné un modèle de profil et un modèle de contexte, contextualisation est définie comme un processus inter-filtrage, exécutez périodiquement sur l'interaction du fichier journal de l'utilisateur pour extraire les associations possibles Un service de contextualisation entre les instances de profil et contexte (appelé préférences contextuelles). Le service de contextualisation fait partie d'un ensemble de services composant le PAM et visant à fournir des méca- nismes de haut niveau pour créer des applications personnalisées. Ce document est organisé comme suit. La section 2 donne un aperçu de l'architecture PAM et des services. L'article 3 définit le service de contextualisation. Section 4 rapports sur les travaux connexes. La section 5 conclut le document avec les résultats actuels et d'autres recherches. 2 PAM: La définition personnalisée modèle d'accès Notre d'un des objectifs du modèle d'accès personnalisé pour fournir un ensemble générique des concepts et des techniques qui peuvent être déployées sur une architecture donnée pour faire des applications adaptables aux profils des utilisateurs et des contextes. La figure 1 donne un aperçu des principales composantes d'un PAM. FIGUE. 1 - accès personnalisé modèle d'architecture Les offres de la couche de stockage avec persistance et l'accès aux profils et aux con- textes. Il comprend le profil et les catalogues de contexte. La couche fonctionnelle se compose de services pour le profil et la gestion du contexte et des services d'accès personnalisés. Enfin, la couche de communication fournit une interface de communication entre le PAM et les utilisateurs ou ap- plications. le ro le de cette couche est, d'une part, pour donner accès aux profils et la base de données de contextes et, d'autre part, pour permettre d'appeler les services PAM. Les composants des trois couches du PAM sont construites autour des modèles de profil et les méta contexte qui sont suffisamment génériques pour être adapté à une large gamme d'applications et qui sont ouverts à intégrer des connaissances spécifiques non inclus au départ. Plus de détails sur le PAM se trouvent dans Abbar et al. (2008). 3 Le service contextualisation Les objectifs de service de contextualisation à identifier les relations entre les préférences utilisateur et contextes. Pour réaliser cette tâche, le processus que nous proposons prend en entrée le comportement de l'utilisateur capturé dans des fichiers journaux. Ce processus est divisé en trois étapes: partitionnement de résultat, l'initialisation de mise en correspondance, et vérification de la spécificité du contexte (voir figure 2). S. ABBAR et al. FIGUE. 2 - Profil global processus de contextualisation 3.1 Définition et notation problème Un profil d'utilisateur est un ensemble de préférences Pu = {p1,. . . , Pn}. Chaque pi de préférence se compose d'un pri-jacente et un poids wi, à savoir pi = (pri, WI). Le poids wi est un nombre réel exprimant l'importance du pri prédicat pour l'utilisateur. Le prédicat caractérise un concept donné. Il est un triplet <concept, opérateur, valeur>, par exemple Genre = « Drama ». A est défini cj contexte comme un ensemble de paires (attribut, valeur), par exemple location = « Paris ». Nous considérons que le contexte est représenté par son identifiant cj. L'ensemble C est l'ensemble des contextes qui sont d'un intérêt pour l'application, à savoir C = {c1,. . . , cm}. Pour un utilisateur donné dont le profil est Pu, la contextualisation de profil consiste à spécifier un SETM des correspondances qui relie les préférences de l'utilisateur avec les contextes de C: M (Pu, C) = {m (pi, cj, δ) | pi ∈ Pu, cj ∈ C, δ ∈ [-1, 1]} où δ est le score de mappage de pi par rapport au cj contextuel. Selon la valeur de δ, il y a trois interprétations de l'appariement m (pi, cj, δ): une application positive (δ> 0) indique que le prédicat pri doit être satisfaite dans le cj de contexte, une cartographie négative (δ <0) rappelle que la prédicats pri ne seraient pas satisfaits dans le cj contexte et une cartographie neutre (δ = 0) précise que le prédicat pri est hors de propos dans le contexte cj. Mappings sont découverts en analysant les historyH utilisateur capturés dans des fichiers journaux. Le processus de profil de contextualisation prend en entrée un profil d'utilisateur non spécifiques Pu, l'historique de l'utilisateur H et l'ensemble C de contextes gérés par une application donnée. Il retourne l'ensemble des applications M (Pu, C). Le reste de cette section détaille chaque étape du processus tel qu'il est présenté dans la figure 2. 3.2 Résultat Partitionnement Cette première étape consiste à identifier le contenu qui l'utilisateur a aimé et qu'il n'aimait pas. Cette phase produit deux ensembles contenant respectivement contenus pertinents et non pertinents contenus pour chaque contexte. Dans ce qui suit, POS (Pu, cj) et NEG (Pu, cj) représentent, respectivement, l'ensemble des matières pertinentes et non pertinentes dans le contexte de cj pour un profil utilisateur Pu. L'identification des contenus pertinents et non pertinents dans le journal se fait en analysant les actions effectuée par l'utilisateur sur ces contenus. En outre, la pertinence des contenus dans les ensembles POS et NEG peut différer en ce qui concerne le type de l'action qui leur est appliquée. Dans POS ensemble, par exemple, un service de contextualisation un produit acheté est plus important pour un utilisateur donné que visualisé un même si l'utilisateur a passé beaucoup de temps là-dessus. Nous proposons donc d'organiser toutes les actions possibles dans une table relationnelle qui décrit les actions avec leurs poids éventuels. Les actions avec des poids positifs sont dites actions positives et le contenu sur lequel ils sont exécutés sont considérés comme pertinents pour l'utilisateur, alors que les actions avec des poids négatifs sont dites actions négatives, et le contenu sur lequel ils sont appliqués sont considérés comme étant hors de propos. 3.3 mappages initialisation La deuxième étape consiste à initialiser les correspondances possibles entre le profil d'utilisateur prédicats et des contextes. th on se fait en calculant les fréquences occurrence δ + δ ij et - ij de chaque prédicat de profil dans POS (Pu, cj) et NEG (Pu, cj), respectivement. La valeur de δij dépend du fait que la fréquence δ + ij (respectivement δij) satisfait à une condition donnée γ + (δ + ij) (respectivement γ- (δij) comme par exemple un seuil. Si seulement γ + (δ + ij) occupe, à savoir une partie suffisante de la satisfait des résultats pertinents du prédicat, un mappage positif m (pi, cj, δ + ij) est créé même, si seulement γ -. (δ-ij) occupe, une cartographie négative m (pi, cj, δ - ij) est créé Si les deux γ + (δ + ij) et γ -. (δ-ij) ne sont pas satisfaits, une cartographie neutre m (pi, cj, 0) est créé Enfin, si. à la fois γ + (δ + ij) et γ -. (δij) en attente, les correspond sous-jacentes à une préférence d'utilisateur qui est toujours valide Ainsi, est créée avec δij égale à une cartographie positif m (pi, cj, de δij) à la moyenne des fréquences de pi dans l'union ensemble des résultats pertinents et non pertinents (δij = Freq (pi, POS (Pu, ci) ∪NEG (Pu, ci))). 3.4 Contexte spécificité vérifier Cette pruneaux étape mappings qui se rapportent le même prédicat profil à tous les contextes. Ces applications corr espond au dernier cas discuté dans la section 3.3 et profil de préoccupation prédicats qui ne sont pas à contextualisées car ils doivent être pris en compte dans tous les contextes. Par exemple, si l'utilisateur regarde des films uniquement en anglais, tous les résultats pertinents satisferont le prédicat « language = « anglais » », mais celle-ci sera vrai pour tous les résultats non pertinents aussi. Ce genre de icates pred- doit être pris en compte dans tous les contextes où elle correspond à une préférences utilisateur générales. Le résultat de cette étape est un ensemble de correspondances valides entre le profil de l'utilisateur et contextes. 3.5 algorithme de construction Cartographie Découvrir automatiquement les préférences contextuelles est une tâche complexe. Par conséquent, nous avons fait une hypothèse simplificatrice pour la conception de l'algorithme: Toutes les actions de l'utilisateur expriment des préférences positives et sont de la même importance. Comme expliqué précédemment, l'algorithme prend en entrée un profil d'utilisateur Pu, les billes de userH, un ensemble de contextes C et un seuil γ pour la validation de la cartographie. La principale étape de l'algorithme consiste à calculer la fréquence dans le journal de chaque prédicat de l'utilisateur pour chaque contexte. Ensuite, suivant la valeur de ces fréquences, les correspondances seront créés. Si le prédicat a une fréquence d'occurence qui satisfait à y dans tous les contextes, puis un mappage positif m (pi, C, AV G (F)) est créé, attestant que prédicat pri cales dans tous les contextes de la série C, AV G ( F) la moyenne de toutes les fréquences d'apparition. Ce prédicat est toujours satisfaite indépendamment du contexte, il peut donc être considérée comme une caractéristique de vue de l'utilisateur. S. ABBAR et al. Algorithme 1 découverte de la cartographie automatique Exiger: le profil de l'utilisateur Pu = {p1,. . . , Pn}, l'utilisateur behaviorH, l'ensemble des contextes possibles C = {c1,. . . , Cm}, le γ de seuil. Assurer: l'ensemble de contextualisation mappingsM 1: M ← ∅ 2: pour tous les pi ∈ Pu do 3: Calculer F = f1,. . . , FM telle que fk = Freq (pri, cj, H) {où Freq (pricj, H) est la fréquence du prédicat pri inH lorsque le contexte est cj.} 4: si ∃i, j | fi ≥ γ ∧ fj <γ puis 5: pour tous fk ∈ F 6 ne: si fk ≥ γ puis 7: M ← M∪m (pi, ck, fk) {le prédicat tient dans le contexte} 8: else 9: M ← M∪m (pi, ck, 0) {le prédicat ne tient pas dans le contexte} 10: else 11: si ∀fk ∈ F, fk <γ puis 12: M ← M∪m (pi, C, 0 ) {prédicat qui ne tient pas dans un contexte} 13: else 14: M ← M∪m (pi, C, AV G (F)) {prédicat qui tient dans tous les contextes (vue utilisateur)} 15: retour M Si fréquence d'apparition d'un prédicat donné satisfait à y, puis un mappage neutre m (pi, C, 0) est créé attestant que le prédicat pri est non contextuelle. En d'autres termes, le prédicat ne concerne pas les contextes de l'ensemble C. Tel est le cas de prédicats utilisés rarement (de temps en temps). Si le prédicat est fréquent dans certains contextes seulement, le prédicat est contextuel. En effet, il convient de tenir compte dans certains contextes, alors les correspondances m (pi, ck, fk) sont créés pour chacun de ces contexte avec le correspondant la fréquence et de l'ignorer dans les autres, où m de la cartographie neutre (pi, ck, 0) sont créés pour chacun des contextes de reste. 4 Travaux connexes Récemment, contextualisation du profil de l'utilisateur et les préférences a attiré l'attention. Dans auteurs (terres Hol- et Kiessling, 2004) propose un cadre de préférences contextuelles, appelées préférences situées. Dans cette approche, les profils utilisateur et les situations sont modélisés dans un modèle Entité-relation, alors, contextualisation des préférences est modélisé de manière unique comme M: relation N (pid, sid), exprimant que le pid de préférence tient dans la situation sid. Aucun détail sont données sur la construction de la relation. Dans leurs recherches, (Stefanidis et Pitoura, 2008) in- contexte troduced dans le champ de base de données. Le contexte qui est un ensemble d'attributs contextuels (par exemple l'âge, le temps) est utilisé pour la base de données de rang tuples w.r.t une requête donnée. préférences contextuelles sont explicitement spécifiés par les utilisateurs et sont de la forme <contextState, preferencePredicate, Score>. La différence avec notre proposition, est que les auteurs considèrent un profil d'utilisateur comme faisant partie du contexte alors que nous nous séparions clairement ces deux concepts, cela peut être dû à différents points de vue, nous considérons. Une découverte automatique des préférences contextuelles a été initiée par (Agrawal et al., 2006). In (Bunningen et al., 2007) les préférences contextuelles sont également automa- Un service de contextualisation quement généré en observant et en analysant l'historique de l'utilisateur. Préférence sont modélisés throught trame de description-logique, et la découverte de préférences contextuelles se fait automatiquement par le calcul de la probabilité qu'un élément soit le meilleur dans un contexte particulier. 5 Conclusion Dans cet article, nous avons présenté un service de personnalisation, appelé contextualisation, qui permet de trouver les dépendances entre les profils utilisateur et contextes en exploitant les journaux d'interaction à l'application de l'utilisateur. Le service peut être utilisé au moment de la conception ou de façon périodique en tant que service d'entretien qui maintient à jour les liens sémantiques entre les profils et contextes pour une application donnée. Le service de contextualisation est un service parmi plusieurs autres qui constituent un modèle d'accès personnalisé cohérent (PAM). D'autres recherches se concentrera sur le service de liaison qui exploitent, au moment de l'exécution, les applications générées par le service de contextualisation pour effectuer des requêtes des utilisateurs sensibles au profil de l'utilisateur, au contexte d'interaction ou les deux. Références Abbar, S., M. Bouzeghoub, D. Kostadinov, S. Lopes, A. Aghazaryan et S. Betge-Brezetz (2008). Un modèle d'accès personnalisé: Concepts et services pour les plates-formes de distribution de contenu. Dans Actes des 24emes Journées Bases de Données Avancées, Guilherand-Granges, France. Agrawal, R., R. Rantzau, et E. Terzi (2006). classement contextuelle. Dans SIGMOD conférence internationale sur la gestion des données, Chicago, IL, USA, pp. 383-394. Bunningen, A. H. V., M. M. Fokkinga, P. M. G. Apers et L. Feng (2007). Classement des résultats de requête à l'aide des préférences sensibles au contexte. En ICDEW'07, Istanbul, Turquie, p. 269-276. Holland, S. et W. Kiessling (2004). préférences contextualisées et dépôts de préférence pour les applications de base de données personnalisées. Dans la modélisation conceptuelle - ER 2004, pp 511-523.. Stefanidis, K. et E. Pitoura (2008). Rapide notation de préférence contextuelle de tuples base de données. 11e conférence internationale sur la technologie de base de données Extension, Nantes, France, pp. 344-355. La CV à provide étau personnalisation à l'Utilisateur Les plus les Contenus par le port Rap- pertinents à divers critères. Informations CÉS en Sont organisées et en profils Contextes Utilisateur d'interaction. Un Problème de Intéressant is DETERMINER Alors une partie du profil Quelle signifi- cative is Dans un Contexte Donné. Cet article propose un service de contextualisation un de Qui Përmet les relations Entre définir les Preferences et les Contextes Utilisateurs. Nous proposons EGA- Une approach lement verser des relations la automatique de bureaux en découverte une nalysant les traces du de l'Utilisateur comportement."
721,Revue des Nouvelles Technologies de l'Information,EGC,2009,Aggregative and Neighboring Approximations to Query Semi-Structured Documents,Structures heterogeneity in Web resources is a constant concern inelement retrieval (i.e. tag retrieval in semi-structured documents). In this paperwe present the SHIRI 1 querying approach which allows to reach more or lessstructured document parts without an a priori knowledge on their structuring.,"Yassine Mrabet, Nathalie Pernelle, Nacéra Bennacer, Mouhamadou Thiam",http://editions-rnti.fr/render_pdf.php?p1&p=1000808,http://editions-rnti.fr/render_pdf.php?p=1000808,en,"(Actes_non_num \ 351rotes.pdf) agrégative VOISINS Approximations à la requête de documents semi-structurés Y. Mrabet * N. Pernelle * N. BENNACER ** M. Thiam * * 4, Rue J. Monod, Parc du Club Orsay Université, 91483 Orsay Cedex first.last@lri.fr, ** Supelec, F-91192 Gif-sur-Yvette Cedex nacera.bennacer@supelec.fr Résumé. hétérogénéité des structures dans les ressources Web est une préoccupation constante dans la recherche d'éléments (à savoir la récupération des marques dans les documents semi-structurés). Dans cet article, nous présentons l'approche d'interrogation SHIRI 1 qui permet d'atteindre plus ou moins des parties de documents structurés sans connaissance a priori sur leur structuration. 1 Requêtes approximatives selon le document Structuration Pour récupérer l'élément marqué le plus adapté selon une requête de l'utilisateur, les approches classiques ont tendance à utiliser une indexation statistique des zones marquées. Mais, alors que cette indexation a montré très efficace pour la récupération de documents, il reste peu satisfaisant pour la récupération des éléments. Les cas où la requête est composée de beaucoup de termes qui ne sont pas nécessairement localisés dans les mêmes parties des documents, ne sont pas bien couverts. En outre, même si les balises voisines sont pris en compte par un dans le document la distance, le classement des pièces récupérées ne pas intégrer toute notion de structuration (par exemple, un nœud de document ne parle que d'une conférence A, peut avoir le même rang en parler noeud à environ trois conférences différentes). Nous vous proposons une solution sémantique pour faire face à l'hétérogénéité des structures en explicitant les niveaux de structuration [Thiam et al. (2008)]. Un nœud de document est donc dit être une partie de la parole (i.e. annoté par les métadonnées PartOfSpeech) si elle contient de nombreux cas de différents concepts. Un autre nœud ne contenant qu'un seul exemple d'un concept donné est annoté comme étant un exemple de ce concept et, respectivement, pour le cas de SETOF, où un noeud contient un ensemble d'instances du même type. En outre, l'imbrication structurelle entre le document nœuds est utilisé pour déduire les relations sémantiques entre les instances annotés. Par exemple. si le nœud « <ul> » est annotés comme une instance du concept de « article » et le prochain « <li> » nœud est annotées comme une instance de la notion personne, la relation <ul, authored_by, li> est créé. Se référant au modèle d'annotation ci-dessus, nous vous proposons deux types d'approximation. La première, appelée approximation agrégative, utilise pour rechercher des parties de documents moins structurés si aucune meilleure structuration se trouve les métadonnées d'agrégation définie dans l'extension de l'ontologie (PartOfSpeech et SetOfConcepti). La seconde approximation, appelée approximation voisine, est utilisée pour couvrir les cas où nous cherchons des relations sémantiques qui ne sont pas récupérés dans la base de mise en anno- (il n'y a pas d'intrication entre deux noeuds de documents qui sont 1SHIRI annotés: Digiteo Labs projet (LRI, SUPELEC) agrégative et voisins Approximation pour interroger documents semi-structurés respectivement par le domaine et de la relation). Les requêtes que nous considérons comme sont en triple modèle RDF (par exemple les requêtes SPARQL). Par exemple, la requête (<? X type Article> et <? X authoredby? Y> et <? Y hasName « V ictor V ianu »>) doit renvoyer les parties de documents sont imbriquent de manière appropriée et annotés par le approprié types de concepts en fonction de la requête. Cependant, cette structuration est très contraignante. Notre premier type d'approximation permet de rechercher des parties de documents annotés par des agrégats au lieu des concepts initiaux spécifiés par la requête utilisateur. Une première approximation de la requête ci-dessus peut donc être (<? X type Article> et <? X? Authored_by_Set y> et <? Y SetOfAuthors type>). Une seconde peut être à la recherche d'un méta-données de PartOfSpeech indexées par les concepts de l'article et l'auteur. D'un autre côté, l'approximation voisine permet de remplacer l'imbrication souhaitée par un voisin de structure plus générale. Dans l'exemple, la requête ci-dessus pourrait être approchée en remplaçant le triple <? x authored_by? y> par <? x ParentOf [N]? y>. Le ParentOf [N] par rapport représente le fait que l'unité structurelle? X et? Y sont à une distance N de l'autre (par exemple <table / tr [1] / td [3]> est à une distance de 3 < Table / tr [2] / td [1]>). Dans nos premières expérimentations que nous avons proposé un algorithme dynamique qui combine les deux types approximations et renvoie les résultats de la requête initiale et ses approximations classées. Nous avons testé cette approche sur trois sources de données distinctes annotés (références bibliographiques de DBLP, HAL et serveur INRIA)! . Les résultats obtenus après l'application d'un ensemble de requêtes plus ou moins complexes ont montré que notre approche permet d'atteindre des structures de données hétérogènes sans connaissance a priori. Nous avons eu un rappel global de 28% avec la requête initiale, de 19,9% avec l'approximation agrégative, 52,3% avec les approximations voisines et 72% avec les approximations combinées. Chevauchement permis de répudier, le rappel total de trois approximations est de 100%. La précision a également été de 100% en raison de la régularité des structures du corpus que nous avons utilisé. Contextuellement, notre approche ne approximations basés sur l'ontologie remplace pas proposée dans [Hurtado et al. (2006); Corby et al. (2006)]). Elle applique comme couche indépendante pour faire face à l'hétérogénéité des structures. travaux futurs comprendront l'amélioration de nos critères de classement, l'expérimentation avec des structures de documents irréguliers et un processus d'annotation sans supervision. Références Corby, O., R. Dieng, F. Gandon et C. Faron-Zucker (2006). Recherche sur le web sémantique: le traitement des requêtes approximative basée sur des ontologies. IEEE systèmes intelligents Journal. Hurtado, C.-A., A. Poulovassilis et P.-T. Wood (2006). Une approche détendue à l'interrogation rdf. Conférence internationale Web sémantique, ISWC. Thiam, M., N. Pernelle et N. Bennacer (2008). Contextuelle et l'approche basée sur les métadonnées pour l'annotation sémantique des documents hétérogènes. ESWC-Semma. L'résumé des structures Dans hétérogénéité les documents Web is un souci constant en recherche d'éléments. Dans papier CE, NOUS Présentons l'Approche d'interrogation Sémantique Shiri Sie PERMET D'des partis de Atteindre documents Plus ou moins sans connaissances heuristiques a structurées a priori sur their Structuration."
722,Revue des Nouvelles Technologies de l'Information,EGC,2009,An approach for handling risk and uncertainty in multiarmed bandit problems,"An approach is presented to deal with risk in multiarmed bandit prob-lems. Specifically, the well known exploration-exploitation dilemma is solvedfrom the point of view of maximizing an utility function which measures thedecision maker's attitude towards risk and uncertain outcomes. A link withthe preference theory is thus established. Simulations results are provided forin order to support the main ideas and to compare the approach with existingmethods, with emphasis on the short term (small sample size) behavior of theproposed method.","Fabrice Clérot, Stefano Perabò",http://editions-rnti.fr/render_pdf.php?p1&p=1000744,http://editions-rnti.fr/render_pdf.php?p=1000744,en,"(Actes_non_num \ 351rotes.pdf) Une approche pour la gestion des risques et de l'incertitude dans les problèmes de bandit manchot Stefano Perabo *, Fabrice Clérot * * France Télécom Division Recherche & Développement 2, avenue Pierre Marzin, 22307 Lannion Cedex stefano.perabo@orange.fr, fabrice .clerot @ orange-ftgroup.com Résumé. Une approche est présentée pour faire face aux risques multiarmed bandit problè- mes. Plus précisément, le dilemme exploration-exploitation bien connu est résolu à partir du point de vue de maximiser une fonction d'utilité qui mesure l'attitude du décideur envers le risque et les résultats incertains. Un lien avec la théorie de la préférence est ainsi établie. les résultats des simulations sont prévues afin de soutenir les principales idées et de comparer l'approche des méthodes existantes, en mettant l'accent sur le comportement à court terme (petite taille de l'échantillon) de la méthode proposée. 1 Introduction Un « problème de bandit manchot » peut être formulé comme suit: donnée pour t = 1, 2. . . T une séquence de vecteurs aléatoires K dimensions R (t) = [r1 (t). . . rk (t)], appelés récompenses et dont la distribution probabilité est inconnue a priori, l'objectif est de déterminer en ligne une sé- quence d'actions a (t) (également appelée stratégie ou politique) où chacun un (t) est un mesure discrète Vari-aléatoire définie sur l'ensemble {1, 2,. . . , K}, qui maximise l'espérance du gain cumulatif, G (T) = E [ΣT t = 1 ra (t) (t)], en observant, pour chaque t un (et seulement un) réalisation ra (t) (t) 1. la principale difficulté du problème réside dans le fait que la fonction objective ne sait pas à l'avance. En fait, si les moyens pA (t) = E [ra (t)] étaient disponibles, la meilleure stratégie serait évidemment de jouer l'action a * (t) = arg Maxa pA (t). Par conséquent, à chaque fois instant t, le choix d'une action est le résultat d'un compromis pour tenter d'estimer (apprendre) la fonction objective (en explorant les actions dont les récompenses moyen n'a pas encore été déterminée avec confiance fiance assez) et, en même temps, à la maximiser (en exploitant ceux qui, sur la base des observations précédentes, sont estimés à fournir les meilleures récompenses). Cela représente un problème de décision prototype où le décideur doit faire face à ce qu'on appelle dilemme exploration / exploitation: tout en poursuivant le deuxième objectif (exploitation) en utilisant, inéluctablement, une stratégie de suboptimale, il pourrait subir des pertes qui pourraient être évités si une meilleure esti- accouple des moyens de récompenses étaient disponibles; au contraire, tout en poursuivant le premier objectif (exploration) en utilisant une autre stratégie de suboptimale, il pourrait renoncer à jouer les supposés 1. Les caractères italiques comme r et représentent des réalisations des variables aléatoires correspondantes qui sont désignés en utilisant des caractères romains comme r et a. risque de manipulation meilleure action dans les problèmes de bandit trouvé jusqu'à présent. Ceci est un problème qui, avec de nombreuses variantes, peut modéliser des tâches de décision de base communément rencontrées dans l'allocation des ressources et à la commercialisation. Un exemple très populaire est emprunté à la communauté de la publicité sur Internet: le décideur doit afficher périodiquement une annonce sur une page web (l'action) en le choisissant à partir d'un ensemble connu d'annonces, l'objectif étant la tion maximisation du nombre de visiteurs clique sur l'annonce affichée (les récompenses), ce qui se traduit par la maximisation des revenus du décideur. Le dilemme exploration-exploitation provient du fait que les utilisateurs les intérêts ne sont pas connus à l'avance et donc, afin de juger quelle annonce attire le plus grand nombre de clics, chaque annonce doit être affichée (testée) un certain nombre de fois. Dans les scénarios plus réalistes, avant de prendre une décision, le décideur pourrait avoir accès à une sorte d'information latérale (contexte souvent appelé), comme dans l'exemple ci-dessus, un profil de l'utilisateur actuellement en visite la page Web ou une liste de mots-clés extrait de la page Web. Comment ces informations pourraient être utilisées afin de mieux choisir l'annonce à l'affichage représente une extension de le problème de bandit qui est généralement appelé un « problème contextuel de bandit ». Il peut être formalisé par l'introduction d'une séquence supplémentaire de quantités aléatoires, les contextes x (t), et en supposant qu'il existe une corrélation entre les contextes et les récompenses r (t). La question de ploration ex consiste donc à estimer cette corrélation afin de mieux prédire quelle est la meilleure action à effectuer conditionnellement à un contexte donné. Dans cet article, une approche est proposée pour faire face au dilemme exploration-exploitation des problèmes de bandit manchot. Son extension aux problèmes de bandit contextuel plus intéressant n'est pas poursuivi. Cependant, il devrait être clair pour les lecteurs intéressés par une telle extension que le même raisonnement pourrait être appliqué sans changements aussi en présence d'un contexte, les principales difficultés étant plus technique plutôt que la nature conceptuelle (ces difficultés, en notamment pour le traitement d'énormes ensembles de données nécessitant simultanément l'application des techniques de classification). De plus, l'accent est mis ici sur le cadre classique, où les récompenses r (t) sont modélisés comme indépendants et identiquement distribuées (IID) variables aléatoires, indépendamment de la stratégie de décideur a (t). En dehors de ces hypothèses, sont considérées comme des récompenses que bornées, qui est le cas ra (t) ∈ [0, 1] sans perte de généralité (cette hypothèse est pas vraiment nécessaire pour la méthode de travail, mais il est dit dans l'intérêt de comparer avec deux autres existants). 2 rapport à l'état de l'art La littérature sur les problèmes de banditisme depuis les travaux fondateurs de Robbins (1952) est abondante. Les principales contributions récentes à la solution du problème de bandit manchot avec IID et récompenses bornées, notamment les stratégies UCB1 (Auer et al., 2002) et UCB-V (Audibert et al., 2007), la garantie que le gain cumulé attendu G ( T) est délimitée inférieure par une fonction de la forme G * (T) - c log T, où c est une constante et G * (T) est la valeur maximale de la fonction objective. Comme le montre la Lai et Robbins (1985), l'expression logarithmique log T c (appelé regret) ne peut pas être améliorée plus loin dans le sens suivant: toute stratégie possible subit un regret qui est délimitée inférieure par une fonction O (log T). D'un autre point de vue, la stratégie SUCCESSIVEELIMINATION (Even-Dar et al., 2006) a été prouvé pour trouver l'action optimale S. Perabo et F. Clérot 10 1 10 2 10 3 10 4 10 5 0 0,2 0,4 0,6 T av . e x p é c é e c u m. g a la Fig. 1 - Un exemple de limites inférieures sur le gain cumulatif moyen attendu pour l'algorithmes de UCB1 (trait plein) et UCB-V (ligne en pointillés). Ces limites sont applicables au cas où des points sont distribués comme décrit dans la section §4 (le deuxième exemple avec K = 5, les récompenses ont support [0, 1] et G * (T) / T = 0,6). a * avec une probabilité prédéfinie arbitraire en un nombre fini d'étapes. Toutes ces stratégies ont été obtenues par le traitement (avec un aperçu mathématique considérable) les inégalités de concentration des sommes de variables aléatoires. Plus précisément, ces inégalités indiquent ce qui suit: si, à l'instant t l'action un a été joué na (t) fois, sa moyenne empirique, définie par pA (t) = Σt τ = 1 1 (un (τ) = a ) ra (τ) / na (t), est à une distance ε à partir de la moyenne vraie uA avec une plus grande probabilité de ou égale à une certaine valeur de seuil α-1. La forme fonctionnelle de ε et α, dépend de la nature de l'inégalité utilisée; cependant, il est généralement vrai que si α est maintenue fixe, alors ε diminue pour augmenter na, alors que si ε est maintenu fixe, puis augmente pour augmenter les a na. Par conséquent, par l'action de jeu une fois de plus, on est sûr d'augmenter la confiance avec laquelle est connu le vrai pA moyenne. L'art de la conception de la stratégie consiste donc à trouver le meilleur compromis entre deux objectifs contradictoires: l'échantillonnage afin de maximiser le gain et l'échantillonnage afin d'augmenter la confiance sur les moyens de récompenses. Le regret des stratégies mentionnées ci-dessus a été prouvé être très proche du mal pour opti- ar distributions de récompenses bitrary, parce qu'ils comptent sur les inégalités de concentration qui sont valables pour les distributions de probabilité arbitraires. Ceci est cependant aussi l'inconvénient principal de ces approches, parce que les régions de confiance provenant de ces inégalités peuvent être, dans certains cas, pratique, très conservatrice, ce qui dans les grandes constantes devant le tor T fac- journal du terme de regret. En conséquence, les bornes inférieures sur le gain deviennent utiles seulement à long terme, qui est la quantité G * (T) - c log T est seulement positif pour un horizon de temps suffisamment grand T. A titre d'exemple, la figure 1 représente graphiquement les moyennes des bornes inférieures (soit la quantité (G * (T) -c log T) / T) pour les algorithmes UCB1 et UCB-V dans le cas particulier où K = 5, et les récompenses sont répartis comme décrit dans la section §4, où seront présentés les résultats de certaines simulations numériques. Il est clair que, pour T <200 ne peut rien dire. La principale critique qui peut être adressée aux approches actuelles pour résoudre les problèmes de bandit est de se concentrer tous les efforts dans la limite supérieure du regret d'un algorithme aussi serré que possi- ble, qui dans les feuilles de fait pas de place pour prendre en compte les préférences de l'utilisateur et les contraintes. Tenez compte, risque de manipulation des problèmes de bandit en particulier, ce qui se passe au cours de la première période suivant le début d'un algorithme de bandit, où l'exploration est l'activité principale. La question est de savoir quand et comment passer de l'exploration à l'exploitation compte tenu des informations fournies par un nombre fini d'échantillons. La réponse dépend clairement de nombreux facteurs, les plus influents, peut-être, être: l'horizon de temps disponible (qui est combien d'autres échantillons peuvent tirer) et l'attitude du décideur à l'incertitude et des choix risqués. L'objectif de cet article est donc de proposer une procédure de conception de stratégie qui permet à ces Ørences et contraintes de préférences avoir un impact sur le comportement de l'algorithme et à prendre en compte en quelque sorte automatiquement. En particulier, le dilemme exploration-exploitation sera lated refor- que le problème de maximiser une fonction d'utilité qui quantifie les préférences du décideur sur un ensemble d'intervalles de confiance appropriés. Plus précisément, chaque intervalle de confiance de remplacement est associée à une estimation du gain futur qui pourrait être obtenu en jouant une certaine stratégie. Le rôle de la fonction d'utilité est donc de mesurer l'attitude face au risque et à l'incertitude, l'attitude qui entraîne le choix d'un intervalle de confiance. Une fonction d'utilité spécifique qui est conçu pour exprimer l'aversion pour les stratégies à risque est prévue. L'algorithme connexe sera appelé LCB1. Les résultats de certaines simulations numériques sont présentées afin de comparer le court terme (petit horizon T) performance de ce nouvel algorithme avec celui des algorithmes UCB1 et UCB-V précité. 3 Description de l'approche Pretend pendant un certain temps que les récompenses des distributions de probabilités sont connues. Il est possible de voir le problème de maximiser le gain G (T) comme un problème de planification d'horizon fini dans un processus de décision dégénéré MARKOV (MDP), qui est le problème de la détermination de la stratégie optimale (souvent appelée politique) dans un PEAD consistant un seul état déterministe 2. Il est connu (Puterman, 1987) que la stratégie optimale peut être trouvée par un algorithme d'induction en arrière. Si elle est appliquée ici, sous les hypothèses d'indépendance énoncés dans l'introduction, il revient simplement de trouver pour chaque instant t les fonctions Q * (a, t) (appelées fonctions d'action valeur optimale dans le jargon MDP) qui permettent de résoudre Q * (a, t) = E [ra (t) (t) + max b Q * (b, t + 1) ‖ a (t) = a] Q * (a, t + 1) = 0 (1) Le un fonctionnement optimal de la valeur d'action Q * (a, t) est égal au gain maximal attendu qui peut être ob- contenue dans l'intervalle [t, T], conditionnée par le fait que, à l'action de temps t a est joué. La formule ci-dessus peut donc se lire comme suit: afin de maximiser le gain futur sur l'intervalle [t, T], il suffit de jouer à l'instant t l'acte ion maximiser la récompense attendue E [ra (t)], puis de suivre la meilleure stratégie dans l'intervalle [t + 1, T]. En d'autres termes, la meilleure stratégie consiste à jouer avec la probabilité d'une action a * (t) = arg maxa Q * (a, t), pour tout t ∈ [1, T]. Dans ce cas, la solution de l'induction arrière donne évidemment Q * (a, t) = pA + (T - t) μ *, où μ * = Maxa pA, et confirme l'interprétation ci-dessus. 2. Dans un MDP il existe un processus stochastique x (t) défini par la probabilité de transition P [x (t + 1) = w‖x (t) = z, a (t) = a] (ce qui donne la probabilité d'atteindre un état suivant possible dans l'état actuel et de l'action) et le vecteur de récompenses dépendent de l'état actuel par la probabilité conditionnelle P [r (t) = r‖x (t) = z]. Le cadre de IID considéré ici est donc obtenue lorsque la transition est autorisée seulement une forme déterministe état x (t) ≡ z à lui-même, quelle que soit la mesure est prise. S. Perabo et F. Clérot Maintenant, supposons que le décideur est autorisé à jouer une stratégie aléatoire, qui est l'action a (t) est tiré d'une variable aléatoire prenant des valeurs dans l'ensemble {1, 2,. . . , K}. Définir pa (t) = P [a (t) = a] pour la probabilité de prendre des mesures une à l'instant t, ainsi que la quantité V * (t, p (t)) = Σ a Q * (a, t) P [a (t) = a] = Σ un μapa (t) + (T - t) * μ (2), où p (t) = [p1 (t). . . Pk (t)]. Il est clair que V * (t, p (t)) représente le maximum de gain attendu que l'on peut obtenir dans l'intervalle [t, T] à chaque fois qu'une stratégie randomisée est lue à l'instant t. En outre, V * (t, p (t)) ≤ maxa Q * (a, t) pour le choix de p (t), le maintien de l'égalité lorsque pa * (t) (t) = 1 (cas où V * (t, p (t)) = Q * (a * (t), t), quantité que l'on appelle souvent fonction de valeur optimale). Par conséquent, dans un problème de planification (qui est lorsque la distribution de récompenses sont connus) il n'y a aucun avantage à adopter une stratégie aléatoire par rapport à la valeur optimale obtenue istic un détermi- par l'induction en arrière décrit ci-dessus. Considérons alors le cas d'aucune connaissance préalable sur la distribution des récompenses. Ni les fonctions Q * (a, t) ni V * (t, p (t)) peuvent être évalués parce que les moyens pA ne sont pas connus. Cependant, on suppose que, pour une donnée α> 0 et pour chaque t, 100 (1 - α)% intervalle de confiance pour V * (t, p (t)), par exemple C (t, p (t)) = [ V * lo (t, p (t)), V * jusqu'à (t, p (t))] (3) peut être calculée à partir de l'ensemble d'observation {ra (s) (s)} sur l'intervalle [1 , t - 1]. Re- appeler le sens d'un intervalle de confiance: il existe un intervalle dont les extrémités sont des fonctions des données empiriques et qui contient avec une probabilité (1 - α) de la valeur réelle. En faisant varier p (t), différents intervalles de confiance sont obtenus. Il est donc raisonnable de définir la stratégie optimale p * (t) que la stratégie telle que la intervalle de confiance C (t, p * (t)) est préférable à tous les intervalles C (t, p (t)) obtenu pour p (t) = 6 * p (t). Par exemple, un intervalle [a + Aa, b + Ab] doit être préféré [a, b] chaque fois que Aa, Ab> 0 parce que le gain maximal prévu de la stratégie correspondante est susceptible d'être plus grande. Il est donc nécessaire de montrer comment ces intervalles peuvent être construits dans la pratique et de définir une relation de préférence sur des intervalles de confiance. En ce qui concerne la première question, ici une approche heuristique est adoptée. On sait que, comme na (t) → ∞, les moyens empiriques pA (t) = Σt τ = 1 1 (un (τ) = a) ra (τ) / na (t) ont tendance à être dis- tribué en tant que N (uA, σ 2 a / na (t)) variable aléatoire, qui est une gaussienne dont la moyenne et la variance sont respectivement le vrai uA moyenne et la variance σ 2 vraie divisée par le nombre d'échan- ples. La variance réelle peut aussi être remplacé par le σ2a empirique de la variance non biaisée (t) = Σt τ = 1 1 (un (τ) = a) (ra (τ) - pA (t)) 2 / (na (t) - 1). Même si ceux-ci sont ré- sultats asymptotiques, prétendre qu'ils représentent des approximations acceptables aussi pour un nombre fini d'échantillons na (t). Par conséquent, V * (t, p (t)) en (2) est sensiblement distribué comme N (μ (t), σ2 (t)) avec μ (t) = Σ un pA (t) pa (t) + ( T - t) μ * (t) σ 2 (t) = σ un σ2a (t) na (t) pa (t) 2 + (T - t) σ2 * (t) n * (t) (4) et où μ * (t) est le plus grand mea empirique n et σ 2 * (t) et n * (t) sont la variance empirique et le nombre d'échantillons de l'action correspondante. Pour un exemple numérique voir la figure 2. Les extrémités d'un 100 (1 - α)% intervalle de confiance sont donc V * lo (t, p (t)) = μ (t) - fσ (t) V * jusqu'à (t, p (t)) = μ (t) + fσ (t) (5) des risques de manipulation dans des problèmes de bandits 50,3 50,4 50,5 20,1 20,15 20,2 td s moyenne. d e v. FIGUE. 2 - Dans le graphique, les croix indiquent les couples (μ (t)), σ (t)) obtenus en faisant varier chaque probabilité de sélection d'action pa (t) dans l'intervalle [0, 1] en étapes de 0,1 sous la con- straint Σ une pa (t) = 1 et en utilisant les données suivantes: K = 3, [pA (t)] = [0,5, 0,4, 0,3], [oa (t) / √ na (t)] = [0,2 , 0,15, 0,1] et (T - t) = 100. les cercles représentent les valeurs ob- CONTENUES lorsque pa (t) = 1 pour un certain a ∈ {1, 2, 3}. où σ (t) = √ σ2 (t) et f est la 100 (1 - α)% quantile d'une gaussienne standard. Ensuite, une relation de préférence est nécessaire qui est en mesure de représenter l'attitude du décideur à l'incertitude représentée par le fait que, pour une stratégie donnée, le gain correspondant ne peut pas être évalué avec précision. Une relation de préférence est une relation binaire qui sera notée, de sorte que si l'intervalle D est préféré à intervalle C, on écrit D C. Le résultat classique est que, sous des hypothèses appropriées, il existe une représentation quantitative d'une relation de préférence en termes d'une fonction d'utilité réelle u définie sur l'ensemble des intervalles, par exemple I, tel que D C ⇐⇒ u (D)> u (C) (6) Outre les hypothèses techniques qui sont nécessaires pour tenir compte du fait que I est dénombrable ONU- (voir par exemple Fishburn (1999) pour plus de détails), il peut être prouvé que ce qui précède repré- sentation si et seulement si la relation sur I est un ordre faible. Cela signifie que est transitif (E D et D C ⇒ E C), irréflexive (C C) et ne détient que la relation ~ (définie par C ~ D si ni C, ni D D C) est aussi transitive. Dans la soi-disant approche normative de la modélisation des préférences (Tsoukias, 2008), qui sera suivie ici, l'existence d'une relation de préférence avec les propriétés ci-dessus est postulée (un décideur adhérant à une telle relation est appelée rationnelle). Ce montant, dans la pratique, pour définir la fonction d'utilité à une forme appropriée, en fonction de l'application à portée de main, en choisissant peut-être dans un ensemble de plusieurs bons choix possibles. En d'autres termes, l'existence d'une fonction d'utilité optimale n'est pas indiqué ici: différentes formes pourraient être raisonnable et se comporter aussi bien dans la pratique. Dans cet article, la suite fonction d'utilité très simple est proposé et testé par simulation numérique: u (t, p (t)) = exp (μ (t) - σ (t) T - t + 1) (7) S. perabo et F. CLEROT une α β uA oa 0,333 0,222 0,600 1 0,393 2 1,200 0,800 0,600 0,283 17,000 12,000 3 0,586 0,090 4 0,222 0,333 0,400 0,393 5 2,000 18,000 0,100 0,066 TAB. 1 - Les paramètres des distributions bêta qui ont été utilisées pour modéliser les résultats de récompenses dans les simulations numériques, ainsi que les attentes correspondantes et des écarts stan- dard Notez qu'il est une fonction paramétrique des intervalles de confiance pour V * (t, p (t)) par μ (t) et σ (t) qui sont les quantités utilisées pour définir l'intervalle lui-même. La raison de ce choix est le suivant. La tangente en un point sur une courbe de niveau de u est égal à 1 partout dans le plan (μ, σ). Par conséquent, le décideur est indifférent entre un intervalle de confiance spécifié par le couple (μ, σ) et un autre spécifié par (μ + Δμ, σ + Δσ), où Δμ et Δσ sont deux incréments liés par Δσ = Δμ. Étant donné que les limites inférieures des intervalles de confiance sont égaux, σ = u- (μ + Δμ) - (σ + Δσ), cela signifie que le décideur accepte de négocier une stratégie avec une autre (autrement dit, ils ont la même utilité) que si les intervalles de confiance partagent la même limite inférieure. limites plus bas sont préférés tandis que les limites supérieures sont ignorées, indiquant ainsi l'aversion aux stratégies risquées. En conclusion, le Propo stratégie sed, qui sera appelée LCB1 (pour « Lower Confidence Bound »), est le suivant: 1. pour 1 ≤ t ≤ 2K échantillon chaque action deux fois, de sorte que les variances empiriques authentiques peuvent être réglées à leur valeur initiale; 2. Pour chaque temps t> 2K choisir l'un d'action avec une probabilité p * a (t), où p * (t) = arg maxp u (t, p), et u (t, p) est la fonction d'utilité dans ( 7). 4 Quelques simulations numériques Dans cette section, les résultats de deux simulations numériques sont prévues. Cinq sont considérés comme des distributions de récompenses, qui appartiennent à la famille des distributions bêta Be (α, β). Les paramètres et les tracés des densités de probabilité correspondants sont présentés dans le tableau 1 et la figure 3 respectivement. L'horizon de temps est T = 200 dans les deux tests et f = 1,96 (ce qui donne un intervalle de confiance de 95% pour une variable aléatoire gaussienne norme). Dans le premier test K = 3, ce qui correspond à la récompense distributions 1 à 3. La figure 4 Les emplacements d'exposition de la distribution de probabilité FG (t) (g) = P [G (t) ≤ g] du gain obtenu après t = 50 , 100, 150 et 200 pas par les algorithmes UCB1 (ligne bleue), UCB-V (ligne noire) et le LCB1 algorithme proposé (ligne rouge). Ces distributions ont été obtenues par l'exécution de ces algorithmes plus de 1000 réalisations différentes des séquences de récompenses. Il n'y a pas de différences remarquables entre les trois algorithmes. En fait, des moyens de récompense sont comparables et la réduction du risque qui pourrait être obtenu par une action d'échantillonnage 3 plus sont souvent contrebalancés par une réduction du risque de manipulation des problèmes de bandits 0 0,2 0,4 0,6 0,8 1 0 2 4 6 0 0,2 0,4 0,6 0,8 1 0 2 4 6 8 FIG. 3 - Emplacements des récompenses densités de probabilité, sur la gauche pour a = 1, 2, 3 (lignes bleues, vertes et rouges, respectivement), et sur la droite pour a = 4, 5 (lignes cyan et magenta, respectivement). gain attendu. Dans le second test K = 5 et actions 4 et 5 ont été ajoutés à des actions 1, 2 et 3 du premier test. Le test a été effectué de la même manière que décrit ci-dessus. Parcelles de la distribution de probabilité du gain sont présentés à la figure 5. Contrairement au cas considéré dans le premier test, ici la performance de LCB1 est supérieure. En fait, comme il peut être vérifié dans la figure 6, LCB1 se débarrasse des actions inférieures 4 et 5 plus rapide que les deux autres algorithmes. 5 Discussion Une approche pour la solution du problème de bandit manchot dans le cadre IID a été proposé. Contrairement aux méthodes existantes bien connues, l'approche est quelque peu heuristique en ce sens qu'elle ne repose pas sur des preuves de convergence rigoureuses. Cependant, les simulations numériques montrent une très bonne performance à court terme, au moins dans les cas considérés, ainsi des analyses plus poussées motivat- ing. Dans les applications pratiques, le comportement à court terme est particulièrement important parce que l'hypothèse d'une répartition identique dans le temps peut être violé à long terme. Deux marges d'améliorations convient d'indiquer. D'une part, les intervalles de confiance de manière sont calculés: mieux établis échantillons techniques statistiques finies telles que le bootstrap, par exemple, pourraient prévoir des intervalles plus corrects et même asymétriques. D'autre part, les fonctions d'utilité peuvent exister que représenterait différents équilibres entre le risque et d'éviter les comportements à la recherche des risques, qui est différentes façons de gérer le dilemme exploration-exploitation. La principale contribution de cet article, cependant, est le lien établi avec la théorie des préférences qui a été et est actuellement un grand sous-champ de recherche en économie. En fait, l'approche présentée ici partage de nombreuses similitudes avec les méthodes de sélection très portefeuille de base (Markowitz, 1952) et la possibilité d'introduire des techniques plus avancées (le soi-disant ou aversion pour le risque sensible au risque) dans le contexte des problèmes de bandit devraient être considérés pour les enquêtes futures. Il convient également de mentionner la possibilité d'étendre le cadre IID pour permettre des récompenses à prix réduit, c'est-à résoudre un dilemme exploration-exploitation où, à chaque instant t, un temps de- S. Perabo et F. CLEROT 25 30 35 0,2 0,4 0,6 0,8 1 Gain G après 50 étapes P ro bab ili té d est tr ib u ti on FG 55 60 65 70 0,2 0,4 0,6 0,8 1 Gain G après 100 pas P ro bab ili té d est tr ib u ti on FG 80 85 90 95 100 0,2 0,4 0,6 0,8 1 Gain G après 150 marches P ro bab ili té d est tr ib u ti on FG 110 120 130 0,2 0,4 0,6 0,8 1 Gain G après 200 pas P ro bab ili té d est tr ib u ti on FG Fig. 4 - Les résultats du premier test (K = 3, les récompenses distributions 1 à 3): distribution du gain obtenu par le UCB1 d'algorithme (ligne bleue), UCB-V (ligne noire) et LCB1 (ligne rouge), après 50 , 100, 150 et 200 pas de temps. pendants fonction objective de la forme G (t) = E [ΣT (t) S = tw (t, s) · ra (s) (s)] doit être maximisée, où T (t) ≥ t et le réel le nombre w (t, s) sont des poids donné (seul le cas T (t) ≡ T et W (t, s) ≡ 1 a été discuté dans le présent document). En fait, les décisions impliquent très souvent des compromis entre les coûts et les avantages incertains qui se produisent à différents points dans le temps sous la contrainte des horizons temporels différents. Dans de tels cas, il est connu (Frederick et al., 2002) que le choix d'une séquence de pondération est liée aux préférences temporelles du décideur: différentes préférences exigent différentes formes de pondération. Il devrait être clair que, chaque fois que ces préférences sont connus (ainsi que la fonction d'utilité sur des intervalles de confiance), l'approche présentée peut être appliquée tout à fait sans détour. Références Audibert, J.-Y., R. Munos et C. Szepesvári (2007). Estimations de la variance et la fonction exploration bandit à plusieurs bras. Rapport technique 07-31, CERTIS, France. Auer, P., N. Cesa-Bianchi et P. Fischer (2002). Analyse finie temps du problème de bandit manchot. Machine Learning 47, 235-256. risque de manutention des problèmes de bandits 20 25 30 0,2 0,4 0,6 0,8 1 Gain G après 50 étapes P ro bab ili té d est tr ib u ti on FG 40 50 60 0,2 0,4 0,6 0,8 1 Gain G après 100 pas P ro bab ili té d est tr ib u ti on FG 70 80 90 0,2 0,4 0,6 0,8 1 Gain G après 150 marches P ro bab ili té d est tr ib u ti on FG 90 100 110 120 0,2 0,4 0,6 0,8 1 Gain G après 200 pas P ro bab ili té d est tr ib u ti on FG Fig. 5 - Les résultats du deuxième test (K = 5, les récompenses distributions 1 à 5): répartition du gain obtenu par le UCB1 d'algorithme (ligne bleue), UCB-V (ligne noire) et LCB1 (ligne rouge), après 50 , 100, 150 et 200 pas de temps. Même-Dar, E., S. Mannor et Y. Mansour (2006). l'élimination d'action et arrêter les condi- tions pour des problèmes d'apprentissage bandit à plusieurs bras et le renforcement. Journal of Machine Learning Research 7, 1079-1105. Fishburn, P. (1999). structures de préférence et leur représentation numérique. Theoretical Computer Science (217), 359-383. Frederick, S., G. Loewenstein et T. O'donoghue (2002). actualisation du temps et le temps la préférence: un examen critique. Journal of Economic Literature 40 (2), 351-401. Lai, T. et H. Robbins (1985). règles d'allocation adaptative asymptotiquement efficace. Adv. Appl. Math. 6, 4-22. Markowitz, H. (1952). sélection du portefeuille. Journal des Finances 7 (1), 77-91. Puterman, M. (1987). Programmation dynamique. Dans l'affaire R. Meyers (Ed.), Encyclopédie des sciences physiques et de la technologie, volume 4, pp. 438-463. Academic Press. Robbins, H. (1952). Certains aspects de la conception séquentielle d'expériences. Taureau. Amer. Math. Soc. 58 (5), 527-535. Tsoukias, A. (2008). De la théorie de la décision à la décision de la méthodologie Complicité. Revue européenne de recherche opérationnelle 187, 138-161. S. perabo et F. CLEROT 20 40 60 80 100 120 140 160 180 200 1 2 3 4 5 A c ti ona UCB1 20 40 60 80 100 120 140 160 180 200 1 2 3 4 5 A c ti ona UCB-V 20 40 60 80 100 120 140 160 180 200 1 2 3 4 5 Temps t A c ti ona LCB1 FIG. 6 - comportements typiques des algorithmes UCB1, UCB-V et LCB1 (de haut en bas) obtenu dans le deuxième test (K = 5, les distributions de récompenses 1 à 5). Un symbole « + » indic ates que l'action correspondante a été sélectionnée à l'instant t. Une approach is résumé présentéisme Pour la prise en considération du Erotisme Dans le Problème du « bandit manchot ». De plus précisement, le dit d'exploration Dilemme-exploitation is reformulée Comme un de maximisation d'Problème Une fonction d'utilité Qui l'attitude de du mesure Décideur le Envers et osée l'incertitude. Un lien Avec la théorie de la ÉTABLI Préférence is Fait. La méthode proposed is Testée et par simulation numérique comparée Avec d'Autres bien connues Dans la littérature, un with interest for the Particulier à comportement de cour terme (petit d'Échantillons Nombre)."
729,Revue des Nouvelles Technologies de l'Information,EGC,2009,Assessing the uncertainty in knn Data Fusion,,"Tomàs Aluja-Banet, Josep Daunis-i-Estadella, Enric Ripoll",http://editions-rnti.fr/render_pdf.php?p1&p=1000794,http://editions-rnti.fr/render_pdf.php?p=1000794,en,"(Actes_non_num \ 351rotes.pdf) Évaluation de l'incertitude dans knn Fusion de données Tomàs Aluja-Banet *, Josep Daunis-i-Estadella **, *** * Enric Ripoll Université Polytechnique de Catalogne, Campus Nord, C5-204, E-08034 Barcelona tomas.aluja@upc.edu ** Universitat de Girona, Campus de Montilivi, Edifici P4, E-17071 Girona josep.daunis@udg.edu *** Institut d'Estadística de Catalunya, Via Laietana 58, E-08003 Barcelona eripoll@idescat.net fusion de données, également connu sous le nom l'appariement statistique, est une opération technologique dont le but est d'intégrer les informations de deux sources de données indépendantes. Soit (X0, Y0) le fichier donneur et (X1) le fichier destinataire, où les X sont les variables communes et Y sont ceux de spécifiques. Le but est de compléter le dossier du destinataire (X1, Y1) de manière telle qu'il peut être un d'une réalisation de la fonction de densité conjointe f (X, Y). Il existe trois approches fondamentales pour la fusion de données. La première consiste à incorporer les courants et les variables spécifiques au sein d'une distribution multivariée paramétrique f (X, Y | θ), en supposant que les donneurs et les récepteurs établir indépendamment et de façon aléatoire à partir de cette distribution. Cette distribution peut être pris en compte dans f (X, Y | θ) = f (Y | X, θY | X) f (X, θX); Par conséquent, il est possible d'estimer ses paramètres θX et θY | X à partir des informations disponibles et de les utiliser pour le bloc manquant imputer des données. La seconde approche consiste à modéliser directement la relation entre les variables Y et les variables X dans le fichier de donneur au moyen d'une fonction de régression: E (Y | X) = r (X) + ε et l'application de ce modèle dans le fichier de destination (explicite la modélisation). La dernière approche consiste à trouver pour chaque individu du fichier bénéficiaire d'un ou plusieurs individus donateurs aussi semblables que possible, puis d'une certaine façon, transférer les valeurs des variables Y à l'individu bénéficiaire (modélisation implicite). Cette méthode est connue sous le pont chaud, un terme emprunté à partir de données d'édition de bases de données. Validité de l'imputation Nous dirons que la fusion de données est valide si l'ensemble de données fusionné (X1, Y1) est une instance de la fonction de distribution f (X, Y). En général, la fonction de distribution f est inconnue, nous sommes obligés donc de comparer des fonctions de distribution empirique (X1, Y1) avec l'ef (X1, Y1). Nous appelons l'écart entre les deux distributions de bruit correspondant; suivant Paass (1985) l'adaptation dépend du bruit sur la justesse de la fonction d'imputation i (X) à des instances approximatives de la distribution conjointe vrai, qui dans le cas paramétrique il dépend de la façon dont la distribution multivariée supposé représente les données réelles, et la méthodologie de pont chaud dépend, comme avant, le modèle d'imputation i supposée (X) et en plus des écarts existants entre les bénéficiaires et leurs bailleurs de fonds correspondants. Cependant, quelle que soit la méthode d'imputation choisie, des données imputées ne sont pas comme les données observées, car il a l'incertitude inhérente, c'est le problème de l'incertitude. Les valeurs imputées sont des estimations ¯y1, donc, pour être réaliste, nous devons tenir compte de la variabilité des données imputées lors de l'analyse elle. Cette variabilité provient de la fluctuation aléatoire de la distribution L'évaluation de l'incertitude dans knn Fusion de données f (Y | X, θY | X) et aussi du fait que les paramètres du modèle θY | X sont inconnus et consé- quent qu'elles véhiculent aussi une fluctuation aléatoire. Imputation multiple est la façon classique de faire face à ce problème (Rubin (2004)). Il consiste à répéter plusieurs fois la procédure d'imputation unique, de la distribution prédictive de f (Y | X, θY | X) dans des conditions réalistes des paramètres θY | X et puis juste concaténer plusieurs fichiers individuels d'imputation. (. Aluja-Banet et al, 2007) Suite des statistiques de validation ASLM: comparaison des moyennes marginales y1 et Y1. ASL: comparaison des écarts marginaux dans ¯y1 et Y1. CIDA: comparaison des corrélations entre les variables par paires spécifiques de ¯y1 et Y1. ACDE: comparaison des corrélations par paire entre les spécifications ific variables et celles des courants dans (X1, Y1) et (X1, Y1). wc: reproduction de la structure propre de Y1 dans ¯y1. ASD: calcul des distances entre les distributions Smirnov empiriques des variables spécifiques Y1 et Y1. τ: Calcul de l'erreur de généralisation individuelle. Application à des données d'enquête officielle sur la sécurité et la victimisation en Catalogne Nous avons pris les données recueillies en 2006 enquête Idescat pour effectuer une opération de fusion de données de certaines variables sur l'enquête de 2007 et de les comparer avec les valeurs réelles collectées en 2007. Nous ont procéder à extraire 400 rééchantillonnage bootstrap de chaque fichier pour évaluer la validité des résultats. Principaux résultats Nous présentons la valeur moyenne et l'intervalle de 95% des différentes statistiques de validation du plus proche voisin comme méthode de référence, le DA-MI habituelle et la méthode KNN-MI proposé. ASLM ASL IDCA Acde wc ASD T pour 1NN 0,021 0,027 0,132 0,040 0,741 0,100 1,934 0,000 0,100 0,000 0,112 0,085 0,209 0,035 0,046 0,190 0,956 0,065 0,137 1,751 2,195 DA-MI 0,055 0,065 0,051 0,031 0,950 0,267 1,894 0,001 0,151 0,004 0,138 0,043 0,059 0,027 0,036 0,931 0,969 0,261 0,274 1,865 1,923 knn-MI 0,054 0,068 0,038 0,031 0,915 0,065 1,935 0,000 0,111 0,000 0,167 0,043 0,100 0,035 0,042 0,708 0,986 0,045 0,086 1,841 2,048 TAB. 1 - Statistiques de validation moyennes L'imputation multiple knn améliore nettement les résultats obtenus par la seule imputation, mais il se situe en deçà des performances de l'imputation multiple paramétrique, sauf pour le bruit correspondant, où la méthode KNN assure réalistes imputations. Références Aluja-Banet, T., J. Daunis-i-Estadella et D. Pellicer (2007). Graft, un système complet pour la fusion de données. Journal de la statistique informatique et analyse des données 52 (2), 635-649. Paass, G. (1985). méthode de couplage d'enregistrements statistiques, état des perspectives d'art et à venir. Bulletin de l'Institut international de statistique, Actes de la 45e session, LI 2. Rubin, D. (2004). Imputation multiple pour la non-réponse dans les enquêtes. New York: Wiley & Sons."
730,Revue des Nouvelles Technologies de l'Information,EGC,2009,Binary Sequences and Association Graphs for Fast Detection of Sequential Patterns,We develop an efficient algorithm for detecting frequent patterns thatoccur in sequence databases under certain constraints. By combining the useof bit vector representations of sequence databases with association graphs weachieve superior time and low memory usage based on a considerable reductionof the number of candidate patterns.,"Dan A. Simovici, Selim Mimaroglu",http://editions-rnti.fr/render_pdf.php?p1&p=1000781,http://editions-rnti.fr/render_pdf.php?p=1000781,en,"(Actes_non_num \ 351rotes.pdf) séquences binaires et graphiques Association pour la détection rapide des motifs séquentiels Selim Mimaroglu *, Dan A. Simovici ** * Université Bahcesehir, Istanbul, Turquie, selim.mimaroglu@gmail.com ** Université du Massachusetts à Boston, Massachusetts 02125, Etats-Unis, dsim@cs.umb.edu Résumé. Nous développons un algorithme efficace pour détecter des motifs fréquents qui se produisent dans des bases de données de séquence sous certaines contraintes. En combinant l'utilisation des représentations de vecteur de bits de bases de données de séquences avec des graphiques d'association nous atteignons le temps supérieure et une faible utilisation de la mémoire basée sur une réduction considérable du nombre de motifs candidats. 1 Introduction Les mines motifs séquentiels a été initialement proposé dans Agrawal et Srikant (1995), où trois algorithmes, (AprioriAll, AprioriSome et DynamicSome) ont été introduits. PrefixSpan, basé sur l'idée de projection de préfixe, a été introduit dans Pei et al. (2001). SPADE Zaki (2001) l'espace PerForm efficace rejoint sur les classes d'équivalence à base de préfixe. PRISM Gouda et al. (2007), utilise le codage des nombres premiers pour le comptage de soutien. Un problème connexe mais distinct (discuté dans Mannila et al. (1997)) est de trouver des épisodes fréquents dans des séquences très longues. SPAM Ayres et al. (2002) trouve des motifs séquentiels en utilisant une représentation en mode point. Une extension de SPAM, qui intègre les contraintes d'espacement et d'expression régulière a été obtenue à Ho et al. (2005). L'algorithme GSP Srikant et Agrawal (1996) est similaire à AprioriAll; en outre, il peut gérer trois types de contraintes: écart minimum et maximum entre les éléments consécutifs d'une séquence (appelée min_gap et max_gap), et la taille de la fenêtre entre les rangées. Lorsque min_gap = 0, max_gap = ∞ et window_size = 0, les motifs séquentiels trouvés par GSP sont les motifs séquentiels classiques comme introduit dans Agrawal et Srikant (1995). L'algorithme cSPADE Zaki (2000) introduit des contraintes similaires, et il est mis en œuvre sur le dessus de SPADE. SPIRIT Garofalakis et al. (1999) est plus générale que les deux GSP et cSPADE car il traite des contraintes d'expression régulière. Dans cette note, nous décrivons SPAG, un algorithme qui combine la double utilisation du bit vecteur RESENTATIONS sentants des bases de données de séquences avec des graphiques d'association pour obtenir des résultats supérieurs dans l'identification des motifs dans les séquences. 2 cadres Apriori Sequence Définit Nous renvoyons le lecteur à Simovici et Djeraba (2008) pour des concepts mathématiques et nota- tions. Soit I un ensemble d'éléments, et laissez-Seq (I) l'ensemble des séquences d'éléments de I. Nous considérons aa poset graduée (P, ≤, h), où P ⊆ Seq (I), et h: P - → N, dénommés ensemble de sterne PAT-, et un ensemble de données D défini comme une séquence de séquences , D = {s1,. . . , Sn} ⊆ Seq (Seq (I)). Une séquence est un cadre Apriori triple ((P, ≤, h), D, σ), où σ est une relation entre sterne PAT- et des données, de telle sorte que t ≤ t 'et (t', s) ∈ σ implique ( t, s) ∈ σ. Détection rapide des motifs séquentiels Si x est une séquence de y, on note ceci en x v y. Pour y ∈ D, et t ∈ P on définit plusieurs fonctions partielles forment φ: (Seq (I)) 2 - → R tel que Dom (φ) = {(t, y) ∈ (Seq (I)) 2 | TVY}: α (t, y) = '(y) - `y (t), β (t, y) =' y (t) -` (t), et ω (t, y) que le nombre de occurrences de t à y, pour t, y ∈ Seq (I). L'α de la fonction (t, y) mesure l'écart externe de t en y, tandis que β (t, y) mesure l'écart interne de t y. Pour un y fixe, si t est dispersée en y alors α (t, y) est relativement faible, et β (t, y) est relativement grande. D'autre part, si t est condensé en y alors α (t, y) est relativement grande, et β (t, y) est relativement faible. Par exemple, si t = A1A2, et y = a1a3a4a5a2, nous avons α (t, y) = 0 et β (t, y) = 3. Si y '= a3a4a1a2a5, alors α (t, y') = 3 et β (t, y ') = 0. Les valeurs des deux fonctions a et β sont modifiés comme prévu, parce que t est condensé en y'. Soit y une séquence de base de données, u, v deux séquences telles que u est un infix de v et supposons que v v y. Si φ est l'une des fonctions partielles a ou ω alors φ (u, y) ≥ φ (v, y). Nous avons aussi β (v, y) ≥ β (u, y). Laisser σmin, k est le rapport qui se compose de ces paires (v, y) pour lesquels il existe une Currence OC- de v en y tels que le moindre écart entre deux symboles consécutifs de v est d'au moins k. De même, si amax, k est constituée de couples (v, y) pour lesquels il existe une occurrence de v en y de telle sorte que le plus grand écart entre deux symboles consécutifs de v est d'au plus k. Ces relations ont été introduites dans Srikant et Agrawal (1996). Le relationsσα, k et σβ, k sont donnés par σα, k = {(v, y) ∈ (Seq (I)) 2 | v v y et α (v, y) ≥ k} et par σβ, k = {(v, y) ∈ (Seq (I)) 2 | v v y et β (v, y) ≤ k}. Il est facile de voir que σmin, k, amax, k, σα, k et σβ, k sont des relations Apriori. Si σ est une relation Apriori et u est un infix de v, puis Supp D, σ (v) ≤ suppD, σ (u). Cela permet une extension directe de l'algorithme bien knownApriori un algorithme défini sur les séquences. 3 graphiques Association et Bit vecteurs vecteurs est bit Stockage des opérations spatiales très efficaces et sont très rapides au niveau du bit, ce qui permet de stocker de grandes bases de données en mémoire. Chaque i de l'article distinct dans la base de données est représenté par un vecteur de bits, noté ibvi (point vecteur de bits), qui contient autant de bits que le nombre de lignes dans la base de données. Si i est présent dans la ligne jème, l'entrée est jème ibvi 1, et 0 sinon. Chaque ligne d'une table qui contient des éléments distincts m est représenté par un ensemble de vecteurs de bits m rangée rbv1,. . . , Rbvm dont la longueur est égale à la longueur l de la ligne. Si rbvj = (BJ1,..., BJL) puis bjh = {1 si le point ij se produit à la position h, 0 sinon, pour 1 ≤ h ≤ l. La double utilisation des vecteurs de bits articles et ligne accélère le processus d'extraction considérablement en fournissant nombre de réponses rapides. Une représentation vectorielle de bits similaire est utilisée dans PRISM Gouda et al. (2007), où les opérations de codage des nombres premiers, et entiers sont utilisés à la place des opérations binaires. Ensuite, nous définir le graphe d'association d'une base de données de séquence. Définition 3.1 Soit D = (s1,..., Sn) est une base de données de séquence sur l'ensemble des éléments I, min_sup le nombre de support minimal, et σ est une relation Apriori. Le graphique de l'association de D, S. Mimaroglu et D. A. Simovici AG = (V, E) est un graphe orienté étiqueté défini comme suit. L'ensemble des sommets V est composé de ces éléments de telle sorte que i | {j | (i, sj) ∈ a et i v sj sj ∈ D, pour 1 ≤ j ≤ N} | ≥ min_sup. L'ensemble d'arêtes E est constitué des couples (i, i ') ∈ V × V tel que | {j | (ii', sj) ∈ σ et ii 'sj v, sj ∈ D, pour 1 ≤ j ≤ N} | ≥ min_sup. Un bord (i, i ') est marqué par une sé- quence L (i, i') = (τ1,..., ΤN), où τp = 1 si (ii ', sp) ∈ σ, et ii' v sp, et τp = 0, sinon, pour 1 ≤ p ≤ N. Entrée: Base de données: D IBV et format RBV, le nombre minimum de soutien: min_sup, relation Apriori: σ sortie: Le graphique Association, AG = (V, E) // Créer les sommets foreach point i dO1 // Sur chaque ligne de telle sorte que ibvi est égal à 1, contrainte de vérification si suppcount D, σ (i) ≥ min_sup lors2 V = V ∪ {i}; // 3 Insérer les bords des étiquettes foreach i ∈ V do4 foreach i 'V ∈ do5 ibvT = ibvi ∩ ibvi' ; 6 si ibvT .count () ≥ min_sup then7 // sur chaque rangée de telle sorte que ibvT est 1, en utilisant les vecteurs de bits de ligne de i et i 'pour calculer si suppcount D, σ (ii') ≥ min_sup then8 E = E ∪ {(i, i ')}; 9 étiquette (i, i') par ibvii '; 10 // sur chaque rangée de telle sorte que ibvT est 1, en utilisant les vecteurs de bits de ligne de i et i' pour calculer si suppcount D, σ (i 'i) ≥ min_sup then11 E = E ∪ {(i', i)}; 12 étiquette (i ', i) par ibvi'i; 13 sortie AG; 14 la Fig. 1: Association Graph Construction Algorithm Dans la mise en œuvre de C ++ de SPAG, classe KFrequentC représente les séquences fréquentes et stocke les membres de la séquence, un vecteur de bits qui est mis à 1 au niveau des indices de ligne contenant cette séquence, et pour chaque rangée contenant cette séquence (il peut y avoir plus d'une occurrence), la position d'extrémité (s) et la longueur (s). Pour prolonger une séquence p ∈ Fi il doit y avoir un avantage du dernier membre de p à un élément fréquent dans le graphe d'association. Depuis Fi p, la longueur de p est i. S'il existe un bord de pi, le dernier élément de p, à un autre élément fréquent z, puis (pi, z) ∈ E dans le graphique de l'association AG. Dans un premier temps, ibvpz est obtenu comme ibvpz = ibvp ∩ ibvpiz. Nous devons avoir le nombre (ibvpz) ≥ min_sup. Détection rapide des modèles d'entrée séquentielle: Base de données: D, le nombre minimum de soutien min_sup, relation Apriori: Sortie σ: séquences fréquentes Lire la base de données D, convertir les éléments en IBV format et convertir les lignes rbv format1; Créer le graphe d'association AG = (V, E) à l'aide de l'association graphique Construction2 algorithme; F1 = V; 3 F2 = {ii '| (i, i') ∈ E}; 4 i = 2; 5 tandis Fi 6 = ∅ DO6 Ci + 1 = {pz | p ∈ Fi, (pi, z) ∈ E, | ibvpz = ibvp ∩ ibvpiz | ≥ min_sup, pruneσ (pz)}; // 7 utiliser des vecteurs de bits de ligne de z, et des informations à KFrequentC de p pour calculer suivant Compute Fi + 1 à partir de Ci + 1; i + 8 + 9; la Fig. 2: Modèle séquentiel Mining avec l'Association Graphique (SPAG) algorithme Comme σ est une relation Apriori, toutes les séquences q tel que q est un infix de pz doit être fréquente. Si une q viole cette condition, cela signifie que pz ne peut être fréquent non plus, donc pruneσ (pz) retourne false. Sur la figure 2, ces étapes sont représentées à la ligne 7. Après cela, les séquences fréquentes réelles sont calculées à partir de l'ensemble candidat comme représenté sur la ligne 8 de la figure 2. vecteurs Row bits de z et les positions d'extrémité de P sont utilisés pour faire en sorte que z suit p. Les lignes violant cet ordre ou la contrainte σ sont à 0 dans ibvpz. Vérification de l'ordre et la contrainte σ doit être fait sur chaque ligne de la base de données où ibvpz est initialement fixé à 1. A la fin de cette procédure, si le nombre (ibvpz) ≥ min_sup, puis pz est placé dans Fi + 1. Les deux Ayres et al. (2002) et Ho et al. (2005) l'utilisation des vecteurs bit et les opérations sur les vecteurs de bits. SPAG combine l'utilisation de vecteurs de bits avec des graphiques d'association et permet l'utilisation de l'information mondiale vitale. Trouver toutes les occurrences des éléments dans un ensemble de données est immédiate en utilisant des vecteurs de bits de l'élément. En outre, des graphiques d'association gérer ce qui peut suivre un élément dans un ensemble de données; ce qui améliore le processus de génération de motif candidat considérablement. Ni Ayres et al. (2002), ni Ho et al. (2005) ont l'information globale fournie par notre double utilisation de vecteurs de bits de l'élément et des graphiques d'association; par conséquent, ils doivent garder une trace de chaque ligne. En outre, l'énorme quantité de transformations de vecteur de bits à la fois faire Ayres et al. (2002) et Ho et al. (2005) mémoire inefficace. SPAG évite ces difficultés en travaillant sur les vecteurs de bits d'origine sans les modifier. 4 Résultats expérimentaux Des expériences approfondies ont été menées sur des ensembles de données séquentielles générées synthétiquement US- un ordinateur Pentium ing 3.0GHz ayant 4 Go de mémoire principale sur Linux. Les résultats de SPAG sont comparés à SPADE Zaki (2001), la mise en œuvre de Ho et al. (2005) (que nous appelons cSPAM), et avec cSPADE Zaki (2000). Il est montré dans Zaki (2001) qui surclasse SPADE GSP en utilisant l'espace efficace rejoint. Bien que cSPAM n'est pas très efficace de l'espace, il est rapide, car il utilise des vecteurs de bits. cSPAM S. et D. Mimaroglu A. Simovici peuvent également gérer les contraintes. cSPADE est mis en oeuvre au-dessus de pique; il peut gérer certaines contraintes. Les résultats expérimentaux montrent que SPAG surclasse SPADE, cSPADE et cSPAM dans presque tous les niveaux de soutien pour chaque ensemble de données. Dans les figures 3 (a), 3 (b) et 3 (c), nous montrons les délais d'exécution (a) Sur une petite base de données (b) Sur une base de données de moyenne (c) Sur une grande base de données (d) Sur une base de données très importante ( e) écart minimal (f) Ecart maximal Fig. 3: Temps de comparaison de cSPAM, pelle, et SPAG pour cSPAM, pelle et SPAG sont présentées pour les bases de données synthétiques qui vont de 1, 000 à 100, 000 lignes, comportant 7 articles par rangée en moyenne, et un total de 30 articles distincts. SPAG a de meilleures performances sur cet ensemble de données pour tous les niveaux de soutien. Sur tous ces ensembles de données SPAG OUT- effectue SPADE. Pour un seul niveau de soutien cSPAM surclasse SPAG. cSPAM et SPAG sont comparables dans deux cas, et pour les 13 autres cas de les plus performant que la détection rapide des motifs séquentiels cSPAM. Des expériences sont élargies pour inclure un ensemble de données plus de 100.000 lignes ayant 1000 des objets uniques et 50 articles par rangée en moyenne. Les résultats des tests pour cet ensemble de données sont représentés sur la figure 3 (d). Dans la plupart des cas SPAG surpasse SPADE par un facteur de 10. Fig. 3 (E) montre que SPAG et cSPADE effectuer de même pour la contrainte de l'écart minimum. SPAG est 4 fois plus rapide que dans la plupart des cas cSPAM. SPAG surpasse clairement à la fois cSPAM et cSPADE considérablement pour la contrainte de l'écart maximum, comme illustré sur la figure 3 (f). 5 Conclusions et travaux futurs Nous présentons SPAG, un algorithme efficace pour détecter des motifs fréquents qui combine la double utilisation des représentations de vecteur de bits de bases de données de séquences avec des graphiques d'association. les résultats montrent que Perimental Ex- SPAG est plus rapide que les autres algorithmes état de l'art (cSPAM, Spade, et cSPADE) à la fois avec des contraintes et sans contraintes. Nous croyons que le SPAG a une application plus large et son utilisation pour la détermination des profils de variabilité ou à des modèles de fréquence doit être étudiée. Références Agrawal, R. et R. Srikant (1995). Exploitation minière motifs séquentiels. Compte rendu de la onzième Conférence internationale sur l'ingénierie des données, 3-14. Ayres, J., J. Flannick, J. Gehrke et T. Yiu (2002). extraction de motifs séquentiels en utilisant une représentation en mode point. En KDD, pp. 429-435. Garofalakis, M. N., R. Rastogi et K. Shim (1999). Esprit: extraction de motifs séquentiels avec des contraintes d'expressions régulières. En VLDB, pp. 223-234. Gouda, K., M. Hassaan, etM. J. Zaki (2007). Prism: Une approche premier codant pour l'extraction de séquences fréquentes. Int IEEE. Conférence sur l'exploration de données. Ho, J., L. Lukov et S. Chawla (2005). extraction de motifs séquentiels avec des contraintes sur grandes bases de données de protéines. Dans Actes de la 12e Conférence internationale sur la gestion des données (COMAD 2005b), pp. 89-100. Computer Society de l'Inde. Mannila, H., H. Toivonen et A. Inkeri Verkamo (1997). Découverte de épisodes fréquents dans les séquences d'événements. Data Mining et Knowledge Discovery 1 (3), 259-289. Pei, J., J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U. Dayal, etM. Hsu (2001). PrefixSpan: Exploitation minière motifs séquentiels par Pattern-Efficiently projeté Prefix. Int IEEE. Conférence sur l'ingénierie des données. Simovici, D. A. et C. Djeraba (2008). Outils mathématiques pour l'exploration de données - la théorie des ensembles, ensembles, Combinatoire partiellement commandé. London: Springer-Verlag. Srikant, R. et R. Agrawal (1996). Exploitation minière motifs séquentiels: Généralisations et l'amélioration des performances indiquées. En EDBT, p. 3-17. Zaki, M. J. (2000). l'extraction de séquences dans les domaines catégoriques: L'intégration de contraintes. En CIKM, pp. 422-429. Zaki, M. J. (2001). Spade: Un algorithme efficace pour l'extraction de séquences fréquentes. Machine Learning 42 (1/2), 31-60. Nous résumé de l'un algorithme des Efficace verser des motifs Détecter Fréquents Qui se pro- duisent des Dans Bases de données sous séquentielles CONTRAINTES CERTAINES. En Combinant L'utilisation des bases de des Représentations Données par séquentielles Avec des Séquences Graphes binaires d'association, NOUS ONU obtenons Temps et Une Meilleur ont utilisation MoiNs Grande de La Memoire basée Sur une réduction considérable des motifs du Nombre candidats."
735,Revue des Nouvelles Technologies de l'Information,EGC,2009,Collaborative Outlier Mining for Intrusion Detection,"Intrusion detection is an important topic dealing with security of in-formation systems. Most successful Intrusion Detection Systems (IDS) rely onsignature detection and need to update their signature as fast as new attacks areemerging. On the other hand, anomaly detection may be utilized for this purpose,but it suffers from a high number of false alarms. Actually, any behaviour whichis significantly different from the usual ones will be considered as dangerousby an anomaly based IDS. Therefore, isolating true intrusions in a set of alarmsis a very challenging task for anomaly based intrusion detection. In this paper,we consider to add a new feature to such isolated behaviours before they can beconsidered as malicious. This feature is based on their possible repetition fromone information system to another. We propose a new outlier mining principleand validate it through a set of experiments.","Goverdhan Singh, Florent Masseglia, Céline Fiot, Alice Marascu, Pascal Poncelet",http://editions-rnti.fr/render_pdf.php?p1&p=1000776,http://editions-rnti.fr/render_pdf.php?p=1000776,en,"(Actes_non_num \ 351rotes.pdf) en collaboration Outlier Mining pour la détection des intrusions Goverdhan Singh *, Florent Masseglia *, Céline Fiot *, Alice Marascu *, Pascal Poncelet ** * INRIA Sophia Antipolis, 2004 route des Lucioles - BP 93, 06902 Sophia Antipolis Prenom .nom @ sophia.inria.fr ** LIRMM UMR CNRS 5506, 161 Rue Ada, 34392 Montpellier Cedex 5, France poncelet@lirmm.fr CV. La détection d'intrusion est un sujet important traitant de la sécurité des systèmes de formation in-. La plupart des succès des systèmes de détection d'intrusion (IDS) __gVirt_NP_NNS_NNPS<__ reposant sur la détection de signature et le besoin de mettre à jour leur signature aussi vite que de nouvelles attaques se font jour. D'autre part, de détection d'anomalie peut être utilisée à cette fin, mais il souffre d'un grand nombre de fausses alarmes. En fait, tout comportement qui est sensiblement différent des habituels sera considéré comme dangereux par un IDS basé sur une anomalie. Par conséquent, l'isolement de véritables intrusions dans un ensemble d'alarmes est une tâche très difficile pour la détection d'intrusion basée sur une anomalie. Dans cet article, nous considérons d'ajouter une nouvelle fonctionnalité à de tels comportements isolés avant de pouvoir être considérés comme malveillants. Cette fonction est basée sur leur éventuelle répétition d'un système d'information à l'autre. Nous vous proposons un nouveau principe de l'exploitation minière des valeurs aberrantes et validons par une série d'expériences. 1 Introduction La protection d'un système contre les nouvelles attaques, tout en gardant un travail automatique et adaptatif DE CADRE est un sujet important dans ce domaine. Une réponse à ce problème pouvait compter sur l'exploration de données. En fait, l'exploration de données à des fins de détection d'intrusion pour fournir de nouveaux outils afin de détecter les cyber-menaces (Luo, 1999;. Dokas et al, 2002; Bloedorn et al, 2001;.. Manganaris et al, 2000; Wu et Zhang, 2003) . Parmi les approches de l'exploration de données, anomalie essais de détection des intrusions de déduire à partir des dossiers atypiques (Lazarevic et al., 2003;. Eskin et al, 2002). Le principe général est en général pour des groupes ou des classes, d'utilisation et de trouver des valeurs aberrantes (à savoir les événements qui ne appartiennent à une classe ou d'un groupe identifiant usage normal). En fait, des valeurs aberrantes objectifs de détection pour trouver des documents qui s'écartent considérablement d'une notion bien définie de la normalité. Il dispose d'un large éventail d'applications, telles que la détection de la fraude pour les cartes de crédit (Aleskerov et al., 1997), les soins de santé, la sécurité informatique (Bloedorn et al., 2001) ou à la sécurité des systèmes critiques (Fujimaki et al., 2005) . Toutefois, le principal inconvénient de la détection des intrusions au moyen d'anomalie (valeurs aberrantes) est le dé- tection taux élevé de fausses alarmes depuis une alarme peut être déclenchée en raison d'un nouveau type d'usages qui n'a jamais été vu auparavant (et est donc considéré comme anormal). Compte tenu de la grande quantité de nouveaux modèles d'utilisation émergents dans les systèmes d'information, même faible pour cent de faux positif donnera une très grande quantité d'alarmes parasites qui seraient overw- pour barrer l'analyste. Par conséquent, l'objectif de cet article est de proposer un algorithme de détection d'intrusion qui est basée sur l'analyse des données d'utilisation provenant de plusieurs partenaires pour la collaboration de détection d'intrusion pour réduire le nombre de fausses alarmes. Notre idée principale est qu'un nouvel usage est susceptible d'être ted rapport aux contexte du système d'information sur lequel il se produit (il ne doit se produire sur ce système). D'autre part, quand un nouveau trou de sécurité a été trouvé sur un système, les pirates veulent l'utiliser en autant de systèmes d'information que possible. Ainsi, une nouvelle anomalie qui se produit sur deux (ou plus) des systèmes d'information est probablement pas un nouveau type d'utilisation, mais plutôt une tentative d'intrusion. Considérons Ax, une anomalie détectée dans l'utilisation du site Web S1 correspondant à une demande de php dans le répertoire personnel pour un nouvel employé: John Doe, qui travaille dans la chambre 204, étage 2, dans le département de R & D. La demande aura la forme suivante: staff.php FName = John \ & LName = Doe \ & chambre = 204 \ & étage = 2 \ & RD = Dpt. Cette nouvelle demande, en raison du recrutement récent de John En raison de ce département, Shoul d pas être consi- dérés comme une attaque. D'autre part, considérons Ay, une anomalie qui correspond à une véritable intrusion. Ay sera basée sur un trou de sécurité du système (par exemple une vulnérabilité php) et pourrait, par exemple, ressembler à: staff.php path = .. / etc / passwd% 00. On peut voir dans cette demande que les paramètres ne sont pas liés aux données accessibles par le script php, mais plutôt à un trou de sécurité qui a été découvert sur le script du personnel. Si deux ou plusieurs entreprises utilisent le même script (par exemple, un script resquesting répertoire acheté à la même société de logiciels), puis l'utilisation de ce trou de sécurité sera certainement répétée d'un système à l'autre et la demande ayant paramètre ../etc/passwd % 00 sera le même pour toutes les victimes. Dans cet article, nous proposons de fournir à l'utilisateur final avec une méthode qui ne prend que un mètre para: n, le nombre d'alarmes souhaitées. Ensuite, en fonction de l'analyse des données d'utilisation provenant des différents partenaires, notre algorithme détecte n valeurs aberrantes communes qu'ils partagent. Ces valeurs aberrantes com- munes sont susceptibles d'être de véritables attaques et déclenchent une alarme. Dans une application réelle de cette technique, la confidentialité préservation sera un enjeu majeur afin de protéger les données des partenaires. Dans cet article, nous nous concentrons sur les techniques de classification et détection des valeurs aberrantes dans un environ- nement distribué. Cependant, les questions de confidentialité dans notre cadre sont présentés dans un autre document actuellement soumis. L'article est organisé de la manière suivante. Dans la section 2, nous présentons la motivation de cette approche et de notre cadre général et de l'article 3 donne un aperçu des œuvres existantes dans ce domaine. La section 4 présente COD, notre méthode de détection de valeurs aberrantes et le déclenchement des alarmes véritables. Même- tuellement, notre méthode est testée à travers une série d'expériences à la section 5 et de l'article 6 donne la conclusion. (. Eskin et al, 2002) 2 Motivation et Principe général IDS Anomaly basé peuvent être divisés en deux catégories; semi-supervisés et non supervisés. Les méthodes semi-supervisés construire un modèle de comportements « normaux » sur le système. Chaque comportement qui n'est pas considéré comme normal est une anomalie et doit déclencher une alarme. Les méthodes non supervisées ne pas utiliser les données étiquetées. En règle générale, basée sur un algorithme de clustering, ils essaient de détecter les valeurs aberrantes et les considèrent comme des anomalies. De toute évidence, anomaly- IDS basé souffriront d'un nombre très élevé de fausses alarmes car un nouveau type de comportement sera considéré comme une anomalie (et une attaque). Dans cet article, nous vous proposons d'améliorer les ré- sultats d'IDS sans supervision au moyen d'un cadre de collaboration entre les différents systèmes à base NETWORK-. La section 3 donne un aperçu des IDS existants sur la base des principes pré- senté ci-dessus et les IDS de collaboration existants. Cependant, au meilleur de nos connaissances, notre proposition est la première IDS sans supervision en utilisant les anomalies communes de multiples partenaires ou- G. Singh et al. der pour détecter les véritables tentatives d'intrusion. L'idée principale de notre proposition est que plusieurs partenaires ne partagent pas les mêmes données, mais ils partagent les mêmes systèmes (le serveur Web peut être Apache ou IIS, le serveur de données peut exécuter Oracle, les scripts d'accéder aux données peuvent être écrites avec PHP ou CGI, etc). Quand un trou de sécurité a été trouvé pour un système (dire des scripts php avec des paramètres spécifiques de conduisant à un accès privilégié au disque dur), cette faiblesse sera la même pour tous les partenaires en utilisant la même technologie. Notre objectif est de réduire le taux de fausses alarmes basées sur cette observation, comme cela est expliqué dans la section 2 Dans cet article, nous présente COD (commune Outlier détection), un cadre et un algorithme destiné à détecter les valeurs aberrantes partagées par au moins deux partenaires dans un IDS de collaboration . Outliers sont généralement de petits groupes. Certaines méthodes utilisées pour les trouver sont présentés à la section 3. Notre objectif est d'utiliser ces listes de valeurs aberrantes de différents systèmes (basés sur un regroupement similaire, impliquant la même mesure de similarité). Si une valeur aberrante se produit pendant au moins deux systèmes, e en elle est considérée comme une attaque. COD est en effet basé sur l'hypothèse selon laquelle une tentative d'intrusion en essayant de trouver une faiblesse d'un script sera similaire pour toutes les victimes de cette attaque. Nous vous proposons de détecter les tentatives d'intrusion dans les archives d'un serveur Web, comme un fichier journal d'accès Apache. fichier conserve un tel enregistrement, pour chaque accès sur le site Web de la propriété intellectuelle, la date, URL demandée et referrer (entre autres informations). Pour la clarté de la présentation, nous présentons notre cadre sur la col- laboration de deux sites Web, S1 et S2 et nous considérons les demandes qui ont été reçues par les scripts de chaque site (cgi, php, sql, etc). Notre objectif est d'effectuer un regroupement sur les habitudes d'utilisation de chaque site et trouver les valeurs aberrantes communes. Cependant, ce ne serait pas suffisant pour répondre à la seconde contrainte de notre objectif: d'exiger qu'un seul paramètre, n, le nombre d'alarmes de retour. Notre mesure de similarité (présentée dans la section 4) permettra des modèles d'utilisation normal d'être regroupés plutôt que groupés avec des motifs d'intrusion. D'autre part, notre mesure simila- rité doit également veiller à distinguer un modèle d'intrusion de modèles d'utilisation normales et d'autres modèles d'intrusion (puisque les différents modèles d'intrusion seront basés sur un autre trou de sécurité et auront des caractéristiques très différentes). Notre algorithme effectue les étapes successives de regroupement pour chaque site. A chaque étape, nous vérifions les valeurs aberrantes correspondant potentiellement entre les deux sites. L'algorithme de clustering est agglomératif et dépend de la dissimila- rité maximale (MD) qui doit être respecté entre deux objets. Ce travail a pour but d'explorer les solutions de surveillance d'un réseau en temps réel. Ensuite, les alarmes potentielles seront déclenchées à chaque étape de la surveillance (par exemple avec une fréquence des une heure). En fonction du nombre d'alarmes vraies ou fausses, l'utilisateur peut vouloir régler n pour l'étape suivante, jusqu'à ce qu'il ne (ou très peu) fausse alarme est renvoyée. Notre hypothèse est que les valeurs aberrantes communes, triées par similitude d'un site à l'autre, donneront les intrusions au début de la liste. Notre défi, dans cet article, est de répondre à des questions importantes sous-jacente notre méthode; quelle est la similitude entre les deux modèles d'utilisation? Comment séparer les grappes afin de donner la liste des valeurs aberrantes? Comment détecter les valeurs aberrantes communes? Notre algorithme principal, ce qui correspond au cadre présenté dans cette section, est donnée à la section 4. 3 Travaux connexes Au fil du temps de nombreuses techniques ont été développées pour détecter les valeurs aberrantes, ce qui conduit à un certain nombre d'enquêtes et d'articles de revue (Hodge et Austin, 2004; Chandola et al., 2008). Certains d'entre eux plus précisément sur le Concentrons thème de la détection des valeurs aberrantes dans le contexte de la détection d'intrusion dans les réseaux informatiques (Lazarevic et al., 2003; Patcha et Park, 2007). Dans cet article, nous nous concentrons sur la détection d'intrusion collaborative ce domaine spécifique et nous vous proposons un système de détection sans supervision basée sur les anomalies. Au contraire des systèmes semi-supervisé détection des anomalies, consistant à décrire viours de comporte- normales pour détecter des motifs déviant (marchette, 1999, Wu et Zhang, 2003; Vinueza et Grudic, 2004), les techniques non supervisées ne nécessitent pas une identification préliminaire des utilisation normale par un expert humain. Notre application sera donc plus utilisable dans un contexte réel. La statistique communautaire a assez largement étudié le concept de outlyingness (Barnett et T. Lewis, 1994; Schölkopf et al, 2001;. Markou et Singh, 2003; Kwitt et Hofmann, 2007). tistique Sta- approche des modèles de distribution construction de probabilité dans lesquelles les valeurs aberrantes sont des objets de faible probabilité (Rousseeuw et Leroy, 1996; Billor et al., 2000; Lee et Xiang, 2001) Toutefois, dans le contexte de la détection d'intrusion, dimensionnalité des données est élevé . Par conséquent, pour améliorer les performances et la précision dans l'ensemble, il est devenu nécessaire de développer des algorithmes d'exploration de données en utilisant toute la distribution des données ainsi que la plupart des fonctionnalités de données (Knorr et Ng, 1998; Breunig et al., 2000; Aggarwal et Yu, 2001). La plupart de ces approches sont basées sur des algorithmes de détection des valeurs aberrantes à base de la mise en grappes (Jain et Dubes, 1988, Ng et Han, 1994;. Ester et al, 1996;. Portnoy et al, 2001; impôt et Duin, 2001; Eskin et al. ., 2002; He et al., 2003; Papadimitriou et al, 2003). Ces techniques reposent sur l'hypothèse que les points normaux appartiennent à des groupes grands et denses en anomalies (ou aberrantes, les instances atypiques) soit ne appartiennent à des groupes (Knorr et Ng, 1998 (Chandola et al., 2008); Ramaswamy et al. 2000;. Duan et al, 2006) ou forment des grappes très petites (ou très rares) (Otey et al., 2003; Chimphlee et al., 2005; Pires et Santos Pereira, 2005. Fan et al, 2006; Ceglar et al., 2007). En d'autres termes de détection des anomalies consiste à identifier ceux parmi les données qui sont loin d'être des groupes importants - soit isolés ou en petits groupes. Au contraire, les techniques d'utilisation abusive (à savoir les approches qui détectent des éléments similaires à usage bien connu malveillant) détecte précisément les attaques, mais ils manqueront chaque intrusion qui diffère de ces signatures d'attaque déjà connues. Par conséquent, certains travaux ont proposé des cadres ratives afin d'collaborations améliorer les performances et les taux d'alarme vrai et faux (Valdes et Skinner, 2001;. Yegneswaran et al, 2004). Ces approches reposent sur la propagation dans une liste noire IP IDS distri- bué après une mauvaise utilisation individuelle ou de détection des anomalies. De plus, cette communication peut conduire à des résultats plus précis, il ne permet pas au système de découvrir totalement inconnus des punaises at- ou pour éviter que les taux d'alarme élevés faux. Pour ces raisons, nous vous proposons dans cet article une approche de détection d'anomalies que les utilisations collaboration entre les systèmes afin de distinguer les attaques de comportements émergents ou utilisation nouvelle, conduisant ainsi à un nombre réduit de fausses alarmes. Selon l'approche, le nombre de paramètres requis pour exécuter l'algorithme peut être élevé et conduira à différentes valeurs aberrantes. Pour éviter cela, certaines œuvres renvoient une liste de classement des valeurs aberrantes potentiels et limiter le nombre de paramètres à préciser (Ramaswamy et al., 2000; Jin et al, 2001;.. Fan et al, 2006). 4 COD: Détection commune Outlier Le principe de COD consiste à effectuer les étapes de regroupement successives sur les modèles d'utilisation des sites partenaires de diffé- rentes, jusqu'à ce que le nombre de valeurs aberrantes communes répond le nombre d'alarmes souhaitées par l'utilisateur. Nous présentons dans cette section un algorithme conçu pour deux systèmes d'information. L'extension de ce travail à plus de deux systèmes nécessiterait un noeud central de coordination les comparaisons et le déclenchement des alarmes, ou un protocole de communication peer-to-peer. Ce n'est pas le but de cet article. Nos objets sont les paramètres donnés aux fichiers de script dans les demandes de G. Singh et al. ceived sur un site Web. En d'autres termes, le fichier journal d'accès est filtré et on ne garde que les lignes correspondant à des requêtes avec des paramètres à un script. Pour chacune de ces lignes, nous séparons les paramètres et pour chaque paramètre, nous créons un objet. Considérons, par exemple, la demande de sui- vantes: staff.php FName = John & LName = Doe. Les objets correspondants sont o1 = John Doe et o2 =. Une fois que les objets sont obtenus à partir des données d'utilisation de plusieurs sites Web, COD est appliquée et donne leurs valeurs aberrantes communes. Algorithme Cod entrée: U1 et U2 les habitudes d'utilisation de sites S1 et S2 et n le nombre d'alarmes. Sortie: I l'ensemble des groupes correspondant à des motifs malveillants. 1. Construction M, la matrice de distance entre chaque motif; 2. ∀p ∈ M, Neighboursp ← liste triée des voisins pour p (le premier motif d'utilisation dans la liste de p est le plus proche de p). 3. DensityList ← liste triée des motifs de densité; 4. MD ← 0; 5. MD ← MD + 0,05; 6. C1 ← Clustering (U1, MD); C2 ← Clustering (U2, MD); 7. O1 ← aberrants (C1); O2 ← aberrants (C2); 8. I ← CommonOutliers (O1, O2, MD); 9. Si | I | ≤ n revenir ensuite I; 10. Si MD = 1 puis retour I; // Pas commun aberrant 11. Sinon retour à l'étape 5; Fin algorithme Cod Comme expliqué en soi ction 2, l'algorithme COD traitera les habitudes d'utilisation des deux sites étape par étape. Pour chaque étape, un résultat de classification est fourni et analysé pour la détection d'intrusion. Tout d'abord, MD est réglé pour obtenir très serré et de nombreux groupes (similitude très courte est autorisée entre deux objets dans un cluster). Ensuite, MD est détendu par une quantité de 0,05 étape après étape afin d'augmenter la taille des clusters résultants, diminuer leur nombre et réduire le nombre d'alarmes. Lorsque le nombre d'alarmes souhaitées par l'utilisateur est atteint, puis se termine COD. 4.1 Clustering Clustering algorithme d'entrée: U, les modèles d'utilisation et MD, la dissemblance maximum. Sortie: C, l'ensemble des grands groupes comme possible, dans le respect MD. 1. i ← 0; C ← ∅; 2. p ← motif suivant non classés dans DensityList; 3. i + +; ci ← p; 4. C ← C + ci; Collaboration de détection d'intrusion 5. q ← motif suivant non classés dans Neighboursp; 6. ∀o ∈ ci Si d (o, q)> MD retourne à l'étape 2; 7. ajouter q ci; 8. Cc ← LCS (Cc, q); // Cc est le centre de C 9. retour à l'étape 5; 10. Si les modèles non classés restent revenir ensuite à l'étape 2; 11. retour C; Fin algorithme Clustering COD Clustering algorithme est basé sur un principe agglomératif. L'objectif est d'augmenter le volume des clusters en ajoutant des objets candidats, jusqu'à ce que la dissemblance maximale (MD) est cassé (par exemple il y a un objet Oi dans le cluster de telle sorte que la similitude entre oi et l'objet candidat OC est supérieur à MD). Similitude entre les objets. Nous considérons chaque objet comme une séquence de caractères. Notre simila- rité est alors basée sur la plus longue séquence commune (LCS), tel que décrit dans la définition 1. Définition 1 Soit S1 et S2 deux séquences. Laissez LCS (s1, s2) la longueur des plus longues séquences communes entre s1 et s2. La différence d (s1, s2) entre s1 et s2 est défini comme suit: d (s1, s2) = 1 - 2 × LCS (s1, s2) | s1 | + | S2 | Exemple 1 Considérons deux paramètres p1 = intrusion et p2 = induction. Le LCS entre P1 et P2 est L = inuion. L a une longueur de 6 et de la similitude entre P1 et P2 est d = 1 - 2 × L | p1 | + | p2 | = 33,33%. Ce qui signifie aussi une similitude de 77,77% entre les deux paramètres. Centre des clusters. Lorsqu'un objet est inséré dans un cluster, nous maintenons le centre de ce groupe, car il sera utilisé dans l'algorithme de CommonOutliers. Le centre d'une grappe de C est le LCS entre tous les objets de C. Lorsque oi d'objet est ajouté à C, son Cc central est mis à jour. La nouvelle valeur de Cc est la LCS entre la valeur actuelle de Cc et Oi. 4.2 Wavelet basée sur Outlier Détection La plupart des travaux antérieurs dans la détection des valeurs aberrantes nécessite un paramètre (Zhong et al, 2007;. Portnoy et al, 2001;.. Joshua Oldmeadow et al, 2004), comme un pour cent des petits groupes qui devraient être considérés comme valeurs aberrantes ou les valeurs aberrantes top-n. Leur idée principale est généralement pour trier les grappes par taille et / ou l'étanchéité. Nous estimons que nos groupes seront aussi serrés que possible, selon notre algorithme de regroupement et nous voulons extraire des valeurs aberrantes par le tri du cluster par taille. Le problème est de séparer « grands » et « petits groupes ». Notre solution est basée sur une analyse de la distribution de cluster, une fois qu'ils sont classés par taille. La répartition habituelle des grappes est illustrée par la figure 1 (capture d'écran faite avec nos données réelles). Nous vous proposons d'utiliser une transformée en ondelettes pour réduire la distribution. Dans la figure 1, l'axe y représente la taille des clusters, alors que leur index dans la liste triée est représentée sur x, et les deux plateaux permettent la séparation des grappes de petits et grands. La transformée en ondelettes est un outil qui coupe des données ou des fonctions ou des opérateurs en différentes composantes de fréquence, puis les études de chaque composant avec une résolution adaptée à l'échelle G. Singh et al. FIGUE. 1 - Détection de valeurs aberrantes au moyen de Haar ondelettes (Daubechies, 1992). Mathématiquement, la transformée en ondelettes continue est définie par: Twavf (a, b) = ∫ 1√ un + ∞ -∞ f (x) ψ * (x-ba) dx où z * désigne le conjugué complexe de z, ψ * ( x) est l'analyse ondelettes, un (> 0) est le paramètre d'échelle et b le paramètre de translation. Nous utilisons les vaguelettes Haar pour illustrer notre méthode de détection des valeurs aberrantes. Considérons les séries de valeurs suivantes: [1, 1, 1, 2, 7, 10, 11, 12]. Sa transformée de Haar ondelettes est illustrée par le tableau suivant: Niveau Approximations Coefficients 8 1, 1, 1, 2, 7, 10, 11, 12 4 1, 1,5, 8,5, 11,5 0, -0,5, -1,5, -0,5 2 1,25 , 10 -0,25, -1,5 1 5,625 -4,375 Ensuite, nous gardons seulement les plus deux coefficients significatifs et nous faisons les autres zéro. Dans notre série de coefficients ([5,625, -4, 375, -0,25, -1,5, 0, -0,5, -1,5, -0,5]) plus deux sont ceux Si- gnificant 5,625 et -4, 375, ce qui signifie que le série devient [5,625, -4, 375, 0, 0, 0, 0, 0, 0]. Dans l'étape suivante, l'opération inverse est calculée et l'on obtient une approximation des données originales [1,25, 1,25, 1,25, 1,25, 10,0, 10,0, 10,0, 10,0]. Cela nous donne deux plateaux et permet de couper la série après index 4 afin de séparer les grandes et les petites valeurs. 4.3 En comparant Outliers Puisque nous voulons que notre algorithme global d'exiger un seul paramètre (le nombre d'alarmes), nous voulons éviter d'introduire un degré de similarité pour comparer deux listes de valeurs aberrantes. Pour cette comparaison, l'algorithme de CommonOutliers utilisera le centre de valeurs aberrantes. Pour chaque paire de valeurs aberrantes, CommonOutliers calcule la similitude entre les centres de ces valeurs aberrantes. Si cette larité est inférieure à simi- MD en cours (CF. paragraphe 4.1), nous considérons ces valeurs aberrantes comme similaires et les ajouter à la liste des alarmes. Algorithme CommonOutliers Entrée: O1 et O2, deux listes de valeurs aberrantes et MD, la dissemblance maximale. Sortie: A, la liste des alarmes (valeurs aberrantes communes). 1. ← ∅ 2. ∀i ∈ O1 faire la détection d'intrusion Collaborative 3. ∀j ∈ Oj faire le centre de 4. centrei (i); 5. centre de centrej (j); 6. Si d (centrei, centrej) <MD Puis un ← A + i ∪ j; 7. fait; 8. fait; 9. Retour A; Mettre fin à l'algorithme CommonOutliers 5 expériences L'objectif de cette section est d'analyser nos résultats (à savoir le nombre de valeurs aberrantes et véritables intrusions et le genre d'intrusions, nous avons détecté). Nos ensembles de données proviennent de deux organismes de recherche différents; (Anonymisées pour la soumission). Nous avons analysé les fichiers journaux d'accès Web à partir du 1er Mars au 31 Mars Le premier fichier journal représente 1,8 Go de données brutes. Dans ce fichier, le nombre total d'objets (paramètres donnés aux scripts) est 30454. Le second fichier journal repré- sente 1,2 Go de données brutes et le nombre total d'objets est 72381. COD a été écrit en Java et C ++ sur un PC (2,33 GHz i686) sous Linux avec 4 Go de mémoire principale. Les paramètres qui sont générés automatiquement par les scripts ont été supprimés des ensembles de données, car ils ne peuvent pas correspondre à des attaques (par exemple « publications.php? Catégorie = Livres »). Cela peut se faire en énumérant toute la génération possible des paramètres dans les scripts d'un site Web. Comme cela est décrit à la section 2, COD procède par étapes et augmente lentement la valeur de MD, ce qui représente une valeur de tolérance lors du regroupement des objets au cours du processus de regroupement. Dans nos expériences, MD a été augmenté par étapes de 0,05 de 0,05 à 0,5. Pour chaque étape, nous présentons nos mesures dans le tableau 1. La signification de chaque mesure est la suivante. O1 (resp. O2) est le nombre d'objets périphériques dans le site 1 (resp. Du site 2). % 1 (resp% 2) est la fraction d'objets périphériques sur le nombre d'objets dans le site 1 (resp du site. 2). Par exemple, lorsque MD est réglé sur 0,3, pour le site 1, nous avons 5.607 objets périphériques, ce qui représente 18,4% du nombre total d'objets (par exemple 30454) dans le site 1. COD est le nombre de valeurs aberrantes communes entre les deux sites et FA% est le pourcentage de fausses alarmes dans les valeurs aberrantes communes. Par exemple, lorsque MD est réglé à 0,05, on trouve 101 alarmes dont 5 sont fausses (qui représente 4,9%). Une première obser- vation est que les valeurs aberrantes ne peuvent pas être directement utilisés pour déclencher des alarmes. De toute évidence, un nombre aussi élevé que 5607 alarmes pour vérifier, même pendant un mois, n'est pas réaliste. D'autre part, les résultats de COD montrent sa capacité à séparer les comportements malveillants de l'utilisation normale. Nos fausses alarmes correspondent aux demandes normales qui sont communes aux deux sites, mais rarement. un des documents de requête peut user de « John Doe » et la demande sera publications.php? FName = John \ & LName = Doe Par exemple, sur le script d'interrogation des références de anonym_lab1,. Si un autre utilisateur demande des papiers de « John Rare » sur le site Web de anonym_lab2), la demande sera biblio.php? FName = John \ & LName = Rare et le paramètre « John » sera donnée en tant que valeur aberrante commune et déclencher une alarme . Comme on peut le voir,% FA est très faible (en général, nous avons au plus 5 fausses alarmes dans nos expériences pour les deux sites Web) par rapport aux milliers de valeurs aberrantes G. Singh et al. 0,05 0,1 0,15 0,2 0,25 0,3 0,35 0,4 0,45 0,5 O1 13197 10860 8839 7714 6547 5607 5184 4410 3945 3532% 1 43,3% 35,6% 29% 25,3% 21,5% 18,4% 17% 14,4% 12,9% 11,6% O2 35983 27519 24032 20948 18152 14664 12738 11680 10179 8734% 2 49,6% 37,9% 33,1% 28,9% 25% 20,2% 17,5% 16,1% 14% 12,1% COD 101 78 74 70 67 71 71 85 89 90% FA 4,9% 5,12% 4% 2,85% 1,5% 2,8 % 2,8% 10,6% 11,2% 16,6% TAB. 1 - Les résultats sur des données réelles ont été filtrés par COD. Une autre leçon de ces expériences est qu'un faible MD implique très petits groupes et de nombreuses valeurs aberrantes. Ces valeurs aberrantes sont partagées entre les deux sites, parmi lesquels certains sont fausses alarmes dues aux rares mais le bon usage normal. Lorsque MD augmente, le processus de regroupement devient plus agglomératif et les alarmes sont regroupées. Ensuite, une alarme peut couvrir plusieurs ceux du même genre (par exemple le cas d'oeufs de Pâques a expliqué plus loin). En même temps, le nombre de valeurs aberrantes correspondant à une baisse d'utilisation normale (car ils sont également regroupés). Finalement, une valeur trop importante de MD implique des groupes de construction qui ne font pas vraiment de sens. Dans ce cas, les valeurs aberrantes sera plus grande, et les critères correspondants sera trop tolérant, ce qui conduit à un grand nombre de valeurs aberrantes correspondant capturant une utilisation normale. Dans un environnement continu impliquant les données réelles de ces expériences, on pourrait décider de conserver 70 comme le nombre d'alarmes souhaitées et de regarder le rapport des fausses alarmes. Si ce ratio diminue, l'utilisateur final doit envisager d'augmenter le nombre d'alarmes souhaitées. 6 Conclusion Dans cet article, nous avons proposé i) un système de classification non supervisée pour isoler les comportements cal atypi-, ii) une méthode de détection des valeurs aberrantes parameterless basée sur vaguelettes et iii) une nouvelle fonctionnalité pour caractériser les intrusions. Cette nouvelle fonctionnalité est basée sur la répétition d'une tentative d'intrusion d'un système à l'autre. En fait, nos expériences montrent que les comportements atypiques ne peuvent pas être directement utilisés pour déclencher des alarmes puisque la plupart d'entre eux correspondent à des demandes normales. D'autre part, ce très grand nombre de valeurs aberrantes peut être filtrée (réduire la quantité de comportements atypiques jusqu'à 0,21%) de manière efficace afin de trouver de véritables tentatives d'intrusion (ou punaises at-) si l'on considère plus d'un site. Finalement, notre méthode garantit un très faible taux de fausses alarmes, rendant ainsi le regroupement sans supervision pour la détection d'intrusion efficace, réaliste et réalisable. Aggarwal Références, C. C. et P. S. Yu (2001). Outlier détection de données de grande dimension. SIGMOD 30 dossiers (2), 37-46. Aleskerov, E., B. Freisleben, et B. Rao (1997). Cardwatch: Un système d'exploitation de base de données sur réseau de neurones pour la détection de la fraude par carte de crédit. Dans IEEE CIFE. Barnett, V. et T. T. Lewis (1994). Outliers dans les données statistiques. John Wiley & Sons. Détection d'intrusion collaborative Billor, N., A. S. Hadi, et P. F. Velleman (2000). BACON: bloqué proposants aberrantes adaptatives efficaces informatiquement. Statistiques de calcul et d'analyse des données 34, 279-298. Bloedorn, E., A. D. Christiansen, W. Hill, C. Skorupka et L. M. Talbot (2001). L'exploration de données pour la détection d'intrusion réseau: Comment commencer. Rapport technique, MITRE. Breunig, M. M., H.-P. Kriegel, R. T. Ng, J. et Sander (2000). Lof: identifier densité-bas ed valeurs aberrantes locales. SIGMOD 29 dossiers (2), 93-104. Ceglar, A., J. F. Roddick, et D. M. W. Powers (2007). Curio: Une valeur aberrante rapide et algorithme de détection des valeurs aberrantes assorti- ment pour les grands ensembles de données. En 2e Atelier international sur l'intégration de l'intelligence artificielle et Data Mining, pp. 37-45. Chandola, V., A. Banerjee, et V. Kumar (2008). détection d'anomalies - une enquête. ACM Computing Surveys Pour apparaître, Pour apparaître. Chimphlee, W., A. H. Abdullah, M. N. Md Sap, et S. Chimphlee (2005). Ano détection Maly sans supervision avec des données non marquées en utilisant le regroupement. Dans la conférence internationale sur les informa- tions et de la technologie de communication. Daubechies, I. (1992). Dix conférences sur vaguelettes. Philadelphia, PA, USA: Société de mathématiques appliquées et industrielles. Dokas, P., L. Ertoz, V. Kumar, A. Lazarevic, J. Srivastava, et P. Tan (2002). L'exploration de données pour la détection d'intrusion réseau. NSF Atelier sur la prochaine génération de Data Mining. Duan, L., D. Xiong, J. Lee, et F. Guo (2006). Un algorithme de regroupement spatial sur la base densité locale de bruit. IEEE Conférence internationale sur les systèmes, l'homme et Cybernétique. Eskin, E., A. Arnold, M. Prerau, L. Portnoy, et S. Stolfo (2002). Un cadre géométrique pour la détection d'anomalie non supervisée: intrusions détecter dans les données non marquées. Applications de l'exploration de données dans la sécurité informatique, 333-342. Ester, M., H.-P. Kriegel, J. Sander, et X. Xu (1996). Un algorithme basé sur la densité pour découvrir les clusters dans les grandes bases de données spatiales avec le bruit. En 2ème Conférence internationale sur la découverte de connaissances et d'exploration de données, p. 226-231. Fan, H., O. R. Zaiane, A. Foss, et J. Wu (2006). Un non paramétrique détection des valeurs aberrantes pour découvrir effica- vement le plus n valeurs aberrantes à partir de données de conception. En PAKDD. Fujimaki, R., T. Yairi, et K. Machida (2005). Une approche de problème de détection d'anomalie de l'engin spatial en utilisant l'espace de fonctionnalité du noyau. Dans 11 SIGKDD. Il, Z., X. Xu, et S. Deng (2003). Découverte de valeurs aberrantes locales à base de cluster. Motif Lettres de reconnaissance 24, 1641-1650. Hodge, V. et J. Austin (2004). Une enquête sur les méthodes de détection des valeurs aberrantes. gence artificielle Intelli Review 22, 85-126. Jain, R. K. et R. C. Dubes (1988). Les algorithmes de clustering de données. Prentice-Hall, Inc. Jin, W., A. K. H. Tung, et J. Han (2001). Mines haut n valeurs aberrantes locales dans les grandes bases de données. 7e conférence internationale SIGKDD sur la découverte des connaissances et l'exploration de données, p. 293-298. Joshua Oldmeadow, J., S. Ravinutala, et C. Leckie (2004). regroupement adaptatif pour la détection d'intrusion réseau. Dans 8 PAKDD. G. Singh et al. Knorr, E. M. et R. T. Ng (1998). Les algorithmes pour l'extraction des valeurs aberrantes basés sur la distance dans les grands ensembles Data-. En 24e Conférence internationale sur les très grandes bases de données, pp. 392-403. Kwitt, R. et U. Hofmann (2007). détection des anomalies dans le trafic non surveillé réseau au moyen de pca robuste. Dans Multi-Conférence internationale sur l'informatique dans le Global Information Technology. Lazarevic, A., L. Ertoz, V. Kumar, A. Ozgur, et J. Srivastava (2003). Une étude comparative des systèmes de détection d'anomalies dans la détection d'intrusion réseau. En 3ème SIAM DM. Lee, W. et D. Xiang (2001). Mesures d'information pour la théorie de détection des anomalies. Dans IEEE Symposium sur la sécurité et la confidentialité. Luo, J. (1999). L'intégration de la logique floue à des méthodes d'extraction de données pour la détection d'intrusion. Manganaris, S., M. Christensen, D. Zerkle, et K. Hermiz (2000). Une analyse de l'exploration de données des alarmes ITR. Réseaux informatiques 34, 571-577. Marchette, D. (1999). Une méthode statistique pour établir le profil du trafic réseau. Dans le 1er USENIX TRAVAiL boutique de détection d'intrusion et de surveillance réseau, p. 119-128. Markou, M. et S. Singh (2003). Détection de nouveauté: un examen - Partie 1: approches statistiques. Traitement du signal 83, 2481-2497. Ng, R. T. et J. Han (1994). méthodes de classification efficace pour l'extraction de données spatiales. Dans la 20ème Conférence internationale sur les très grandes bases de données, pp. 144-155. Otey, M., S. Parthasarathy, A. Ghoting, G. Li, S. Narravula, et D. Panda (2003). Vers Intrus base nic- détection d'ions. 9e conférence internationale SIGKDD sur la découverte des connaissances et l'exploration de données, p. 723-728. Papadimitriou, S., H. Kitagawa, P. Gibbons, et C. Faloutsos (2003). LOCI: détec- tion rapide aberrante en utilisant l'intégrale de corrélation locale. Dans 19 CIED. Patcha, A. et J.-M. Park (2007). Une vue d'ensemble des techniques de détection d'anomalies: Les solutions existantes et des dernières tendances technologiques. Comput. Réseaux 51, 3448-3470. Pires, A. et C. Santos-Pereira (2005). En utilisant le regroupement et estimateurs robustes pour détecter les valeurs aberrantes dans les données à plusieurs variables. Dans la Conférence internationale sur les statistiques robustes. Portnoy, L., E. Eskin, et S. Stolfo (2001). Détection d'intrusion avec des données non marquées à l'aide de l'anneau cluste-. ACM CSS Atelier sur l'exploration de données appliquée à la sécurité. Ramaswamy, S., R. Rastogi, et K. Shim (2000). Des algorithmes efficaces pour les valeurs aberrantes extraction de grands ensembles de données. SIGMOD 29 dossiers (2), 427-438. Rousseeuw, P. et A. M. Leroy (1996). La régression robuste et Outlier détection. Wiley-IEEE. Schölkopf, B., J. Platt, J. Shawe-Taylor, A. Smola, et R. W. (2001). L'estimation du soutien de la distribution de grande dimension. Neural Computation 13, 1443-1471. Impôt, D. M. J. et R. P. W. Duin (2001). La combinaison de classificateurs d'une classe. Lecture Notes in Computer Science 2096, 299-317. Valdes, A. et K. Skinner (2001). corrélation d'alertes probabilistes. Dans les progrès récents en matière de détection d'intrusion, pp. 54-68. Vinueza, A. et G. Grudic (2004). Unsupervised détection des valeurs aberrantes et l'apprentissage semi-supervisé. Rapport technique CU-CS-976-04, Univ. du Colorado à Boulder. Collaboration de détection d'intrusion Wu, N. et J. Zhang (2003). L'analyse factorielle basée détection des anomalies. Dans l'atelier IEEE sur l'assurance de l'information. Yegneswaran, V., P. Barford, et S. Jha (2004). la détection globale d'intrusion dans le système de recouvrement de domino. En réseau et Symposium de sécurité distribuée. Zhong, S., T. M. Khoshgoftaar, et N. Seliya (2007). intrusion réseau basé détec- tion Clustering. International Journal de la fiabilité, la qualité et l'ingénierie de sécurité 14, 169-187. Résumé La détection d'intrusion un domaine is important de la sécurité verser des Systèmes d'Informa- tion. Les Systèmes de détection d'intrusion (IDS) Les plus sur la utilisés reposent de signatures et détection Ontario Besoin de jour à Fréquentes mises verser un Défendre les contre Système nouvelles Attaques. D'un Autre Côté, la d'détection may Compenser CE anomalie Besoin, de Mais compositions provoquent Fausses alarmes Nombreuses. En effet, un comportement de Devie Qui sig- nificative des Manière Comportements sérums habituels par Considéré Comme un IDS dangereux les anomalies Utilisant. Isoler les intrusions Véritables Dans un ensemble d'un Fait is alarmes importante défi pour tous, IDS. Dans this article, nous considérons Une nouvelle characteristic verser les isoler Comportements malicieux. This is characteristic sur their possible basée d'un redites d'information à Système Un autre. Nous proposons un nouveau Principe de détection des objets Atypiques et le validons PAR UNE Série d'Expérimentations."
738,Revue des Nouvelles Technologies de l'Information,EGC,2009,Constraint Programming for Data Mining,,Luc De Raedt,http://editions-rnti.fr/render_pdf.php?p1&p=1000731,http://editions-rnti.fr/render_pdf.php?p=1000731,en,"(Actes_non_num \ 351rotes.pdf) La programmation par contraintes pour l'exploration de données Luc De Raedt Département des sciences informatiques Katholieke Universiteit Leuven, Belgique luc.deraedt@cs.kuleuven.be Dans cet exposé, j'explore la relation entre l'exploitation minière basée sur les contraintes et la programmation par contraintes . Je montrerai en particulier, comment les contraintes typiques utilisées dans le secteur minier de modèle peuvent être formulés pour une utilisation dans des environnements de programmation par contraintes. Le cadre résultant est étonnamment souple et permet de combiner un large éventail de contraintes minières de façon de dif- férents. L'approche est mise en œuvre dans les systèmes de programmation par contraintes impromptu et évalué de manière empirique. Les résultats montrent que l'approche est non seulement très expressif, mais fonctionne aussi bien sur les problèmes de référence complexes. En plus de fournir un compte rendu détaillé de nos premiers résultats réels pour l'extraction de l'élément-ensemble, je soutiendrai également que l'utilisation des techniques et des méthodes de programmation par contraintes fournit un paradigme nouveau et intéressant pour l'exploration de données. Le travail que je ferai un rapport sur un travail conjoint avec Tias armes à feu et Siegfried Nijssen. Références De Raedt, L., T. Guns et S. Nijssen (2008). La programmation par contraintes pour l'exploitation minière itemset. Dans Proc. du SIGKDD."
765,Revue des Nouvelles Technologies de l'Information,EGC,2009,Handling Texts ? A Challenge for Data Mining,"The amount of data in free form by far surpasses the structured records in databases in theirnumber. However, standard learning algorithms require observations in the form of vectorsgiven a fixed set of attributes. For texts, there is no such fixed set of attributes. The bag ofwords representation yields vectors with as many components as there are words in a language.Hence, the classification of documents represented as bag of word vectors demands efficientlearning algorithms. The TCat model for the support vector machine (Joachims 2002) offers asound performance estimation for text classification.The huge mass of documents, in principle, offers answers to many questions and is oneof the most important sources of knowledge. However, information retrieval and text classi-fication deliver merely the document, in which the answer can be found by a human reader ?not the answer itself. Hence, information extraction has become an important topic: if we canextract information from text, we can apply standard machine learning to the extracted facts(Craven et al. 1998). First, information extraction has to recognize Named Entities (see, e.g.,Roessler, Morik 2005). Second, relations between these become the nucleus of events. Ex-tracting events from a complex web site with long documents allows to automatically discoverregularities which are otherwise hidden in the mass of sentences (see, e.g., Jungermann, Morik2008).",Katharina Morik,http://editions-rnti.fr/render_pdf.php?p1&p=1000732,http://editions-rnti.fr/render_pdf.php?p=1000732,en,"(Actes_non_num \ 351rotes.pdf) Manipulation de textes? Un défi pour l'exploration de données Katharina Morik Université technique de Dortmund Département Informatique VIII 44221 Dortmund (Allemagne) katharina.morik@uni-dortmund.de La quantité de données sous forme libre dépasse largement le dossiers structurés dans des bases de données de leur nombre. Cependant, les algorithmes d'apprentissage standards exigent des observations sous forme de vecteurs donnés un ensemble fixe d'attributs. Pour les textes, il n'y a pas ensemble d'attributs fixes. Le sac donne de la représentation des mots avec des vecteurs autant de composants que de mots dans une langue. Par conséquent, la classification des documents représentée comme sac de vecteurs de mots exige des algorithmes d'apprentissage efficaces. Le modèle TCAT pour la machine à vecteur de support (Joachims 2002) offre une estimation de la performance sonore pour la classification texte. La masse énorme de documents, en principe, apporte des réponses à de nombreuses questions et est l'une des sources les plus importantes de la connaissance. Cependant, la recherche d'information et le texte clas- sement fournissent simplement le document, dans lequel la réponse peut être trouvée par un lecteur humain? pas la réponse elle-même. Par conséquent, l'extraction de l'information est devenue un sujet important: si nous pouvons extraire des informations de texte, nous pouvons appliquer une machine standard d'apprentissage aux faits extraits (Craven et al., 1998). Tout d'abord, l'extraction de l'information doit reconnaître les entités nommées (voir, par exemple, Roessler, Morik 2005). D'autre part, les relations entre ceux-ci deviennent le noyau d'événements. événements tractantes d'un site Ex- web complexe avec de longs documents permet de découvrir automatiquement régularités qui sont par ailleurs cachés dans la masse des phrases (voir, par exemple, Jungermann, Morik 2008). Références Craven, M., D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam et S. Slattery (1998). Apprendre à extraire des connaissances du world wide web. Dans Proc. de la Conférence nationale 1998 sur l'intelligence artificielle. Joachims, T. (2002). Apprendre à Classifier texte à l'aide Support Vector Machines. Kluwer. Jungermann, F. et K. Morik (2008). Des services améliorés pour la recherche d'information ciblée par extraction d'événements et l'exploration de données. Dans Proc. de la 13e Conférence internationale sur les applications de langage naturel pour les systèmes d'information NLDB. Marc Roessler, K. M. (2005). L'utilisation des textes non marqués pour la reconnaissance d'entités nommées. Dans Proc. de l'atelier sur ICML multiples Vue d'apprentissage."
770,Revue des Nouvelles Technologies de l'Information,EGC,2009,"Logiciel « DtmVic » Data and Text Mining: Visualisation, Inférence, Classification",,Ludovic Lebart,http://editions-rnti.fr/render_pdf.php?p1&p=1000815,http://editions-rnti.fr/render_pdf.php?p=1000815,en,"(Actes_non_num \ 351rotes.pdf) Logiciel «DtmVic» Data et Text Mining: Visualisation, Inférence, Classification Ludovic Lebart-Telecom ParisTech, 46 rue Barrault, 75013, Paris ludovic@lebart.org~~V~~plural~~3rd 1 Description Ce logiciel Brève is à la visualisation consacré des Donnees multidimensionnelles, Qué bureaux Données Numériques Soient, OU nominales textuelles. Les limites de la Version: 22 actuelle 500 Sont (INDIVIDUS, lignes observations), 1000 (variables NUMERIQUES colonnes, les variables nominales - Variable = Une Nominale juin), 100 Colonne 000 characters Pour Les d'un Réponses textuelles Individu. Pour ce faire, dix-huit de la base des enchaînements à l'Sont propose Utilisateur. On en deux décrira. De l'exemple enchaînement «PCA»: Analyse en Composantes Principales, la classification des INDIVIDUS en k classes, la description des cours Automatique Par les actifs des variables Illustratives ET. Le volet «VIC» (visualisation, Inférence, Classification) d'obtain Përmet des Alors Graphiques, des zones de confiance bootstrap, des Cartes auto-organisées, etc. de l'Exemple enchaînement «VISUTEX»: Analyse des correspondances de la Table de contingence les tranches de viande textes (de la base Données) et les mots les plus de fréquente, des mots characteristics textes, phrases characteristics ous lignes des textes. Sériation de la Table (re-ordonnancements des lignes et des colonnes). À partir Mêmes du compléments VIC volet. 2 Spécificité, Le Domaine d'Accès demande «coeur de cible» EST «le treatment des statistique des enquêtes des questions fermees comportant et ouvertes». ! Complémentarité Systématique des techniques de visualisation (Analyser en Composantes Principales, Analyse des Correspondances simples et multiples) et de la classification automatique (méthode mixte Combinant classification Hiérarchique [critere de Ward] centres et mobiles [k-means]; Cartes auto-organisées de Kohonen ). ! Validation des techniques de visualisation: Ré-sampling (bootstrap, PARTIEL bootstrap, au total bootstrap, bootstrap variables). Sur ! Mise en oeuvre des methods d'Analyse de contiguïté et methods connexes. ! Prétraitement de texte (de la langue indépendant): Fusions et de répressions mots. La version of this present is Accompagnée logiciel d'académique Une 27 batterie de jeux de Données. [Les premiers ministres Treize d'exemples de un tutoriel intégré l'application de Sont au logiciel commentés]. Pour la partie numérique, l'ouvrage de référence Est: «Exploratoire Statistique multidimensionnelle - Visualisation et Inférence en Fouilles de Données» par L. Lebart, M. Piron, A. Morineau, Dunod, 2006. Pour la partie textuelle: «Statistique textuelle »par L. Lebart et A. Salem, Dunod, 1994, en pdf téléchargeable à partir du site. Libre de DtmVic Téléchargement: (version 4.1 de DTM) Site www.lebart.org."
776,Revue des Nouvelles Technologies de l'Information,EGC,2009,Online and Adaptive Anomaly Detection: Detecting Intrusions in Unlabelled Audit Data Streams,,"Wei Wang    , Thomas Guyet, Rene Quiniou, Marie-Odile Cordier, Florent Masseglia",http://editions-rnti.fr/render_pdf.php?p1&p=1000802,http://editions-rnti.fr/render_pdf.php?p=1000802,en,
778,Revue des Nouvelles Technologies de l'Information,EGC,2009,Privacy and Data Mining: New Developments and Challenges,"There is little doubt that data mining technologies create new challenges in the area of dataprivacy. In this talk, we will review some of the new developments in Privacy-preserving DataMining. In particular, we will discuss techniques in which data mining results can reveal per-sonal data, and how this can be prevented. We will look at the practically interesting situationswhere data to be mined is distributed among several parties. We will mention new applica-tions in which mining spatio-temporal data can lead to identification of personal information.We will argue that methods that effectively protect personal data, while at the same time pre-serve the quality of the data from the data analysis perspective, are some of the principal newchallenges before the field.",Stan Matwin,http://editions-rnti.fr/render_pdf.php?p1&p=1000730,http://editions-rnti.fr/render_pdf.php?p=1000730,en,"(Actes_non_num \ 351rotes.pdf) Vie privée et Data Mining: nouveaux développements et défis Stan Matwin École de technologie et du génie de l'information (SITE) Université d'Ottawa (Canada) stan.matwin@live.com Il y a peu de doute que les données des technologies minières créent de nouvelles défis dans le domaine de la protection des données. Dans cet exposé, nous allons passer en revue quelques-uns des nouveaux développements dans Protection des données préservant les mines. En particulier, nous discuterons des techniques dans lesquelles les résultats d'exploration de données peuvent révéler des données per- sonnelle, et comment cela peut être évité. Nous examinerons les situations pratiquement intéressantes où les données à extraire est réparti entre plusieurs parties. Nous mentionnerons de nouvelles applications dans lesquelles les données spatio-temporelles minières peuvent conduire à l'identification des renseignements personnels. Nous soutiendrons que les méthodes qui effectivement protéger les données personnelles, tout en même temps avant la qualité servent des données du point de vue de l'analyse des données, sont quelques-uns des principaux nouveaux défis avant le terrain."
779,Revue des Nouvelles Technologies de l'Information,EGC,2009,Probabilistic Multi-classifier by SVMs from voting rule to voting features,,"Anh Phuc Trinh, David Buffoni, Patrick Gallinari",http://editions-rnti.fr/render_pdf.php?p1&p=1000790,http://editions-rnti.fr/render_pdf.php?p=1000790,en,"(Actes_non_num \ 351rotes.pdf) probabilistes multi-classificateur par SVM de la règle de vote au vote dispose Anh Phuc TRINH, David BUFFONI, Patrick Gallinari * * Laboratoire d'Informatique de Paris 6 104, avenue du Président Kennedy, 75016 Paris. {Anh-phuc.trinh, david.buffoni, patrick.gallinari} @ lip6.fr, 1 multi-classificateur probabilistes par SVM Définition des probabilités a posteriori pour le problème Soit multiclassent S = {(x1, y1), (x2, y2) ,. . . , (Xm, YM)} un ensemble d'exemples de formation m. Nous supposons que chaque exemple xi est tracé à partir d'un domaine X ∈ R n et chaque classe yi est un nombre entier de la série Y = {1,. . . , K} k> 2. Les probabilités postérieures du problème est multiclassent une probabilité conditionnelle de chaque classe y ∈ Y donné une instance x P (y = i | x) = pi (1) sous réserve kΣ i = 1 pi = 1 pi> 0 ∀i (2) Il existe deux approches, soit un contre-un ou un contre-repos, dans la résolution de la pro- blème multi-classe par SVM. Suite à la mise de l'approche d'un contre-un, nous avons la méthode de vote proposée par (Taxes, 2002) à l'aide de valeurs de décision (x la) de SVM pour estimer si les probabilités a posteriori. Un autre procédé de (Wu T-F, 2004) obtient pi de la probabilité de paires de (Platt, 2000). 2 De règle de vote à vote dispose Définition du vote comporte suppose que S = {(x1, y1), (x2, y2),. . . , (Xm, ym)} est l'ensemble des m exemples de formation élaborés à partir d'une distribution indépendante et identique. Une représentation de la fonction de vote Θ: C (fij (x)) × Y → B d est une Θ fonction qui fait correspondre une configuration de valeurs de décision c (fij (x)) ⊂ C (fij (x)) et une classe yi ∈ Y pour un vecteur caractéristique de dimension d, donc l'ensemble des fonctions de vote est notée VF. Les probabilités postérieures DÉFINIES sur l'ensemble des caractéristiques vote VF pi = P (y = i | x, λ) = exp (Pd l = 1 λl × Θl (x, y = i)) P ky = 1 exp (P dl = 1 × λl Θl (x, y)) est estimé à maximiser le logarithme de la hotte likeli- conditionnelle (Nigam et McCallum, 1999) et est résolu par problème d'optimisation non contrainte. Probabilistes multi-classificateur par SVM Fig. 1 - taux de précision de trois méthodes différentes sur sept jeux de données de test UCI et deft08 sont obtenus en utilisant le noyau polynomiale; La règle de vote, notre et les méthodes de Wu sont figurés respectivement par le violet, les colonnes rouge et jaune 3 expériences pour comparer les performances de notre méthode avec les autres, nous avons sélectionné sept ensembles de données à partir du référentiel de données d'apprentissage UCI 1, et l'ensemble de données DEFT08 2. Nigam Références, J. K. L. et A. McCallum (1999). En utilisant l'entropie maximale pour la classification texte. IJCAI-99 Atelier sur l'apprentissage automatique d'information sur le filtrage 1, 61-67. Platt, J. (2000). sorties probabilistes pour machines à vecteurs supports et comparaisons régulière- Ized méthodes de vraisemblance. Les progrès de la grande marge classificateurs 14, 61-74. Fiscale, D. R. P. W. D. (2002). En utilisant classificateurs à deux classes pour la classification multiclassent. Conférence Inter- tional sur la reconnaissance des formes 2, 124- 127. Wu T-F, Chih-Jen Lin, R. C. W. (2004). Les estimations de probabilité de classification multi-classe par couplage par paires. Conférence internationale sur la reconnaissance des formes 5, 975-1005. 1http: //mlearn.ics.uci.edu/MLSummary.html 2http: //deft08.limsi.fr/"
799,Revue des Nouvelles Technologies de l'Information,EGC,2008,A spatial rough set for extracting the periurban fringe,"To date the availability of spatial data is increasing together withtechniques and methods adopted in geographical analysis. Despite this tendency,classifying in a sharp way every part of the city is more and more complicated.This is due to the growth of city complexity. Rough Set theory maybe a useful method to employ in combining great amounts of data in order tobuild complex knowledge about territory. It represents a different mathematicalapproach to uncertainty by capturing the indiscernibility. Two differentphenomena can be indiscernible in some contexts and classified in the sameway when combining available information about them. Several experiencesexist in the use of Rough Set theory in data mining, knowledge analysis andapproximate pattern classification, but the spatial component lacks in all theseresearch streams.This paper aims to the use of Rough Set methods in geographical analyses.This approach has been applied in a case of study, comparing the resultsachieved by means of both Map Algebra technique and Spatial Rough set. Thestudy case area, Potenza Province, is particularly suitable for the application ofthis theory, because it includes 100 municipalities with a different number ofinhabitants and morphologic features.","Beniamino Murgante, Giuseppe Las Casas, Anna Sansone",http://editions-rnti.fr/render_pdf.php?p1&p=1001234,http://editions-rnti.fr/render_pdf.php?p=1001234,en,"jeu rugueux de RNTI_Caquard A pour extraire la frange périurbaine Beniamino Murgante *, Giuseppe Las Casas *, Anna Sansone *, * Université de Basilicate, Viale dell'Ateneo Lucano 10, 85100, beniamino.murgante@unibas.it~~V~~singular~~3rd Résumé Potenza. À ce jour, la disponibilité des données spatiales augmente en même temps que les techniques et les méthodes adoptées dans l'analyse géographique. En dépit de cette ten- dance, la classification d'une manière précise à chaque partie de la ville est de plus en plus com- plexe. Cela est dû à la croissance de la complexité de la ville. théorie prévidage peut être une méthode utile d'employer à combiner de grandes quantités de données afin d'acquérir des connaissances complexes sur le territoire. Il représente une autre approche mathéma- tique à l'incertitude en capturant l'indiscernabilité. Deux phénomènes peuvent être indiscernable dans certains contextes et classés de la même manière lorsque l'on combine les informations disponibles à leur sujet. Plusieurs expériences existent dans l'utilisation de la théorie prévidage dans l'extraction de données, l'analyse des connaissances et de la classification des formes approximative, mais la composante spatiale manque dans tous ces domaines de recherche. Ce document vise à l'utilisation de méthodes prévidage dans les analyses géographiques. Cette approche a été appliquée dans un cas d'étude, en comparant les résultats obtenus au moyen de la technique comme l'algèbre Carte et jeu rugueux spatial. La zone de cas d'étude, la province de Potenza, est particulièrement adapté à l'application de cette théorie, car elle comprend 100 communes avec un nombre différent d'habitants et les caractéristiques morphologiques. 1 Introduction Dans quelques années temps une transition a été produit de la ville traditionnelle, carac- ized par des contextes sociaux statiques, à la ville d'aujourd'hui plus dynamique et très difficile à contrôler dans les petits détails. Jusqu'à il y a quelques décennies la structure sociale de la ville a été caractérisée par une popu- lation ayant des liens sociaux forts, dont la vie a été orientée par les institutions, les règles, les autorités. Ces habitants quittent maintenant la partie historique des villes de plus en plus occupés par la population de transition (étudiants, touristes, etc.). Souvent, le centre de la ville est un grand centre commercial avec des musées, des bibliothèques et d'autres services, mais sans les résidents. La nouvelle population n'a ni racines dans ces lieux, ni la perspective d'y vivre pour toute la vie. Anciens habitants ont quitté la zone urbaine en créant une sorte de forme spatiale dispersée (Indovina, 1990). Ce phénomène se produit en marge des zones urbaines à travers « coagulation » progressive des bâtiments. Quartiers sans centre et avec les navires sociaux pauvres ont RELATION été réalisés. Ces formes, l'étalement urbain a été encouragé par l'augmentation du nombre d'infrastructures, la croissance des revenus et de la demande de biens et services. L'étalement urbain peut être considérée comme une tendance à long terme pour les systèmes économiques territoriaux avec succès (Camagni, et al. 2002), caractérisées par une perte de production de la consommation du sol de la compétitivité des activités agricoles (Murgante, et al. 2008). - 101 - RNTI-E-13 Un ensemble rugueux spatiale pour extraire la frange périurbaine Le terme périurbains zone a été récemment inventé pour représenter ce genre de ville transi- tion et il a été fréquemment utilisé dans les documents de planification. On peut se demander: « qu'est-ce que la zone périurbaine moyenne, exactement? ». Ce terme est pas entièrement compris des planificateurs, parce que les systèmes de planification ne donnent pas une définition claire et sans ambiguïté. Les planificateurs urbains ont deux approches différentes: le premier considère le phénomène d'un point de vue théorique, par rapport aux concepts consolidés de la ville et zone rurale; l'autre tient compte de l'augmentation de la valeur économique de l'immobilier, en raison de la transformation. Ces différents points de vue génèrent une incertitude dans la définition exacte du bord de ces zones. Par conséquent, la frange périurbaine peut être considéré comme un objet avec des limites de durée indéterminée (Burrough et Frank 1996). Un definitio clair n de la frange périurbaine peut être obtenue compte tenu de cette zone comme une zone avec ses propres règles organiques intrinsèques, comme celles urbaines et rurales et non comme une zone de transition entre zones urbaines aux zones rurales. Par exemple, la proximité des zones urbaines, contiguïté au réseau routier, la présence de services publics et les services urbains, la densité de population plus élevée que dans les zones rurales peut générer un ensemble de règles de sion inclu- et si certaines de ces règles sont respectées, la zone peut être inclus dans la frange périurbaine. De même, les règles d'exclusion considèrent les sites archéologiques, les zones du patrimoine, des zones de préservation, terrains envi- ronnement de fortes pentes, les zones de glissement de terrain, les zones d'érosion. Si même un de ces règles est satisfaite, la zone ne peut pas être inclus dans la frange périurbaine. La possibilité de fournir différents degrés d'aptitude des terres est devenue de plus en plus réalisable dans le choix du site par l'utilisation de systèmes d'information géographique combinée avec des méthodes d'évaluation. Dans la littérature sur la classification des terres, un nombre pertinent d'expé- riences combinant des méthodes SIG multicritère ou approche des ensembles flous EXISTE (Malczewski 1999, Eastman 1999, Thill 1999, Leung 1988, Murgante et Las Casas 2004). Seulement quelques expériences existent dans l'utilisation de Set rugueux pour calculer l'information spatiale. Cette approche a été testée dans le cas de l'étude de la province Potenza. Le phénomène de l'étalement urbain est très commun, malgré le petit nombre d'habitants dans la région étudiée. 2 Une vue d'ensemble de la théorie des ensembles rugueux théorie des ensembles rugueux (Pawlak, 1982) est basée sur l'hypothèse que chaque élément est asso- ATED à plusieurs informations dans l'univers du discours. Certains objets sont indiscernables des autres si elles sont classées de la même manière selon leurs informations associées. En d'autres termes, deux éléments différents peuvent être indiscernable dans certaines circonstances, alors que dans d'autres contextes, ils peuvent appartenir à différentes classes. Cette méthodologie est basée sur les concepts de relation indiscernabilité, approximation supérieure et inférieure et la précision de l'approximation. Dans un système d'information IS = (U, A), U est un ensemble d'objets définis U = X1, X2, ...., Xn et A est un ensemble d'attributs A = a1, a2, ..., an. U et A sont des ensembles non vides. nous associons un ensemble de valeurs Va appelé domaine d'un pour chaque attribut a∈A,. Il est possible de définir un attribut pour classer tous les cas. Une décision du système DS = (U, A∪d) est un système d'information dans lequel un attribut de déci- sion, d, affecte la classification. - 102 -RNTI-E-13 Murgante et al. FIGUE. 1 - Limite supérieure et inférieure Approximation d'un ensemble X. Si l'on considère un ensemble d'attributs B⊂A,  il est possible de définir la relation suivante ity indiscernibil- Ind (B): deux éléments (Xi, Xj) ∈ Ind (b) sont indiscernables par l'ensemble des attributs b dans A, si b (Xi) = b (Xj) pour chaque b⊂B. La classe d'équivalence de Ind (B) est appelé ensemble élémentaire B. inférieur et approximation supérieure (figure 1) sont définis comme les éléments contenus avec certitude dans l'ensemble (équation 1) et que les objets qui appartiennent probablement à l'ensemble (équation 2), respectivement. La différence entre approximation supérieure et inférieure définit l'aire de jeu bound- X (équation 3). [] {} XxUxLX tribule terrestre ⊂∈ =) (EQ 1 []} {0) (≠ ∩∈ = XxUxUX tribule terrestre EQ 2 LXUXBX -... = EQ 3 BB - 103 - RNTI-E-13 Un ensemble spatial rugueuse pour . l'extraction de la frange periurbain Si BX = 0 alors l'ensemble X est croquante la précision (équation 4) est définie comme le rapport de car- dinality d'approximation inférieure et supérieure:.) (/) () (UXcardLXcardXB = μ EQ 4 la résultat doit être compris entre 0 et 1. 3 Une extension spatiale de la théorie des ensembles rugueux applications géographiques de cette méthodologie ne sont pas beaucoup plus expérimenté. Une première approche théo- a été développé-théorique par Worboys et Duckham (2004) et Beaubouef et al. (2007 .) Les informations spatiales peuvent être classés selon deux critères: la nature des données un nd formation in- fiabilités. Une classification d'incertitude dans l'information spatiale est rapporté dans la figure 2. L'information géographique peut être certain ou non. Dans le cas des données bien définies, un degré d'incertitude peut être éliminé en utilisant la théorie des probabilités ou, en cas de plusieurs solutions de rechange, il peut être résolu en adoptant des méthodes multicritère. les données mal définies ont été classés en trois groupes. FIGUE. 2 - Une classification d'incertitude dans l'information spatiale (adapté de Murgante et Las Casas, 2004). - 104 -RNTI-E-13 Murgante et al. Certaines données sont dites ambiguës si elles peuvent avoir au moins deux interprétation particulière. mène à un Ambiguïté dans discordances classification des données en raison d'une perception différente du phénomène. Imprécision produit l'incertitude dans le cas de faible qualité des données, en raison d'un certain degré d'erreur. Une grande partie de la fonctionnalité SIG est basée sur des opérateurs booléens qui reposent sur une logique à deux valeurs. Manque de précision (Erwig et Schneider, 1997) prend en compte la logique à valeurs multiples et il est basé sur le concept de « région frontalière », qui comprend tous les éléments qui ne peuvent pas être classés comme appartenant à un ensemble ou son complément (Pawlak, 1998). Trois approches théoriques Exister vague: la première est basée sur la théorie des ensembles flous (Zadeh, 1965) qui représente l'adhésion partielle des éléments à un ensemble; la seconde est d'œuf jaune d'oeuf Théorie (Cohn et Gotts 1996, Hazarika et Cohn 2001) en se basant sur les notions de « œuf », à savoir l'extension maximale d'une région, et « jaune », à savoir la limite de la région intérieure; la troisième approche est la théorie des ensembles rugueux. Il est possible de définir l'objet rugueux par analogie avec l'expérience de Cheng et al. (2001). Objets traditionnels avec des limites nettes peuvent être définies comme des objets Crisp-Crisp (objets CC-). objets bruts sont regroupés dans les trois catégories suivantes (figure 3): - Objets Crisp-rugueux (CR-objets) avec des limites bien définies et Leur contenu incertain; - des objets rugueux-Crisp (RC-Objects) avec un contenu précis et spatial bord indéfini; - Objets rugueux-rugueux (RR-objets) avec incertain à la fois le contenu et les limites. FIGUE. 3 - Les différentes classes d'objets rugueux. Les relations topologiques sont définies par le modèle 9 d'intersection bien connu (Egenhofer et Herring 1991). Plus tard Clementini et Di Felice (2001) mis au point une extension de ce modèle, appelé « modèle large des limites ». Une large limite est constituée de bords intérieurs et extérieurs qui augmentent le 8 cas de modèle 9-intersection de 44 cas. En outre, un modèle topologique a été développé (Wang et al., 2004) dans la théorie des ensembles rugueux. B. - 105 - ensemble brut de RNTI-E-13 A pour extraire la frange périurbaine Fig. 4 - relation Crisp-Crisp (adapté de Wang et al., 2004). () () () () () () () () () () () () () () () () () ()           ∩∩∩ ∩ ∩∩ ∩∩∩ BNegANegBBndANegBPosANeg BNegABndBBndABndBPosABnd BNegAPosBBndAPosBPosAPos; ; ; ; ; ; EQ. 5 Ce modèle (équation 5 et les figures 4, 5, 6) est basé sur les concepts de la région positive (approximation Basse), la région limite et la région négative (univers sans approximation supérieur). De la même manière que pour la formalisation des objets rugueux, il est possible de distinguer trois types de relations difficiles: - Crisp-Crisp (CC), les relations difficiles entre les entités nettes (fig. 4); - rugueux-Crisp (RC), les relations difficiles entre les entités brutes et nettes des entités (fig 5.); - rugueux-rugueux (RR), les relations difficiles entre les entités rugueuses (fig. 6). Les relations Crisp-Crisp compte tenu des objets avec des limites nettes ont la même formalisation du modèle 9 intersection. - 106 -RNTI-E-13 Murgante et al. FIGUE. 5 - relation Irrégulier-Crisp (adapté de Wang et al., 2004). Dans les relations Irrégulier-Crisp, A est un ensemble rugueuse avec Lr inférieure (A) et supérieure Ur (B) approximation, U est l'univers et B Ur (B) est un ensemble de wh croustillant Lr avant coïncide avec Ur. Dans les relations Rough bruts A et B sont des ensembles rugueux avec Lr inférieure (A) Lr (B) et supérieure Ur (A) Ur (B) approximation et U est l'univers. B. - 107 - ensemble brut de RNTI-E-13 A pour extraire la frange périurbaine Fig. 6 - (. Adapté de Wang et al, 2004) Rugosité relation approximative. La méthode proposée par Wang et al. (2004) peut être considéré comme un cas particulier de Clementini et l'étude de Di Felice. Dans le cas des relations RC et RR le nombre de matrices possibles ne dépasse pas huit comme dans le modèle 9 intersection. Si les données spatiales sont dans l'approximation bas ils appartiennent sûrement à l'ensemble et ne peuvent pas être inclus dans d'autres types d'approximations. Si les entités géographiques sont situées à l'extérieur du rapprochement supérieur ils ne sont pas dans l'ensemble. Il est possible de chevauchement (figure 7), seules des approximations supérieures avec d'autres approximations supérieures (Ahlqvist et al., 2000). Une approximation inférieure peut être sur- rodée qu'avec l'équivalent approximation supérieure (Ahlqvist et al., 1998). En conséquence directe: AB ≠ ∀∀ = ∩ = ∩ A 0; Lr (B) Ur (A); 0 Lr (B) Lr (A) - 108 -RNTI-E-13 Murgante et al. FIGUE. 7 - Classification spatiale brute. 4 Techniques d'analyse point Pattern Habituellement, un phénomène géographique peut être analysée en termes de première et deuxième propriétés d'ordre. propriétés de premier ordre décrivent les événements géographiques en termes de densité ou inten- sité, tandis que le second propriétés d'ordre analyser les relations entre les phénomènes spatiaux en termes de distance. Dans cette étude, deux techniques d'analyse spatiale ont été utilisés: la densité du noyau et le plus proche voisin de la distance. Ces deux approches sont appelées première et effets de second ordre et ils considèrent le nombre d'événements observés par unité de surface et la distance entre eux, respective- ment. 4.1 Estimation du noyau Densité Densité du noyau est une technique d'analyse de motif de points, chaque point qui considère comme un événement et les attributs associés que l'intensité du phénomène. Bailey et Gatrell (1995) ont développé un ensemble de techniques d'analyse spatiale appliquées dans le domaine de la propagation des épidémies. Ces techniques sont basées sur la première loi de la géographie (1970) Waldo Tobler: « Tout est lié à tout le reste, mais les choses sont plus près que les choses lointaines liées ». Par rapport aux approches statistiques classiques, il est important de localiser les données, compte tenu des événements comme les occurrences spatiales du phénomène étudié. Chaque événement Li se trouve dans l'espace d'une manière non ambiguë par ses coordonnées (xi, yi). Un événement Li (équation 6) est une fonction de sa position et les attributs caractérisant et quantifier son intensité: B. - 109 - RNTI-E-13 Un ensemble rugueux spatial pour extraire la frange périurbaine) A .. ,, A, A , y, (xL n21iii ... = EQ. 6 Bien que la simple fonction de densité considère le nombre d'événements pour chaque élément de la grille régulière composant la région d'étude R, la densité du noyau prend en compte une surface tridimensionnelle mobile, qui pèse les événements en fonction de leur distance par rapport au point de l'évaluation de l'intensité, la densité de la distribution au point l peut être définie par l'équation suivante (Gatrell et al., 1996).       - = Σ = ττ 1 (l) λ n 1i 2 iLLk) EQ. 7 où, λ (L) est l'intensité de la distribution des points, quantifiée au point L; Li est ième événement, k () représente la fonction du noyau et τ est la bande passante. τ peut être défini comme le rayon d'un cercle générés à partir de l'intersection entre la surface et le plan contenant la région d'étude R. Deux facteurs importants influent sur les résultats: dimension de la grille et la bande passante (Batty et al., 2003). La bande passante produit une surface en trois dimensions, correspondant plus ou moins au phénomène, ce qui permet d'analyser la distribution à différentes échelles. choix de la bande passante influence notablement la surface de densité estimée. Si la bande passante est élevée, la densité du noyau est plus proche de t il apprécie la densité simple. Avec une largeur de bande étroite, la surface va capturer des événements locaux, ayant une densité proche de zéro pour les éléments du réseau lo- quée loin de chaque événement. La bande passante droit peut être déterminée en estimant la nomène et nomène, s'il est important, en mettant en évidence des pics de distribution ou lisses tions spatiales varia-. 4.2 Distance la plus courte distance voisine analyse entre les événements représente généralement une alternative aux mesures fondées sur la densité, mais dans plusieurs cas, il pourrait être une donnée d'entrée pour KDE. Distance la plus courte voisin est la méthode la plus basée sur la distance commune et il fournit des informations sur l'interaction entre les événements à l'échelle locale (deuxième propriété de commande). Distance la plus courte voisin consi- Ers la distance événement d'événement voisin le plus proche, choisi au hasard. La distance entre les événements peut être calculée en utilisant le théorème de Pythagore: (. L EQ ji 8 Si dmin (Li) () () 2ji2ji YYXX - - + =) L, d est la distance du plus proche voisin pour un événement Li, il est possible de considérer la plus proche distance voisine moyenne définie par Clark et Evans (1954) que:. n Ld n 1i iΣ ==) (d minmin EQ 9 5 l'étude de cas province Potenza est situé en Italie du Sud, il a une faible densité de population ( 400.000 habitants, plus de 650 000 hectares) -.. 110 Le chef-lieu est Potenza avec 70.000 habitants, tandis que les autres municipalités peuvent être classées en trois groupes Douze villes comptent plus ou moins 12.000 habitants -RNTI-E-13 Murgante et al, vingt. les municipalités ont une population d'environ 5 000 habitants, la population des 67 municipalités restantes varie de 700 à 2000 habitants. en général, l'étalement urbain est plus fréquente dans les régions métropolitaines, alors qu'il est peu fréquent dans les régions à faible populatio n densité. Ce phénomène dans les petites municipalités est ATED par l'gener- abandon des centres urbains anciens, alors que dans les grandes villes, il est produit par des coûts élevés d'appartements. Une analyse précise du phénomène a été réalisée en utilisant toutes les potentialités des systèmes d'information géographique. Tous les polygones qui représentent Ings Build- ont été convertis en points afin d'utiliser des techniques statistiques spatiales. Le nombre de logements pour chaque bâtiment indique l'intensité de l'événement afin de réaliser une surface continue de densités spatiales. Dans l'application d'une valeur de largeur de bande de 400 m a été utilisé avec une dimension de cellule de grille de 10 m. La figure 8 - Densité des colonies dispersées par rapport au réseau routier. L'intensification du phénomène a été évalué calculer la densité du noyau de tographies à 2004 et car- 1987. Cette comparaison a mis en évidence plus précisément des zones à la plus forte croissance du phénomène de l'étalement urbain. Après la localisation des zones où le phénomène est plus important, il est important de comprendre les facteurs qui pourraient conduire à son augmentation. B. - 111 - ensemble brut de RNTI-E-13 A pour extraire la frange périurbaine densité du noyau peut donner d'autres interprétations du phénomène de dispersion de règlement (figure 8). La croissance a été développé comme une couronne entourant la zone urbaine ou le long du réseau routier et il est indifféremment situé sur les glissements de terrain et des pentes raides. La plus forte augmentation de l'étalement urbain a eu lieu dans les zones situées pour la plupart sur monta- gnes, alors que la croissance urbaine est considérée comme une menace dans les zones ayant des liens activi- agricoles intensives. Dans le cas de l'étude, la densité du noyau a été considéré selon les catégories suivantes: - il est raisonnable de classer une région rurale si la présence d'appartements est inférieur à 1 / ha; - de 1 à 5 appartements / ha, il est possible de définir la classe périurbains; - caractéristiques urbaines prédominent au-delà de 5 appartements / ha. La deuxième classe générée par la densité du noyau est exactement l'une des règles d'inclusion adoptées dans l'application. Le phénomène de la croissance urbaine a été observée dans diverses municipalités de tailles différentes et cette étude indic ates que les nouveaux bâtiments sont complètement à l'intérieur d'une distance de 200 m à partir du réseau routier. A proximité du réseau routier a été cal- culée à l'aide de distance en ligne droite, en attribuant une valeur de distance pour chaque cellule. Les zones où la distance entre les bâtiments est inférieure ou égale à 100 m. ont été obtenus au moyen de la distance du plus proche voisin. Distances de la route et entre les bâtiments complètent l'ensemble des règles d'inclusion. Les règles d'exclusion suivantes ont été prises en compte: zone comprise dans un rayon de 150 m des rivières et des cours d'eau, la pente supérieure à 35%, les sites Natura 2000, zones à risque géologique Hydro-. Ces règles d'inclusion et d'exclusion ont été combinées avec des règles de contiguïté. Profondeur de la zone de contiguïté, pour chaque centre, a été localisé en utilisant un indice de forme pour la limite de la zone urbaine. Indice de forme est le rapport entre le périmètre de la zone urbaine et le périmètre du cercle qui inscrit lui. Il est évident que cet indice peut supposer Val- ues plus d'un. Plus la valeur est supérieure à un, plus la forme de la RÈ GLEMENT SEt- sera longue, en dents de scie et étroite. Un bon niveau de compacité correspond à un indice de forme compris entre 1 et 1,6, un niveau de moyenne à des valeurs comprises entre 1,61 et 2,4, un niveau bas à un plus grand index de 2,4. Dans le tableau 1, les 100 communes de Province Potenza sont regroupés en trois catégories, selon le taux de compacité. Ce tableau met en évidence le faible niveau de compacité des zones urbaines de la province Po- Tenza. Valeur de l'indice Nombre de centres Bonne compacité 1-1,6 12 compacité moyenne 1,61-2,4 68 Mauvaise compacité 2,41-4,81 20 TAB. 1 - Centre des classes en fonction du taux de compacité. Deux critères ont été pris en considération pour contiguïté: le premier est le rapport entre la surface et le périmètre de la région urbaine, le second est le rapport entre la surface et le périmètre du cercle de la région urbaine inscrire. Toutes ces règles avec les mêmes seuils ont été adoptés dans les deux techniques Carte soutif suivants: Algérie Rugueux Spatial. - 112 -RNTI-E-13 Murgante et al. 5.1 classification des terres avec l'algèbre de carte Trois zones périurbains ont été identifiés en combinant les règles précédentes avec l'algèbre de carte: - le premier bord a été obtenu compte tenu de toutes les règles précédentes (inclusion-exclusion) et la première contiguïté, qui prend en compte le rapport entre la zone et le périmètre de la région urbaine; - la deuxième limite a été atteint compte tenu de toutes les règles précédentes et la deuxième contiguïté, qui considère la zone et le périmètre du cercle de la région urbaine inscrire; - la troisième zone ne prend pas en compte toute contiguïté. La figure 9 - Schéma de la procédure pour l'emplacement de la frange périurbaine. Comme prévu, les résultats sont assez différents selon le type de zone périurbaine. Dans la plupart des nicipalités mu-, la plus petite surface est obtenue compte tenu de la première des cas mentionnés ci-dessus. La plus grande région est le rendement sans tenir compte de toute règle de contiguïté. B. - 113 - ensemble brut de RNTI-E-13 A pour extraire la frange periurbain FIG.10 - Comparaison entre les trois franges péri dans la municipalité Potenza. La zone intérieure (trappe en pointillé avec limite gras) ne considère pas la contiguïté. La zone intermédiaire (trappe carré) adopte la première règle de la contiguïté, la zone extérieure (de trappe avec des lignes) utilise la deuxième règle de contiguïté. Cette tendance est complètement inversée dans le cas de la municipalité Potenza (figure 10). La frange périurbaine obtenue sans tenir compte de toute règle de contiguïté est la plus petite, parce que la fonction noyau capture une faible densité de bâtiments dans ces zones. Ce résultat implique que les zones proches de la région urbaine sont représentés par les établissements avec au plus 2 appartements pour les bâtiments. Pour confirmer cette hypothèse, la relation entre les deux franges péri et les deux règles de contiguïté maintenir la même séquence. Cependant, la plus grande partie des cas suit l'ordre « premier contigui règle ty -. seconde règle de la contiguïté -sans aucune règle de contiguïté » Compte tenu de la morphologie du système de règlement de Potenza province, constitué dans la plupart des cas par les zones urbaines situées sur la flèche de Apennins et des établissements dispersés situés près du réseau routier le long des vallées , la règle de la contiguïté ne pas tout à fait saisir le phénomène qui conduit à une mauvaise interprétation. Considérant commune Avigliano (figure 11), l'utilisation des règles de contiguïté implique l'exclusion des colonies du nord-ouest et sud-est. a partir de la figure 11, il est facile sous - support pour que la transition de la première à la deuxième règle de la contiguïté ne provoque pas une grande -.. 114 -RNTI-E-13 Murgante et al augmenter l'extension Sur cette figure une augmentation énorme de la frange periurbain est le rendement sans aucune règle de la contiguïté, ce qui représente mieux la situation réelle. Dans tous les cas, la région plus fiable est le troisième qui est obtenue sans aucune règle de guity conti-, car il identifie le sont comme dans lequel les nouvelles transformations sont plus susceptibles de se produire. FIG.11 - Avigliano, commune de taille moyenne, la trappe au carré adopte la première contiguïté, la trappe avec des lignes utilise la deuxième règle de la contiguïté et la trappe en pointillé ne considère pas la contiguïté. 5.2 classification des terres avec la théorie spatiale brute prévidage a été appliqué pour la classification des différentes couches géographiques. Dans le cas de l'étude, l'ensemble du territoire provincial représente l'univers [U]; règles d'inclusion et d'exclusion sont les attributs [A] (tableau 2) et des objets (cellules de la grille) ont été classi- fiés selon les règles précédemment définies. les relations de indiscernabilité ont été calculées en [U] considérant attributs, pour obtenir le sous-ensemble [X]. Par conséquent, toutes les cellules qui satisfont aux mêmes exclusion du temps et des règles d'inclusion sont considérées comme un sous-ensemble [X], contenu dans [U]. Le système de décision dans ce cas est constitué par l'ensemble X, à savoir l'ensemble d'attributs où la variable de décision est la distance du plus proche voisin. B. - 115 - RNTI-E-13 Un ensemble rugueux spatial pour extraire les franges péri attributs et variables déci- sion valeurs Inclusion des règles 0 Nature 2000 sites B = Nature 2000 sites 1 différente de Nature 2000 sites 1 différents de sites Nature 2000 1 si la distance est inférieure à 150 m. 2 si la distance est comprise ENTRE 150 m. et 800 m. 2 si la distance est comprise entre 150 m. et 800 m. Fi = Hydrographie 3 si la distance est supérieure à 800 m. 3 si la distance est plus grande que 800 m. 0 catégories R3 R4 ed Fr = zones à risques géologiques Hydro- une autre des classes R3 R4 ed une autre des classes R3 R4 ed 1 si la pente est inférieure à 23,6% 1 si la pente est inférieure à 23,6% 2 si la pente est comprise interpolation BE- 23,6% et 35% P = Slope 3 si la pente est supérieure à 35% 2 si la pente est comprise interpolation 23,6% et 35% BE- 1 si la densité est inférieure à 1 appartements par hectare 2 si la densité est comprise entre 1 et 5 appartements par hectare D = masse volumique 3 si la densité est plus grande que 5 appartements par hectare 2 si la densité est comprise entre 1 et 5 appartements par hectare 1 si la distance est moins de 200 m. 2 si la distance est comprise ENTRE 200 et 700m. V = réseau routier 3 si la distance est inférieure à 700 m. 1 si la distance est inférieure à 200 m. C1 = Real contiguïté 0, 1 1 C2 = Idéal contiguïté 0, 1 1 1 si la distance minimale est inférieure à 100 m 1 si la distance minimale est inférieure à 100 m 2 si la distance minimale est CONCLUE in- comprise entre 100 et 200 m Nei = le plus proche distance voisine (variable de décision) 3 si la distance minimale est supérieure à 200 m 2 si la distance minimale est comprise entre 100 et 200 TAB. 2 - Les attributs et les variables de décision pour la classification grossière. Set X a été obtenu selon les variables de décision (IEN) afin d'atteindre son approximation inférieure et supérieure. Trois classifications ont été faites . Deux d'entre elles tiennent compte de la première et la deuxième règles de contiguïté, respectivement; le troisième ne considère aucune règle de contiguïté. Inférieur et supérieur approximation et précision Nei égal à 1 ont été calculés pour chaque cas. Dans le cas du Real contiguïté, rapprochement inférieur appartient à l'ensemble X et en même temps, il est inclus dans la première ceinture de contiguïté avec nei égal à 1. rapprochement supérieur suit les mêmes règles du rapprochement inférieur à l'exception de Nei qui peut être égal à 1 ou à 2 (tableau 3). - 116 -RNTI-E-13 Murgante et al. B Fi Fr DPV C1 Nei 1 Nei 2 Total Inférieur Supérieur Précision 1 2 1 2 1 1 1 103914 103914 138056 435856 31,67% 1 3 1 2 1 1 1 297750 50 297800 1 2 1 2 2 1 1 8946 8946 1 3 1 2 2 1 1 25196 25196 TAB. 3 - Table de décision, rapprochement inférieur et supérieur et la précision avec ity réel Contigu-. Comme défini précédemment, la précision est le rapport entre la cardinalité de proximation inférieure et supérieure. Dans le cas de contiguïté réel, la précision est égale à 31,67% (tableau 3). B Fi Fr DPV C2 Nei Nei 1 2 Total Inférieur Supérieur Précision 1 2 1 2 1 1 1 121089 32 121121 44214 523862 8,44% 1 3 1 2 1 1 1 358 388 139 358 527 1 2 1 2 2 1 1 12127 12127 3 1 1 2 2 1 1 32087 32087 TAB. 4 - Table de décision, rapprochement inférieur et supérieur et la précision avec ity Idéal Contigu-. La même procédure a été suivie pour le cas de Ideal contiguïté. Dans ce cas Ac- cure est égale à 8,44% (tableau 4). La dernière classification a été appliquée sans tenir compte de toute règle de contiguïté. Dans ce cas, la précision est égale à 7,84% (tableau 5). B Fi Fr DPV Nei Nei 1 2 Total Inférieur Supérieur Précision 1 2 1 2 1 1 231 461 102 231 563 72 857 928 856 7,84% 1 3 1 2 1 1 624 127 309 624 436 1 2 1 2 2 1 20390 20390 3 1 1 2 2 1 52467 52467 TAB. 5 - Table de décision, rapprochement inférieur et supérieur et précision sans contiguïté. La comparaison des résultats des trois cas, la règle réel contiguïté produit les meilleurs résultats (BLES ta- 3, 4, 5) avec une meilleure interprétation du phénomène (figures 12, 13, 14, 15, 16, 17). 6 Résultats et discussion finale L'extension spatiale de la théorie prévidage est particulièrement adapté à la définition de la ceinture d'interdiction periur- car il peut être inclus dans des objets rugueux-rugueux (RR-objets), suite à l'état brut précédent objets classement, étant donné une définition claire ainsi que des règles non équivoques pour définir le bord manque. Le phénomène est périurbains un cas avec un niveau élevé d'incertitude, des données mal définies pour toute la région et pas de règles bien claires. paradoxe sorite décrit parfaitement le phénomène périurbains: - deux ou trois bâtiments ne font pas une ville; B. - 117 - ensemble bruts RNTI-E-13 A pour extraire la frange périurbaine - un million de bâtiments font une grande ville; - si les bâtiments n ne font pas une ville des bâtiments ni ne (n + 1); - si les bâtiments n font une ville des bâtiments, donc faire (n-1). La première propriété associée à la troisième implique qu'un million de bâtiments ne font pas une ville, en contradiction avec la deuxième propriété. De la même manière, une combinaison des deuxième et quatrième propriétés montre que deux ou trois bâtiments font une ville, dans la contradiction première propriété (Fisher, 2000). La comparaison entre les résultats obtenus en utilisant la technique algèbre et la méthode prévidage est le plus intéressant (question figures 12, 13, 14, 15, 16, 17). FIG.12 - Comparaison entre la frange périurbaine obtenue avec algèbre et Set rugueux considérant la contiguïté réelle pour la municipalité de Lauria. La frange périurbaine réalisée au moyen d'ensemble grossière spatiale est le plus petit dans tous les nicipalités mu- et avec toutes sortes de contiguïté. Cette tendance peut être due à plusieurs facteurs: • comment certaines combinaisons d'attributs sont liés; • l'algèbre de carte adopte des opérateurs booléens, basée sur la logique vrai-faux, produisant soit trop étroites ou trop larges zones; • influence les variables de décision se traduit par une certaine façon, pour cette raison, il est plus important d'adopter une variable de décision qui est une sorte de facteur clé pour la représenter phé- nomène. - 118 -RNTI-E-13 Murgante et al. FIG.13 - Comparaison entre la frange périurbaine obtenue avec algèbre et Set rugueux considérant contiguïté idéal pour la municipalité Lauria. FIG.14 - Comparaison entre la frange périurbaine obtenue avec algèbre et Set rugueux sans tenir compte de toute règle de contiguïté pour la municipalité de Lauria. B. - 119 - ensemble rugueux de RNTI-E-13 A pour extraire la frange périurbaine FIG.15 - Comparaison entre la frange périurbaine obtenu avec carte et prévidage con- Sidering réelle contiguïté pour la municipalité Picerno. FIG.16 - Comparaison entre la frange périurbaine obtenue avec algèbre et Set rugueux considérant contiguïté idéal pour la municipalité Picerno. - 120 -RNTI-E-13 Murgante et al. FIG.17 - Comparaison entre la frange périurbaine obtenue avec algèbre et Set rugueux sans tenir compte de toute règle de contiguïté pour la municipalité de Picerno Les résultats ont montré dans les figures 12 à 17 ans sont confirmées dans le tableau 6, qui résume les résultats obtenus avec chaque méthode de classification, la prise en compte le nombre de cellules inclus dans la frange périurbaine. Algèbre prévidage Précision inférieure supérieure Sans contiguïté 1721768 72857 928856 7,84% réel contiguïté 766864 138056 435856 31,67% Idéal contiguïté 1208171 44214 523862 8,44% TAB. 6 - Comparaison entre le nombre de cellules incluses dans la frange periurbain pour chaque méthode de classifi- cation. Malgré largement répandue que les évaluations Rugueux REGLER sont utilisés pour obtenir des contraintes plus souples, nos résultats montrent que la classification avec algèbre produit une frange périurbaine avec une extension plus grande, alors que les classifications brutes génèrent une frange plus étroite. Parmi les classifications approximatives, réelle contiguïté produit une précision acceptable et il est plus proche de la réalité. La zone obtenue représente une manière indiscernable la région où se trouvent les nouvelles zones de développement. B. - 121 - ensemble brut de RNTI-E-13 A pour extraire la frange periurbain 7 Conclusion théorie des ensembles rugueux réduit la complexité cognitive (Gorsevski et Jankowski, 2008) en utilisant la relation de indiscernabilité, où les objets caractérisés par des valeurs d'attributs identiques se produire dans les informations système. L'univers peut être divisé en blocs d'objets indiscernables, appelés ensembles élémentaires. La combinaison de ces blocs définit les sous-ensembles de l'univers appelé approximations inférieure et supérieure. La connaissance d'un monde réel ou abstrait peut être construit en combinant les ensembles élémentaires, suivant certaines règles (Greco et al., 2001). Approximations des ensembles sont des opérations de base dans la théorie des ensembles rugueux et sont utilisés comme principaux outils pour traiter des données vagues et incertaines (Pawlak, 1997). Dans la plupart des cas, les données sont obtenues adoptent des techniques statistiques spatiales, où un certain niveau d'incertitude est due à plusieurs facteurs. Par exemple, la densité du noyau, adoptée dans la présente demande, est fonction du choix de certains paramètres clés, tels que la résolution de la grille, la fonction du noyau et, surtout, la bande passante. Une meilleure compréhension des relations spatiales et non-spatiales entre les variables géographiques dépendantes dépendantes et in- est une question fondamentale pour un développement correct de l'approche prévidage de classification des terres. Précisément, en utilisant ces variables joue un rôle important dans la classification des terres par jeu rude spatiale. Le choix combinaison d'attributs et variable de décision sur un côté, affecte les résultats whist de l'autre côté des simulations produisent plus près de la réalité. approche prévidage permet une meilleure interprétation du phénomène périurbain. Cette méthode ne produit pas une frontière nette qui définit clairement le bord périurbains, mais il permet de localiser les cellules avec les mêmes valeurs d'attributs pris en compte pour le classement d'une manière indiscernable. Cette zone, appelée limite, exactement r epresents la zone de transition entre urbain à la partie rurale de la ville, où phénomène de l'étalement urbain pourrait être plus con- centrated. Références Ahlqvist O., J. Keukelaar, Oukbir K., (1998). En utilisant la classification brute pour représenter la certitude Un- dans des données spatiales. SIRC98, Vers la prochaine décennie de l'information spatiale Recherche Re-. 10e Colloque annuel du: information spatiale du Centre de recherche, Université d'Otago, Dunedin, Nouvelle-Zélande. Ahlqvist O., Keukelaar J., Oukbir K., (2000). classification grossière et évaluation de la précision. International Journal of Information Science géographique, vol. 14, no. 5, 475 - 496. Londres: Taylor et Francis. Bailey T. C., A. Gatrell C., (1995). l'analyse des données spatiales interactives, Prentice Hall. M. Batty, Besussi E., K. Maat, Harts J., (2003). Représenter les villes multifonctionnels: den- sité et la diversité dans l'espace et le temps. CASA Working Papers. Beaubouef, T., Petry, F. et Ladner, R., (2007). Méthodes de données spatiales et les régions: une approche vagues Set rugueux. Applied Soft Computing Journal, vol. 7, pp. 425-440 Elsevier Science Publishers. Burrough P. A., Frank A. U. (1996). Les objets géographiques avec des limites de durée indéterminée, Londres: Taylor & Francis. - 122 -RNTI-E-13 Murgante et al. Camagni R., Gibelli M.C., Rigamonti P., (2002). Je Costi collettivi della città dispersa. Firen- ze: Alinea. Cheng T., M. Molenaar, Lin H., (2001). Formalisant objet flou résultats de la classification incertaine, International Journal of vol science géographique. 15, no 1, 27-42, Londres: Tay- lor et Francis. Clark P.J., Evans F.C., (1954). Distance au plus proche voisin comme une mesure de tionships rela- spatiales au sein des populations, l'écologie, 35: 445-453, 1954. Clementini E., Di Felice P., (2001). Un modèle spatial pour des objets complexes en requêtes larges limites d'appui sur des données aléatoires. Données et ingénierie des connaissances, 37 (3): 285-305. Cohn A.G., Gotts N. M. (1996). La représentation « vitellins » des régions dont les limites sont indéterminées » dans Burrough P. A., Frank A. U., (dir.). Les objets géographiques avec indétermination limites, Londres nate: Taylor & Francis. Eastman J.R. (1999). évaluation multicritère et SIG, en Longley P.A., Goodchild M. F., Maguire D.J. Rhind D.W. Système d'information géographique et de la science. Jonh Wiley & fils. Egenhofer M. J., Herring, J., (1991). Catégorisation relations topologiques binaires entre les régions, des lignes et des points dans des bases de données géographiques, Rapport technique, Département de génie d'arpentage, Université du Maine, Orono. Erwig, M., Schneider M., (1997). Régions vagues. Actes du 5 sium international Sympo- sur les progrès dans les bases de données spatiales (__gVirt_NP_NN_NNPS<__ SSD'97), LNCS 1262 pp.298-320 Fisher P., (2000). Sorite paradoxe et zones géographiques vagues, ensembles flous et des systèmes 113, 7 18, Elsevier Science. Hazarika S.M., Cohn A.G., (2001), une taxonomie pour Spatial imprécision, une alternative jaune d'œuf, Proceedings of COSIT 2001, LNCS 2205 pp.92-107 Gatrell A. C., T. Bailey C., P. J. Diggle, Rowlingson B. S., (1996). analyse de modèle de point spatial et son application en épidémiologie géographique, la transaction de l'Institut de la Colombie-Géographe. Gorsevski P. V., Jankowski P., (2008), la susceptibilité aux glissements Discerner en utilisant des ensembles rugueux. Ordinateurs, Environnement et Systèmes Urbains 32, 53-65. Elsevier Science Publishers. Greco S., B. Matarazzo, Slowinski R., (2001). La théorie des ensembles rugueux pour l'analyse de décision multicritère. Revue européenne de recherche opérationnelle, 129, 1-47. Elsevier Science Publish- Ers. Indovina F. (1990). La città diffusa. Venezia: DAEST. Leung Y .. (1988). L'analyse spatiale et de la planification sous l'imprécision. Amsterdam, Pays--Bas: Elsevier Science Publishers. J. Malczewski (1999). Gis et multicritère Analyse de décision. John Willey & Sons Inc. B. - 123 - RNTI-E-13 Un ensemble bruts pour extraire la frange périurbaine Murgante B., Las Casas G., M. Danese, (2008). La ville périurbaine: les méthodes géostatistiques pour sa définition dans Rumeur M., Coors V., Fe Nde E. M., S. Zlatanova (Eds), urbaine et la gestion des données régionales. Londres: Taylor et Francis. Murgante B., Las Casas G .. (2004). G.I.S. et ensembles flous pour la Terre Adéquation Analyse, Conférence Note dans Computer Scienze. Vol. 3044. Berlin: Springer Verlag. Pawlak Z., (1982). Ensembles rugueux. International Journal of Sciences de l'Information et informatique 11, 341-356, 1982. Pawlak Z., (1997). approche ensemble Rough aide à la décision fondée sur le savoir, European Journal of Operational Research, Volume 99, Numéro 1, pp. 48-57, Elsevier Science Publishers. Pawlak Z., (1998). théorie Rough Sets et ses applications à l'analyse des données, Cybernétique et systèmes An International Journal, 29, 661- 688. Londres: Taylor et Francis. Thill J.C., (1999). Spatiale multicritère prise de décision et d'analyse, Ashgate, Aldershot. Tobler, R. W., (1970). Un modèle informatique Simuler la croissance urbaine dans la région de Detroit, Géographie économique, 46: 234-240. Wang S., Yuan H., Chen G., Li D., Shi W., (2004). Irrégulier Spatial interprétation. Dans Tsu- moto S., Słowiński R., J. Komorowski, J. Grzymala-Busse W. (eds), « ensembles bruts et les tendances actuelles en informatique ». Lecture Notes in Computer Science vol. 3066. Berlin: Springer Verlag. Worboys, M. F. et Duckham, M., (2004). SIG: une perspective informatique. CRC Press. Zadeh, L. (1965). ensembles flous. Contrôle de l'information. 8, 338-353. De nos jours résumé la availability des informations Spatiales ne Cesse d'increase with the improvement des techniques et des methods Dans les analyses Adoptees Géographiques. This tendance du Malgré, classificateur précisement each partie d'Une Ville est une tache de plus de en plus de difficile, en raison de la Complexité du tissu urbain croissante. La théorie des ensembles may consti- tuer approximatifs Une approach Intéressante verser un grand volume de regrouper de in the mais Données d'Une Connaissance Elaborer complexe d'Une zone géographique. Elle Constitué Une approche nouvelle du principe d'incertitude, en capturant l'indiscernabilité. Deux Phénomènes indiscernables Être PEUVENT Différents Dans des Contextes particuliers, de la Mais same classifiés Manière les combiner quand on se les infor- mations concerning. Il exists several de l'expériences traitant de la théorie emploi des ensembles les phases Dans approximatifs de fouille de Données, d'analyse des connaissances heuristiques et de la classification des Mode- les approximatives. Néanmoins, tous bureaux de recherche ne Domaines pas en prennent la considéra- tion composante spatiale. - 124 -RNTI-E-13 Murgante et al. Le mais l'article de this is l'emploi des methods sur la théorie fondées des en- sembles les analyses Dans approximatifs Géographiques. This approach was à un appliquée No CAS concret, en les comparant en adoptante Résultats obtenus les techniques de l'Algèbre des Cartes Avec en adoptante Ceux obtenus la théorie des ensembles approximatifs. La zone d'étude correspondent à la province de Po- Tenza en Italie, et particuliérement is Pour l'application indiquée of this théo- RIE, voiture 100 Municipalities Elle Comprend des populations et Avec des morphologiquement très logies Différentes. B. - 125 - ensemble brut de RNTI-E-13 A pour extraire la frange periurbain - 126 -RNTI-E-13"
815,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Data mining for activity extraction in video data,"The exploration of large video data is a task which is now possible because of the advances made on object detection and tracking. Data mining techniques such as clustering are typically employed. Such techniques have mainly been applied for segmentation/indexation of video but knowledge extraction of the activity contained in the video has been only partially addressed. In this paper we present how video information is processed with the ultimate aim to achieve knowledge discovery of people activity in the video. First, objects of interest are detected in real time. Then, in an off-line process, we aim to perform knowledge discovery at two stages: 1) finding the main trajectory patterns of people in the video. 2) finding patterns of interaction between people and contextual objects in the scene. An agglomerative hierarchical clustering is employed at each stage. We present results obtained on real videos of the Torino metro (Italy).","Monique Thonnat, Jose Luis Patino, Etienne Corvée, François Brémond",http://editions-rnti.fr/render_pdf.php?p1&p=1000632,http://editions-rnti.fr/render_pdf.php?p=1000632,en,"EGC2008 Data mining pour l'extraction de l'activité dans les données vidéo JoseLuis PATINO, Etienne CORVEE François BREMOND, Monique THONNAT INRIA 2004 route des Lucioles, 06902 Sophia Antipolis (FRANCE) {jlpatino, Etienne.Corvee, Francois.Bremond, Monique.Thonnat}@sophia. inria.fr http://www-sop.inria.fr/orion/ Résumé. L'exploration des grandes données vidéo est une tâche qui est désormais possible en raison des progrès réalisés sur la détection et le suivi objet. les techniques d'extraction des données telles que le regroupement sont généralement employés. Ces techniques ont principalement été appliquées pour la segmentation / indexation de la vidéo mais la traction de la connaissance de l'activité contenue dans la vidéo n'a été que partiellement pris en compte. Dans cet article, nous présentons comment l'information vidéo est traitée dans le but ultime de parvenir à la découverte de connaissances des personnes activité dans la vidéo. Tout d'abord, jects ob- d'intérêt sont détectés en temps réel. Puis, dans un processus hors ligne, nous visons à effectuer la découverte de connaissances à deux étapes: 1) trouver les principaux modèles de trajectoire des gens dans la vidéo. 2) trouver des modèles d'interaction entre les objets et per- PLE contextuels de la scène. Une ing cluster- ascendante hiérarchique est utilisé à chaque étape. Nous présentons les résultats obtenus sur des vidéos réelles du métro de Turin (Italie). 1 Introduction De nos jours, plus que jamais, les progrès techniques et scientifiques exige des opérateurs humains pour traiter de plus en plus grandes quantités de données. Pour traiter cette énorme quantité de données, la plupart des travaux peuvent maintenant être effectuées dans le domaine d'exploration de données pour synthétiser, analyser l'information et de mesure de l'extrait, qui est généralement caché dans les données brutes. Clustering est l'une des techniques les plus couramment utilisées dans les données minières pour effectuer des tâches de découverte de connaissances sur grande quantité de données sans connaissance préalable de ce qui pourrait être caché dans les données. Il existe de nombreuses techniques de regroupement dans la littérature, et l'objectif principal de toutes ces techniques est d'obtenir une partition des données en organisant automatiquement dans des groupes séparés où les objets à l'intérieur d'un groupe spécifique sont plus semblables les uns aux autres (en ce qui concerne leur caractéristiques extraites et mesurées, ou de variables) que pour les objets des autres groupes. Extraction de documents texte (Blatak 2005, Lemoine et al., 2005; Xing et Ah-Hwee 2005) et l'exploitation des données liées au Web pour l'extraction de l'activité dans l'information de données vidéo (Chia-hui et Kayed 2006, Facca et Lanzi 2005; McCurley et Tomkins 2004) sont deux champs d'application bien connus de l'exploration de données. L'application des techniques d'exploration dans les grandes données vidéo est désormais possible aussi en raison des progrès réalisés sur la détection d'objet et (ing Track- Fusier et al., 2007, Vu et al., 2003). Des recherches antérieures ont mis l'accent sur la classification vidéo sémantique pour l'indexation et la recherche (Oh et Bandi 2002. Ewerth et al 2007) (. Benini et al, 2006) ou la création de résumé vidéo, mais l'extraction de connaissances sur l'activité contenue dans la vidéo a été seulement partiellement pris en compte. Récemment, il a été démontré que l'analyse du mouvement des objets mobiles détectés dans la vidéo peut donner des informations sur la trajectoire normale et anormale (Porikli, 2004; Naftel et Khalid 2006, Anjum et Cavallaro 2007). Comment- a été fait jamais petite enquête pour trouver les modes d'interaction entre les objets mobiles détectés et les objets contextuels de la scène. Un premier travail a été présenté par (et al Patino. 2007) où les techniques de clustering ont été utilisées pour trouver des modèles de ries trajecto- et modèles d'activité. Dans cet article, nous présentons un ensemble de fonctionnalités plus riches et de définir de nouvelles distances entre les caractéristiques symboliques. Nous appliquons la classification ascendante hiérarchique 1) pour trouver les principaux modèles de trajectoire des gens dans la vidéo. 2) pour extraire les relations complexes entre les personnes et les objets contextuels de la scène. Cette recherche a été fait dans le cadre du GARDIEN proje ct, qui est une initiative européenne de fournir un outil efficace pour la gestion des grands multimédia de collec- tions. À l'heure actuelle, il est testé sur de grands enregistrements vidéo souterrains (métro GTT, Turin, Italie et métro ATAC, Rome, Italie). Dans ce travail, nous présentons les résultats obtenus sur des vidéos réelles de trois caméras du métro de Turin (Italie). La structure générale de notre système est présenté dans la section 2. détection d'objets et d'événements est expliqué dans la section 3. Trajectoire analy- sis des objets mobiles est expliqué à la section 4 alors que l'extraction des interactions significatives entre les personnes et les objets contextuels de la scène est décrite dans section cinq. Dans la sixième section, nous présentons les résultats obtenus. Notre conclusion est donnée est l'article 7. 2 Structure générale de l'approche proposée comporte trois éléments principaux qui définissent notre approche: l'acquisition de données, l'analyse ON- ligne de flux vidéo et l'analyse hors ligne. Une vue d'ensemble du système est illustré à la figure 1. Les flux vidéo sont directement introduits dans notre système d'analyse en ligne pour la détection en temps réel des objets et des événements de la scène. Cette procédure va sur une base image par image et les résultats sont stockés dans une base de données spécifique en ligne. A ce niveau, les événements détectés contiennent déjà des informations sémantiques décrivant l'interaction entre les objets et les informations contextuelles de la scène. Ceci est une première couche d'information sémantique dans notre sys- tème. L'analyse des objets détectés et événements récupérés à partir de la base de données en ligne fournira de nouvelles informations difficiles à voir directement sur les flux vidéo. Cela constitue une couche OND d'information sémantique sec-. Dans cette couche les trajectoires prises par les utilisateurs sont caractérisés. Ces informations sont mis en place dans un modèle approprié de représentation des connaissances à partir de laquelle les relations complexes peuvent être découvertes entre les objets mobiles et les objets contextuels de la scène. Les mesures statistiques telles que les chemins fréquents la plupart, le temps passé par les utilisateurs à interagir avec les objets contextuels de la scène peuvent être inférées. J. L. Patino et al. RNTI - X - page 3 Fig. 1 - Aperçu de l'approche proposée. 3 Objet et événement de détection La première tâche de notre démarche d'exploration de données est de détecter en temps réel des objets présents dans la vidéo et les événements d'intérêt. 3.1 suivi d'objets suivi plusieurs objets mobiles évoluant dans une scène est une tâche difficile à réaliser. Les détecteurs de mouvement ne parvient souvent à la détection d'objets en mouvement avec précision appelés « mobiles » qui induit mistracks des mobiles. De telles erreurs peuvent être causées par des ombres ou plus important en statique (si un objet mobile est masquée par un objet d'arrière-plan) ou dynamique (lorsque plusieurs projections mobiles sur le chevauchement de plan d'image) d'occlusion (Georis et al., 2003). En bref parler un algorithme de détection de mouvement permet la détection d'objets avant d'être classés et suivis au fil du temps. Les segments de détecteur de mouvement, de l'image de référence d'arrière-plan, les pixels d'avant-plan qui appartiennent à des objets en mouvement par une simple opération Threshold olding. Les pixels de premier plan sont alors regroupées en régions spatialement mobiles repré- ressentie par les boîtes englobantes. Ces régions sont ensuite classées en classes d'objets sémantiques en fonction de leur taille 3D. L'algorithme de suivi construit un graphique temporel des objets CONNECTE con- au fil du temps pour faire face aux problèmes rencontrés pendant le suivi. Les objets tégé de dé- sont connectés entre chaque paire d'images successives par un cadre de bâti (F2F) traqueur (Avanzi et al., 2005). Le graphique des objets liés est analysé par l'algorithme de suivi, aussi appelé Tracker à long terme, qui construit des chemins potentiels pour chaque mobile selon les liens établis par le tracker F2F. Le meilleur chemin est alors reconnu et les objets détectés liés par cette voie sont marquées avec le même identifiant. L'exploration de données pour l'extraction de l'activité dans les données vidéo 3.2 détection des événements Les événements d'intérêt sont définis en fonction de la langue sémantique introduite par Vu et al., (2003). Cela suppose une ontologie où les objets d'intérêt « o »; zone d'intérêt « z » et les objets contextuels d'intérêt « eq » (objets contextuels font partie du modèle de scène vide correspondant à l'environnement statique) sont définis. les relations spatio-temporelles sont ensuite construits pour former les événements d'intérêt: - inside_zone (o, z) »: quand un objet « o » est dans la zone « z ». - 'stays_inside_zone (o, z, T1)': lorsque l'événement 'INTÉRIEUR_ zone (o, z)' est détecté Suc-cessivement pendant au moins secondes T1 - 'close_to (o, eq, D)': lorsque le 3D distance d'un emplacement de l'objet sur le plan de masse est inférieure à la distance maximale autorisée, D, à partir d'un objet d'équipement « eq » - « stays_at »: lorsque l'événement « fermer (o, eq, Dmax, T2) » est détecté consécutivement pendant au moins T2 secondes. - « crowding_in_zone »: lorsque l'événement « stays_inside_zone (foule, z, T3) » est détectée pendant au moins secondes T3. Dans notre application particulière, nous avons utiliser les variables suivantes: - objet o = {p, g, c, l, t, u} avec p = personne, g = groupe, c = foule l = bagages, t = train, et u = inconnu. - zone z = {plate-forme, validating_zone, vending_zone} - eq équipement = {g1, ..., G10, VM1, VM2} où « gi » est la porte de la i-ième et vmi est le i-ième distributeur automatique. - T1 = 60 s, D = 1m50, T2 = 5 s, T3 = 120 s. 4 Trajectoire Analyse Pour la caractérisation du motif trajectoire de l'objet, nous avons sélectionné un dations détaillées, compact, et la représentation souple convient aussi pour une analyse plus poussée, par opposition à de nombreux systèmes vidéo qui stockent réellement la séquence d'emplacements d'objets pour chaque image de la vidéo , qui est une représentation lourde sans information sémantique. Si l'ensemble de données est composé d'objets m, la trajectoire pour l'objet i dans ce jeu de données est définie comme l'ensemble des points [xi (t), yi (t)]; x et y sont des vecteurs de séries chronologiques dont la longueur n'est pas égal pour tous les objets que le temps qu'ils passent dans la scène est variable. Deux points clés définissant ces séries temporelles sont le début et la fin, [xi (1), yi (1)] et [xi (fin), yi (fin)] comme ils définissent où l'objet vient et où il est aller à. Nous construisons un vecteur caractéristique de ces deux points. De plus, on inclut les informations de direction donnée en tant que [cos (θ), sin (θ)], où θ est l'angle qui définit le vecteur joignant [xi (1), yi (1)] et [xi (fin), yi (fin)]. Nous nourrissons le vecteur de caractéristique formé par ces six éléments dans un algorithme de classification ascendante hiérarchique (Kaufman et Rousseeuw, 1990). Pour un ensemble de données constitué d'objets m il y a m * (m-1) / 2 paires dans l'ensemble de données. Nous employons la distance euclidienne comme mesure de calculer similitude la distance entre toutes les caractéristiques de trajectoire. trajectoires d'objets avec la distance minimale sont regroupés ensemble. Lorsque deux ou plusieurs trajectoires sont définies ensemble leur barycentre global est pris en compte pour le regroupement plus. La fusion successive de J. L. Patino et al. RNTI - X - page 5 grappes est répertorié par le dendrogramme. L'évaluation du dendrogramme est généralement subjective par adjugeant qui seuil de distance semble créer le plus naturel regroupement des données. Pour cette raison, nous avons créé une interface qui permet à l'utilisateur d'explorer le gramme dendro-. Le nombre final de grappes est réglé manuellement et les valeurs typiques sont comprises entre 12 et 25 pour un ensemble de données de 1000 à 2500 objets mobiles. Comme les préformes d'acquisition dans un environnement multi-caméra les classes obtenues peuvent être généralisés à différentes vues de la caméra grâce à une matrice d'étalonnage 3D appliquée au cours du système d'analyse en ligne. Afin d'évaluer notre approche d'analyse de trajectoire, nous avons défini un ensemble de données contenant la vérité au sol de plus de 300 trajectoires. L'ensemble de données a été annotés manuellement. Les attributs sémantiques tels que « Du sud Portes des distributeurs automatiques » ont été enregistrées dans la base Data-. Il y a une centaine de ces annotée ETM attributs. antic En général, chaque description sémantique est associée à une trajectoire qui correspond le mieux à cette description. En outre, deux trajectoires définissent les limites de confiance au sein de laquelle on peut encore associer cette description tic seman-. La figure 2 montre quelques exemples des trajectoires de base de données et de leur asso- ATED sens sémantique. Nous calculons deux mesures de performance pour valider la qualité de l'approche de regroupement proposée, à savoir, de fusion et de rappel. Le premier donne une indication sur le nombre d'étiquettes sémantiques du rez-de-vérité (ou classes) sont mis ensemble dans un cluster unique résultant de la procédure agglomératif. Ces étiquettes sémantiques rez-de-vérité (contenant trois trajectoires par étiquette) sont: 'la vérité au sol associée au cluster. Idéalement, tous les jectoires de tradi- rez-de-vérité associés à la même étiquette sémantique doivent être inclus dans le même cluster. La dernière mesure de la performance (Rappel) indique le nombre de trajectoires vérité terrain match- ing un groupe donné par rapport au nombre de «vérité terrain associée au cluster. La figure 3 représente l'évolution de ces deux facteurs en fonction du nombre de grappes qui est sen cho- lors de l'exécution de l'algorithme de clustering. FIGUE. 2 - rez-de-vérité pour deux groupes sémantiques différents. Parce que toutes les trajectoires ne peuvent être également observés par la caméra (par exemple tous les tourniquets établir la distinction dans le coin supérieur gauche nécessiterait une résolution spatiale plus grande), il est en réalité très difficile à réaliser une bijection entre les étiquettes sémantiques et l'extraction de données résultant pour l'extraction de l'activité dans les groupes de données vidéo. Cependant, nous visons à avoir le niveau le plus bas possible et de fusion le plus élevé de rappel Percent- âge. A partir de la figure 3, on peut observer qu'un bon compromis est atteint pour un certain nombre de groupes d'environ 21. 3 - Evolution de la mesure de la qualité de groupement de fusion (fusion d'étiquettes sémantiques de réalité de terrain) et de rappel (récupération des trajectoires réalité de terrain avec la même étiquette sémantique) en fonction du nombre de grappes. 5 Interaction Analysis 5.1 Clustering de table d'objet mobile dans un deuxième étape de l'analyse hors ligne, on analyse la trajectoire de jects ob- mobiles détectés en même temps que d'autres caractéristiques utiles qui donnent des informations sur l'interaction entre les mobiles et les éléments contextuels de la scène. Les caractéristiques de l'objet mobile suivantes sont utilisées. - m_id: l'étiquette d'identification pour l'objet. - m_type: la classe l'objet appartient à: Personne, groupe, Foule ou bagages. - m_start: temps l'objet est d'abord vu. - m_duration: le temps dans lequel l'objet est observé. - m_significant_event: événement le plus important parmi tous les événements. Il est calculé comme événement le plus fréquent lié à l'objet mobile. - m_trajectory_type: le motif caractérisant la trajectoire de l'objet. J. L. Patino et al. RNTI - X - page 7 5.2 Clustering de la table d'objet mobile Une fois que toutes les mesures statistiques des activités de la scène ont été calculées et l'information a été mis dans le format modèle proposé correspondant, nous visons à découvrir- ing relations complexes qui peuvent exister entre mobiles eux-mêmes des objets et entre les objets mobiles et les objets contextuels de la scène. Pour cette tâche, nous courons une nouvelle procédure de regroupement où agglomératif l'ensemble de données est l'ensemble de la table d'objet mobile, il auto. Chaque enregistrement de la table est ainsi défini cinq caractéristiques comme l'étiquette d'identification ne sont pas prises en compte pour l'algorithme de clustering. Il faut remarquer que ce processus de regroupement, l'ensemble des Tures fea- contient numérique (par exemple le temps de début et de la durée d'un objet) et sym- valeurs Bolić (par exemple, le type d'objet et l'événement significatif) par opposition à le regroupement des trajectoires où toutes les fonctions sont numériques. Pour appliquer l'algorithme de classification agglomérante, nous avons défini une spécifi c métrique pour les valeurs symboliques: Pour le type d'objet () () () () () ()         ⊃⊃⇔ ⊃⊃⇔ = ⇔ = - sinon CrowdtypeobjopPersonGroutypeobjo PersontypeobjopPersonGroutypeobjo typeobjotypeobjo oo ji ji ji ji 1 '' _, '' _ 5,0 '' _, '' _ 5,0 __0 pour le type de trajectoire jiji tctcoo - = - où tci, TCJ sont les centres ou les prototypes des grappes de trajectoire respectivement corres- pondante à oi et oj et résultant de la dernière étape de regroupement trajectoire. Pour l'événement important, nous avons une comparaison logique () ()      = ⇔ = - sinon eventsigoeventsigo oo ji ji 1 __0 6 Résultats Nous présentons le résultat de notre approche sur une séquence vidéo de la durée de métro Torino 48 minutes. Au total notre algorithme détecté 2052 objets mobiles. La figure 4 montre par exemple une personne tracked étiquetés 1 et une foule Suivies des gens étiquetés 534. La figure 5 montre également une personne tracked, étiqueté 58 avec deux nouveaux objets: un groupe de personnes marqué 24 et une exploration de données pour l'extraction de l'activité dans les données vidéo non classés objet suivi marqué 68. en raison de la faible partie inférieure contraste du groupe de personnes, ce groupe a été segmenté en deux objets suivis (au lieu d'une) marquées 24 et 68. Fig. 4 - Une personne avec une étiquette « 1 » et une foule avec étiquette « 534 » sont suivis. a été détecté séjours « fils » à per- portes l'événement. La figure 5 montre la personne suivie étiquetée 1 qui sont restés assez longtemps devant les distributeurs automatiques de billets de validation portant la mention « Portes » pour que l'événement « stays_at » est détecté. Le groupe de personnes à chenilles marquée 24 sur la figure 3 est en interaction avec le numéro de distributeur automatique 2 assez longtemps pour que l'événement « stays_at » à détecter. Dans ces deux chiffres, n'a pas montré l'événement primitif « inside_zone », mais a également été détectée pour les autres objets présents dans la salle. FIGUE. 5 - Une personne avec une étiquette « 1 » et une foule avec étiquette « 534 » sont suivis. Les « séjours de groupe à un distributeur automatique » de l'événement a été détecté. Ensuite, nous avons regroupé les trajectoires des objets mobiles détectés 2052 en 21 grappes employant l'algorithme de clustering hiérarchique. Chaque groupe représente alors un type de trajectoire. La caractérisation des trajectoires donne des informations importantes sur le comportement et les flux de personnes. Par exemple, groupe trajectoire 21 montre que les gens qui viennent utilisé les machines distributrices et déplacer ensuite directement aux portes. 13 grappes Trajectoire montre que les gens qui viennent de portes / portails nord et sortant par les portes sud (voir figure 6). J. L. Patino et al. RNTI - X - page 9 Fig. 6 - groupe Trajectoire 21. Les gens se déplacent des distributeurs automatiques aux portes (à gauche);. groupe Trajectoire 13. Les gens qui viennent des portes va portes au sud (à droite) Quelques connaissances qui peuvent être déduites à partir du regroupement des trajectoires est la suivante: - 64% des gens vont directement aux portes sans s'arrêter à la machine de billets - 70% des personnes viennent de l'entrée nord - aux heures de pointe les gens sont 40% plus rapide pour acheter un billet - la plupart des gens passent 10 secondes dans la salle; FIGUE. 7 - Groupe 38 résultant de l'agrégation de la table d'objet mobile. Le panneau de droite indique les trajectoires des prototypes impliqués dans ce cluster. Une fois que les trajectoires d'objets mobiles a été caractérisée, toutes les informations sont ted formatage selon le tableau sémantique donnée dans la section 4.2. Il est alors possible d'exécuter à nouveau l'algorithme de clustering agglomératif cette fois sur la table d'objet mobile. Certains des groupes trouvés sont maintenant détaillées. Figure 7 (Cluster 38) représente le plus grand groupe trouvé; sa description détaillée est donnée dans le tableau 1. Cluster 38 est constitué de « objets inconnus » pour lesquels aucun cas aussi pu être détectés au cours de la phase de suivi (section 2). Ces objets sont principalement associés à des trajectoires de type 4 « sortie portes - aller à la n orth portes (ligne de trajectoire épais- est). En effet, il est aux portes que la reconnaissance d'objets est le plus difficile à réaliser que la plupart des activités a lieu là-bas. En fait, les deux plus grands groupes (non représentés) de l'extraction de données pour l'extraction de l'activité dans les données vidéo suivantes ont la description similaire, mais impliquent respectivement « personne » et « Groupe personne ». Nous avons donc les connaissances que les portes du Nord sont les plus utilisés par les utilisateurs. Cluster 38 Cluster 6 Nombre d'objets 385 15 types de types d'objets: { 'Unknown'} fréq: 385 types: { 'Personne'} fréq: 15 Heure de début (min) [0,1533, 48,4633] [28,09, 46,79] Durée (s) [0,04, 128,24] [2,04, 75,24] types trajectoire types: { '4' '3' '7'} freq: [381 1 3] types: { '13' '12' '19'} freq: [13 1 1] importants types d'événements: { 'vide '} Freq: 385 types: {' inside_zone_Platform'} Freq: 15 TAB. 1 - Propriétés groupe 38 et le groupe 6 après clusterisation de l'objet mobile ta- ble. La figure 8 présente un autre groupe (6) dont seulement 15 mobiles, mais ils comme étant à l'intérieur de la plate-forme tous en com- mon type d'objet étant la personne et tous ont été détectés. Inter- estingly Ceci nous montre en grappe que les trajectoires de type « 12 » et « 19 » peuvent être liés à tra- jectory « 13 » (représentée sur la figure 6). Dans les trois trajectoires prototypes du point de sortie sont des portes sud. FIGUE. 8 - Groupe 6 résultant de l'agrégation de la table d'objet mobile. Le panneau de droite indique les trajectoires des prototypes impliqués dans ce cluster. 7 Conclusion Dans cet article, il a été démontré comment les techniques de regroupement peuvent être appliquées sur des données vidéo pour l'extraction d'informations significatives. regroupement hiérarchique première a été appliquée afin d'obtenir les trajectoires des prototypes qui caractérisent les flux de personnes dans le sous-sol. Ensuite, nous appliquons dans une deuxième étape à nouveau la classification hiérarchique dans le but de réaliser la prise de découverte de connaissances en compte d'autres informations significatives en plus tion comme le mo- type de l'objet détecté et son événement important. Pour cette fin, nous J. L. Patino et al. RNTI - X - page 11 a créé un format de modélisation des connaissances spécifiques qui rassemble toutes les informations à partir d'objets suivis d'intérêt sur la scène. Ce type de représentation permet à l'utilisateur final d'explorer les interactions entre les personnes et les objets contextuels de la scène. De cette façon, il est effectivement possible d'obtenir des statistiques sur l'activité souterraine et ainsi optimiser res- sources disponibles. Nous avons défini des distances sémantiques qui nous permettent de relier différents types d'objets et types de trajectoires différentes. Ce faisant nous travaillons directement sur toutes les fonctions CHAR acterising objets mobiles et d'analyser les variables hétérogènes. Cela nous permettra de trouver des relations entre les gens, leurs trajectoires et leurs occurrences. Dans notre travail d'avenir, nous allons inclure une phase d'apprentissage pour mieux définir les modèles d'objet de la scène et de diminuer le nombre de détection « inconnu » des objets. Nous chercherons à ajouter des fonctionnalités plus significatives qui peuvent caracté- terise une trajectoire. Nous travaillerons aussi pour améliorer les distances sémantiques que nous avons implémentées par exemple que de meilleures relations peuvent être extraites. Références Anjum N., Cavallaro A. (2007). calibrage de la caméra à l'unité d'analyse de comportement sur la base trajectoire, Proceedings of IEEE conférence sur la base surveil- lance et le signal vidéo de pointe, AVSS'07, pp 6. Avanzi A., F. Bremond, Tornieri C, Thonnat M. (2005). Conception et évaluation d'une plate-forme de surveillance des activités ligent tuelle, EURASIP, 2359-2374. Benini, S .; Bianchetti, A .; Leonardi, R .; Migliorati, P. (2006). Extraction des importants résumés vidéo par Dendrogramme Analyse, IEEE Conférence internationale sur le traitement de l'image, 133-136. J. Blatak (2005). Premier ordre motifs fréquents dans Text Mining, Proceedings of EPIA, 344 -. 350. Chia-Hui CH, Kayed, M., Giri, M. R., Shaalan, K.F. (2006). A Surve y de l'information sur le Web Systèmes d'extraction, IEEE Transactions sur les connaissances et l'ingénierie des données, 18: 1411- 1428. Ewerth R., Freisleben B. (2007). Apprentissage semi-supervisé pour la recherche vidéo sémantique, ceedings Pro- de la 6e conférence internationale ACM sur la recherche d'images et de vidéo CIVR '07. Facca FM., Lanzi PL. (2005). Exploitation minière connaissances intéressantes de weblogs: une enquête, génie DataKnowledge, 53: 225-241. Fusier F., Valentin V., F. Brémond, M. Thonnat, Borg D., M. Thirde, Ferryman J. (2007) Comprendre la vidéo pour l'activité complexe de reconnaissance. Vision et machine de Applica- Journal, 18: 167-188. Georis B., F. Bremond, M. Thonnat, Macq B. (2003). Utilisation d'une méthode d'évaluation et de diagnostic pour améliorer le suivi Performances, Actes de la 3e Conférence Inter- IASTED tional sur la visualisation, l'imagerie et de traitement d'image (VIIP), 2. Kaufman L. et P. Rousseeuw J. (1990). Groupes de données dans la recherche. Introduction à l'analyse Cluster. New York: Wiley. L'exploration de données pour l'extraction de l'activité dans les données vidéo Lemoine J., H. Benhadda, Ah-Pine J. (2006). Classement non de documents supervisee Hétérogènes: Application au corpus '20 Newsgroups’, la 11e Conférence internationale IPMU, 3: 2538-2544. Marcotorchino F., P. Michaud (1981). Agrégation des en classification automa- similarités Que, Revue de Statistique Appliquée, 30. McCurley K.S., Tomkins, A. (2004). Mines et découverte de connaissances à partir du Web, ceedings Pro- du 7e Symposium international sur les Architectures parallèles, les algorithmes et les réseaux, 4-9. Naftel A., Khalid S. (2006). Objet le classement des trajectoires spatio-temporelles en utilisant l'apprentissage dans le VISED sans sur- coefficient caractéristique espace, tranactions. des systèmes multimédias, 12: 45-52. J. Oh, Bandi B. (2002). cadre d'exploration de données multimédia pour des séquences vidéo brutes, des données multimédia extraction MDM / KDD, 1-10. Patino J.L, Benhadda H., Corvee E., F. Bremond, Thonnat M. (2007) Modélisation vidéo-données et Discovery, Conférence internationale sur l'ingénierie de l'information visuelle VIE 2007, pp 9. Porikli, F. (2004). L'apprentissage des modèles de trajectoire de l'objet par le regroupement spectral, IEEE Interna- tional Conference sur Multimedia and Expo, ICME '04, 2: 1171-1174. Vu VT., Bremond F., Thonnat M. (2003). Interprétation automatique vidéo: Un nouvel algorithme de reconnaissance de scénario temporel, Actes du IJCAI'03, 1295-1302. J. Xing, Ah-Hwee T. (2005). Exploitation minière connaissance ontologique du texte spécifique à domaine docu- ments, cinquième IEEE Conférence internationale sur l'exploration de données, 4 pp. Résumé L'exploration de larges bases de Données techniques vidéo is une tache Qui DEVIENT possible Grâce aux Avancées Dans la détection et le Suivi d « objets. Les methods d'information de fouille le regroupement Comme Sont typiquement employés. Celles-ci were principa- la répandrai lement segmentation Appliquées / indexation vidéo l'extraction de Mais sur l'activité Connaissances Dans la vidéo Présente was only Partiellement adressee. Dans mes this article Présentons commentaire NOUS techniques SCÉ PEUVENT Être utilisées répandrai de l'informations Traiter vidéo répandrai l'extraction de connaissances heuristiques. Tout d'Abord, les objets d'en interest temps Sont détectés réel. Salle de bains, Dans un treatment Supplémentaire, nous recherchons à des nou- velles Extraire en deux etapes Connaissances: 1) l'extraction des motifs characteristics des Trajectoires des personnes Dans la vidéo. 2) l'extraction des motifs d'interaction Entre les personnes et les objets contextual Dans la scène. Dans les deux CAS, nous appliquons un regroupement hiérarchisée Que agglomératif. Nous Présentons des Résultats sur des vidéos obtenus du métro de Turin (Italie)"
819,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Discretization of Continuous Features by Resampling,"Les arbres de décision sont largement utilisés pour générer des classificateurs à partir d'un ensemble de données. Le processus de construction est une partitionnement récursif de l'ensemble d'apprentissage. Dans ce contexte, les attributs continus sont discrétisés. Il s'agit alors, pour chaque variable à discrétiser de trouver l'ensemble des points de coupure. Dans ce papier nous montrons que la recherche des ces points de coupure par une méthode de ré-échantillonnage, comme le BOOTSTRAP conduit à des meilleurs résultats. Nous avons testé cette approche avec les méthodes principales de discrétisation comme MDLPC, FUSBIN, FUSINTER, CONTRAST, Chi-Merge et les résultats sont systématiquement meilleurs en utilisant le bootstrap. Nous exposons ces principaux résultats et ouvrons de nouvelles pistes pour la construction d'arbres de décision.","Taimur Qureshi, Djamel Abdelkader Zighed",http://editions-rnti.fr/render_pdf.php?p1&p=1000622,http://editions-rnti.fr/render_pdf.php?p=1000622,en,"Discrétisation des fonctions continues par rééchantillonnage Taimur Qureshi *, D.A.Zighed * * Université de Lyon 2 - Lab ERIC 5, Avenue Pierre Mendès France, 69676 Bron Cedex - France taimur.qureshi, abdelkader.zighed@univ-lyon2.fr CV. Les arbres de décision verser Sont largement utilisés des classi ficateurs Générer à partir d'un ensemble de Données. Le construction de l'Est Processus Une récursif de l'partitionnement ensemble d'apprentissage. Dans CE Contexte, les at- tributs Continus discrétisés fils. Il mio, les verser variables Alors each à discrétiser de l'Ensemble des Trouver Reportages des points de coupure. Dans papier nous montrons CE Que la recherche des bureaux des points de coupure par Une méthode de ré-échantillonnage, le BOOTSTRAP conduit Comme à des Meilleurs Résultats. Nous Avons tested this approach with the methods de discrétisation Principales MDLPC Comme, FUS- BIN, FUSINTER, CONTRASTE, Chi-Merge et les systématique- ment Résultats Sont en Utilisant le Meilleurs bootstrap. Nous exposons bureaux et Ouvrons principaux de Résultats pistes nouvelles Pour la construction d'arbres de décision. 1 Introduction Dans le processus de découverte de connaissances à partir d'un ensemble de données brutes, nous avons d'abord prétraiter les données pour éliminer le bruit et la poignée manquant champs de données. Ensuite, la transformation des données, telles que la réduction du nombre de variables et la discrétisation des attributs définis sur un do- principal continu, est souvent réalisée, qui est ensuite fourni à un algorithme d'extraction de données. L'un des problèmes les plus importants et complexes dans l'exploration de données est en relation avec le processus de transformation tel que discrétisation qui consiste à convertir des données numériques sous une forme symbolique ou discrète. Ku Siak [9] a souligné que la qualité de la découverte de connaissances à partir d'un ensemble de données peut être améliorée par discrétisation parce que beaucoup des techniques de découverte de connaissances sont très sensibles à la taille des données en termes de complexité. Ainsi, le choix de la technique de discrétisation a des conséquences importantes sur le modèle d'induction utilisé comme le CART [2]. En outre, les plages de valeurs numériques ne sont pas assez facile pour les fonctions d'évaluation à la poignée dans un domaine nominal; par exemple, les versions originales de la machine populaire apprentissage al- gorithms ID3 peuvent être utilisés que pour les données qualitatives et Quinlan [11] ont dû transformer les nuous en valeurs nents discrètes dans son apprenant l'arbre de décision C4.5. De nombreux algorithmes de classification du monde réel sont difficiles à résoudre, sauf si les attributs continus sont discrétisées. Il est difficile de les intervalles de- Termine une discrétisation des attributs numériques qui a un nombre infini de candidats. Une procédure de discrétisation simple, divise la plage d'une variable continue en intervalles égaux de largeur égale ou à des intervalles de fréquence. Fayyad et al. [6] a proposé un algorithme qui pendent classe de- réduire le nombre de valeurs attribuées en maintenant la relation entre les valeurs de classe et d'attribut. Liu et al. [10] méthodes classées de discrétisation de cinq points de vue différents: par rapport supervisé sans supervision, statique vs dynamique, global vs local, haut vers le bas vs bas vers le haut, et directement contre incrémental. méthodes ne font pas non surveillés utilisation des informations de classe dans le processus de discrétisation alors que les méthodes supervisées utiliser. Si aucune de mation de classe est disponible, discrétisation non supervisée est la seule méthode possible. Les méthodes dynamiques exercent une discrétisation des valeurs continues durant le processus de classification, tandis que la discrétisation des méthodes statiques avant processus de classification. méthodes locales utilisent la région locale de l'espace d'instance alors que les méthodes globales utilisent tout l'espace. Top-down méthodes comme FUSBIN, MDLPC et CONTRASTE [5-7] commencent par un intervalle et des intervalles séparés dans le processus de discrétisation et sont basées principalement sur la binarisation dans un sous-ensemble de données de formation. Bien que, les méthodes ascendantes comme FUSINTER [5] et Chi-fusion [4] divisé complètement toutes les valeurs continues des intervalles d'attributs et de fusion dans le processus de discrétisation. Dans ce article, nous nous concentrons sur ces deux types de stratégies pour déterminer de meilleurs points de discrétisation et de fournir des comparaisons en termes de taux de qualité et de prévision [1]. Notre objectif est de trouver un moyen de produire de meilleurs points de discrétisation. Auparavant, diverses études ont été réalisées pour estimer les points de discrétisation à partir d'échantillons. De manière significative, dans [1], un ensemble d'échantillons d'apprentissage sont utilisés pour approcher les meilleurs points de discrétisation de l'ensemble de la population, mais également valoir que l'échantillon d'apprentissage est une approximation de l'ensemble de la population, de sorte que la solution optimale construit sur un seul ensemble échantillon est pas nécessairement celle qui est globale. Cela conduit d'in- terprétation nous d'utiliser une approche de ré-échantillonnage [3] pour déterminer de meilleures distributions des points de discrétisation, où chaque point a une probabilité d'être le point de discrétisation exacte vers l'ensemble de la population. Ce faisant, nous essayons d'améliorer la qualité de discrétisation et une meilleure estimation des points de discrétisation de l'ensemble de la population, ainsi, traiter le problème de discrétisation dans le domaine statistique avec de nouveaux résultats. Dans cet article, nous montrons que en effectuant rééchantillonnage en utilisant bootstrap [8], nous déterminons une meilleure estimation de la distribution des points de discrétisation sur l'ensemble de la population, ce qui est montré améliorer le taux de prédiction de la discrétisation atteint. De plus, nous améliorons encore la qualité et le taux moyen de tion obtenu à partir rééchantillonnage prédictions en appliquant un protocole de sélection de point de discrétisation. Ce protocole sélectionne les points de coupe en fonction de certains critères (par exemple) de l'entropie de la distribution de point de fréquence d'amorçage de ré-échantillonnage obtenues à partir de rééchantillonnage n fois et améliore encore le taux de prédiction. De plus, nous comparons les taux de prédiction de différentes descendante et les stratégies ascendantes en utilisant rééchantillonnage. Dans la section 2, nous exposons le cadre de discreti- sation et de définir les ensembles de données utilisés dans nos calculs. En 3, nous donnons une illustration de notre travail et des résultats en appliquant la méthodologie à un exemple d'ensemble de données, puis à un beaucoup plus dé- coupé la queue, le jeu de données d'onde de Breiman [2]. Nous comparons également plusieurs critères basés stratégie descendante et ascendante comme dans [1], comme Chi-fusion basé sur le droit statistique χ2, FUSBIN et FUSIN- TER sur la base du principe d'incertitude, MDLPC basée sur le gain d'information et CONTRASTE que prend en compte l'homogénéité des classes et aussi la densité de points. En fin de compte, nous concluons avec des observations, des déductions et des propositions pour les travaux futurs. 2 Définitions et cadre et formulation Notations: (.) ​​Soit X une valeur d'attribut sur la ligne réelle <. Pour chaque exemple ω d'un ensemble d'apprentissage Ω, X (ω) est la valeur prise par l'attribut X (.) À ω. L'attribut C (.) Est appelée la variable endogène ou classe et est généralement symbolique et si un exemple appartient à une classe c, nous avons C (ω) = c. Nous supposons également que C (ω) est connue pour tout ω de l'échantillon d'apprentissage ensemble Ω. Ainsi, nous essayons de construire un modèle, noté Φ, de telle sorte que, idéalement, nous avons: (.) ​​C = Φ (X1 (), ..., Xp ().).). La discrétisation de X (.) Consiste à diviser le domaine Dx de l'attribut continu X (.), En k intervalles Ij, j = 1, ...., k, avec k ≥ 1. On note Ij = [dj-1 , dj] avec les d'js appelés les points de discrétisation qui sont déterminés en tenant compte de l'attribut particulier C (.). Taux de prédiction: Nous mesurons la qualité de discrétisation en tenant compte du taux de prédiction, qui est calculé comme suit: τj = carte {ω∈Ωt / C (ω) = C (ω)} carte {Qt} On note τjs les bon taux de prédiction résultant de la discrétisation de Xj obtenue en appliquant la méthode de l'échantillon q co S ou τjt par application sur l'échantillon d'essai Qt. Ensemble de données: Dans cet article, nous utilisons deux ensembles de données différentes. En premier lieu, on utilise un petit ensemble de données de 110 individus correspondant à un problème à deux classes représenté sur la figure 1. Le deuxième grand ensemble de données utilisé pour des comparaisons et des résultats est l'ensemble de données de forme d'onde de la Breiman [2] ayant 4590 individus et les 21 attributs X (. ), qui correspondent à une trois problèmes de classe. FIGUE. 1 - Runs Ri et points limites dj pour un échantillon de 2 classes ""x"" et ""o"". 3 Résultats et comparaisons 3.1 Illustration en utilisant l'exemple de données de la figure 1 Considérons un ensemble de données de la figure 1 de 110 personnes ayant deux classes. Nous effectuons discrétisation FUSBIN avec λ = 0,91 sur chaque échantillon aléatoire et d'amorçage de la taille 30 et de générer des 500 échantillons. La figure 2 nous donne la répartition des points de discrétisation de 500 bootstrap et des échantillons aléatoires. On voit que la discrétisation obtenue à partir bootstrap semblent être un peu plus généralisé et bien défini sur les quatre petits intervalles; 4,5 à 6, 6,5 à 9, de 12,5 à 14,5 et 22,5 à 27. Bien que, dans l'échantillonnage aléatoire, la distribution des points ne semble pas être mal définie dans une grande région de valeurs de 18 à 27. Nous pensons en outre que cette différence augmente que l'ensemble de données devient plus grande que nous verrons avec l'ensemble de données de Breiman. Nous avons également calculé le taux de prédiction moyen = 1100 mV à-dire Σ100 j = 1 τ v j par l'estimation des valeurs moyennes de chacun des échantillons ci-dessus 500. Nous avons trouvé les taux d'amorçage et d'échantillonnage aléatoire 22 et 21,1 respectivement montrant que les échantillons de bootstrap meilleures performances, avec cette différence augmente encore avec la complexité ajoutée et la taille de la population, comme indiqué dans le paragraphe suivant. FIGUE. 2 - distribution de points de discrétisation de 500 échantillons aléatoires et bootstrap. Ensuite, nous améliorons le taux de qualité et de prévision en introduisant une notion de protocole de sélection de point de discrétisation. Ce protocole sélectionne les points de discrétisation à partir d'une distribution de fré- quence point donné, comportant plus grande probabilité d'occurrence, et se divise sur ces points si un critère (par exemple l'entropie) est satisfaite. Pour illustrer, à partir de la figure 2, nous voyons que le point le plus élevé est bable pro- 25.5; si nous prenons ce point et de diviser la population si un certain critère (entropie FUSBIN) est satisfaite. Nous continuons notre processus sur les scissions obtenus de manière descendante, jusqu'à ce que le critère permet le fractionnement ou encore tous les points de la distribution de fréquence ont été al- prêt choisi. Nous avons appliqué ce protocole à la fois sur l'amorce et des échantillons aléatoires et il sélec- ted 6 sur 30 et 8 sur 36 points de discrétisation des deux répartitions de points de fréquence respectivement. Nous avons calculé le taux de prédiction que 22 pour bootstrap et 19,5 pour Pling échan- au hasard, ce qui démontre la meilleure qualité de discrétisation obtenue par sélection de bootstrap. Nous soutenons en outre que l'échantillonnage nous donne beaucoup de variation des taux de prédiction pour savoir échantillons bootstrap le taux de prédiction varie du 17 au 26 et donc, il est difficile d'obtenir une estimation généralisée des points de discrétisation de la population d'origine. Ici, notre protocole permette d'obtenir des points de discrétisation bien définis et donc, donnent une meilleure estimation des points de discrétisation d'origine. 3.2 Analyse et résultats à l'aide des données de signal de Breiman Pour cette section, nous utilisons l'ensemble de données d'ondes du Breiman. Nous avons généré 100 bootstrap et des échantillons aléatoires et cob co S; s = 1, ..., 100 des 300 points chacune et cot un échantillon d'essai de 4590 points. Pour tout ω partir de l'échantillon, on a un vecteur de 21 éléments désignés par (X1 (ω), ..., X21 (ω)) et une étiquette C (ω). Nous avons répété le processus décrit ci-dessus avec l'ensemble de données de forme d'onde. Nous avons pris chaque variable de l'ensemble de données et d'amorçage généré et des échantillons de dom ran- comme ci-dessus. Ensuite, nous avons réalisé FUSBIN à la fois les 100 bootstrap et des échantillons aléatoires et obtenu des taux de prédiction moyenne de 196 et 180 respectivement, montrant une meilleure formance per- avec un échantillonnage bootstrap. , Nous avons appliqué ensuite notre protocole de point de discrétisation de sélection sur la répartition des points obtenus et sélectionné les meilleurs points (en utilisant le critère de FUSBIN) des deux méthodes d'échantillonnage. Nous avons trouvé un taux de prédiction de 309 pour les points obtenus à partir de la distribution d'amorçage et d'une valeur moindre de 271 pour l'échantillonnage aléatoire montrant une quantité importante d'amélioration du taux de prédiction en utilisant rééchantillonnage (ou bootstrapping). Enfin, nous comparons FUSINTER, FUSBIN, CONTRASTE, MDLPC et Chi-fusion par rééchantillonnage. Cela se fait en deux par deux selon le mode opératoire suivant; Soit u et v les deux méthodes pour comparer. Tout d'abord, on obtient des points de discrétisation de 100 échantillons bootstrap et créer une distribution de points de fréquence pour chaque variable. Puis, en utilisant notre protocole de sélection, nous Diff dans Mean P-Rate MDLPC ChiMerge CONTRASTE FUSBIN FUSINTER MDLPC X 52,7 10,6 6,9 7,3 ChiMerge X -42,1 -45,4 -45 CONTRASTE X -3,7 -3,3 FUSBIN X 0,4 X FUSINTER TAB. 1 - calculées Résultats: Différence de prédiction taux moyen de sélection de points de discrétisation μuv de ces fréquences de distribution de points, en appliquant le critère de la méthode respective (à partir de laquelle les points de discrétisation initiaux ont été obtenus). Nous avons ensuite le calcul des taux de prédiction τjt des points de discrétisation sélectionnés de chaque méthode par rapport à l'ensemble de l'échantillon de test Qt. Nous formons la différence Γuv des deux taux de prédiction obtenus et conclure que u est meilleur que v si Γuv est significativement supérieure à 0. Le tableau 1 présente la comparaison en termes de la différence des moyens μuv des taux de prédiction de tous les riables va- . Les valeurs positives de μuv indiquent que la méthode de la rangée est meilleur que le procédé dans la colonne. En dehors de la méthode Chi-fusion dont les résultats sont relativement pauvres, toutes les autres méthodes ont des différences relativement plus petites. Cependant, parmi ces méthodes MDLPC semblait être le meilleur avec une complexité beaucoup moins de temps. FUSBIN et FUSINTER ont également eu une complexité temporelle plus faible par rapport à CONTRASTE qui avait une complexité quadratique qui devait être pris en compte lorsque le nombre d'exemples devient trop élevé. 4 Conclusion L'échantillon d'apprentissage est une approximation de l'ensemble de la population, de sorte que la sation optimale discreti- construite sur un ensemble unique de l'échantillon est pas nécessairement celui global optimal. Rééchantillonnage donne une meilleure estimation de la répartition des points de discrétisation en termes de parvenir à une répartition bien définie. L'application de notre protocole de point de discrétisation de sélection sur la distribution de fréquence obtenue par ré-échantillonnage, améliore considérablement la qualité de discrétisation et le taux de prédiction et donc, à l'approche d'une solution globale optimale. De plus, le même protocole, lorsqu'il est appliqué à la distribution de point de fréquence d'échantillons aléatoires, obtenu des améliorations beaucoup moins du taux de prédiction par rapport à bootstrap. Nous avons appliqué notre protocole (après rééchantillonnage) à diverses méthodes. À l'exception de Chi-fusion, toutes les autres méthodes offrent de faibles variations en termes de taux de prédiction. MDLPC réalise le meilleur et FUSBIN réalise la meilleure complexité temporelle, ce qui est un point clé en traitant avec beaucoup d'exemples. Comme les travaux futurs, nous appliquerons cette approche de discrétisation dans le contexte des arbres de décision, pour voir si elle améliore la performance globale ou non. Mais, en même temps, la réalisation de cette approche doit répondre à d'autres questions telles que la complexité du temps. Cela peut également conduire à appliquer les éventuels points de discrétisation dans le contexte de discrétisation floue ou douce [12] dans les arbres de décision. Références bibliographiques 1. D.A.Zighed, S.Rabaséda, R.Rakotomalala. Méthodes de discrétisation en ning Supervisé. Mémorisation en Encyclopédie des sciences informatiques et de la technologie, vol40, pp 35-50, 1998. 2. L.Breiman, J.H.Friedman, R.A.Olshen, C.J.Stone. Et régression des arbres. Wadsworth International, San Francisco, 1984. 3. L.Wehenkel. Une décision fondée sur la qualité de l'information Arbre Méthode Élagage. Compte rendu de la 4ème Conférence internationale sur le traitement de l'information et la gestion de la sécurité dans Un- fondée sur la connaissance des systèmes, IPMUŠ92 (1992). 4. R.Kerber. Discrétisation des attributs numériques. Actes de la Confé- rence nationale Dixième sur l'intelligence artificielle, MIT Press, Cambridge, MA, 1992, pp.123-128. 5. D.A.Zighed, R.Rakotomalala et S.Rabaséda. Procédé de discrétisation attributs continus dans l'induction des graphiques. Actes du 13e Rencontres européennes sur les tics Cyberne- et recherche sur le système, 1996, pp.997-1002. 6. U.M.Fayyad, K.Irani. mult i-intervalle discrétisation des attributs continu Apprécié pour la classification d'apprentissage. Actes de la 13e Conférence internationale conjointe sur ficielle Intelligence ar-, Morgan Kaufmann, San Mateo, Californie, 1993, pp1022-1027 7. T.Van de Merckt. Les arbres de décision dans les espaces d'attributs numériques. Actes de la 13e Conférence internationale conjointe sur l'intelligence artificielle, Morgan Kaufmann, San Mateo, Californie, 1993, pp 1013-1021. 8. Mooney, C Z Duval, R D (1993). Bootstrapping. Une approche non paramétrique statis- tique Inference. Série de documents de l'Université Sage sur les applications quantitatives des sciences sociales, 07-095. Newbury Park, CA: Sage. 9. A. Kusiak. méthodes de transformation des fonctions dans l'exploration de données. IEEE Trans. sur l'électronique fabrication d'emballages, 24 (3): 214-221, 2001. 10. H. Liu, F. Hussain, C. L. Tan, et M. Dash. Discrétisation: Une technique permettant. Data Mining et Knowledge Discovery, 6 (4): 393-423, 2002. 11. J. R. Quinlan. Une meilleure utilisation des attributs continus dans C4.5. Journal of gence artificielle Intelli Recherche, 4: 77-90, 1996. 12. Y. Peng et P. Flach. Douce discrétisation pour améliorer la décision continue Arbre à induction. Intégration des aspects de l'exploration de données, aide à la décision et méta-apprentissage, pages 109-118, les notes de l'atelier ECML / PKDD'01, Septembre 2001. Résumé induction des arbres de décision a été largement utilisé pour générer des classificateurs à partir des données de formation par un processus de division récursive la l'espace des données. Dans le cas de la formation sur les données d'une valeur continuous-, les attributs associés doivent être discrétisées à l'avance ou au cours du processus d'apprentissage. Nous générons points de discrétisation en effectuant rééchantillonnage sur l'ensemble de données d'origine, puis produire une sélection de points de discrétisation en utilisant notre sélection de ré-échantillonnage col proto-. Nous générons aussi des points de discrétisation par échantillonnage aléatoire ordinaire et on calcule le taux de prédiction des points de discrétisation obtenus à l'aide à la fois les techniques d'échantillonnage et rééchantillonnage. Ce processus est répété en utilisant les différentes stratégies de discrétisation mentionnées ci-dessus. Ainsi, l'objectif de cet article est d'observer si la technique rééchantillonnage peut conduire à de meilleurs points de discrétisation, ce qui ouvre un nouveau paradigme pour la construction d'arbres de décision."
823,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Enhancing Personal File Retrieval in Semantic File Systems with Tag-Based Context,"Recently, tagging systems are widely used on the Internet. On desktops, tags are also supported by some semantic file systems and desktop search tools. In this paper, we focus on personal tag organization to enhance personal file retrieval. Our approach is based on the notion of context. A context is a set of tags assigned to a file by a user. Based on tag popularity and relationships between tags, our proposed algorithm creates a hierarchy of contexts on which a user can navigate to retrieve files in an effective manner.","Ba-Hung Ngo, Frédérique Silber-Chaussumier, Christian Bac",http://editions-rnti.fr/render_pdf.php?p1&p=1000558,http://editions-rnti.fr/render_pdf.php?p=1000558,en,"Microsoft Word - TaggingSystemEGC2008-Final.doc Amélioration de fichiers personnels de récupération dans le fichier sémantique des systèmes avec Tag-Based Contexte Hung Ba Ngo 1,2, Silber-Chaussumier Frédérique 1, Christian Bac 1 Institut National des Télécommunications-France 1, Université Cantho-Vietnam 2 {hung.ngo_ba, frederique.silber-Chaussumier, Résumé christian.bac}@int-edu.eu. Récemment, des systèmes de marquage sont largement utilisés sur Internet. Sur les ordinateurs de bureau, les balises sont également pris en charge par certains systèmes de fichiers sémantiques et des outils de recherche de bureau. Dans cet article, nous nous concentrons sur l'organisation de balise personnelle pour améliorer la récupération de fichiers personnels. Notre approche est basée sur la notion de contexte. Un contexte est un ensemble de balises affectées à un fichier par un utilisateur. Sur la base de la popularité de l'étiquette et des relations entre les balises, notre algorithme proposé crée une hiérarchie des contextes sur lesquels un utilisateur peut naviguer pour récupérer des fichiers de manière efficace. 1 Introduction De nos jours, les systèmes de marquage tels que (délicieux) sont largement utilisés sur Internet. Ces systèmes de marquage permettent aux utilisateurs d'ajouter des mots-clés (ou tags) aux ressources Internet sans compter sur un vocabulaire contrôlé. Sur le bureau, les balises sont également pris en charge par certains systèmes de fichiers sémantiques et des outils de recherche de bureau. Les utilisateurs de l'EPA (Padioleau, 2005), par exemple, peuvent balises attribuer manuellement un fichier Jpeg pour annoter les noms des personnes dans cette photo pour une récupération ultérieure. Avec les étiquettes, les utilisateurs sont flexibles pour décrire leurs opinions et leurs intérêts sur les fichiers (ou ressources). Par conséquent, les fichiers personnels des utilisateurs sont classés par balises et chaque utilisateur dispose d'un vocabulaire personnel de balises. Les utilisateurs peuvent ensuite récupérer les fichiers en utilisant des expressions logiques de tags. Par défaut, les systèmes de marquage sont plus appropriés pour la récupération de fichiers à l'aide de la navigation interrogation. Cependant, des expériences dans la gestion des informations personnelles (et al Barreau., 1995), et (Khoo et al., 2007) montrent que la plupart des utilisateurs préfèrent la navigation que l'interrogation (recherche logique) que la récupération de leurs fichiers à partir d'un ordinateur de bureau. Telle est la raison pour laquelle récemment des systèmes de marquage comme Delicious sur le Web ou l'EPA (Padioleau, 2005) et TagFS (Bloehdorn et al., 2006) sur le concentré de bureau sur l'organisation de l'étiquette pour aider les utilisateurs à des balises de navigation pour la récupération de fichiers. Nous continuons d'améliorer la récupération des fichiers personnels dans les systèmes de marquage avec la recherche contextuelle. Un contexte dans notre approche est un ensemble de balises affectées à un fichier (ou ressource) par un utilisateur. Sur la base de la popularité de l'étiquette et des relations entre les balises, notre algorithme proposé crée une hiérarchie des contextes sur lesquels un utilisateur peut naviguer pour récupérer des fichiers de manière efficace. Dans cet article, nous présentons d'abord les techniques intéressantes pour l'organisation de l'étiquette à l'article 2; introduire TAG- contexte basé et comment améliorer les systèmes de marquage avec la recherche contextuelle dans la section 3. Notre algorithme pour la création d'un graphe acyclique orienté de Tags (DAGOT) basé sur la popularité de l'étiquette et la relation entre les balises est dans la section 4. Ce DAGOT est utilisé d'organiser des contextes dans une structure hiérarchique afin que nous puissions améliorer la récupération de fichiers personnels avec la recherche contextuelle. Une mise en œuvre et les résultats expérimentaux utilisant des données réelles sont présentées à la section 5. Notre conclusion et les perspectives sont dans la dernière section. Amélioration de la récupération de fichiers personnels avec le contexte dans SFS basée sur les balises 2 Travaux connexes (délicieux) est un serveur de signets en ligne bien connu où les utilisateurs peuvent utiliser leurs propres étiquettes pour organiser et récupérer leurs signets. Dans Delicious, sont considérés comme liés à deux étiquettes ou plus associées au même signet. Le nombre de signets associés à une étiquette par un utilisateur est appelé la popularité de l'étiquette. Quand une étiquette est choisie, une liste de signets marqués avec et une liste de ses balises connexes sont retournées. Tags connexes sont la façon de naviguer entre les balises de revisiter des signets intéressants. Toutefois, lorsque le nombre d'augmentation des signets et des étiquettes, l'analyse du résultat pour un signet ou choisir une étiquette de droite rel balises ées pour résultat affiner la recherche devient une tâche énorme pour un utilisateur. Sur les ordinateurs de bureau, les balises sont également utilisés pour la récupération de fichiers personnels. Pleins feux sur les utilisateurs (Apple Computer, 2005) peut attribuer un mot-clé à un ensemble de fichiers sémantiquement liés et récupérer ces fichiers à l'aide d'un simple mot-clé de recherche. Dans le domaine du système de fichiers, l'EPA (Padioleau, 2005) permet aux utilisateurs de fichiers associés avec des balises représentant les propriétés du fichier. EPA soutient axiomes entre les balises comme une relation parent-enfant. Les utilisateurs peuvent créer manuellement axiomes entre leurs balises. Des axiomes, une taxonomie des balises est créé. Les utilisateurs peuvent naviguer sur la taxonomie pour la récupération de fichiers comme ils le font avec les répertoires traditionnels. TagFS (Bloehdorn et al., 2006) organise des balises dans les balises connexes comme la façon de Delicious. Par conséquent, TagFS a la même difficulté lorsque le nombre de fichiers et augmente les balises. ROOT article (5) VERSION FINALE (3) RIVF (2) vacances (2) Hanoi (1) Paris (1) 2007 (8) file1.doc file2.pdf file3.doc file4.pdf contexte supérieur inférieur contexte JDIR (4) { 2007, vacances, Hanoi} {file8.jpg 2007, vacances, Paris} {file7.jpg 2007, VERSION FINALE, RIVF} {article file6.pdf 2007, RIVF} {article file5.doc, JDIR 2007,} fichier4 VERSION FINALE. pdf {article, JDIR 2007,} file3.doc {VERSION FINALE article, JDIR 2007} {article file2.pdf, JDIR 2007} file1.doc Balises attribué à fileFile {2007, vacances, Hanoi} {file8.jpg 2007, vacances, Paris} {file7.jpg 2007, VERSION FINALE, RIVF} {article file6.pdf 2007, RIVF} {article file5.doc, JDIR 2007,} file4.pdf {VERSION FINALE article, JDIR 2007,} file3 VERSION FINALE. doc {article, JDIR, 2007} {file2.pdf article, Jdir, 2007} file1.doc Mots clés assignés à fileFile 1Hanoi 1Paris 2Vacation 2RIVF 3FinalVersion 4JDIR 5Article 82007 PopularityTag 1Hanoi 1Paris 2Vacation 2RIVF 3FinalVersion 4JDIR 5Article 82007 PopularityTag A. - Ex d'une affectation amplement faites par un utilisateur. B. - Popularité de tagsC. - Le correspondant DAGOT figure. 1 - Exemple d'un DAGOT créé de popularité tag et la relation entre les mots clés. 3 Définition du contexte basée sur les balises Un utilisateur dans un système de marquage permet un poste d'assigner un fichier avec un ensemble de balises. Chaque étiquette représente un concept ou un objet lié au fichier. Et l'ensemble des balises affectées au fichier représente un sujet ou un sujet que le propriétaire du fichier pense que le problème de fichier. Par exemple, un utilisateur H. B. Ngo et al. RNTI - X - peut affecter l'ensemble des balises {vacances, Paris, 2007} dans le fichier maphoto.jpg à rappeler que la photo a été prise pendant les vacances de l'été '07 à Paris. Nous appelons un ensemble de balises affectées à une ressource par un utilisateur un contexte basée sur les balises (ou le contexte pour faire court). Le sens d'un contexte est agrégée de ses éléments. Un contexte est plus significatif qu'une balise. Par exemple, le contexte {vacances, Paris, 2007} est tout à fait plus pertinent que l'étiquette 2007. En fait, lors de l'attribution d'un ensemble de balises à un fichier, un utilisateur veut classer le fichier en utilisant le contexte représenté par cet ensemble de tags pour une récupération ultérieure. Ainsi, des systèmes d'étiquetage devrait fournir aux utilisateurs récupération de fichiers répondant sur le contexte - les caractéristiques utilisées pour classer les fichiers. La figure 1.A est un exemple de messages effectués par un utilisateur. Si l'utilisateur effectue une recherche basée sur des balises avec l'étiquette article, cinq fichiers (de fichier1 à fichier5) seront retournés. Ces fichiers appartiennent à trois contextes {article, Jdir, 2007}, {article, JDIR 2007,} et {VERSION FINALE article 2007, RIVF}. Une étiquette participe habituellement dans de nombreux contextes. La figure 1.A ci-dessus montre que dans Jdir tag participe deux contextes connexes {article, Jdir, 2007} et {article, Jdir, 2007,} VERSION FINALE. Le premier est plus général que celui-ci. Dans la section suivante, nous vous proposons une méthode pour classer les contextes dans une structure hiérarchique: du général à des contextes spécifiques de. 4 Récupération de fichiers à base contextuelle Nous proposons un système qui transfère automatiquement un utilisateur dans le contexte le plus populaire qui participe un tag donné dans. Les fichiers qui ont frappé le contexte seront retournés. Si pas satisfait, l'utilisateur peut descendre à un contexte plus spécifique pour affiner sa demande ou de modifier jusqu'à un contexte plus général. Notre solution est basée sur la popularité de l'étiquette et de la relation entre les balises pour classer les contextes dans une structure hiérarchique: du général à des contextes spécifiques de. Nos classifie algorithme proposé balises personnelles dans un graphe acyclique orienté selon la popularité tag. Ce graphe acyclique orienté de Tags (DAGOT) permet d'organiser automatiquement les fichiers dans des contextes appropriés, afin d'identifier le contexte plus général d'une étiquette et permettre à un utilisateur de naviguer d'un contexte à un autre pour récupérer des fichiers d'une manière efficace. Un DAGOT a trois types de noeuds: noeuds de balise, nœuds feuilles et un nœud racine. Un noeud de balise représente une étiquette créée par un utilisateur. Il a une étiquette et la popularité. Un nœud de balise peut avoir de nombreux parents et beaucoup d'enfants. Un noeud feuille représente un fichier étiqueté par un utilisateur. Il bénéficie d'un emplacement, comme une URL, à partir de laquelle le fichier est accessible. Un noeud feuille a un ou plusieurs noeuds de balise en tant que noeuds parents. Un noeud racine est le début d'une DAGOT. Un nœud racine est le nœud le plus populaire de l'étiquette. Il existe trois types de bords: bords concernés, bords moins populaires et les bords des feuilles. Lorsque deux balises sont affectées au même document, on dit qu'ils sont deux balises connexes. Un bord lié relie deux noeuds de variables liées ensemble. La direction du bord est de la plus populaire noeud de balise (noeud de balise supérieure) à celle moins populaire (noeud d'étiquette inférieure). Si deux nœuds d'étiquette connexes ont la même popularité, celui qui a la plus petite étiquette est la partie supérieure. L'étiquette sans parent prendra le nœud racine en tant que parent. A moins populaire bord est un bord connexe qui relie un noeud de balise et de ses noeuds supérieurs moins populaires. Les moins populaires noeuds supérieurs d'un nœud de balise sont appelés les parents du nœud de balise. Les parents d'un nœud de balise ne doivent pas être liés les uns aux autres. Nous soutenons qu'une étiquette t a trois noeuds de balise supérieur qui ont les étiquettes « t1 », « t2 » et « t3 » et correspondant popularités 3, 2 et 2. Dans ce cas, « t2 » et « t3 » sont candidats à la parents de t. Si « t2 » et « t3 » ne sont pas des balises connexes, ils sont tous deux acceptés comme les parents de t. Dans le cas contraire, seule la balise « t3 » sera acceptée en tant que parent de t parce que l'étiquette « t3 » est supérieure à l'étiquette « t2 ». Lorsqu'une balise est affecté à un fichier, une feuille Amélioration de la récupération des fichiers personnels avec le bord de SFS contexte basé sur les balises est créé à partir du nœud de balise au noeud de feuille respectivement. La figure 1.C représente la DAGOT des balises dans la figure 1.A. Le mince, l'épaisseur et les flèches de points représentent respectivement le connexe, les moins populaires et les bords des feuilles. Pour être concis, nous montrons que les fichiers associés à étiquette JDIR. Les DAGOT montre que balise JDIR accepte étiquette article comme parent et son enfant comme VERSION FINALE. Jdir participe dans les deux contextes {2007, article, JDIR} et {2007, article, JDIR,} VERSION FINALE. Le premier est le plus populaire contexte qui contient ses balises connexes les plus populaires. Tel est le contexte que le système retourne lorsqu'un utilisateur effectue une recherche contextuelle avec le tag donné JDIR. De JDIR, l'utilisateur peut affiner sa requête de recherche en déplaçant sa balise enfant VERSION FINALE. Les moins populaires bords maintiennent une relation hiérarchique entre les contextes. Ils sont utilisés comme guide pour l'utilisateur de passer d'un contexte à l'autre. (1) Res (t) ← {r∈R | (R, t) ∈P}: Les bords des feuilles d'une étiquette à toutes ses ressources (2) Tag (r) { ← t∈T | (R, t) ∈P}: Les bords des feuilles de tous les tags associés à une ressource (3) Pop (t) ← carte ({r∈R | (r, t) ∈P}): La popularité d'une balise ( 4) Rel (t1, t2) ← ∃r∈R | (R, t1) ∈P et (r, t2) ∈P: Vérifiez si un avantage lié existe entre les balises (5) supérieur (t ») ← {t  | Rel (t », t) et Pop (t)> Pop (t »)}: bords connexes arrivent à l'étiquette t »(6) supérieur (t ») ← {t | Rel (t 't) et Pop (t) = Pop (t ') et l'étiquette (t) <label (t')}: Si les étiquettes t' et t ont la même popularité, l'étiquette de t doit être plus petite que l'étiquette de t »(7) PMOP (t) ← min {Pop (p) | p∈Upper (t)}: La plus petite popularité parmi les balises supérieures de t (8) Parent (t) ← {p | p∈Upper (t)} & Pop (p) = PMOP (t) &! ∃P » & p'∈Upper (t) et Rel (p, p ') & Pop (p') = PMOP (t) & étiquette (p ') <Etiquette (p)}: les moins populaires bords arrivent à l'étiquette t (9) Enfants (t) { ← c∈T | t∈Parent (c)}: les moins populaires bords à partir de l'étiquette t TAB. 1 - Modèle formel pour une DAGOT. Pour chaque utilisateur, un système de marquage est formellement représenté sous la forme d'un tuple U: = (R, T, P), où R et T sont des ensembles finis qui représentent les fichiers et les balises gérées par l'utilisateur. P représente les affectations effectuées par l'utilisateur. Un affichage représente la relation entre une ressource et une étiquette, P = RxT. Le modèle formel pour une DAGOT est décrite dans TAB 1. (10) Rsat (t) ← {r∈  Res (t) | Tag (r) ⊂ (supérieur (t) ∪ {t})} (11) Psat (t) ← Parent (t) (12) Csat (t) ← Enfants (t) (13) CBFR (t) ← [Rsat (t), PSAT (t), Csat (t)] TAB. 2 - récupération de fichiers basé sur le contexte simple. Dans un cas simple, la récupération de fichiers basé sur le contexte peut être défini comme dans TAB 2. Etant donné une étiquette t, sa récupération fichier à base de contexte CBFR (t) contient trois types d'informations: un ensemble de fichiers Rsat (t) qui a frappé le contexte le plus populaire contenant t, une liste des balises parentes Psat (f) guidant à des contextes plus généraux, et une liste des enfants Csat (f) guidant à des contextes plus spécifiques. Dans l'exemple ci-dessus, nous avons CBFR (JDIR) = [{file1.doc, file2.pdf}, {article}, {} VERSION FINALE]. En fait, Rsat ne revient pas toujours une valeur pour chaque balise. Il y a quelques balises pour lesquels Rsat est vide. Nous appelons t une balise vide si son Rsat (t) est vide et il n'a qu'un seul parent et un H. B. Ngo et al. RNTI - X - enfant. Nous vous proposons de passer par une balise vide. Le résultat recherche d'une balise vide est automatiquement remplacé par le résultat de la recherche de son enfant unique. Dans l'exemple ci-dessus, l'article est une balise vide. Par conséquent CBFR (article) est automatiquement remplacé par CBFR (JDIR). En outre, le rôle de rôle des parents et des enfants d'une balise vide sont aussi remplacés par son parent et l'enfant. Ainsi, la récupération de fichiers basé sur le contexte est redéfinie comme les expressions TAB 3 ci-dessous. (14) vide (t) ← carte (Rsat (t)) = 0 & carte (Children (t)) = 1 & carte (Parent (t)) = 1 (15) CBFR (t) ← [Rsat (t), PSAT ( t), Csat (t)] | Videz (t) (16) CBFR (t) ← CBFR (c) | Vide (t) et c∈Children (t) (17) Psat (t) ← {p∈Parent (t) | Videz (p)} (18) Psat (t) ← Psat (p) | p∈Parent (t) et vide (p) (19) Csat (t) ← {c∈Children (t) | ! Vide (c)} (20) Csat (t) ← Csat (c) | c∈Children (t) et vide (c) TAB. 3 - complète récupération de fichiers basé sur le contexte. 5 La mise en œuvre et tester d'abord nous avons téléchargé les messages de 46 personnes au hasard de (délicieux) pour calculer le nombre de balises et de ressources connexes par personne et le nombre de ressources et de balises connexes par étiquette. Ensuite, nous avons mis en œuvre l'algorithme de création DAGOT et a utilisé les données de marquage ci-dessus pour tester l'algorithme. Nous avons fait des statistiques sur les 46 DAGoTs créées afin de valider notre approche. Le TAB 4 compare les valeurs utiles pour la récupération de fichiers dans deux cas. Les valeurs moyennes des caractéristiques par rapport au modèle en fonction du contexte à l'aide DAGOT sont plus petites que celles du modèle délicieux. Dans le modèle délicieux, un utilisateur sur 717 balises à choisir. Inversement, un utilisateur dans le modèle de récupération de fichiers basé sur le contexte ne compte que 145 contextes à choisir. Cela prouve que le modèle DAGOT mieux aide les utilisateurs à la récupération de fichiers personnels. Les plus petites plages de valeurs de caractéristiques par rapport au modèle basé sur le contexte en utilisant le DAGOT montrer que le DAGOT a une structure de l'équilibre. Cela évite des cas où il y a des milliers de ressources de vie ou des centaines de tags en rapport avec un tag donné. Délicieux Modèle (désarchivage basé Tag-) Modèle DAGOT (désarchivage base contextuelle) Moyenne: 717 Moyenne: 145 tags par utilisateur Plage: 2-4590 Contextes par utilisateur Gamme: 2-663 Moyenne: 4.6 Moyenne: 2.2 Ressources par étiquette Plage : 1-1426 Ressources par contexte Gamme: 1-96 moyenne: 16,5 moyenne: 1.1 Re balises lated par tag Plage: 1-3715 parents par tag Gamme: 1-23 moyenne: 2,7 enfants par étiquette Gamme: 1-113 TAB 4. - Comparaison entre le modèle délicieux et le modèle DAGOT. Amélioration de la récupération de fichiers personnels avec le contexte dans SFS basée sur les balises 6 Conclusion et perspectives Nous avons proposé d'améliorer la récupération des fichiers personnels avec la recherche contextuelle. Nous soutenons que chaque utilisateur dispose d'un vocabulaire personnel des balises qui sont sémantiquement regroupées dans des contextes différents. L'ensemble des balises associées à un fichier par un utilisateur crée un contexte. Nous avons proposé un algorithme créant automatiquement un DAGOT basé sur la popularité de l'étiquette et des relations de balise. Cette DAGOT permet d'identifier automatiquement le contexte de succès pour un tag donné. L'utilisation DAGOT, les utilisateurs peuvent naviguer d'un contexte à d'autres pour récupérer ses fichiers de manière efficace. Pour l'avenir, nous allons intégrer ce système de marquage dans le système de fichiers basé sur ontologies (Ngo et al., 2007) et de proposer une méthode complète pour la récupération du fichier dans lequel on tient compte à la fois la sémantique des fichiers extrinsèques et la sémantique des fichiers intrinsèques. Références Apple Computer (2005). Inc: Tiger Developer Series Présentation - Travailler avec Spotlight. http://developer.apple.com/macosx/spotlight.html. Barreau, D. et B. Nardi, (1995). Trouver et rappeler: l'organisation des fichiers à partir du bureau. SIGCHI Bulletin, 27 (3), 39-43. Bloehdorn, S., O. Görlitz, S. Schenk, et M. Völkel, (2006). TagFS --- Tag Sémantique pour systèmes de fichiers hiérarchique. Compte rendu de la 6e Conférence internationale sur la gestion des connaissances (I-06 KNOW), Graz, Autriche, Septembre 2006. Delicious. http://del.icio.us/ Khoo, C., B. Luyt, C. Ee, J. Osman, H.H. Lim, Yong et S. (2007). Comment les utilisateurs organisent des fichiers électroniques sur leurs postes de travail dans l'environnement de bureau: une étude préliminaire du comportement de l'organisation des renseignements personnels. La recherche de l'information, 12 (2), le papier 293. Ngo, H. B., C. Bac et F. Silber-Chaussumier (2007). Vers une ontologie des systèmes de fichiers sémantiques basés. Compte rendu de la 5 e Conférence internationale sur la recherche, l'innovation et vision pour l'avenir, Hanoi, Vietnam. Padioleau, Y. (2005). Logic File System, un Système de fichier sur la basons logique. Thèse de doctorat, Université de Rennes 1. CV DEPUIS peu, les Étiquettes Sont utilisées identifiant des répandrai largement also bien sur Contenus le bureau informatique des sur les Utilisateurs Qué Sites Coopératifs du dit Web 2.0. Notre recherche se Focalise sur l'organisation des assisted d'Étiquettes Improving Personnelles la AFIN des pertinence de Recherches à Fichiers personnels des associés Étiquettes. Notre proposition d'utiliser la notion de point central Contexte Comme. Un is Constitué à Contexte partir d'un ensemble d'un par Étiquettes affectées à un fichier Utilisateur. Nous proposons une infrastructure à juin Qui Përmet un de Utilisateur à Travers les Naviguer Contextes verser de SES Retrouver Fichiers."
828,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Extraction d'itemsets compacts,"L'extraction d'itemsets fréquents est un sujet majeur de l'ECD et son but est de découvrir des corrélations entre les enregistrements d'un ensemble de données. Cependant, le support est calculé en fonction de la taille de la base dans son intégralité. Dans cet article, nous montrons qu'il est possible de prendre en compte des périodes difficiles à déceler dans l'organisation des données et qui contiennent des itemsets fréquents sur ces périodes. Nous proposons ainsi la définition des itemsets compacts, qui représentent un comportement cohérent sur une période spécifique et nous présentons l'algorithme DEICO qui permet leur découverte.","Bashar Saleh, Florent Masseglia",http://editions-rnti.fr/render_pdf.php?p1&p=1000628,http://editions-rnti.fr/render_pdf.php?p=1000628,en,"Extraction d'itemsets compacts Bashar Saleh * Florent Masseglia * * INRIA Sophia-Antipolis Méditérranée Equipe-Projet AxIS 2004 route des Lucioles - BP 93 FR-06902 Sophia Antipolis {Prénom.Nom}@sophia.inria.fr, http: // www -sop.inria.fr/axis~~V~~singular~~3rd CV. L'extraction d'itemsets Fréquents is a sujet de l'ECD majeur et fils, mais de l'Est des découvrir Entre les corrélations d'un ensemble Enregistrements de Données. Cependant, le soutien en Calculated is de la fonction de la taille de base Dans fils intégralité. Dans this article, montrons Qu'il est possible NOUS de en Prendre des compte Difficiles à DÉCÉL Périodes Dans l'organisation des et Qui Données des itemsets Fréquents contiennent sur SCÉ Périodes. Nous proposons la définition des AINSI itemsets compacts, Qui representent un sur Une comportement period cohérent et nous Présentons Spécifique l'DEICO Qui Përmet Algorithme their découverte. 1 Introduction Le problème de la recherche de rules d'association, Agrawal et introduit Dans al. (1993), sur l'EST basons extraction de Fréquentes Entre les corrélations et Enregistrements de NOM- breuses connait des applications Dans le marketing, la gestion financière ou L'analyse décisionnelle (par exemple). Au cœur de CE Problème, la découverte dâ € ™ un itemsets Fréquents domaine de Représente recherche très etudie. Dans l'analyse du panier de la ménagère, par exemple, les itemsets Fréquents mais de verser en Ontario des ensembles d'découvrir correspondant à objets Sie ONU Nombre de postes de clients significatif. Si C'EST SUPERIEUR A Nombre un soutien defini (par l'Utilisateur) itemset is this Alors Considéré Comme fréquent. Cependant, Dans la définition des itemsets Fréquents Initiale, sur la l'extraction de l'Est éffectuée la base Entière Données Toute (c.-à-minsupp Soit, le minimum de soutien Donné, Par l'Utilisateur les itemsets Extraits doivent apparaitre | MoiNs la DANS D Au | × minsupp Enregistrements de D). Toutefois, il EST possible Que des itemsets interessants des Reste characteristics ne tient pas compte Particulières du Malgré (y compris de soutien). Effectivement, les itemsets interessants correspondent Sont Souvent au moment Liés de Qui à their observation. Sur could be Prendre sa verser le comportement des exemple d'Utilisateurs site un de commerce en ligne Pendentif Une offre sur les DVD spéciale et les CD vierges versent Une publicité is Laquelle faité diffusion par. De la same Manière, le site de Web d'Une conférence may voir le Nombre de increase Dans Une Connaissance de fenêtre avant la Quelques heures la date limite de soumission. état à la Une Nécessaire de découverte de CE de type Liée à is Données l'aspect des Données temporel. aspect this was a du déjà abordé verser les Règles d'association Dans Ale et Rossi (2000); Extraction d'itemsets compacts Lee et al. (2001). Dans Ale et Rossi (2000), les auteurs proposent la notion de rules d'asso- tion Temporelles. Consiste à their idée les itemsets Extraire Qui sont Fréquents sur des periodes Par la Durée définies de vie de each article (les ne Périodes pas Sont Fait découvertes Mais utilisées Comme contrainte). Dans this article, nous proposons de découvrir les itemsets Qui sont sur un Fréquents Sous- ensemble contigü de la Base de Données. Par exemple, les navigations Sur les pages Web des CD DVD et de Façon vierges apparaissent sur toute l'distribuée année. Cependant, la fréquence de très AUGMENTE CE comportement pendant les Certainement Quelques heures (ou jours) Qui le suivent envoi. AINSI, le Consiste à Pressothérapie défi la fenêtre Qui va Optimiseur temporelle le soutien de CE comportement. Que le mailing Considérons Soit le 3 mars Envoyé et les INFLUENCE un Qu'il clients pendant deux jours. Notre mais Est de découvrir Que: « 25% des clients, le 3 et Entre le 5 mars, la page par Ontario des Demandé CD vierges, la Page des DVD vierges et FINALEMENT la page par des Offres spéciales ». Le soutien de la CE is comportement trop Sûrement l'extraction de verser Faible fils sur l'année permettre Entière Toute Mais this Connaissance (à savoir le et la p comportement ériode sur il is fréquent Laquelle) très utile can be verser les Décideurs Qui veulent découvrir CE Certainement et sa period comportement de frequency verser FINALEMENT faire le lien with the mailing. 2 Définitions La définition 1 le concept de d'reprend itemset de Agrawal et fréquent al. (1993). Nous y AJOUTE la notion Avons d'Estampille (transaction juin may Fait les dates de couvrir PLUSIEURS). Définition 1 Soit I = {i1, i2, ..., in} d'ensemble des Nations Unies articles D'. Soit X = {i1, i2, ..., ik} / k ≤ n et ∀j ∈ [1..K] ij ∈ I. X is a jeu d'éléments (ou un k-itemset). T = {Soit t1, t2, ..., tm} un ensemble d'estampilles, sur un Lesquelles ordre linéaire <T est defini, et ti where <T tj ti PRECEDE Que signifié tj. Une transaction T est un couple de T = (TID, X) OU tid is l'opération de la Identifiant et X is l'itemset qu'associé. À l'article each i de X is qu'associé l'Estampille ti Qui la Date d'Représente apparition de i Dans T. Une transaction T = (tid, I) Supporte un jeu d'éléments X ∈ I si X ⊆ I. Une base de transactions de D est ensemble de l'ONU transactions. La couverture d'un itemset X sur D is l'ensemble des opérations de Dans les identifiants D X Qui supportent: Couverture souple (X, D) = {tid / (tid, I) ∈ D, X ∈ I}. Le support d'un itemset X Dans D is the number of transactions in the X sur de couverture D: support (X, D) = | Couverture souple (X, D) |. La Frequence d'un ensemble item- X sur D is le rapport between de la taille de X sur couverture D et la taille de D: frequency (X, D) = support (X, D) | D | . Soit γ ∈] 0..1] minimum le support Donné par l'Utilisateur, un jeu d'éléments X is dit fréquent si fréquence (X, D) ≥ γ. 2 L'ensemble Définition F des itemsets Fréquents de D Avec minimum de support un γ is Note F (D, γ) = {X ∈ I / fréquence (X, D) ≥ γ}. Etant donnés ensemble ONU D'articles I, la base de transactions D juin support et un minimum γ, Le problème de l'extraction d'itemsets fréquente étau à TROUVER F (D, γ) AINSI Que le soutien des jeux d'éléments de F. L'exemple 1 donne des Une illustration Dans la section de concepts this. B. Saleh et F. Masseglia 1 La figure Exemple 1 (a) un exemple de Montrésor Base de Données D. Verser simplificateur la conférence, nous les transactions Qué supposons de D par ordre Sont affichées de (ie T1 is enregis- TREE avant T2, etc.) et Estampille is unique, Qu'une à tous les Associée objets d'opération juin (Qué in the Alors 1 article each définition is estampillé). Avec γ = 12, les articles fréquente (en gras Dans Les transactions de la figure 1 (a)) are a, b et c. Les jeux d'éléments de Fréquente are D (a), (b), (c) Avec un support de 610, et (a, c), Avec un support de 1 2. Fig. 1 (a) itemsets fréquente. Fig. 1 (b) jeux d'éléments compacts. FIGUE. 1 - itemsets et Fréquents itemsets compacts sur D Avec γ = 12 Notre basons sur is Problème les estampilles et à l'étau des itemsets Qui Extraire Fréquents sur des Sont Particulières de D. Périodes Nous Présentons les notions d'Maintenant itemset et d'itemset temporel compact , Qui sont au cœur de cet article. 3 Une period définition P = (Ps, Pe) par jour definie is juin de Ps et la date départ de fin juin Pe. L'Ensemble des transactions à Qui appartiennent juin period P is defini par Tr (P) = {T / T ⊆ D, ∀i ∈ T, Ps ≤ Pi ≤ Pe} Avec Pi l'Estampille de l'article i Dans la transaction T. Enfin, PR is l'ensemble des D. sur Périodes possibles En d'Autres Termes, l'ensemble des opérations à P Qui appartiennent is l'ensemble des opérations tous les articles Do not estampilles are DANS LES P de Limites. Définition 4 Un jeu d'éléments temporel x is un tuple (xi, xp, xσ) Où xi is a itemset, xp is juin period Associée à xi et xσ is le soutien de xi sur xp. K la taille Soit de xi, x is a Alors k-temporel itemset. Γ Soit, le soutien minimum, nous Présentons les characteristics d'un itemset Dans compact la 5. Définition 5 définition x un itemset Soit temporel. x is a itemset compact (IC) ssi les conditions Suivantes de les respectées: 1) xσ ≥ γ 2) ∀p2 ∈ PR / xp ⊆ p2 alors on observe a) OU b) ous les deux: support a) (xi, p2) <γ b) couverture (xi, p 2) = couverture (xi, xp) 3) ∀p2 ∈ PR / p2 ⊆ xp, couverture (xi, p2) <couverture (xi, xp) Soit k la taille de xi, x is Alors un k-itemset compact. Enfin, sik Est l'Ensemble de Tous les compacts k-itemsets. Extraction d'itemsets compacts La première état de la 5 Rassurer Que définition x un itemset Qui Représente is sur sa period fréquent. La état la seconde assure Qué xp de taille is maximale. En fait, si en plus grande period Une exists, Alors, sur this period, xi Ne est pas ou La couverture fréquent de xi Reste Identique (à savoir la period étendre xp à p2 n'apporte au soutien rien). Enfin, la condition third assurer Que la taille de xp is also minimale. En effet, si xi is supportAchat par la première et la Dernière transaction de xp, Alors si il exists un period, plus petite sur Laquelle xi is fréquente, la sera de couverture, plus Faible (ie passer de xp à p2 implique d'Ignorer des Qui transactions xi et supportent doivent Être Fait gardées). Une illustration of this is définition Dans l'exemple Donnée 2. La figure 2 Exemple 1 (b) les k-Montrésor itemsets compacts Qui sont extraits = 0 γ Avec, 5. Sur may y les itemsets Que constater compacts de taille 1 Sont ( a), (b) et (c), Que their soutenir is de 610, et Que their period correspondent à la base D Entière. Salle de bains, sur observateur may compacts de trois jeux d'éléments 2 Taille: - (ac), Avec support un de 510 et juin period qui couvre l Toute la base de D. - (ab) et (bc), sur la period [7..10] Avec un support de de 34. FINALEMENT, il y a un jeu d'éléments de taille compacte 3: (a b c) Qui apparait in the period [7..10] Avec un support de 34. Définition 6 L'ensemble des Jeux d'éléments compacts Maximaux (ICM) is defini Comme suit: Soit x un circuit intégré, x is a ICM si les conditions respectées Sont Suivantes: ∀y ∈ TR / x 6 = y si xi yi ⊆ Alors xp = 6 yp. Dans la suite de papier CE, un NOUS proposons Pour la Optimisé Algorithme de l'ensemble Découverte des ICM, Comme Par la définition décrits 6. 3 DeICo: DEICO général Príncipe un nouveau introduit Príncipe de les verser itemsets comptage Candidats. Derons consi- un jeu d'éléments temporel t Qui pas ne est compact (à savoir tσ <γ). Tout surensemble u = (ux, haut, uσ) / tx ⊆ ux ∧ jusqu'à ⊆ tp t de ne pas may Être un jeu d'éléments compact (à savoir uσ <γ). DEICO etend le prince d'apriori de AFIN itemsets des compacts Générer et COMP- ter Candidats their support. Le principe de l'Est REGULATIONS génération l'un d'ajout sur les sections filtre inter possibles (c.-à-Entre si Candidats deux ensembles d'éléments compacts de taille k un Ontario ne préfixe commun Mais pas la same partagent period, ne Alors their may pas croisement Générer un itemset compact). Cependant, l'étape de d'apriori ne comptage may pas s'appliquer ment directe- Dans notre CAS. C, l'ONU Considérons itemset. Temporel candidat Une solution à Compter le consisterait d'apparitions de Nombre C dans cp. Ne est de Ce pas une solution Une correcte. Consi- rons en effet le candidat c = ((ab), [1..10], cσ) (Généré à partir de x = ((a), [1..10], 610) et y = ((b ), [1..10], 610)). c Ne est pas cσ compact voiture = 4 10. Toutefois, sur cp, il exists un jeu d'éléments c compact '= ((a b), [7..10], 34). Notre mais, pendant le comptage, Est de construct des noyaux correspondant aux Qui de frequency des Périodes itemsets temporels Candidats. Salle de bains, bureaux noyaux will be in the mais fusionnés d'obtain les itemsets compacts. La CÉS 7 Précise concepts définition. This bien s'adapte définition récursive au fait Que l'Effectué sur des cols suc- cessives Sur les Donnees de AFIN les periodes Sie Trouver Reportages compacts correspondant aux itemsets de. B. Saleh et F. Masseglia En effet, la Façon Dont Une passe is éffectuée (Dans l'Ordre Soit Sequentiel des transactions) de découvrir les implique noyaux « à la volée ». 7 Un noyau définition is juin period. L'ensemble K (x, P, γ) des noyaux de l'élément x sur la period P verser un soutien γ is defini Comme suit: k ⊆ P Soit Une period x ⊆ Que Telle Tr (ks) ∧ Tr (ks) la première apparition is de x sur P. Si k n'exi ste pas, Alors K = ∅. Sinon, Soit N l'ensemble des estampilles Telles Que ∀n ∈ N, n ∈ P ∧ n> ks ∧ fréquence (x, [ks..n]) <γ (en d'Autres Termes, N is l'ensemble des estampilles de P Telles Que Toute l'extension de k à une estampille de N implique la perte de fréquence for x). Si N is vide, Alors ke is defini Comme la Dernière apparition de x Dans P et K (x, P, γ) = {k}. Sinon (à savoir N 6 = ∅), Soit m ∈ N / ∀n ∈ N, n> m (m is la première estampille Telle Que la fréquence de x is perdue sur [ks..m]). Ke is defini Alors est comme la apparition de x DerniÃ¨re sur [ks..m] et K (x, P, γ) = {k} ∪k (x, P - [ks..ke], γ). Exemple 3 Considérons l'itemset temporel candidat de taille 1, c = ((b), [1..10], cσ). La figure 2 donne la fi Table des apparitions de booléenne b. Il y a deux noyaux de (b) sur cp ([1..3] et [6..10]). CES noyaux fusionnés PEUVENT Être un écoulement get compact jeu d'éléments ((b), [1..10], 610). FIGUE. 2 - Noyaux et period de l'itemset (b) l'algorithme MERGEKERNELS Bien que (∃q, r ∈ K / couverture (x, q) | + | x (Couverture, r) || q∪r | ≥ γ) K ← K + q ∪ r - q - r; Couverture souple (x, q ∪ r) = (x, Couverture q) ∪ couverture (x, r) End While End algorithme MERGEKERNELS Notre base de d'extraction Algorithme se sur le principe de la génération de Candidats. Pendant la passe sur les Données, le mais de l'extraction d'Algorithme Est de jour Mettre à les informations sur les noyaux des itemsets la Do not temporels period Candidats EnGlobe l'Estampille de la transaction de Courante. A la fin de each passe de l'algorithme nous tous les noyaux obtenons de each this passe répandrai candidat. A la fin de each passe, les noyaux each itemset verser obtenus temporel candidat verser obtain Sont des fusionnés itemsets compacts. Extraction D'compacts de itemsets 4 Expérimentations Les Expérimentations sur les Sont réalisées Web de Fichiers connecter l'Inria Sophia de Mars 2004 à Juin 2007 Qui representent 253 Go de 36 et Données brutes 710 616 après le navigations prétraitement. VOICI un de exemple aux comportement itemsets Trouvé compacts Grâce. 1) Joan Miro: Début: Jeu 20 avril 07: 05: 39 2006; fin: Jeu 20 avril 17: 21: 06 2006; fré- quence: 0.024565; couverture: 120; itemset: (préfixe « oméga / personnel / Christophe.Berthelot / ») - {css / style.css, Omega / JoanMiro / joanmiro.html} Pour CE interprète comportement, il savoir Faut Que: 1) this page is Joan Miro à dédiée (un artiste Célèbre) 2) Joan Miro is né le 20 avril 1893 et ​​3) la Page de Christophe est classé fifth Dans les Google Résultats avec les mots clés « Joan Miro » (au moment où de bureaux riences rimentations). Notre conclusion is Fait Que l'anniversaire de verser Miro (20 avril), les inter- nautes des Ont massivement FAIT Recherches sur l'artiste en partie ET SONT Passés Par la Page de Christophe. Ce comportement is also en 2004, 2005 et 2007. 5 Conclusion Nous Avons Une nouvelle Proposé la répandrai définition d'itemsets Qui découverte corres- pondent à des Fréquences sur des Périodes élevées precises sans sur Connaissance préalable bureaux Périodes. This la découverte posait de découvrir en Difficulté same temps les itemsets et de their Optimales frequency Périodes. De plus, le possible Nombre de combinaisons et devait nous Être Réduit les bases Apporte Avons nécéssaires à la théoriques du résolution Problème. Notre algorithme, basons sur la découverte de « noyaux » et their fusions et se est capable Révélé d'Efficace CE nouveau de type Extraire de Connaissance de Manière Précise et exhaustive. D'après Nos Expérimentations, les jeux d'éléments compacts constituant un et lisible Résultat instructif for better Comprendre les Données étudiées. Agrawal Références, R., T. Imielinski, A. et N. Swami (1993). Exploitation minière règles d'association entre ensembles d'éléments dans les grandes bases de données. En SIGMOD, Washington, DC, États-Unis, pp. 207-216. Ale, J. M. et G. H. Rossi (2000). Une approche à la découverte de règles d'association temporelle. pp. 294-300. Lee, C.-H., C.-R. Lin, et M.-S. Chen (2001). Sur l'exploitation minière règles d'association temporelles générales dans une publication da tabase. pp. 337-344. Résumé minier fréquent modèle est un sujet très important de découverte de connaissances, destiné à extraire les corrélations entre les éléments enregistrés dans les bases de données. Cependant, ces bases de données sont généralement considérées comme un tout et, par conséquent, itemsets sont extraites sur l'ensemble des dossiers. Dans cet article, nous présentons la définition des itemsets solides, qui représentent un ensemble cohérent et compact comportement sur une période spécifique."
835,Revue des Nouvelles Technologies de l'Information,EGC ,2008,From Mining the Web to Inventing the New Sciences Underlying the Internet,"As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business. However, because of its novelty and the scale of data sets involved, few companies have figured out how to properly make use of this data. In this talk, I will review some of the challenges and opportunities in the utilization of data to drive this new generation of marketing systems. I will provide several examples of how data is utilized in critical ways to drive some of these capabilities. The discussion will be framed with theMore general framework of Grand Challenges for data mining : pragmatic and technical. I will conclude this presentation with a consideration of the larger issues surrounding the Internet as a technology that is ubiquitous in our lives, yet one where very little is understood, at the scientific level, in defining and understanding many of the basics the Internet enables : Community, Personalization, and the new Microeconomics of the web. This leads to an overview of the new Yahoo ! Research organization and its aims : inventing the new sciences underlying what we do on the Internet, focusing on areas that have received little attention in the traditional academic circles. Some illustrative examples will be reviewed to make the ultimate goals more concrete.",Usama M. Fayyad,http://editions-rnti.fr/render_pdf.php?p1&p=1000550,http://editions-rnti.fr/render_pdf.php?p=1000550,en,"E: /EGC2008/Articles/invités/fayyad.dvi De Mining Web à Inventer les nouvelles sciences sous-jacentes Yahoo !, Internet Usama Fayyad1 Californie, États-Unis Résumé. Alors que l'Internet continue de changer la façon dont nous vivons, trouver des informations, commu- niquer et faire des affaires, il a également pris un rôle de plus en plus radicalement dans le marketing et la publicité. Contrairement à tout support de masse avant, l'Internet est un moyen unique en matière d'interactivité et offre la capacité à cibler et à la messagerie du programme au niveau individuel. Supplia son cou- caractère unique de la richesse des données disponibles pour mesurabilité, dans la variété de façons d'utiliser les données, et dans la grande dépendance de marketing efficace sur applica- tions qui sont fortement axée sur les données, rend l'exploration de données et l'analyse des données statistiques, la modélisation et les rapports d'une partie essentielle à la mission essentielle de la gestion des affaires en ligne. Cependant, en raison de sa nouveauté et l'ampleur des ensembles de données en cause, peu d'entreprises ont compris comment faire correctement l'utilisation de ces données. Dans cet exposé, je vais passer en revue quelques-uns des défis et opportunités dans l'utilisation des données pour conduire cette nouvelle génération de systèmes de commercialisation. Je vais vous donner quelques exemples de la façon dont les données sont utilisées de façon critique pour conduire certaines de ces capacités. La discussion sera encadré par le cadre plus général des grands défis pour l'exploration de données: pragmatique et technique. Je terminerai cette présentation avec un examen des grandes questions entourant l'Internet comme une technologie qui est omniprésente dans notre vie, mais celui où très peu est compris, au niveau scientifique, dans la définition et la compréhension de la plupart des bases Internet permet: Communauté, personnalisation, et les nouveaux microéconomie du web. Cela conduit à une vue d'ensemble de la nouvelle Yahoo! organisation de la recherche et ses objectifs: inventer les nouvelles sciences ONU- ce que nous faisons jacents sur Internet, en se concentrant sur les zones qui ont reçu peu d'attention dans les milieux universitaires traditionnels. Quelques exemples seront examinés afin de rendre les objectifs ultimes plus concrets. 1DR. Fayyad est usama Yahoo! Est vice-président exécutif des données de recherche et solutions stratégiques. Il supervise également le Yahoo! organisme de recherche qui comprend des bureaux à Sunnyvale, Burbank et Berkeley, Californie, ainsi que New York, en Europe et en Amérique du Sud. Avant de rejoindre Yahoo !, il a cofondé et dirigé le DMX Group, une société de conseil de l'exploration de données et de la stratégie de données. Au début de 2000, il a co-fondé et servi en tant que PDG de digiMine Inc. (maintenant Revenue Science, Inc.). expérience professionnelle de Il comprend également cinq années menant l'exploration de données et le groupe d'exploration à la recherche de Microsoft. De 1989 à 1996 Fayyad a tenu un rôle de premier plan au Jet Propulsion Laboratory (JPL) de la NASA, où son travail lui a valu le prix d'excellence de la recherche en haut qui attribue Caltech aux scientifiques du JPL, ainsi qu'une médaille du gouvernement américain de la NASA. Fayyad a obtenu son doctorat en génie de l'Université du Michigan, Ann Arbor (1991), et détient également BSE à la fois génie électrique et informatique (1984); MSE en informatique et en génie (1986); et d'une maîtrise en mathématiques (1989). Il a publié plus de 100 articles techniques dans les domaines de l'exploration de données et de l'intelligence artificielle, est membre de l'Association américaine de l'intelligence artificielle, a publié deux ouvrages influents sur l'exploration de données et a lancé et a servi comme rédacteur en chef de ces deux revue scientifique primaire dans le domaine et le bulletin primaire dans la communauté technique publié par l'ACM. Il donne régulièrement des keynotes et des conférences à des conférences du gouvernement, l'industrie et universitaires du monde entier."
854,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Mining Implications from Lattices of Closed Trees,"We propose a way of extracting high-confidence association rules from datasets consisting of unlabeled trees. The antecedents are obtained through a computation akin to a hypergraph transversal, whereas the consequents follow from an application of the closure operators on unlabeled trees developed in previous recent works of the authors. We discuss in more detail the case of rules that always hold, independently of the dataset, since these are more complex than in itemsets due to the fact that we are no longer working on a lattice.","José L. Balcázar, Albert Bifet, Antoni Lozano",http://editions-rnti.fr/render_pdf.php?p1&p=1000625,http://editions-rnti.fr/render_pdf.php?p=1000625,en,"Implications Exploitation minière de Treillis de José L. Fermé Arbres Balcázar *, Albert Bifet *, Antoni Lozano * * Departament de Llenguatges i Sistemes Informàtics Université Polytechnique de Catalogne {balqui, abifet, antoni} @ lsi.upc.edu Résumé. Nous vous proposons une façon d'extraire les règles d'association de haute confiance des ensembles de données comprenant des arbres non marqués. Les antécédents sont obtenus par un calcul semblable à une transversale hypergraphe, alors que les suivent conséquens d'une application des opérateurs de fermeture sur les arbres non marqués développés dans des travaux récents precedentes des auteurs. Nous discutons plus en détail le cas des règles qui tiennent toujours, indépendamment de l'ensemble de données, puisque ceux-ci sont plus complexes que dans itemsets en raison du fait que nous ne travaillent plus sur un réseau. 1 Introduction Dans le domaine de l'exploration de données, l'une des principales notions qui contribuent au succès de la région a été celle des règles d'association. De nombreuses études de divers types ont fourni une grande avancée de la connaissance humaine au sujet de ces concepts. Une famille particulière d'études se fonde sur les notions précédentes de concepts formels, Galois et lattices implications, qui correspondent aux règles d'association de confiance maximale. Ces notions ont permis pour les travaux et Algorithmics plus efficaces en réduisant le calcul des ensembles fréquents, une importante étape habituelle vers des règles d'association, au calcul des soi-disant ensembles fréquents fermés, un calcul plus rapide de la taille de sortie beaucoup plus facile à gérer, mais ne perdant pas informations du tout en ce qui concerne des ensembles fréquents. Il a été réalisé il y a quelque temps que le modèle unique relationnel simple pour les données, comme em- ployé par le calcul soit d'ensembles fermés ou des règles d'association, alors utile dans une certaine mesure, était un peu limité dans son application par le fait que , souvent, les données de la vie réelle ont une sorte de structure interne qui se perd dans le cadre transactionnel. Ainsi, les études de l'exploration de données dans des structures combinatoires ont été entreprises, et a été fait des progrès considérables ces dernières années. Notre travail est encadré ici dans cette entreprise. Dans des travaux antérieurs, nous avons proposé une clarification mathématique de l'opérateur de fermeture sous-tend la notion d'arbres fermés dans des ensembles de données d'arbres; l'opérateur de fermeture ne fonctionne plus sur les arbres isolés, mais sur des ensembles d'entre eux. Dans un sens, il fait précis, les arbres fermés ne constituent pas un réseau. peut être défini un réseau de remplacement mathématiquement précise, cependant, comme l'a démontré (Balcázar et al., 2006), consistant non plus d'arbres, mais des ensembles d'arbres, et avec la propriété particulière qui, dans toutes les expériences avec des données réelles que nous ont entrepris, ils se révèlent être en réalité des arbres lattices, dans le sens que chaque ensemble fermé d'arbres était, dans tous les cas pratiques, un singleton. Conséquences de l'exploitation minière de Treillis de fermé les arbres Algorithmics pour construire ces ensembles fermés ont été étudiés dans plusieurs références comme (Chi et al., 2005b), (Balcázar et al., 2007a), (Balcázar et al., 2007b), (Termier et al, 2004). voir les références dans l'enquête (Chi et al., 2005a). Nous continuons ici cette ligne de recherche en abordant la plus prochaine étape naturelle: l'identification des conséquences sur le réseau d'ensembles fermés d'arbres. Nous décrivons une méthode, le long de la ligne des travaux similaires sur des séquences et des ordres partiels (Balcázar et Garriga (2007b), Balcázar et Garriga (2007a)) pour construire des implications des ensembles fermés d'arbres, et nous caractériser mathématiquement, en termes de propositionnelle théories corne, les implications que l'on trouve. Nous expliquons ensuite une différence majeure de notre cas avec les travaux précédents: les règles qui ne serait pas trivial dans d'autres cas deviennent redondants, et donc inutile, dans le cas des arbres, en raison du fait qu'ils sont implicites dans les combinatoires des structures . Un exemple montrera mieux notre point ici; s'il vous plaît garder pour l'instant avec quelques notions non définies qui seront plus tard précisé. Considérons une règle intuitivement décrite comme suit: Naturellement signifie que chaque fois qu'un arbre dans l'ensemble de données en cours d'exploration a comme (de haut en bas) les deux sous-arbres arbres dans l'antécédent, il a aussi celui de la conséquence. Un arbre ayant une bifurcation à la racine, tel que requis par le premier antécédent, et une branche de longueur au moins deux, comme l'exige la deuxième, doit avoir la conséquence comme un sous-arbre (top-down). Par conséquent, la règle dit, en fait, rien du tout à propos de l'ensemble de données, et n'est pas digne d'apparaître dans la sortie d'un algorithme d'exploration de la règle sur les arbres. Notre deuxième contribution majeure est donc une étude de certains cas où l'on peut DE- téger ces règles implicites et les supprimer de la sortie, avec un faible temps de calcul. Considérant que d'autres travaux théoriques pourrait être utile, nos contributions à ce jour détectent déjà la plupart des règles implicites dans les ensembles de données de la vie réelle, à des niveaux de soutien assez faible, et avec une efficacité raisonnable. Nous rapportons quelques faits sur le comportement empirique de nos mises en œuvre à la fois l'algorithme de trouver des règles et des heuristiques pour supprimer des règles implicites. 2 Préliminaires Les arbres sont reliés graphes acycliques, arborescences sont des arbres à un sommet du doigt comme la racine, et les arbres non classées sont des arbres avec arité sans bornes. Nous disons que t1,. . . , Tk sont les composantes de l'arbre com- t si t est constitué d'un noeud (la racine) rejoint aux racines de tous de l'ti. On peut distinguer entre les cas où les composants au niveau de chaque noeud forment une séquence (arbres commandés) ou tout simplement pour un ensemble (arbres non ordonnées). Nous traiterons enracinés, non classées, arbres désordonnées. Nous ne supposons pas la présence d'étiquettes sur les noeuds. Le (infini) ensemble de tous les arbres seront désignés avec T, mais en fait tous nos développements se déroulera dans un sous-ensemble fini de T qui agira comme notre univers du discours. Pour les structures à base de liaison-comparaison, nous sommes intéressés par une notion de sous-arbre où la racine est préservée. Un arbre t 'est un sous-arbre de haut en bas (ou simplement un sous-arbre) d'un arbre t (écrit t' t) si t 'est un sous-graphe connexe de t qui contient la racine de t. Cette notation peut être étendue à des ensembles d'arbres A B: pour tout t ∈ A, il y a un t '∈ B qui t' t. Deux J. Balcázar et al. arbres, t 'on dit être comparable si t' t ou t 't. Dans le cas contraire, ils sont incomparables. Aussi t ≺ t 'si t est un sous-arbre approprié de t' (qui est, t t 'et t 6 = t'). L'entrée de notre processus d'extraction de données est un ensemble de données limité D de transactions, où ∈ D de chaque transaction donnée se compose d'un identificateur de transaction, TID, et un arbre dont la racine non marqué. Tids sont censés fonctionner de manière séquentielle de 1 à la taille de D. De cet ensemble de données, notre univers du discours U est l'ensemble de tous les arbres qui apparaissent comme sous-arbre d'un arbre en D. Après usage standard, nous disons que les supports d'une transaction un arbre t si t est un sous-arbre de l'arbre correspondant à la transaction s. Le nombre de transactions dans l'ensemble de données D que le soutien t est appelé le support de l'arbre t. Un sous-arbre t est appelée fréquemment si son support est supérieure ou égale à une min_sup de seuil donnée. Le problème de l'exploitation minière de sous-arbre fréquent est de trouver tous les sous-arbres fréquents dans un ensemble de données. Toute sous-arbre d'un arbre fréquent est aussi fréquente et, par conséquent, toute superarbre d'un arbre nonfrequent est également nonfrequent. Nous définissons un arbre fréquent t fermé si aucune de ses propres supertrees a le même soutien qu'il a. En général, il y a beaucoup moins d'arbres fermés que les fréquents. En fait, nous pouvons obtenir tous les sous-arbres fréquents avec leur soutien de l'ensemble des sous-arbres fréquents fermés avec leurs supports. L'ensemble de données définit un opérateur de fermeture sur le powerset de U, notée rD, résultant d'une connexion de Galois et développé en (Balcázar et al., 2006). Définition La paire de connexion Galois: • Pour fini A ⊆ D, σ (A) = {t ∈ T || ∀ t '∈ A (t' t)} • Pour B fini T ⊂, pas nécessairement dans D, xd ( B) = {t '∈ D || ∀ t ∈ B (t t')} Proposition 2.1 La composition rD = σ ◦ xd est un opérateur de fermeture. Théorème 2.2 Un arbre t est fermé pour D si et seulement si elle est maximale dans RD ({t}). Nous allons construire des règles d'association sous une forme standard à partir, et montrer qu'ils cor- respondent à une certaine théorie corne; En outre, nous allons prouver l'exactitude d'une construction semblable à la base instantanée de Wild (1994) et Pfaltz et Taylor (2002). 3 règles d'association Après utilisation standard sur Galois lattices, nous considérons maintenant des implications (parfois appelées règles d'association déterministe, voir par exemple Pfaltz et Taylor (2002)) de la forme A → B pour les ensembles d'arbres A et B de U. , Nous considérons précisément l'ensemble des règles suivantes: A → rD (A). Sinon, nous pouvons diviser les {A en conséquens → t || t ∈ rD (A)}. Il est facile de voir que D obéit à toutes les règles suivantes: pour chaque A, tout arbre de D qui a comme sous-arborescences tous les arbres de A a aussi sous-arbres tous les arbres de rD (A). Nous voulons fournir une caractérisation de cet ensemble d'implications. Nous opérons sous une forme similaire à Balcázar et Garriga (2007a) et Balcázar et Garriga (2007b), traduisant cet ensemble de règles dans une théorie propositionnelle spécifique que nous pouvons caractériser, et pour lesquels nous pouvons trouver une « base »: un ensemble de règles qui sont suffisantes pour déduire toutes les règles qui attente dans l'ensemble de données D. les détails techniques partent quelque peu Balcázar et Garriga (2007b) dans ce que nous sautons Répercussions sur l'exploitation minière de Treillis de fermé arbres certaine condition de maximalité imposée là, et sont encore plus différents de ceux des Balcázar et Garriga (2007a). Ainsi, nous commençons par l'association d'un vt variable propositionnelle à chaque arbre t ∈ U. De cette façon, chaque implication entre les ensembles d'arbres peut aussi être vu comme une conjonction propositionnelle des implications Klaxon, comme suit: la conjonction de toutes les variables correspondant à l'ensemble du côté gauche implique chacune des variables correspondant à la fermeture à le côté droit. Nous appelons cette implication Corne propositionnelle la traduction propositionnelle de la règle. En outre, maintenant un ensemble d'arbres A correspond d'une manière naturelle à un modèle propositionnelle mA: en particulier, mA (vt) = 1 si et seulement si t est un sous-arbre d'un arbre à A. Nous abbreviate m {t} comme mt. Notez que les modèles obtenus de cette manière Obey la condition suivante: si t 't et vt = 1, puis vt' = 1 aussi. En fait, cette condition identifie les modèles mA: si un m modèle remplit, alors m = mA pour l'ensemble A des arbres t pour lesquels vt = 1 m. Sinon, A peut être considéré comme l'ensemble des arbres maximaux pour lesquels vt = 1. Notez que nous pouvons exprimer cette condition par un ensemble de clauses de Horn: = {vt R0 '→ vt || t', t ∈ U , t '∈ U}. Il est facile de voir que ce qui suit est: Lemme 3.1 Soit t ∈ D. Alors mt satisfait R0 et aussi toutes les traductions propositionnel des implications de la forme A → rD (A). Depuis rD ({t}) = {t '∈ T || t' t} par définition, si mt | = A, alors A t, d'où rD (A) rD ({t}), et mt | = rD (A). Pour R0, la définition même de mt assure la réclamation. Nous recueillons toutes les implications à base de fermeture-dans l'ensemble suivant: R'D = ⋃ C {A → t || rD (A) = C, t ∈ C} Pour une utilisation dans nos algorithmes ci-dessous, nous précisons également un ensemble de règles de béton parmi ceux qui viennent de l'opérateur de fermeture. Pour chaque ensemble fermé d'arbres C, considérer l'ensemble des « prédécesseurs immédiats », qui est, des sous-ensembles de C qui sont fermés, mais où aucun autre intervenant ensemble fermé existe entre eux et C; et, pour chacun d'eux, disent Ci, définir: Fi = {t || t C, t 6 Ci} Ensuite, nous définissons HC comme une famille d'ensembles d'arbres qui remplissent deux propriétés: chaque H ∈ HC coupe chaque Fi, et tout le H ∈ HC sont minimes (par rapport à) sous cette condition. Nous prenons maintenant l'ensemble suivant des règles RD, RD = ⋃ {C H → t || H ∈ HC, t ∈ C} comme un sous-ensemble de l'ensemble beaucoup plus large de règles R'D définies ci-dessus, et de l'état de notre résultat principal: théorème 3.2 Compte tenu de l'ensemble de données D des arbres, les formules suivantes sont propositionnel logiquement équivalentes: i / la conjonction de toutes les formules de Horn satisfaits par tous les modèles mt pour t ∈ D; ii / la conjonction de R0 et toutes les traductions propositionnelles de formules dans R'D; J. Balca ZAR et al. iii / la conjonction de R0 et toutes les traductions propositionnelles de formules en RD. Notons tout d'abord la preuve que je / est vu facilement impliquer ii /, parce que Lemme 3.1 signifie que tous les adverbes conjonctifs dans ii / également appartiennent à i /. De même, ii / trivialement implique iii / parce que tous les adverbes conjonctifs en iii / appartiennent également à ii /. Il reste à dire que la formule iii / implique que i /. Choisissez une formule Corne H → v qui est satisfaite par tous les modèles mt pour t ∈ D: qui est, chaque fois mt | = H, mt | = v Soit v = vt '. Cela signifie que, pour tout t ∈ D si H t alors t 't ou, de manière équivalente, t' ∈ rD (H). Nous montrons qu'il ya H 'H qui coupe peu tous les ensembles de la forme Fi = {t || t C, t 6 Ci} pour C = fermé rD, et pour son ensemble de ses prédécesseurs immédiats Ci. Une fois que nous avons un tel H ', puisque t ∈ C, la règle H' → t est en RD. En collaboration avec R0, leurs traductions propositionnel communes entail H → T: un modèle arbitraire rendant vrai H et épanouissante doit faire H R0 vrai à cause de H 'H et, si H' → t vaut pour elle, t est aussi vrai dans ce . Depuis R0 et H '→ T sont disponibles, H → t détient. Par conséquent, nous avons juste besoin de prouver que tel H 'existe H. Notez que H croise déjà tous les Fi: H rD (H) = C; supposons que pour une bonne prédécesseur Ci, H ne Intersection Fi. Cela signifie que Ci t pour tout t ∈ H, et donc, le plus petit ensemble fermé au-dessus de H, qui est, rD (H) = C, doit être inférieure à l'ensemble fermé Ci ou coïncident avec, et ni est possible. Par conséquent, il suffit de considérer tous les ensembles d'arbres H '', où H '' H, qui encore Intersection tous les Fi. Ce n'est pas une famille vide puisque H est lui-même en elle, et il est une famille finie; Il a donc au moins un élément minimal (par rapport à), et l'un d'eux peut être choisi pour notre H '. Ceci termine la preuve. 4 sur la recherche des règles implicites pour Subtrees Nous définissons formellement les règles inplicit comme suit: Définition donnée trois arbres t1, t2, t3, on dit que t1 ∧ t2 → t3 est une règle Corne implicite (abbreviately, une règle implicite) si pour chaque arbre t elle détient t1 t2 t ∧ t ↔ t3 t. On dit que deux arbres t1, t2, ont des règles implicites s'il y a un t3 arbre pour lequel t1 t2 ∧ → t3 est une règle implicite de Horn. Une généralisation naturelle pourrait être considérée comme ayant plus de deux antécédents; nous tances cunscribe notre étude à des règles implicites de deux antécédents. L'objectif des définitions suivantes est de fournir des outils formels, pour une règle implicite. Définition Un arbre c est un superarbre commun minimal de deux arbres a et b si c, b c, et d pour chaque ≺ c, soit d'un 6 d ou b 6 d. Dans l'exemple de règle implicite donnée dans l'introduction, l'arbre à droite du signe est un cation impli- superarbre commune minimale des arbres sur la gauche. Conséquences de l'exploitation minière de Treillis de Définition Fermé Arbres Etant donnés deux arbres a, b, nous définissons a⊕ b comme superarbre minimal commun a et b. Comme il peut y avoir plus d'un superarbre commun minimal de deux arbres, nous choisissons celui avec la plus petite représentation naturelle, comme indiqué dans (Balcázar et al., 2007b) afin d'éviter l'ambiguïté de la définition. Définition Un composant c1 est d'un maximum si l'un des composants d'un c2 c2 c1 satisfait, et il est maximal en l'absence de composant c2 telle que c1 c2 ≺. Notez qu'un arbre ne peut pas avoir des composants, mais le maximum, dans le cas où il a plus d'un, tous doivent être égaux. Les faits suivants sur les composants seront utiles plus tard. Les preuves ne sont pas difficiles et seront fournis dans une version ultérieure de cet article, à cause du manque d'espace. Lemme 4.1 Si un arbre n'a pas de composante maximale, il doit avoir au moins deux composantes parabole maximale incom-. Lemme 4.2 Deux arbres ont des règles implicites si et seulement si elles ont un superarbre commun minimal unique. Lemme 4.2 on peut calculer des règles implicites d'une manière algorithmiquement coûteuse, l'obtention d'supertrees minimales communes, qui a coûté quadratique. Pour éviter cela, nous vous proposons plusieurs heuristiques pour accélérer le processus. Une conséquence de simples de ces lemmes est: Cor 4.3 Tous les arbres dispensable corollaire a, b telle que b ont des règles implicites. Un cas particulièrement utile lorsque l'on peut prouver formellement les règles implicites, et qui permet de détecter une grande quantité d'entre eux dataset l'exploitation minière, se produit lorsque l'un des arbres a dans la vie réelle d'un seul composant. Théorème 4.4 Supposons que a et b sont deux arbres incomparables, et b a une seule composante. Ensuite, ils ont des règles implicites si et seulement si a une composante maximale qui est une sous-arborescence du composant b. Preuve Supposons que a et b sont deux arbres incomparables tels que décrits dans la déclaration: a a des composants a1,. . . , Un, et b a seulement le composant b1. Nous représentons leurs structures graph- comme aa n1 quement ... b: 1b a: Supposons qu'un composant a un maximum qui est un sous-arbre de b1. Sans perte de généralité, on peut supposer qu'un composant est un tel. Ensuite, nous prétendons que ∧ b → c est une règle implicite, où c est un arbre avec des composants a1,. . . , An-1, et b1. Autrement dit, le juge Balcázar et al. c: b: a: 1 n 1 n-1 b 1AA 1a ...... a Pour montrer que c'est en fait une règle implicite, supposons que, pour un arbre x, a x et x b. Du fait que x, nous gagnons un aperçu de la structure de x: elle doit contenir des composants où les composants de A et de B peut cartographier, et ainsi, il doit y avoir au moins n composants x. Alors, laissez-x1,. . . , Sont les composants xm de x, avec m ≥ n, et supposons que ai xi pour tout i tel que 1 ≤ i ≤ n. Etant donné que B est également un sous-arbre de x, b1 doit être une sous-arborescence de certains xi avec 1 ≤ i ≤ m. Nous montrons maintenant que, pour toutes les valeurs possibles de i, c doit être un sous-arbre de x puis, a ∧ b → c est une règle implicite: • Si i ≥ n, ak XK pour tout k ≤ n 1, et xi b1. • Si i <n, puis - ak xk pour k 6 = i et 1 ≤ k ≤ n 1 - ai un xn - b1 xi Dans les deux cas, c x, et nous fait. Pour montrer l'autre sens, supposons que n'a pas une composante maximale qui est un sous-arbre de b1. Nous montrerons que, dans ce cas, il y a deux supertrees communes minimales différentes a et b. Ensuite, par le Lemme 4.2, nous obtiendrons la conclusion souhaitée. La condition précédente sur les composants maximale peut être divisé en deux possibilités: 1. Tree a ne dispose pas d'une composante maximale. D'après le lemme 4.1, il doit y avoir deux composantes maximales d'un qui sont incomparables, disons-ai et aj. Maintenant, nous prétendons que les deux arbres c et d dans la figure ci-dessous sont deux supertrees communes minimales différentes a et b: ... ...... aa a1 nj ... ...... aa an1 i + + d: ia b1 ja 1b c: En premier lieu, nous montrons que c et d sont différents. On suppose qu'ils sont égaux. Ensuite, étant donné que b1 ne peut pas être un sous-arbre selon l'une quelconque ak, 1 ≤ k ≤ n (parce que a et b sont supposés être incomparable), les composants contenant b1 doivent correspondre. Mais alors, les multijeux suivants (le reste des composants c et d) doivent être égaux: {al | 1 ≤ l ≤ n ∧ l 6 = i} = {al | 1 ≤ l ≤ n ∧ l 6 = j}. Mais l'égalité si et seulement si ai = aj, ce qui est faux. Ensuite, c 6 = d. En second lieu, nous montrons que c contient a et b peu. Appel c1,. . . , Cn pour les composants de c dans le même ordre dans lequel elles apparaissent: ck = ak pour tout k ≤ n sauf pour k = i, pour laquelle des mines Implications de grilles d'arbres fermé ck = ai ⊕ b1. Supposons maintenant que l'on supprime une feuille de c, c obtenir '≺ c, dont les composantes sont C'1,. . . , C 'n (qui sont comme les ck est à l'exception de celle contenant la feuille supprimé correspondante). Nous verrons que c 'ne contient pas ou b en analysant deux possibilités pour l'emplacement de la feuille supprimé, soit (a) dans le composant ci = ai ⊕ b1 ou (b) dans tout autre composant: (a) Supposons que la feuille est supprimé à partir de ci = ai ⊕ b1 (qui est, c'i ≺ ci). Alors, que ce soit AI 6 c'i ou b1 6 c'i. Dans le cas où b1 6 c'i, nous avons que b 6 c 'depuis b1 ne figure pas dans tout autre composant. Donc, supposons que ai 6 c'i. Dans ce cas, tenir compte du nombre d'occurrences s d'Aï dans un. Etant donné que ai est une composante maximale, les occurrences d'AI dans un sont les seuls composants qui contiennent ai comme un sous-arbre. Par conséquent, le nombre de compone nts de c qui contiennent ai est exactement s, mais il est s - 1 en c 'en raison de la feuille supprimée dans ai ⊕ b1. Ensuite, un 6 c. (B) On suppose maintenant que la feuille est supprimé à partir de la visualisation pour k 6 = i. Dans ce cas, il est clair que ak 6 c'k, mais nous devons nous assurer que 6 c 'par quelque mappage matchs avec une composante ak de c' différent de c'k. Pour contradiction, Supposons qu'il existe un tel cartographie, qui est, pour une permutation π de du groupe symétrique de n éléments, nous avons h c'π (m) pour chaque m ≤ n. Soit l la longueur du cycle contenant k dans la représentation de cycle de π (ainsi, nous avons πl (k) = k, et a une autre valeur de k pour les exposants 1 à L - 1). Nous avons que ak aπ (k) aπ2 (k). . . aπl-1 (k) pour chaque depuis h de la chaîne précédente, sauf le dernier, si π (m) 6 = i, puis h c'π (m) = aπ (m); tandis que si π (m) i =, puis am aπ (m) parce que ai est une composante maximale. De la chaîne précédente de confinements, nous concluons que ak aπl-1 (k). Mais aπl-1 (k) c'πl (k) = c 'k. Mettre ensemble, nous obtenons ak c'k, ce qui est une contradiction. Par conséquent, un 6 c. Maintenant, à partir de (a) et (b), nous pouvons conclure que c est une superarbre minimale commune a et b. De toute évidence, la même propriété peut faire valoir pour d de manière symétrique, et puisque c et d sont différents, Lemme 4.2 implique que a et b ne peut pas avoir des règles implicites. 2. Tree a des composants a un maximum, mais ils ne sont pas sous-arbres de b1. Nous considérons maintenant les arbres suivants: 1a-1 a un bana1 1bn1 ... + e: ... f: Nous montrerons (a) cet arbre est un e superarbre commune minimale a et b, et (b) cet arbre f est un superarbre commun de a et b et ne contient pas e. De (a) et (b), nous pouvons conclure que a et b doit avoir deux sous-arbres communs minimaux différents. Prenez e comme l'un d'entre eux. Pour l'autre, soit f 'être un arbre obtenu à partir de f en supprimant les feuilles jusqu'à ce qu'il soit minime (qui est, en supprimant un autre congé ne contiendraient pas a ou b). Depuis e 6 f (à partir du point (b)), on estime que e 6 f '. D'autre part, si nous avions f 'e, puisque e est minime, nous aurions e = f', puis e f, J. Balcázar et al. qui contredit le point (b). Par conséquent, e et f doivent être deux supertrees communes minimales incomparables de a et b, et le théorème suivant. Pour compléter la preuve, il est seulement à gauche indique: (a) l'arbre est un e superarbre minimale commune a et b. Notez que la preuve dans le cas précédent 1, montrant que c est une superarbre minimale commune a et b, applique à l'e aussi. L'argument en faveur c était fondé sur la maximalité de AI, mais un maximum est e, puis il est également maximale, de sorte que la preuve s'applique. (B) l'arbre f est un superarbre commun de a et b, et ne contient pas e. Il est clair que, par définition, f est un superarbre commun de a et b. Maintenant, nous allons faire valoir que e 6 f. Pour cette inclusion est vrai, un ⊕ b1 devrait être une sous-arborescence de certains composants de f. Il ne peut pas être un sous-arbre de l'un des composants de l'ak (k ≤ n) depuis lors b1 et b ak a, ce qui est faux. D'autre part, un ⊕ b1 ne peut pas être un sous-arbre de b1 non plus, parce que cela signifierait qu'une b1, ce qui est faux dans ce cas. Par conséquent, f ne contient pas e. Depuis que nous avons prouvé l'existence de deux supertrees communes minimales également pour ce cas, une nouvelle application du lemme 4.2 complète la preuve. Corollaire 4.5 Deux arbres avec un composant ont chacune des règles implicites si et seulement si elles sont comparables. En fait, un fragment de l'argumentation de ce théorème peut également être appliqué directement aussi bien à certains cas qui apparaissent dans la pratique: Définition Étant donné deux arbres a, b, on note a + b l'arbre construit reliant les racines de tous les composants de a et b à un seul noeud racine. Définition Étant donné deux arbres a et b, un arbre avec des composants a1, · · ·, un arbre et b avec des composants b1, · · ·, bk, et n ≥ k, on note a] b l'arbre construit par récursive joignant les arbres aI] bi pour 1 ≤ i ≤ k, et ai pour k <i ≤ n, à un seul noeud racine. Si b ne dispose que d'un nœud puis a] b = a. Dans le cas où n <k, a] b est défini comme b] a. Proposition 4.6 La règle a ∧ b → c est pas un impli cit règle si c 6 a + b ou c 6 a] b. Preuve Si c 6 a + b ou c 6 a] b, alors a + b ou] b sont supertrees a et b qui ne sont pas supertrees de c et par la définition de la règle implicite, la règle a ∧ b → c est pas implicite. En utilisant la proposition 4.6, nous avons mis en place une heuristique récursive supplémentaire qui peut être expliqué comme suit: pour chaque règle a ∧ b → c nous construisons a + b et a] b et si nous nous rendons compte que l'un d'entre eux est pas un superarbre de c, alors la règle n'est pas implicite. 5 Validation expérimentale Nous avons testé nos algorithmes sur deux ensembles de données réelles. La première est CSLOGS Dataset (Zaki (2002)). Il se compose de fichiers journaux Web collectés plus d'un mois au département des implications minières informatique de Treillis de Sciences Fermé Arbres de l'Institut Polytechnique Rensselaer. Les journaux ont touché 13.361 pages Web uniques et ensemble de données CSLOGS contient 59.691 arbres. La taille moyenne des arbres est 12. Le deuxième ensemble de données est Gazelle, un ensemble de données de la Coupe KDD 2000 (Kohavi et al., 2000). Cet ensemble de données est un fichier journal web d'un véritable centre commercial Internet (gazelle.com), a une taille de 1,2 Go et contient 216 attributs. Nous utilisons l'attribut « session ID » à associer à chaque session utilisateur un arbre unique. Les arbres enregistrent la séquence des pages Web qui ont été visités dans une session utilisateur. Chaque arbre noeud représente un contenu, assortiment et le chemin produit. Les arbres ne sont pas construits à l'aide de la structure du site Web, au lieu qu'ils sont construits suivant la diffusion en continu de l'utilisateur. Chaque fois qu'un utilisateur visite une page, s'il n'a pas visité avant, nous prenons cette page comme un nouveau nœud plus profond, sinon, nous backtrack au nœud cette page correspond à, si elle est le dernier nœud visité sur une profondeur de béton. L'ensemble de données résultant est constitué de 225.558 arbres. Sur ces ensembles de données, nous avons calculé les règles d'association après notre méthode. Nous avons ensuite analysé un certain nombre de questions. Tout d'abord, nous avons vérifié combien de règles redondantes pourraient être évités par un système de production de règles plus sophistiquées le long des lignes de base Guigues Duquenne-; Cependant, la structure de ces ensembles de données conduit à peu ou pas de redondance pour cette raison, et nous omettons examen plus approfondi de cette considération. Ensuite, nous avons mis en place une étape de détection de règle implicite basée sur tous les critères DE- tracée dans la section précédente. considérations de synchronisation sont plutôt non pertinentes, en ce que le temps de surcharge imposée par cette étape de détection de la règle implicite est relativement faible. Nous comparons le nombre de règles obtenues, le nombre de règles implicites détectées et non implicite, et le nombre de règles non implicites. La figure 1 montre les résultats pour l'ensemble de données CSLOGS, et l'ensemble de données Gazelle. Nous observons que lorsque le support minimum des sous-arbres fréquents fermés diminue, le nombre de règles augmente et le nombre de règles détectées diminue. Le nombre de règles tégé dé- dépend de l'ensemble de données et sur le support minimum. A titre d'exemple, nos détecte méthode si une règle est implicite ou non dans 91% des règles obtenues à partir d'ensemble de données CSLOGS avec un support de 7,500 et 32% des règles obtenues à partir de Gazelle dataset avec un support de 500. Le nombre de non règles implicites sont plus de 75% dans les deux ensembles de données. 6 Conclusions Nous avons mis au point, sur la base d'un opérateur de fermeture sur des ensembles d'arbres pour un ensemble de données, étudié dans nos précédents travaux, une nouvelle forme d'implication (ou règle d'association déterministe) entre les arbres. Nous avons présenté une caractérisation mathématique et proposé une méthode pour obtenir une base. Ensuite, nous avons discuté pourquoi les combinatoires particulières de notre application de la base conduisent toujours à des informations redondantes dans la sortie: les règles implicites qui sont construits par notre méthode, mais, en fait, en raison des combinatoires des arbres, tiendrez dans tous les ensembles de données et parler rien sur l'ensemble de données en cours d'analyse. Alors qu'une caractérisation complète de ces licenciements est, jusqu'à présent, hors de la portée de notre travail, nous avons été en mesure de fournir une caractérisation exacte pour un cas particulier, où l'un des deux arbres impliqués dans la antece dents a un seul composant. Nous avons démontré, par une mise en œuvre et une analyse empirique sur des ensembles de données de la vie réelle, que nos offres de développement un bon d'équilibre entre la sophistication mathématique et l'efficacité dans la détection des règles implicites, puisque avec seulement notre caractérisation et deux heuristiques nous attrapons un grand rapport de règles implicites. J. Balcázar et al. 0 200 400 600 800 5000 10000 15000 20000 25000 30000 Soutien CSLOGS Nombre de règles Nombre de règles non implicite nombre de règles détectées 0 100 200 300 400 500 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 support GAZELLE Nombre de règles Nombre de règles pas implicite Nombre de règles détectées figure. 1 - résultats expérimentaux de données réelles sur des jeux de données CSLOGS et Gazelle Références Balcázar, J. L., A. Bifet et A. Lozano (2006). Intersection des algorithmes et un opérateur de fermeture sur les arbres non ordonnées. En MLG 2006, 4e Atelier international sur l'exploitation minière et l'apprentissage avec des graphiques. Balcázar, J. L., A. Bifet, et A. Lozano (2007a). L'exploitation minière des arbres désordonnées fermés fréquents à travers des représentations naturelles. Actes de la 15e Conférence internationale sur les structures (CIEC Con- 2007 conceptuels). Balcázar, J. L., A. Bifet, et A. Lozano (2007b). les tests et l'exploitation minière sous-arbre d'arbre fermé à travers des représentations naturelles. Atelier Les progrès dans la connaissance conceptuelle Engineer- ing. Balcázar, J. L. et G. C. Garriga (2007a). implications caractérisantes des ordres partiels injectifs. Dans Actes de la 15e Conférence internationale sur les structures conceptuelles (Implications CIEC Mines de Treillis de 2007) Fermé les arbres. Balcázar, J. L. et G. C. Garriga (2007b). Corne axiomatisations pour les données séquentielles. Théorique- cal Computer Science 371 (3), 247-264. Chi, Y., R. Muntz, S. Nijssen, et J. Kok (2005a). l'exploitation minière fréquente de sous-arbre - une vue d'ensemble. Fondam. Inf. 66 (1-2), 161-198. Chi, Y., Y. Xia, Y. Yang et R. Muntz (2005b). L'exploitation minière des arbres sous fréquents fermés et maximale de bases de données d'arbres enracinés étiquetés. IEEE Transactions sur les connaissances et l'ingénierie des données 17 (2), 190-202. Kohavi, R., C. Brodley, B. Frasca, L. Mason, et Z. Zheng (2000). KDD-Cup 2000 Rapport des organisateurs: Peeling l'oignon. SIGKDD explorations 2 (2), 86-98. Pfaltz, J. L. et C. M. Taylor (2002). découverte de connaissances scientifiques à travers des formations trans itératives du concept lattices. Dans l'atelier sur Discrete Math. et l'exploration de données à la Conférence SIAM DM, pp. 65-74. Termier, A., M.-C. Rousset, et M. Sebag (2004). DRYADE: une nouvelle approche pour découvrir des arbres fréquents fermés dans les bases de données d'arbres hétérogènes. En ICDM, pp. 543-546. Sauvage, M. (Septembre 1994). Une théorie des espaces de fermeture finis en fonction des implications. Advances in Mathematics 108, 118-139 (22). Zaki, M. J. (2002). l'exploitation minière des arbres fréquents dans Efficacement une forêt. Dans 8 SIGKDD Conférence nationale sur les connaissances inter-découverte et l'exploration de données. Nous proposons Résumé Une Manière à des rules d'Extraire association de haute confiance d'en- sembles de se Données d'arbres non Composant étiquetés. Les ANTÉCÉDENTS par un Sont obtenus à un calcul transversal apparenté d'hypergraphe, TANDIS Que les conséquens se suivent d'applications Une des Opérateurs de sur les arbres fermeture non des Dans étiquetés développés des travaux Précédents auteurs. Nous Discutons en plus de le détail des rules trivialement CAS valides, ci-Sont PUISQUE Celles, plus complexes Que in the des itemsets CAS, being nous ne Donné Que Travaillons en plus treillis AVEC ONU."
872,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Semantics of Spatial Window over Spatio-Temporal Data Stream,"Dans les systèmes DSMS (Data Stream Management Systems), les données en entrée sont infinies et les requêtes sur celles-ci sont actives tout le temps. Dans le but de satisfaire ces caractéristiques, le fenêtrage temporel est largement utilisée pour convertir le flux infini de données sous forme de relations finies. Mais cette technique est inadaptée pour de nombreuses applications émergentes, en particulier les services de localisation. De nombreuses requêtes ne peuvent pas être traitées en utilisant le fenêtrage temporel, ou seraient traitées plus ecacement à l'aide d'un fenêtrage basé sur l'espace (fenêtrage spatial). Dans cet article, nous analysons la nécessité d'un fenêtrage spatial sur des flux de données spatio-temporels, et proposons, sur la base du langage de requêtes CQL (Continuous Query Language), une syntaxe et une sémantique associées au fenêtrage spatial.","Yi Yu, Talel Abdessalem",http://editions-rnti.fr/render_pdf.php?p1&p=1000570,http://editions-rnti.fr/render_pdf.php?p=1000570,en,"Sémantique de la fenêtre spatiale sur spatio-temporelle du flux de données Yi Yu *, **, Talel ** * Centre Abdessalam CIMS, Université de Tongji à Shanghai, Chine ** Ecole Nationale Supérieure des Télécommunications ASLD - UMR CNRS 5141 46, rue Barrault, 75013 Paris - France First.Last@enst.fr CV. Dans les DSMS Systèmes (Data Stream Management Systems), les en entrée Données et les Sont infinies sur requests-ci Celles actives tout le Sont temps. Dans le mais de bureaux characteristics Satisfaire, le fenêtrage is temporel largement le Utilisée verser flux Convertir de Données sous infini forme de rela- tions finies. Mais la technique this is de verser inadaptée Emergentes, applications Nombreuses en les services de Particulier localisation. De ne requests Nombreuses pas PEUVENT Être en Utilisant le traitées fenêtrage temporel, ainsi que ous EFFICACEMENT seraient à traitées l'aide d'un fenêtrage basons sur l'espace (espace fenêtrage). Dans this article, La nécéssité NOUS analysons D'un fenêtrage spatiale des flux de Sur Donnees Spatio-temporels, ET proposons, sur la base de du langage de requests CQL (Query Language continue), juin et syntax juin Associées au fenêtrage sémantique spatiale. 1 Introduction Les systèmes de gestion de flux de données (DSMS) ont vu le jour pour répondre aux besoins de traitement de changement continu, les données et les réponses sans bornes en temps réel. Les applications comprennent le matériel cité, le traitement des ventes, contrôle des flux de réseau, le déplacement des objets de surveillance [Abdessa- LEM et al. (2007), Moreira et al. (2000)], etc. Dans ces cas, les caractéristiques communes consistent à: 1- les sources de données sont infinies et en temps réel l'évolution, 2- les requêtes sur les données doivent produire des réponses continues. Pour faire face à la première fonction, le concept de fenêtre est proposée. L'idée consiste à transformer flux de données dans les tables de unbounded données bornées, puis les requêtes peuvent être traitées comme dans un système de base de données traditionnelle. Pour la deuxième fonction, doivent être exécutées thodes moi- d'évaluation requête résultant en continu dans un temps réel l'évolution de la réponse. Comme nous l'avons mentionné ci-dessus, les techniques de fenêtre sont proposées pour résoudre deux problèmes dans le traitement des flux de données: sources de données infinies et requête continue. Dans DSMS actuelles, la ration ope- fenêtrage est effectué en utilisant les horodatages des données d'entrée (à savoir des attributs temporels). Par exemple, dans une application de surveillance du trafic réseau, il est impossible de stocker et d'analyser en ligne l'ensemble des données d'entrée. Nous pouvons simplement surveiller en permanence la situation pendant un intervalle de temps borné, Sémantique de fenêtre spatiale, par exemple la dernière 1 heure. Dans ce cas, nous avons un flux infini de données de trafic et une fenêtre poral tem- 1 heure. L'ensemble des données appartenant à la fenêtre spécifiée est fini. Les requêtes sont ensuite évaluées périodiquement (par exemple, toutes les 2 minutes) sur l'ensemble de données appartenant à la fenêtre. Ainsi, le résultat des requêtes changera en continu (toutes les 2 minutes). DSMS proposent également des opérateurs fenêtré tels que les opérateurs globaux, rejoindre l'opérateur, sélectionnez l'opérateur, etc. pour permettre de répondre à des requêtes complexes. En plus des premières applications de flux de données, le développement de dispositifs sensibles à l'emplacement a également stimulé la surveillance en ligne ser- vices géolocalisées et en temps réel des objets en mouvement applications. Outre les caractéris- tiques communes de DSMS, ces applications plus besoin de traiter les données spatiales et spatio-temporelle pour les services de localisation. Dans ce cas, les données d'entrée est supposé être constitué d'au moins deux tributs at-: un attribut spatial et un horodatage. Dans les précédentes littératures, deux approches sont envisagées pour le traitement continu des requêtes sur les flux de données spatio-temporelles. Dans la première approche [Patroumpas et Sellis (2004)], l'opération fenêtrage est effectuée en utilisant uniquement l'attribut Ti- de mestamp. Ensuite, les restrictions spatiales sont évaluées sur chaque ensemble de données composant les fenêtres obtenues « fenêtres temporelles ». Le problème avec cette approche est que l'or- der fixe d'opérations (temporelle / spatiale restr ictions) dans un plan d'évaluation de la requête peut avoir un impact important sur les performances du système [Elmongui et al. (2006)]. En d'autres termes, il peut être inefficace dans certains cas, lorsque la restriction temporelle est toujours fait avant celle spatiale. De plus, de nombreux services de localisation ne comportent pas d'attribut temporel du tout et ne se soucient de l'information spatiale, par exemple Plage ou requête kNN [Mokbel et Aref (2005)], etc. Dans la deuxième ap- proche, tel que proposé dans le projet PLACE [Mokbel et al. (2005)], l'opération de fenêtrage est effectué en utilisant l'attribut spatial. Ensuite, les restrictions temporelles sont évaluées sur chaque ensemble de données composant le Spatial obtenu de Windows. Pour souligner la nécessité de fenêtres spatiales pour les applications spatio-temporelles, nous prenons l'exemple suivant. Compte tenu de la situation d'un véhicule en mouvement sur la route. Le véhicule envoie les mises à jour de sa position périodiquement aux DSMS, et nous aimerions connaître sa vitesse moyenne dans les 50 derniers kilomètres. Temporelle fenêtrage n'est pas efficace pour répondre à ce genre de requête, car la taille de la fenêtre temporelle ne peut être décidée à l'avance. Le temps passé par l'objet observé pour traverser les 50 kms, peut être modifié de manière significative le long de la trajectoire de l'objet. est plus approprié Un fenêtrage spatial pour répondre à ce genre de requête. On peut se replier sur la fenêtre spatiale (spécifiée par le critère « dans les 50 derniers kilomètres ») pour attraper la tion de données finis à partir du flux de données sans bornes. Seule la sémantique des changements fenêtrage opération parce que nous utilisons la dimension spatiale au lieu de le temporel. Lors du traitement des flux ming INCO, fenêtre spatiale conserve son contenu en fonction de l'attribut spatiale des données d'entrée (tuples). Seules les données dans un certain périmètre de zone sont d'intérêt et sont conservés dans la fenêtre. Dans cet article, nous nous concentrons sur l'extension de DSMS à la gestion des données spatio-temporelles. Nous vous proposons deux types de fenêtres spatiales: une fenêtre spatiale statique et une fenêtre spatiale en mouvement. Ces opérations sont présentées comme une extension possible du CQL de langage de requête (Query Language continue) [Arasu et al. (2006); Arasu et Widom (2004)]. Leur sémantique est définie en fonction de la sémantique abstraite de CQL et leur expressivité est démontrée à l'aide des exemples de requêtes. Ce document est organisé comme suit. La section 2 présente la requête en langage de CQL. La section 3 décrit notre extension de CQL pour soutenir fenêtrage spatiale, et l'article 4 conclut le document. Yi YU et al. 2 Préliminaires Dans cette section, nous présentons le modèle de données et les opérations de base proposées dans CQL. Ce langage de requête a été défini dans le projet STREAM [Arasu et al. (2006); Arasu et Widom (2004)]. Sa syntaxe est proche de la syntaxe SQL-99 et il est spécialement conçu pour les données continues de traitement sur les flux de données. CQL suppose que les tuples d'entrée sont disponibles dans une séquence ordonnée de temps et l'opération de fenêtrage est traitée sur l'horodatage de chaque tuple. Tout d'abord, nous présentons les fonctions de base et les domaines de données, nous donnons les définitions de flux et de la relation et leurs opérateurs connexes. - T: le domaine temporel; T est un domaine discret, le temps commandé. Un instant de temps est une valeur de T. Le temps des attributs appartiennent à T. - TP: domaine Tuple; Un tuple est une séquence finie de valeurs atomiques. - Σ: Tuple domaine multi-ensembles; C'est le domaine de finis, mais sans bornes, sacs de tuples. - S: domaine alluvionnaire; Ceci est le domaine des multi-ensembles plus TP × T (voir définition 2.1) - R: domaine de parenté; R: T → Σ, c'est le domaine des fonctions des instants de carte à sacs de tuples (voir Définition 2.2). - R2ROp: Σ × ... × Σ → Σ Ce domaine des fonctions qui produisent un sac de tuples d'un ou plusieurs sacs de tuples. Par exemple, les opérateurs d'algèbre relationnelle (par exemple π, σ, etc.). - S2ROp: S × T → Σ C'est le domaine des fonctions qui dialoguent un flux de sacs de tuples. Par exemple, les opérateurs de CQL RANGE et SLIDE. - R2SOp: Σ × T → S Ceci est le domaine des fonctions qui conversent sacs finis de tuples à un cours d'eau. Tel fonctions CQL sont IStream, DSream et RStream. Définition 2.1: Un flux de S est un sac (éventuellement infinie) (multi-ensembles) d'éléments <s, t>, où s est un tuple appartenant au schéma de S et t ∈ T est l'estampille temporelle de l'élément. Définition 2.2: Une relation R est une application de chaque instant t ∈ T à un sac fini mais sans bornes de tuples, noté R (t), appartenant au schéma de requêtes R. CQL sont composées des opérateurs appartenant à trois classes: les opérateurs de Relation Relation-to-(domaine de R2ROp), les opérateurs jet-à-Relation (du domaine S2ROp), et les opérateurs de parenté-à-Stream (de domaine R2SOp). Rebâtissons l'exemple donné dans l'introduction, et nous considérons que nous recherchons la vitesse moyenne d'une voiture dans les 60 dernières minutes toutes les 10 minutes. Dans cette situation, Sémantique temporelle de glissement de fenêtre fenêtre spatiale peut être utilisé pour extraire tuples de 60 dernières minutes avec la valeur de glissement de 10 minutes. En utilisant CQL, phrase de requête peut être exprimée comme suit: Q1: SELECT RStream (AvgSpd (R)) DE R = Stream_car [RANGE '60 minutes', SLIDE '10 minutes'] = OU R.ID MonAuto Cette requête est construite à partir trois classes d'opérateurs: Gamme '60 minutes' et SLIDE '10 minutes' design respectivement la taille de la fenêtre et l'étape de déplacement de la fenêtre spatiale. Ils appartiennent au domaine S 2ROp. Un opérateur de restriction relationnelles limite tuples d'avoir la valeur d'ID égal à mycar 'après la conversion du flux de relation. au cours des 60 dernières minutes en utilisant uplets répondant à la restriction précédente à un opérateur global 'AvgSpd () calcule la vitesse moyenne de' mycar. Ces deux opérateurs appartiennent au domaine R2ROp. Enfin, un opérateur de RStream convertit le contenu de la relation de résultat (ID d'objet et de la vitesse moyenne après culation de cal- précédente) pour diffuser en continu et transmet aux utilisateurs. Cet opérateur appartient au domaine R2SOp. Dans cet exemple, les trois catégories d'opérateurs peuvent être exprimées par la sémantique de CQL. La sémantique de CQL est spécifiée en utilisant une signification functionM [Arasu et Widom (2004)]. La fonction de sens prend toute requête Q appartenant à CQL et renvoie une fonction ""entrée-sortie"" M ~ Q (r, s, t) après calcul par Q. M ~ Q (r, s, t) prend les cours d'eau et les relations référencées dans Q et un instant t de temps (par exemple maintenant) comme entrée. Ensuite, il spécifie la sortie produite par Q à l'instant t. Laissez-nous consisder l'exemple de requête Q1 donnée ci-dessus. Dans la première étape, une conversion à jet de parenté est effectuée sur la tream_car du flux de données. La fonction de sens de cette opération est la suivante: MS 2R ~ Stream_car [RANGE '60 minutes', SLIDE '10 minutes'] = {λStream_car.λt tp:. (Tp, t ') ∈ Stream_car ∧ (Tb ≤ t' ) ∧ (t '≤ cuisse)}, où cuisse = bt / 10 minutesc x 10 minutes, et Tbas = max {cuisse - 60 minutes, 0} l'expression cuisse = bt / 10 minutesc × 10 minutes calcule le plus grand temps multiple instant de 10 minutes et plus petit puis t. Intuitivement, la conversion jet à sa sortie de parenté définit tuples toutes les 10 minutes, seuls les tuples dans les 60 dernières minutes sont contenues dans la sortie. Sur la base de la relation de sortie obtenue, l'évaluation de l'opération où se fait selon la fonction signification suivante: MR2R ~ Où R.ID = mycar = λR.λ ID.λmycar. {R: r ∈ R∧ r.ID = mycar} La fonction sens de l'opérateur AvgSpd (R) est: MR2R ~ AvgSpd (R) = λ R. {Spd: Spd = distance / 60 minutes} Dans ce cas, la distance représente le chemin que la voiture a surveillé passé dans les dernières 60 nutes mi-. Il est calculé en utilisant les informations de position qui doit contenir chaque tuples dans le dow gagnant. Enfin, RStream opérateur du domaine R2SOp convertit sa relation d'entrée à un flux. Sa fonction est ce qui signifie: Yi YU et al. MR2s [RStream (R)] = {λR.λt <e, t '>: t' ≤ t ∧ e ∈ R (t)} 3 Ajout d'une opération de fenêtre spatiale pour CQL Dans cette section, nous proposons une extension à CQL en Afin de soutenir fenêtrage spatial. Nous considérons deux catégories de fenêtres spatiales: fenêtres fixes et mobiles fenêtres. Les coordonnées spatiales d'une fenêtre fixe d o ne change pas au fil du temps. Cependant, les coordonnées spatiales d'une fenêtre mobile peut changer au fil du temps. Pour illustrer cela, nous allons examiner les deux requêtes spatiales suivantes: • Stationary spatiale de Windows Supposons que nous nous intéressons à la surveillance des trajectoires des navires de pêche dans la mer de Chine orientale. Dans ce cas, la zone d'intérêt peut être considérée comme une fenêtre spatiale fixe et la trajectoire de chaque navire de pêche se compose dans ses positions continues dans ce domaine. • Déplacement spatial de Windows Recall l'exemple de l'article 1 et l'étendre: « Interrogez la vitesse moyenne d'un véhicule dans les 50 derniers kilomètres à chaque fois après qu'il se déplace à 10 km ». La fenêtre spatiale a ici la taille de fenêtre de 50 kms et l'étape de glissement de 10 km, ce qui indique que la fenêtre spatiale est en mouvement. Sur la base de cette description informelle des fenêtres spatiales, nous étendons en 3.1 le modèle de données CQL et ses opérations de base présentés à la section 2. Ensuite, nous définissons en 3.2 la syntaxe et la sémantique de l'opération de fenêtre spatiale. 3.1 Extension du modèle de données Pour pouvoir traiter les données spatiales, nous ajoutons les domaines suivants modèle de données CQL. - G: domaine spatial; Ceci est le domaine des valeurs spatiales. Les attributs spatiaux des flux de données spatio-temporelles appartiennent à ce domaine. - R = Rt ∪ Rg: domaine de parenté; Rg: G → Σ, c'est le domaine des fonctions que la carte de restriction spatiale sacs de tuples (voir Définition 3.2). Rt: T → Σ, tel est le domaine des fonctions que la carte de restriction temporelle de sacs de tuples (ce domaine est noté R dans le modèle de données de CQL, voir la section 2). - S: domaine alluvionnaire; Ceci est le domaine des multi-ensembles sur TP G × × T (voir Définition 3.1) - S2ROp: S × (G ∪ T) → R; C'est le domaine des fonctions de fenêtrage qui dialoguent un flux spatio-temporelle des sacs de tuples. Cela correspond aux nouveaux opérateurs de fenêtrage GAMME DE ... [RATTR SPACE | TIME] et SLIDE PAR ... [sAttr SPACE | TEMPS]. - R2SOp: Σ × G × T → S; Ceci est le domaine des fonctions qui dialoguent sacs finis de tuples dans un flux spatio-temporelle, en ajoutant un tampon spatial et un timbre temporel pour chaque tuple. Cela corres- pond aux nouvelles fonctions de CQL IStream, RStream et DSTREAM. Sémantique de fenêtre spatiale Un flux de données spatio-temporelle se compose d'un courant de n-uplets, chacun d'eux est marqué avec un attribut temporel et un attribut spatial (à savoir, chaque tuple possède deux timbres). opérateurs temporels de fenêtrage sont exécutées sur la base des timbres temporels, et les opérateurs de fenêtrage spatiales sont exécutées en fonction des timbres spatiales. Nous définissons le flux et le modèle de relation dans le cas des données spatio-temporelles comme suit. Définition 3.1: flux A S dans le cas spatio-temporelle est un sac (possible infini) d'éléments <s, g, t>, où s est un tuple appartenant au schéma de S, g ∈ G est un tampon spatial correspondant par exemple pour les coordonnées spatiales d'un objet en mouvement, et t ∈ T est l'estampille temporelle de l'élément. Définition 3.2: Une relation R (g, t) est une application de chaque emplacement g ∈ G et chaque t ∈ instant de temps T à un fini mais sac sans bornes de tuples, appartenant au schéma de R. Le contenu d'un rapport des changements de R dans l'espace et le temps. Dans un flux de données S, on désigne par g du tampon spatial, et on désigne par t l'estampille temporelle de chaque tuple. Ces timbres sont nécessaires pour les opérations de fenêtrage spatiales et temporelles. Les mises à jour provenant de différentes sources (par exemple, des objets en mouvement) peuvent circuler dans des courants séparés ou peuvent être incorporés dans le même flux. Lorsque l'on utilise une opération de fenêtrage spatial, les tuples sont filtrés dans des sacs de tuples en fonction de leurs timbres spatiales. Lors de l'utilisation d'une opération de fenêtrage temporelle, ils seront filtrés en fonction de leurs timbres temporels. Une requête en continu sur un flux spatio-temporel peut être composé de seulement les opérations de fenêtrage spatiales ou temporelles. Il peut également être composé en même temps des deux opérations de fenêtrage temporelles et spatiales. 3.2 sémantique et la syntaxe de fenêtre spatiale depuis un flux de données est infini et la taille de la mémoire est limitée, l'approche fenêtrage est fondamentale pour le traitement des requêtes continues Dsms [Patroumpas et Sellis (2006)]. Dans la syntaxe CQL [Arasu et al. (2006); Arasu et Widom (2004)], le fonctionnement de la fenêtre est indiquée par les mots-clés RANGE, ROW et coulisser. Dans TelegraphCQ [Chandrasekaran et al. (2003)], l'opération de fenêtre est désignée par l'expression GAMME DE ... SLIDE PAR et dans [Li et al. (2005); Maier et al. (2005)] GAMME, SLIDE sont utilisés pour désigner l'opération de fenêtre mots-clés même CQL. Dans cet article, nous utilisons une syntaxe similaire à TelegraphCQ pour illustrer nos opérations spatiales de la fenêtre. La syntaxe que nous considérons pour l'opération fenêtrage est la suivante: GAMME DE v1 RATTR ESPACE | TEMPS, SLIDE DE L'ESPACE v2 sAttr | TIME V1 désigne la taille de la fenêtre (attribut de gamme) et v2 représente un pas entre deux fenêtres successives (attribut de glissement). La portée et les caractéristiques de glissement peuvent être des valeurs temporelles ou spatiales. Ceci est indiqué par les mots-clés RATTR SPACE et RATTR TIME pour l'attribut plage, et par les mots-clés sAttr SPACE et sAttr TIME pour l'attribut diapositive. La portée et l'attribut de glissement peuvent appartenir au même domaine (T ou G) ou non. Dans ce qui suit, nous considérons que le cas où ces deux attributs sont des valeurs spatiales (v1 et v2 ∈ G ∈ G). Yi YU et al. 3.2.1 fixe Fenêtre spatiale Prenons l'exemple de la fenêtre spatiale stationnaire donnée ci-dessus au début de la section 3. Comme nous soucions seulement sur les trajectoires des navires de pêche dans une certaine région (à l'est de la mer de la Chine), on peut utiliser un espace fenêtre correspondant à la mer est de la Chine pour que des captures du flux des tuples d'intérêt. Les trajectoires sont constitués d'ensembles d'informations de position. Pour un objet o1 donné, la requête suivante donne sa trajectoire dans la mer de Chine orientale. Q2: SELECT RStream (Timbres (*)) à partir de Moving_Objects_Stream [RANGE PAR East_China_Sea RATTR SPACE] WHERE ID = « o1 » Dans ce cas, Moving_Objects_Stream désigne un courant spatio-temporel. Le schéma de ce flux est composé de l'attribut ID et d'autres attributs qui indiquent la vitesse et l'orientation des objets en mouvement surveillés. Timbres de fonction (*) renvoie les timbres spatiales et temporelles (G et T) de chaque tuple, qui indiquent la trajectoire de l'objet observé. Cette requête contient une plage en valeur sans SLIDE BY. Cela signifie que la fenêtre spatiale est stationnaire et ne changent pas au fil du temps ou de l'espace. Formellement, la sémantique de la fenêtre spatiale utilisée dans Q2 de requête est spécifié par la fonction de signification sui- vantes: MS 2R ~ Moving_Objects_Stream [RANGE PAR East_China_Sea RATTR SPACE] = λ Moving_Objects_Stream. λ East_China_S ea. {(Tp, g, t): (tp, g, t) ∈ Moving_Objects_Stream ∧ (g intérieur East_China_S ea)} Lors du traitement de la requête Q2, seuls les tuples dans l'intervalle spatial East_China_Sea sont servis avant dans la fenêtre. Le contenu de la fenêtre est mis à jour lorsqu'un nouveau tuple entre en jeu. Le fait qu'un tuple est qualifié pour la fenêtre ou non est déterminé par l'opérateur spatial à l'intérieur, qui détermine si l'emplacement spatial g (tampon spatial) est à l'intérieur de la zone spatiale East_China_Sea notée. Après l'opération Stream à la relation (opération fenêtrage), un traitement supplémentaire sur le contenu de la fenêtre se fera. Dans cet exemple, la clause Where et l'opérateur de timbres seront effectués. La fonction sens de la clause where peut être déduit facilement de la sémantique de CQL présenté dans la section 2. La fonction de sens de l'opérateur de timbres est comme suit. Timbres MR2R ~ (R) = {λR (r.g, t.a.): r ∈ R}. A la dernière étape, le résultat de Q2 de requête est retourné à l'utilisateur dans un format de flux. Cela se fait par l'opération de RStream. Pour chaque tuple ajouté à sa relation d'entrée, l'opérateur RStream réévaluera complètement sa sortie et retourner tous les tuples qui composent le flux de sortie. Formellement, la sémantique de RStream est la suivante: Sémantique de fenêtre spatiale F. 1 - Stream à Relation F de fonctionnement. 2 - Relation à Relation opertation pour le contenu Fenêtre MR2s ~ RStream (R) = λR. {E: e ∈ R (g, t)} La figure 1 représente un exemple de fenêtre spatiale statique sur un flux spatio-temporelle (e S2ROp). Trois objets mobiles (o1, o2 et o3) sont respectées. Chaque nouveau tuple représente une observation et contient l'ID de l'objet observé, sa vitesse et son orientation. Le timbre temporel du nuplet indique l'instant de l'observation, et le timbre spatial indique l'emplacement de l'objet à cet instant. Les trajectoires de la figure 1 représentent les emplacements successifs des objets contrôlés. La figure 1.a représente les observations reçues jusqu'à présent dans le flux, et la figure montre de 1.b le sous-ensemble d'observations qui sont situés à l'intérieur de la zone de la fenêtre spatiale East_China_Sea. La figure 2 montre le résultat de l'opération de filtrage R2ROp. La clause limite les observations que celles correspondant à o1 objet. La figure 3 illustre la conversion de la relation de courant par R2SOp. Dans cet exemple, la sortie de la volonté de RStream tous les tuples de sa relation d'entrée. Dans le cas d'une opération IStream, la relation de résultat sera comparé au précédent et que les nouveaux tuples composera la sortie. F. 3 - opertation Relation à flux pour le contenu fenêtre Yi YU et al. F. 4 - Fonctionnement flux à Relation pour le déplacement Fenêtre 3.2.2 Déplacement fenêtre spatiale Prenons l'exemple de la fenêtre spatiale en mouvement donné au début de l'article 3. Nous ajoutons des restrictions supplémentaires à cet exemple, en supposant que la voiture se déplace dans un ligne droite et garde la même direction. Cette requête peut être exprimée comme suit: Q3: SELECT RStream (mycar, AvgSpd (R)) à partir de R = Stream_mycar [RANGE PAR '50 km 'RATTR SPACE, SLIDE PAR '10 km' sAttr SPACE] L'opération de fenêtrage spatiale est effectuée par l'opération Stream à Relation GAMME PAR. . . PAR SLIDE. La fonction sens de cette opération est la suivante. MS 2R ~ Stream_car [RANGE PAR '50 km de RATTR ESPACE, SLIDE PAR '10 km sAttr ESPACE »] = λStream_car.λt. {(Tp, g, t '): (tp, g, t') ∈ Stream_car ∧ g ≤ Dhigh ∧ g ≥ Dlow}, où Dhigh = bdt / 10 KMSC × 10 kms, dt = Σti = 1 (gi - gi-1), et Dlow = max {Dhigh - 50 kms, 0} les tuples de la relation sont mises à jour en fonction du paramètre DISTANCE PAR et SLIDE BY. Chaque fois que la voiture se déplace surveillé 10 km avant, le résultat de sortie est une réévaluation et que les tuples ayant un g de timbre spatial situé à l'intérieur des 50 derniers kms sera conservée dans la fenêtre. La figure 4 illustre le fonctionnement de la vitre mobile dans cet exemple. La voiture se déplace le long d'une surveillée route. Au T1 temps, la voiture a terminé les 50 premiers kms et les tuples reçus dans le flux jusqu'à T1 composent la première fenêtre. Au moment T2 instant, la fenêtre de requête se déplace à 10 km de l'avant, et seuls les tuples correspondant aux 50 derniers kilomètres sont conservés dans la fenêtre. Notez que le temps nécessaire par la voiture pour traverser le « slide par » la distance est pas constante: T3 - T2 mon être pas égal à T2 - T1. De manière similaire à la requête Q1, la relation de parenté à l'opérateur AvgSpd (R) et la relation à opérateur flux RStream sont utilisés ici pour calculer la vitesse moyenne et à délivrer en sortie le résultat dans un format de flux. Leurs fonctions sens peuvent être déduites facilement de l'exemple de Q1. Sémantique de fenêtre spatiale 4 Conlusion Le but de cette étude était de définir la sémantique et la syntaxe du langage général pour une opération de fenêtrage spatial sur les flux de données. opération de fenêtrage spatial est utile pour l'interrogation de flux de données spatio-temporelles. Basé sur le CQL de langage de requête continue, nous avons proposé une syntaxe pour exprimer les fenêtres spatiales et défini la sémantique de cette opération. c'est la principale contribution de cet article. Les prochaines étapes seront la mise en œuvre de l'opération fenêtrage présentée dans cet article, l'analyse de la performance sur une application réelle, et l'analyse des cas de fenêtrage spatiales plus complexes. Références Abdessalem, T., R. Chiky, G. Hébrail, et J. L. Vitti (2007). Traitement de Données de consom- mation électrique par un Système de gestion de flux de Données. Dans M. Noirhomme-Fraiture et G. Venturini (Eds.), Actes des Cinquièmes journées Extraction et Gestion des Connais- sances (EGC'07), Namur, Belgique, 23-26 janvier 2007, Volume RNTI-E-9 de la Revue des Nouvelles Technologies de l'information, pp. 521-532. Cépaduès-EDITIONS. Arasu, A., S. Babu, et J. Widom (2006). Le langage de requête continue CQL: fondements sémantiques et exécution de la requête. VLDB Journal 15 (2), 121-142. Arasu, A. et J. Widom (2004). Une sémantique dénotationnelle pour les requêtes continues sur les flux et les relations. SIGMOD fiche 33 (3), 6-11. Chandrasekaran, S., O. Cooper, A. Deshpande, M. J. Franklin, J. M. Hellerstein, W. Hong, S. Krishnamurthy, S. Madden, V. Raman, F. Reiss, et M. A. Shah (2003). TelegraphCQ: traitement continu de flux de données pour un monde incertain. Lors de la conférence sur les systèmes de données de recherche innovante (CIDR'03). Elmongui, H. G., M. Ouzzani, et W. G. Aref (2006). Défis dans l'optimisation des requêtes de flux. Spatiotemporelle Dans PK Chrysanthis, CS Jensen, V. Kumar, A. et Labrinidis (Eds.), Cinquième ACM Atelier international sur l'ingénierie de données pour l'accès sans fil et mobiles (MobiDE'2006), le 25 Juin 2006, Chicago, IL, USA, Proceedings , pp. 27-34. Li, J., D. Maier, K. Tufte, V. Papadimos, et P. A. Tucker (2005). Sémantique et les techniques d'évaluation pour les agrégats de fenêtre dans des flux de données. Dans F. Özcan (Ed.), Actes de la SIGMOD Conférence internationale sur la gestion des données, Baltimore, Maryland, USA, 14-16 Juin 2005, pp. 311-322. Maier, Li, Tucker, Tufte, et Papadimos (2005). Sémantique de flux de données et les opérateurs. Dans la 10e Conférence internationale sur la théorie de base de données (ICDT'05). Mokbel, M. F. et G. W. Aref (2005). GPAC: traitement générique et progressive des requêtes mobiles sur les données mobiles. Dans P. K. Chrysanthis et G. Samaras (Eds.), 6e Confé- rence internationale sur la gestion des données mobiles (MDM'2005), Ayia Napa, Chypre, 9-13 mai 2005, p. 155-163. Mokbel, M. F., X. Xiong, M. A. Hammad, et W. G. Aref (2005). le traitement des requêtes en continu des flux de données spatio-temporelles en place. GeoInformatica 9 (4), 343-365. Moreira, J., C. Ribeiro, et T. Abdessalem (2000). opérations requête pour déplacer des objets systèmes de base Data-. En K.-J. Li, K. Makki, N. Pissinou, et S. Ravada (Eds.), Actes du Yi YU et al. Huitième Symposium ACM sur les progrès dans les systèmes d'information géographique (ACM-GIS'00) 10-11 Novembre 2000, Washington D.C., aux États-Unis, pp. 108-114. Patroumpas, K. et T. K. Sellis (2004). Gestion des trajectoires d'objets mobiles comme des flux de données. Dans J. Sander et M. A. Nascimento (Eds.), 2e atelier international sur la gestion des bases de données spatio-temporelle (STDBM'04), Toronto, Canada, le 30 Août 2004, p. 41-48. Patroumpas, K. et T. K. Sellis (2006). Spécification de la fenêtre sur les flux de données. Dans T. Grust, H. Höpfner, A. Illarramendi, S. Jablonski, M. Mesiti, S. Müller, P.-L. Patranjan, K.-U. Sat- tler, M. Spiliopoulou, et J. Wijsen (Eds.), Ateliers EDBT, Volume 4254 de Lecture Notes in Computer Science, pp. 445-464. Springer. Résumé Dsms (flux de données des systèmes de gestion), les données d'entrée est infinie et les requêtes sur elle sont actifs tout le temps. Afin de faire face à ces caractéristiques, la fenêtre en fonction temps est largement utilisé pour convertir flux de données infini dans les relations bornées. Mais, cette technique ne suffit pas pour de nombreuses applications émergentes, en particulier pour les applications de service de localisation. De nombreuses requêtes ne peuvent pas être traitées par les fenêtres en fonction temps ou peut-être plus efficace en utilisant les fenêtres spatiales (fenêtres spatiales). Dans cet article, nous analysons la nécessité d'une fenêtrage spatiale sur les flux de données spatio-temporelles et, sur la base de CQL de langue DSMS (Requête continue Langue), nous vous proposons une syntaxe et la sémantique des fenêtres spatiales."
876,Revue des Nouvelles Technologies de l'Information,EGC ,2008,Structure Inference of Bayesian Networks from Data: A New Approach Based on Generalized Conditional Entropy,"We propose a novel algorithm for extracting the structure of a Bayesian network from a dataset. Our approach is based on generalized conditional entropies, a parametric family of entropies that extends the usual Shannon conditional entropy. Our results indicate that with an appropriate choice of a generalized conditional entropy we obtain Bayesian networks that have superior scores compared to similar structures obtained by classical inference methods.","Dan A. Simovici, Saaid Baraty",http://editions-rnti.fr/render_pdf.php?p1&p=1000621,http://editions-rnti.fr/render_pdf.php?p=1000621,en,"bay14R.dvi Structure Inference de réseaux bayésiens de données: une nouvelle approche basée sur Entropie Généralisée conditionnelle Dan A. Simovici *, Saaid Baraty * * Univ. du Massachusetts Boston, Massachusetts 02125, Etats-Unis {DSim, sbaraty} @ cs.umb.edu Résumé. Nous vous proposons un nouvel algorithme pour extraire la structure d'un réseau bayésien à partir d'un ensemble de données. Notre approche est basée sur entropies conditionnelles généralisées, une famille paramétrique de entropies qui prolonge l'habituel Shannon entropie conditionnelle. Nos résultats indiquent que, avec un choix approprié d'une général- isé conditionnelle entropie on obtient des réseaux bayésiens qui ont des scores supérieurs par rapport à des structures similaires obtenues par des méthodes classiques d'inférence. 1 Introduction Une structure de réseau de croyance bayésien (BBN) est un graphique acyclique orienté, qui représente les dépendances probabilistes entre un ensemble de variables aléatoires. Une structure BBN induire pour l'ensemble des attributs d'un ensemble de données est un problème bien connu et un défi en raison de l'énormité de l'espace de recherche. Le nombre de structures de BBN possibles croît de façon exponentielle super par rapport au nombre des nœuds. Dans Cooper et Herskovits (1993), où l'algorithme heuristique K2 est introduit, une me- sûr de la qualité de la structure est dérivée en fonction de sa probabilité postérieure en présence d'un ensemble de données. Une autre approche pour calculer une structure BBN est basée sur le principe des- cription minimum Longueur (LDM) introduit en Rissanen (1978). Les algorithmes de Lam et Bacchus (1994) et Suzuki (1999) proviennent de ce principe. Nous vous proposons une nouvelle approche pour induire des structures BBN des ensembles de données basés sur la notion d'entropie β-généralisée (β-GE) et son entropie conditionnelle β-généralisée correspondant (β- GCE) introduit dans Havrda et Charvat (1967) et axiomatisée dans Simovici et Jaroszewicz (2002) comme une famille à un paramètre de fonctions définies sur les partitions (ou la probabilité de des distributions). La flexibilité qui en découle nous permet de générer RDBA avec de meilleurs scores que les résultats publiés. Un avantage important de notre approche est que, contrairement à Cooper et Herskovits (1993), il ne repose sur aucune hypothèse distributive pour le développement de la formule. 2 Generalized Entropy et structure Inference L'ensemble des partitions d'un ensemble S est notée par la partie (S). La trace d'une partition π sur un sous-ensemble T de S est la partition πT = {T ∩ Bi | i ∈ I et T ∩ Bi 6 = ∅} de T. L'ordre habituel entre les partitions ensemble est désigné par « ≤ ». Il est bien connu que (PARTIE (S), ≤) est une inférence bayésienne délimitée de réseau des réseaux. La borne inférieure de deux partitions π et π '= {Bj | j ∈ J} sur S, notée avec π ∧ π', est la partition {Bi ∩ Bj | i ∈ I, j ∈ J, Bi ∩ Bj 6 = ∅} sur S. Le moindre élément de ce réseau est la partition aS = {{s} | s ∈ S}; la plus grande est la partition = {S co S}. La notion d'entropie généralisée ou β-entropie a été introduite dans Havrda et Charvat (1967) et axiomatisé pour les partitions en Simovici et Jaroszewicz (2002). Si S est un ensemble fini et π = {B1,. . . , Bm} est une partition de S, la β-entropie de π est le nombre Hp (π) = 1 à 21 janvier-β (1 - Σmi = 1 (| Bi | | S |) β) pour β> 1 . L'entropie de Shannon est obtenu sous forme d'une limβ → Hp (π). Pour β ≥ 1 la fonction Hp: PARTIE (S) - → R≥0 est anti-monotones. Ainsi, Hp (π) ≤ Hp (aS) = 1-nβ-1 (21-β-1) · nβ-1, où n = | S |. Laissez π, σ ∈ PARTIE (S) deux partitions, où π = {B1,. . . , Bm} et σ = {C1,. . . ,} Cn. L'entropie de β-conditionnelle de π et σ est Hp (π | σ) = Σn j = 1 (| Cj | | S |) β Hp (πCj). Il est immédiat que Hp (tc | co S) = Hp (π) et que H (tc | aS) = 0. En outre, dans Simovici et Jaroszewicz (2006), il est démontré que Hp (π | σ) = Hp (π ∧ σ) -Hβ (σ), une propriété qui étend la propriété similaire de l'entropie de Shannon. Lorsque β ≥ 1, le β-GCE est doublement anti-monotone par rapport à son premier argument et est monotone par rapport à son second argument. De plus, nous avons Hp (π | σ) ≤ Hp (π). Soit D un ensemble de données avec ensemble d'attributs attr (D). Le domaine d'attribut Ai ∈ Attr (D) est Dom (Ai). La projection d'un tuple t ∈ D sur X est la restriction t [X] de t à l'ensemble X. L'ensemble des attributs X définit une partition πX sur D, qui regroupe les tuples qui ont les projections égales sur X. Soit A un attribut et laissez-X de parents pour A, où Dom (A) = {v1, v2, ...,} et Dom vl (X) = Π B∈X Dom (B) = {u1, u2, ..., um}. Définir pij = P (t [A] = vi | t [X] = uj). Nous avons 1 nβ-1 ≤ Σni = 1 p β ij ≤ 1 pour β ≥ 1. X est considéré comme un ensemble parent « bien » A si connaître la sa valeur nous permet de prédire la valeur de A avec une forte probabilité , qui est, si aj = Σni = 1 p β ij est proche de 1 pour chaque j où P (t [X] = uj) est suffisamment grande. De toute évidence, X est un parent « parfaite » si Σm j = 1 aj = m. Les captures β-GCE exactement cette mesure de la qualité de la hotte. Parent- En effet, supposons que πA = {Bi | 1 ≤ i ≤ n} et πX = {Cj | 1 ≤ j ≤ m}, où T ∈ Bi nous avons t [A] = vi, et de s Cj nous avons s [X] = uj. Ensuite, pij = P (t [A] = vi | t [X] = UJ) = P (t ∈ Bi | t ∈ Cj) = | Bi∩Cj | | Cj | , Ce qui implique Hp (πA | πX) = 1 à 21 jan-β Σm j = 1 P β (Cj) (1 - aj). Ainsi, ce qui réduit Hp (πA | πX) revient à réduire les valeurs de (1 - aj), autant que possible pour les j est où | Cj | est grand, qui est, P (Cj) = P (t [X] = uj) est non triviale. Nous nous référons à la quantité Hp (πA | πX) comme l'entropie du noeud A, en présence de X ensemble. Cependant, même si X = argminX (Hp (πA | πX)), la valeur du minimum lui-même peut être trop élevé pour assurer une bonne prévisibilité. Une alternative consiste à me- assurer la réduction de l'entropie du noeud A à la suite de la présence du groupe X comme Hp (π A | πX) Hp (πA). Depuis 0 ≤ Hp (πA | πX) ≤ Hp (πA), nous avons 0 ≤ Hp (π A | πX) Hp (πA) ≤ 1. Si X est un ensemble de parent parfait A, puis aj = 1 pour 1 ≤ j ≤ m, de sorte Hp (πA | πX) = 0. soit ∈ [0, 1] un numéro appelé seuil de prédiction. Nous considérons X en tant que parent -convient de A si Hp (π A | πX) Hp (πA) ≤. Pour éviter les cycles dans le réseau, nous partons d'une séquence d'attributs A1, A2, ..., Ap et nous cherchons l'ensemble des parents Ai dans l'ensemble Φ (Ai) = {A1,. . . , Ai-1}, une RNTI hypothèse fréquente - X - D. et S. A. Simovici Baraty figure. 1 - Visualisation de l'algorithme (voir Cooper et Herskovits (1993), Suzuki (1999)). De plus, nous avons établi un r lié au nombre maximum de parents. L'ensemble Φ (Ai) peut contenir plusieurs sous-ensembles qui sont -convient. Une solution possible est de choisir un ensemble parent -convient X ⊆ Φ (Ai) avec β-GCE minimum Hp (π A | πX). Par la propriété de β-monotonicity GCE par rapport au deuxième argument que nous avons Hp (πAi | πΦ (Ai)) ≤ Hp (πAi | π {A1, A2, ... Ai-2}) ≤ · · · ≤ Hp ( πAi | π {A1}) ≤ Hp (πAi). Ensuite, pour une donnée, si X a le minimum Hp (πAi | πX) parmi tous les parents -Convient de Ai, alors X a la taille maximale possible. Pour simplifier la structure, nous échangeons une certaine prévisibilité pour la simplicité en adoptant une approche heuristique qui trouve un ensemble minimal de parents pour un noeud avec la plus grande réduction possible de l'entropie de ce nœud enfant sur sa présence. Définir Θl (Ai) = {X ⊆ Φ (Ai) | X est un parent de -convient Ai et | X | = L} et μ = min {n ∈ N | Θn (Ai) 6 = ∅}. Lorsque μ ≤ r, on a la séquence des collections non vides de jeux d'attributs Θμ (Ai), Θ μ + 1 (Ai), ..., Θ r (Ai) par la propriété de monotonicité de β-GCE . Laissez X` = argminX∈Θ `(Ai) (Hp (π Ai | πX)) le premier jeu de taille` (dans l'ordre lexicographique) qui réduit Hp (πAi | πX). Nous limitons notre recherche parent à la séquence des ensembles s = (Xμ, Xμ + 1,..., Xr), où les ensembles sont énumérés dans l'ordre croissant de taille. Pour la séquence S = (... Xμ, Xμ + 1,, Xr) défini ci-dessus, nous avons Hp (πAi | πμ) ≥ Hp (πAi | πXμ + 1) ≥ · · · ≥ Hp (πAi | πXr). L'ensemble de points {(0, Hp (πAi))} ∪ {(p, Hp (πAi | πXp)) | μ ≤ p ≤ r} dans R 2 peut être placé sur une courbe non croissante avec une hauteur h = Hp (πAi) - Hp (πAi | πXr), comme illustré sur la figure 1. On initialise le parent courant réglé Xu à ∅ et iterate membres plus de S dans l'ordre croissant de leur taille. Les membres Xv ∈ S conduit à une amélioration non négligeable dans la prévisibilité sur Xu si Hp (π Ai | πXu) -Hβ (π Ai | πXv) Hp (πAi) -Hβ (πAi | πXr) ≥ v-u r. Thi s se produit si la diminution de Hp (πAi | πX`) lorsque l'ensemble mère de Ai est changé de Xu à Xv est supérieure ou égale à décroissance linéaire par rapport aux deux points d'extrémité de la courbe non croissante correspondant comme représenté sur la Figure 1. les points d'extrémité de la courbe sont (0, Hp (πAi)) et (r, Hp (πAi | πXr)) et la diminution linéaire par rapport aux deux points d'extrémité de la courbe lorsque l'on passe de u à v de est h · axe x qui correspondent à des ensembles parent Xu et Xv (v-u) r = (Hp (π Ai) -Hβ (π Ai | πXr)) · (v-u) r. Notez que v = u + w 1 ≤ w ≤ r - u. Cela donne à penser que nous ne cessons pas le processus si Xu + 1 ne satisfait pas l'inégalité ci-dessus car il peut y avoir un ensemble parent Xv ∈ S où v> u + 1 avec une amélioration non négligeable en termes de prévisibilité par rapport au jeu de parent actuel Xu. RNTI - X - inférence bayésienne de réseaux Algorithme 1: Entrée BuildBayesNet: Jeu de données D, β réel,, r // ∈ [0, 1] est le seuil de prédiction. // β ≥ 1 est le paramètre β-entropie. // r est le nombre maximum des parents. // Attr (D) est une liste d'attributs de D où si // 1 ≤ i <j ≤ | Attr (D) | le ième élément de la liste peut // être un parent de l'élément jème, mais pas vice versa. Sortie: une structure de réseau D NetworkStructure N pour i ← | Attr (D) | pour faire 1 nœud Ai ← Attr (D) [i]; Nombre entier de 0 ←, m ← min (r, i- 1) Vrai H [m + 1] Set S [m + 1] H [0] ← Hp (π Ai) pour j ← m à 1 ne calculent Θj (Ai) si Θj (Ai) = ∅ alors briser le reste S [j] ← argminx∈Θ j (Ai) (Hp (π Ai | πx)) H [j] ← Hp (π Ai | πS [j ]) ← μ j N.addNode (Ai) si μ 6 = 0, alors u Entier ← 0 pour v ← μ m à faire si H [u] -H [v] v-u ≥ H [0] -H [m ] m puis u ← v forall x ∈ S [u] ne N.addEdge (x → Ai) revenir N; // fin de l'algorithme L'augmentation de la taille de l'ensemble des parents est pénalisé en rendant la stricte condition pour les jeux de parents plus grands. En outre, si aucun des ensembles de parents en S de la taille μ à r - 1 satisfont l'inégalité, puis sera Xr. 3 Résultats expérimentaux Nous avons comparé les résultats obtenus avec des structures bayésienne bien connues dans la littérature en utilisant deux systèmes de notation, LDM utilisé par Lam et Bacchus (1994) et Suzuki (1999) et la méthode de notation de Cooper et Herskovits (1993). Les expériences impliquées l'ensemble de données sur les tumeurs cérébrales (Cooper (1984)), le cancer du sein (Blake et al (1998a).), ALARME (Beinlich et al. (1989)), et IRIS (Blake et al. (1998b)). Les résultats expérimentaux sont présentés dans le tableau 1. La dernière ligne de chaque table contient les deux scores pour les structures publiées (selon Williams et Williamson (2006) et Beinlich et al. (1989)). Nous partons du principe que la distribution sur prieurs des structures pour un ensemble de données est uniforme et Cooper Herskovits (1993). Des expériences ont été réalisées sur une machine avec un processeur Intel Xeon 64 bits. Les scores pour les structures de réseau générées dépend de β et dans de nombreux cas est meilleur que les scores des structures établies (partitions C-H sont des scores plus élevés et MDL sont plus bas). La figure 2 représente quatre structures différentes pour le cerveau jeu de données de la tumeur. Structure A est l'une RNTI - X - D. A. et S. Simovici Baraty TAB. 1 - Résultats expérimentaux générés Structures 10000 lignes de la log r (CH Score) MDL Note Temps (ms) 1,0 1,0 3 -7483 13631,52 57 1,0 0,8 2 -7506 13474,37 51 1,6 0,7 2 -7588 13680,31 45 2,1 0,5 3 -7588 13693,21 55 Structure d'origine -8115 14410,10 - Structures Création 286 lignes log ß r (CH Score) MDL Score Temps (ms) 1,1 0,5 2 1,0 144 -1.172 3210,22 0,6 3 -1197 8640,41 202 1,7 0,3 2 1,8 121 -1.207 3669,88 0,7 3 -1214 3859,67 196 1,0 0,5 3 -1215 3511,35 202 1,2 0,4 2 -1224 4968,50 133 1,0 0,7 3 -1256 13667,40 202 Structure originale -1201 4142,03 - cancer du cerveau Résultats Résultats du cancer du sein Generated Structures 20002 lignes de log de r (CH Score) Mdl Score Temps (s) 1,2 0,5 3 542 1,2 -114931 270298,25 0,5 4 -114981 271590,92 12801 1,2 0,6 4 -116.081 12.802 272665,06 1,1 0,7 3 546 -116914 271469,89 Structure d'origine -159306 378.518,37 - Structures générés 150 lignes de log r (CH de Score) MDL Temps Score (ms) 1,0 0,4 2 109 1,8 -902 127543,87 0,7 3 -905 173 13279,40 Structure originale -932 261481,02 - A Larm Résultats Résultats Iris introduits par G. F. Cooper. Structures B (β = 1,0, α = 1,0, r = 3), C (β = 1,0, α = 0,8, r = 2) et D (β = 2,1, α = 0,5, r = 3) sont ceux générés par notre approche. 4 Conclusions Nous avons développé une approche pour générer une structure de réseau bayésien à partir des données sur la base notion d'entropie généralisée. Les meilleures relations parent-enfant entre les attributs sont obtenus à des valeurs de β qui dépendent fortement de l'ensemble de données, ce qui suggère que l'approche de GCE est préférable d'utiliser l'entropie de Shannon. Références Beinlich, I., H. Suermondt, R. Chavez, et G. F. Cooper (1989). Réglementation générale du système de surveillance d'alarme: Une étude de cas avec deux techniques d'inférence probabiliste pour les réseaux de croyance. Rapport technique KSL-88-84, l'Université de Stanford, laboratoire Knowledge System. Blake, C., D. Newman, S. Hettich, et C. Merz (1998a). dépôt UCI de la machine LEARN bases de données ing. Un ensemble de données de l'Institut de Ljubljana Oncology fourni par l'UCI, disponible à http://www.ics.uci.edu/ mlearn / MLRepository.html. Blake, C., D. Newman, S. Hettich, et C. Merz (1998b). dépôt UCI des bases de données d'apprentissage de la machine. Un ensemble de données créé par R.A. Fisher et offert par Michael Marshall, disponible à http://www.ics.uci.edu/ mlearn / MLRepository.html. Cooper, G. F. (1984). NESTOR: Une aide de diagnostic médical informatisé qui intègre les connaissances décontractée et probabiliste. Ph. D. thèse, Université de Stanford. Cooper, G. F. et E. Herskovits (1993). Procédé bayésien pour l'induction de réseaux de données probabilistes. Rapport technique KSL-91-02, l'Université de Stanford, laboratoire Knowledge System. Havrda, J. H. et F. Charvat (1967). Les méthodes de quantification des processus de classification: con- cepts de l'entropie de l'α structurel. Kybernetica 3, 30-35. RNTI - X - Inférence de Bayesian Networks figure. 2 - Structures de tumeurs cérébrales Lam, W. et F. Bacchus (1994). Apprentissage des réseaux bayésiens: une approche fondée sur le principe de LDM. Computational Intelligence 10, 269-293. Rissanen, J. (1978). Modélisation par la plus courte description des données. Automatica 14, 456-471. Simovici, D. A. et S. Jaroszewicz (2002). Un axiomatization de l'entropie de la partition. transac- tions sur 48 Théorie de l'information, 2138-2142. Simovici, D. A. et S. Jaroszewicz (2006). Un nouveau critère de division métrique pour les arbres de décision. International Journal of Parallel, Emergent et Systèmes Distribués 21, 239-256. Suzuki, J. (1999). Apprentissage des réseaux bayésiens basés sur le principe de: MDL Un algorithme efficace en utilisant la branche et de la technique liée. IEICE Trans. Systèmes d'information et, 356-367. Williams, M. et J. Williamson (2006). La combinaison de l'argumentation et les filets bayésiens pour le pronostic du cancer du sein. Journal de logique, du langage et de l'information 15, 155-178. Nous proposons un résumé nouvel Algorithme la structure de verser d'Extraire un réseau bayésien d'un ensemble de Données. Notre approach is sur les entropies basée conditionnelles généralisées, Une famille d'entropies Qui conditionnelle etend l'entropique de Shannon.Nos RE- conditionnelle sultats indiquent Que, with un choix d'Une entropique approprié conditionnelle généralisée, nous obtenons des Bayésiens Qui réseaux Ontario obte- Nues aux structures de scores de par des des methods d'inférence classiques. RNTI - X -"
920,Revue des Nouvelles Technologies de l'Information,EGC,2007,Clustering : from model-based approaches to heuristic algorithms,"Les méthodes du 'clustering' ont pour but de diviser un ensemble (large) d'objets dans un petit nombre de groupes homogènes (clusters), basé sur des données relevées ou observées qui décrivent les (dis-)similarités qui existent entre les objets – en espérant que ces clusters soient utiles pour l'application concernée. Il existe une multitude d'approches, et cette contribution présente quelques-unes qui sont les plus importantes ou actuelles.",Hans-Hermann Bock,http://editions-rnti.fr/render_pdf.php?p1&p=1001287,http://editions-rnti.fr/render_pdf.php?p=1001287,en,"Microsoft Word - 0D-HBock.rtf Clustering: des approches basées sur des modèles à des algorithmes heuristiques Hans Hermann Bock 1 Institut de statistique, Université RWTH Aachen, D-52056 Aachen, Allemagne, bock@stochastik.rwth-aachen.de Résumé. Les methods du 'regroupement', mais de verser en Ontario ensemble de un Diviser (grand) d'un petit objets Dans Nombre de Homogènes (clusters Groupes), basons sur des Données relevées ous les personnes observées Qui décrivent (dis-) Qui existe Entre similarités les objets - en esperant Que bureaux grappes l'répandrai Soient Utiles appli- cation concernée. Il exists Une multitude d'approaches, et contribution this Pré--Sente Quelques lignes Qui sont les plus de Actuelles OU IMPORTANTES. Les approaches Qui sont sur un modèle basées (cluster à base de modèles) Une vue d'partent Dans probabiliste il exists Une Laquelle classification et les inconnue des Données Variables Aléatoires are DonT la répartition de la Dépend Classe des objets correspondants. NOUS présenterons les Modèles '-cloison fixe', 'partition aléatoire' et le modèle de mélange. Each Mène à un critere de clas- sification à Optimiseur. Nous esquissons des algorithmes, des mathé- matiques Propriétés, et Quelques CAS spéciaux, Mais Importants. Il is facile de des critères heuristiques définir de classification des Dans where il N'y CAS a pas un modèle probabiliste, et TANDIS Que les methods Précédentes se concentrent sur des classifications de type 'partition', sur des may also construct classifications hiérarchiques OU structurées . - aux methods Qui Contrairement Une classification exhaustive construisent l'ensemble verser au total de tous les objets DONNÉS, nous considérerons le FINALEMENT sur where se CAS Contente à des Pressothérapie only (Quelques) et Isolés Groupes d'singuliers objets Qui sont bien, plus Entre Eux similaires moyenne Qu'en. CÉS methods à la base de Sont de Beau- coup d'applications en des fouille (marketing Données, biotechnologie, journaux Web). 1 Etudes de mathématiques en 1958-1965 à Karlsruhe, Paris, Fribourg (Diplome); positions universitaires aux Universités de Freiburg, Hanovre, et Aachen (Aix-la-Chapelle); Profes- seur en Probabilité et à Aachen Statistique 1978 DEPUIS; : Analyse des Spécialités Données, le regroupement et la classification, reliability; de la Société président de classification Allemande (GFKL; 1986-1995), président de la Fédération internationale des sociétés de classification (IFCS; 1985-1987); Editeur de la revue 'Les progrès dans l'analyse des données et la classification (ADAC)' et de la série 'La classification, l'analyse des données et de l'Organisation de la connaissance' (Sprin- ger Verlag)."
940,Revue des Nouvelles Technologies de l'Information,EGC,2007,Finding interesting queries in relational databases,"La découverte de motifs dans des bases de données relationnelles quelconques est un problème intéressant pour lequel il existe très peu de méthodes efficaces. Nous présentons un cadre dans lequel des paires de requêtes sur les données sont utilisées comme des motifs et nous discutons du problème de la découverte d'associations utiles entre elles. Plus spécifiquement, nous considérons des petites sous-classes de requêtes conjonctives qui nous permettent de découvrir des motifs intéressants de manière efficace.",Bart Goethals,http://editions-rnti.fr/render_pdf.php?p1&p=1001285,http://editions-rnti.fr/render_pdf.php?p=1001285,en,"Microsoft Word - 0C-BGoethals.rtf Trouver des requêtes intéressantes dans les bases de données relationnelles Bart Goethals 1 Université d'Anvers, Département de mathématiques et d'informatique Middelheimlaan 1, B-2020 Anvers, Belgique Résumé. La découverte de motifs Dans des Bases de données relationnelles is a quelconques intéréssant verser Problème il exists très Lequel de ME- thodes peu Efficaces. Nous Présentons un cadre Dans des paires de Lequel requests sur les Données des Sont utilisées Comme motifs et nous Discutons du de la Découverte Problème d'associations Entre ELLES Utiles. De plus spécifiquement, nous des petites sous considérons classes de requests conjonctives Qui nous tente de découvrir permet- des motifs interessants de Manière Efficace. Sommaire. modèles dans des bases de données relationnelles arbitraires reste un problème intéressant pour lequel il existe des techniques que très peu efficaces. Nous présentons un cadre dans lequel les paires de requêtes sur les données sont utilisées comme modèles et discuter du problème de trouver des associations intéressantes entre eux. Plus précisément, nous considérons les petites sous-classes de requêtes conjonctives qui permettent encore nous de trouver des modèles intéressants efficacement. 1 Bart Goethals a obtenu son doctorat sur Motif fréquent minier de l'Université transnationale du Limbourg, après quoi il a déménagé à l'Université d'Helsinki depuis deux ans. Son principal intérêt de recherche sur des méthodes efficaces pour l'extraction du modèle et l'intégration des données dans les systèmes de minières base de données. Il est l'organisateur de plusieurs ateliers parmi lesquels les FIMI ateliers sur de fréquentes mises en œuvre minières itemset. Il est membre du comité de rédaction du Data Mining et Knowledge Discovery Journal, il a siégé à presque tous les comités de programme de conférence minière de données et est responsable du programme de ECML / PKDD en 2008. Actuellement, il est chercheur post-doctoral dans la base de données avancée groupe de recherche de recherche et de modélisation (adrem) à l'Université d'Anvers dont il dirige l'équipe Data Mining."
944,Revue des Nouvelles Technologies de l'Information,EGC,2007,Interestingness in Data Mining,"Interestingness measures play an important role in data mining regardless of the kind of patterns being mined. These measures are intended for selecting and ranking patterns according to their potential interest to the user. Good measures also allow the time and space cost of the mining process to be reduced. Measuring the interestingness of discovered patterns is an active and important area of data mining research. Although much work has been conducted in this area, so far there is no widespread agreement on a formal definition of interestingness in this context. Based on the diversity of definitions presented to date, interestingness is perhaps best treated as a broad concept, which emphasizes conciseness, coverage, reliability, peculiarity, diversity, novelty, surprisingness, utility, and actionability. This presentation reviews interestingness measures for rules and summaries, classifies them from several perspectives, compares their properties, identifies their roles in the data mining process, gives strategies for selecting appropriate measures for applications, and identifies opportunities for future research in this area.",Howard J. Hamilton,http://editions-rnti.fr/render_pdf.php?p1&p=1001283,http://editions-rnti.fr/render_pdf.php?p=1001283,en,"Microsoft Word - 0B-HHamilton.rtf Interestingness dans le secteur minier de données Howard Hamilton 1 Department of Computer Science, Université de Regina, 3737 Wascana Parkway, Regina, SK, Canada S4S 0A2 Résumé. mesures d'intérêt jouent un rôle important dans les données minières quel que soit le type de motifs étant minées. Ces mesures sont destinées à la sélection et le classement des modèles en fonction de leur intérêt potentiel pour l'utilisateur. De bonnes mesures permettent également le temps et le coût espace du processus d'extraction à réduire. Mesure de la interestingness des modèles découverts est une zone active et importante de la recherche sur l'exploration de données. Bien que beaucoup de travail a été mené dans ce domaine, jusqu'à présent il n'y a pas un large consensus sur une définition formelle de interestingness dans ce contexte. Sur la base de la diversité des définitions présentées à ce jour, interestingness est peut-être mieux traité comme un concept large, qui met l'accent sur la concision, la couverture, la fiabilité, particularité, la diversité, la nouveauté, surprisingness, l'utilité et actionability. Cette présentation des mesures de critiques pour les règles et les résumés, les classifie de plusieurs points de vue, compare leurs propriétés, identifie leur rôle dans le processus d'extraction de données, donne des stratégies de sélection des mesures appropriées pour les applications et identifie les opportunités pour la recherche future dans ce domaine. 1 Howard J. Hamilton est professeur au Département des sciences informatiques à l'Université de Regina, Regina, au Canada, où il a servi depuis 1991. Il a obtenu son baccalauréat ès sciences et d'une maîtrise en science informatique de l'Université de la Saskatchewan, et son doctorat en sciences informatiques de l'Université Simon Fraser. Il est le directeur de l'Institut pour la découverte informatique de l'Université de Regina. Ses recherches portent notamment sur la découverte de connaissances dans les bases de données, l'apprentissage de la machine, l'application de l'intelligence artificielle à l'animation par ordinateur et les jeux informatiques, et la représentation temporelle et le raisonnement. Il est co-auteur de _Knowledge Découverte et mesures de Interest_ et co-rédacteur en chef de quatre autres livres, y compris les mesures de _quality pour les données Mining_, Springer, 2007. Certaines questions étudiées dans le souci de découverte de connaissances interestingness, particularité, les mesures basées sur l'utilité, la généralisation de domaine graphiques, jeux d'éléments à base d'actions et la visualisation des résultats."
955,Revue des Nouvelles Technologies de l'Information,EGC,2007,Optimal histogram representation of large data sets: Fisher vs piecewise linear approximation,"Histogram representation of a large set of data is a good way to summarize and visualize data and is frequently performed in order to optimize query estimation in DBMS. In this paper, we show the performance and the properties of two strategies for an optimal construction of histograms on a single real valued descriptor on the base of a prior choice of the number of buckets. The first one is based on the Fisher algorithm, while the second one is based on a geometrical procedure for the interpolation of the empirical distribution function by a piecewise linear function. The goodness of fit is computed using the Wasserstein metric between distributions. We compare the proposed method performances against some existing ones on artificial and real datasets.","Antonio Irpino, Elvira Romano",http://editions-rnti.fr/render_pdf.php?p1&p=1001314,http://editions-rnti.fr/render_pdf.php?p=1001314,en,"Microsoft Word - IRP_ROM_EGC_07_FINAL.doc optimale représentation de l'histogramme des grands ensembles de données: Fisher vs linéaire par morceaux approximation. Antonio Irpino *, Elvira Romano ** * Dipartimento di studi europei e mediterranei Université de Naples II Via del Setificio, 15 Complesso Monumentale Belvedere - San Leucio I-81020 Caserta (CE) irpino@unina.it ** Dipartimento di Matematica e STATISTICA Universita degli Studi di Napoli ""Federico II"" Via Cintia - Complesso Monte Sant'Angelo I-80126 Napoli elvrom@unina.it~~V~~plural~~3rd Résumé. représentation Histogramme d'un grand ensemble de données est une bonne façon de résumer et de visualiser des données et est souvent effectuée afin d'optimiser l'estimation de la requête dans SGBD. Dans cet article, nous montrons les performances et les propriétés des deux stratégies pour une construction optimale de sur un histogrammes réel sin- gle descripteur d'une valeur sur la base d'un choix préalable du nombre de ets sarrasin. La première est basée sur l'algorithme de Fisher, tandis que le second est basé sur une procédure géométrique pour l'interpolation de la fonction de distri- bution empirique par une fonction linéaire par morceaux. La qualité de l'ajustement est calculé en utilisant la métrique Wasserstein entre les distributions. Nous comparons les performances de la méthode proposée par contre certains jeux de données sur celles qui existent déjà artificiels et réels. 1 Introduction mécanisme de l'information de stockage Aujourd'hui, ne parvient pas à capturer une grande quantité de données et processus pré-les dans leur intégralité, alors que seulement un résumé est stocké. Dans cet histogramme contexte joue le rôle d'un outil pour produire une description récapitulant appropriée et répondre rapidement aux demandes d'aide à la décision. À la suite de l'expression de guidage « Une image dit plus d'un Hon- mots Dred », l'histogramme représente un outil simple et graphique intuitive pour décrire la distribution des données. Il aplanit les données à afficher la forme générale d'une distribution empirique. Le problème est qu'il peut donner une fausse impression de la forme de la distribution de jeux de données, car sa construction dépend du choix du nombre et la longueur des sous-intervalles - seaux usu- allié appelés ou bacs - des lignes réelles sur lesquelles la histogramme est basé. il pourrait idéalement avoir la situation dans laquelle des grands bacs de la nature de l'ensemble de données est bimodale et pour les petits bacs le terrain réduit à la représentation unimodale. La question en jeu ici concerne le genre de largeur de bac qui peut prendre en compte la meilleure représentation graphique du SGBD sous-jacent et comment il peut être construit avec une approximation d'erreur minimale. représentation de l'histogramme optimal des grands ensembles de données dans la communauté de base de données, et en particulier dans le cadre de l'optimisation des requêtes, la recherche d'un bon histogramme pour la représentation d'un grand ensemble de données est mieux connu comme le problème « d'estimation de sélectivité ». Les estimations peuvent être utilisées pour sélectionner le meilleur plan parmi beaucoup les concurrents. Il existe deux grandes classes de méthodes d'estimation de sélectivité: méthodes et méthodes d'échantillonnage Tical statis-; dans ce document, le deuxième type (méthodes statistiques non paramétriques) est prise en compte. Dans Sec. 2 quelques-unes des méthodes d'histogramme sont brièvement passés en revue, tandis qu'une excellente taxonomie des histogrammes se trouve dans Poosala et al. (1996). De nombreux ensembles de données ont des attributs continus valeur tels que des ensembles de données scientifiques et statistiques. Les domaines état de l'art Histogrammes traite implicitement avec la valeur d'attribut discrètes ou catégoriques dans lesquels il y a relativement peu de valeurs distinctes dans l'attribut, ces méthodes sont utilisées pour estimer rejoindre sélectivités aussi (voir Ioannidis et Poosala (1995)). En l'absence de nombreuses valeurs en double dans de nombreux ensembles de données scientifiques et statistiques, une équi-jointure entraînera efficacement dans l'ensemble vide, à l'origine de ces méthodes sont inefficaces. A partir de ce point, notre approche tente de saisir les caractéristiques variables statistiques, afin que nous puissions considérer un bas modèle statistique approche ed, étant donné que notre objectif est de rapprocher (en fonction) la fonction cumulative par polynôme par morceaux (modèle géométrique). Les méthodes proposées tentent de résoudre le calcul de l'histogramme en présence d'ensembles de données OUs presque continuités selon deux approches différentes: la première est basée sur l'algorithme de Fisher pour la partition de données ordonnées, celle-ci est basée sur la meilleure interpolation de la fonction de distribution des données. La sensibilité de l'algorithme de remplacement proposée est étudiée en utilisant plusieurs ensemble de données et de la qualité de l'approximation est calculée en proposant une qualité de mesure en forme sur la base de la métrique L2 Wasserstein entre deux fonctions de distribution. Une application sur un artificiel et sur deux ensemble de données réel est effectuée afin de confirmer notre procédure. 2 Touches de bienséances histogramme et une petite revue des techniques de Isting ex Examinons la définition de l'histogramme. Définition 1 Un histogramme sur une variable X est réalisé en divisant la distribution de données en sous-ensembles appelés seaux et rapprocher les fréquences f et les valeurs dans chaque godet d'une façon commune (Ioannidis, 1993). Dans cette définition, il n'est pas mentionné comment dessiner des classes spécifiques d'histogramme et qui sont les principaux aspects à prendre en compte pour sa construction. Il y a six principaux aspects à considérer dans la construction de l'histogramme: Règle de partition, l'algorithme de construction, fréquence d'approximation, approximation Valeur, erreur Guarantes (Ioannidis, 1995). Dans la première proposition se rapproche de la largeur des casiers sont également espacés et les propositions ont été essentiellement fondé sur le choix du nombre de bacs (Wand, 1997). Néanmoins, ces méthodes ont l'inconvénient de perdre les détails de la partition à haute densité de données. Au cours des dernières années, plusieurs types d'histogramme ont été proposées pour résoudre ce problème. Dans tous la ligne directrice commune est de trouver le meilleur emplacement des points de coupe, en plus du nombre de bind A. Irpino et E.Romano RNTI - X - largeur estimation fonction de densité (Kooi, 1980). Ce problème a reçu une attention non seulement dans les statistiques et la communauté de base de données, mais aussi dans l'analyse numérique, où la densité est approchée par une classe de polynôme de piecewise un certain degré fixe. Les régimes communs pour la construction dans histogrammes SGBD diffèrent en termes de contraintes de accu- racé de partition. La première proposition remonte à la thèse de doctorat de Kooi. Il présente un concept couramment utilisé dans la littérature statistique, la forme la plus simple de l'histogramme, dans laquelle le jeu de valeurs est divisée en intervalles de longueur égale, que l'on appelle histogramme équi-largeur. Dans ues Val- et des fréquences particulières au sein de chaque godet, sont approchées par la hauteur du godet. Comment- jamais, histogrammes equi-largeur avaient pas une bonne amélioration par rapport à la répartition uniforme AS- pour l'ensemble consommation de valeur, qui est la raison pour laquelle de nouvelles propositions ont été faites. Le soi-disant hauteur équi- ou histogrammes équi-profondeur (Piatetski-Shapiro, 1984) est l'un de ceux-ci. il con- siste en particulier de diviser l'ensemble de l'attribut dans des seaux qui ont comme approximativement le même nombre de tuples. Après ces propositions l'attention a été déplacé à l'étude de la façon dont les erreurs d'approximation initiale est maintenue dans la base de données d'estimation par ces techniques. Les histogrammes V-optimales (Ioannidis, 1995) ont été proposées pour minimiser l'erreur quadratique moyenne pour problème d'estimation de la sélectivité. Dans cette technique, la partition de la distribution des données est calculée de telle sorte que la variance d'une valeur-paramètre de la source à l'intérieur de chaque godet est minimisé. Outre les contraintes de séparation V-optimale, les méthodes d'autres, comme ce dernier, ont été mis au point ayant pour but principal de regrouper rapidement plusieurs valeurs de paramètres source dans le même seau. Parmi eux, on peut distinguer MaxDiff (Ioannidis Y., 1993), qui place les limites du godet entre la source paramètre adjacent. En plus de ces solutions pour les contraintes de séparation, numérique Solu tion pour capturer la distribution de forme n'a reçu que peu d'attention. Parmi eux, il a été proposé de trouver splines linéaires pour chaque bac par un moindre carré pro- blème de régression (Konig, 1999), mais pas beaucoup d'attention a été consacrée au nombre de paramètres à estimer et au coût de la construction efficace. Sur la base de la règle de la partition, histogrammes peuvent être classés en fonction de leurs propriétés mutuellement (orthogonalement Poosala et al., 1997). Le tableau 1 résume et en même temps décrit comment peut être colloqué les méthodes existantes. Dans ce cadre notre méthode a lieu dans le contexte des méthodes qui utilisent les valeurs de variable observée et la fréquence cumulée relative. SOURCE SORT PARAMETER paremeter propagation (S) Fréquence (F) zone (A) cumulation. Fréq. (C) Valeurs (V) Valeurs (V) Equi-Somme Equi-Somme V-Max-Optimal .diff comprimé V-Max-Optimal .diff comprimé à base Spline V-optimale Piecewise Fisher fréquence (F) V-OPTIMALE MaxDiff Zone (A) MaxDiff TAB. 1. - Carte des principales approches de la construction de l'histogramme. Les algorithmes proposés dans le présent document sont soulignés. représentation d'histogramme optimal de grands ensembles de données 3 Les techiques proposées Soit X une variable numérique dont le domaine est constitué de V ordonné de valeurs. Soit (x1, x2, ..., xn) une liste d'observation N (tuples) pour la variable X, tandis que (v1, v2, ..., vV) est l'ensemble de valeurs distinctes assumée par X dans l'ensemble de données. La fonction de masse empirique de X est définie par: {}: 1, / ij si J.-J. N xv N = # ≤ ≤ = On décrit la fonction de répartition empirique de X: 1 iijjcf = = Σ De la même manière, nous définissons la masse d'un intervalle]] (), ab ⊆ -∞ + ∞ comme:]] () {}, 1, / si abjj N axb N = # ≤ ≤ <≤ Si le domaine est divisé en seaux bêta, en supposant la distribution uniforme dans les ets sarrasin, on calcule la densité empirique de la j-ième (1 j β≤ ≤) seau comme: () () (), jjjjj jd bbfbbbb⎤ ⎤ ⎤ ⎤ = -⎦ ⎦ ⎦ ⎦. La densité peut être affichée par un histogramme, qui est un graphique à barres dans lequel la proportion dans la liste sont représentées par les zones de différentes barres. 3.1 L'algorithme Fisher L'algorithme Fisher peut être considéré comme un algorithme V-optimale (V, V). En effet, la fonction Ject ob- qui est réduite au minimum est la somme de la variance intra-godets. V-Optimal algo- rithme, étant fixé un certain nombre de godets ß diviser le domaine de valeurs, optimise une fonc- tion du paramètre de source selon la formule suivante: 1 min () HHHW VAR X = Σ β où x est la source paramètre et w est un poids du paramètre source. Si le paramètre source est le domaine des valeurs et w = 1, il correspond à la minimisation de la variance dans les seaux et conduit à la mise en œuvre de l'algorithme dynamique de partition en raison de Fisher (1958) pour les données commandées. On définit les quantités suivantes: () 1 1 2 1 1 1: [,]: [,] (1) ([,]) ([,]) où ([,]) i V i VV ii VV iiivvvivvv VAR k VAR vvfv AVG vv AVG vvfv ∈ ∈ = = = - = Σ Σ la vi borne supérieure de la nouvelle kième godet est choisie en fonction de la formulation dynamique suivante: () () () () 1 1 min () | 1 et,,, i H ijjjiijvh Arg VAR h VAR jjkvbb VAR VAR bv vb - = ⎧ ⎫⎡ ⎤ ⎡ ⎤ ⎡ ⎤- ∈ - ∈ + + ⎨ ⎬⎣ ⎦⎣ ⎦ ⎣ ⎦⎩ ⎭ Σ où 1 H k 1j β ≤ ≤ - ≤ ≤ et, j jb b⎡ ⎤⎣ ⎦ est la notation d'intervalle de la j-ième seau. Le coût de calcul est en termes d'opérations dans le pire des cas est égal à O (V2β). A. Irpino et E.Romano RNTI - X - 3,2 interpolation par morceaux de la fonction de distribution empirique Le procédé commence à partir de l'histogramme trivial -one seau histogramme ou l'uniforme ap- proximation- et à chaque pas la limite du nouveau godet est choisi sur la base de cette valeur pour laquelle on observe la distance maximale L2 entre la valeur prédite et la valeur observée. La distance c un être non pondérée (dans la version standard de l'algorithme), ou pondérée par le nombre d'observations dans les seaux. Dans le second cas, si les deux valeurs ont la même erreur, il est choisi de la valeur de la plus godet peuplée, en fonction de la motivation pratique qu'il est préférable de supprimer une erreur d'un seau qui se rapprochent d'un grand ensemble d'observation à partir d'un plus petit. Nous commençons compte tenu de l'histogramme trivial tel que: 0 (,) VTriv U vv~ où v0 est un point d'artificiel ajouté à l'ensemble de données de telle sorte que [] () 0 1 1, fvvf = Afin d'identifier le meilleur point de coupe appartenant à la vis dans des seaux k nous résolvons l'algorithme sui- vantes à chaque étape afin de trouver β points de coupure. Pour l'algorithme standard (PWST) et pour la version pondérée (PWW), nous avons: () {} () {} 2 2 * (PWST) ou () * (PWW) iiiiiivv Argmax vv Argmax fjvv- - où f (j ) est la fréquence de la j-ième seau qui comprend vi et où * iv est calculée au moyen de la fonction de quantile: () *, pour 1, ..., 1 () jjijjiijjbbvbcbcvbbjk fj - ⎡ ⎤⎡ ⎤ = + - ↔ ∈ = -⎣ ⎦ ⎣ ⎦ le coût de calcul en termes d'exploitation est, dans le pire des cas, égale à O (vp). 4 La représentation de mesure de la qualité Dans le paragraphe suivant, nous présentons une façon plus cohérente pour calculer le carré d'erreur moyenne de l'histogramme obtenu et la distribution des données selon une métrique entre la distribution. Nous développons une mesure de prise de précision en considération la somme des différences entre les carrés prédite et la valeur observée, compte tenu du (émis l'hypothèse) la nature Con- tinu du modèle par rapport aux valeurs observées discrètes. Lorsque nous utilisons une fonction continue avec un histogramme (à savoir un mélange de ß uniformes avec des supports non chevauchants) pour interpoler une fonction continue droite discrète nous mit toujours une ad- d'estimation d'erreur. Étant donné un vecteur [v1, .., vi, ..., vV] des valeurs avec la fonction de masse égale à fi, le meilleur histogramme est constitué de godets V, et peut être représentée par une fonction linéaire par morceaux, où la pièce linéaire général a i iv bornes F (()) i iv F v et 1 1 (, ()) + v +. L'histogramme est la meilleure dans le sens d'interpolation linéaire par morceaux. Notre proposition est d'évaluer la précision de la procédure au moyen d'un calcul de distance entre le modèle obtenu du mélange d'uniformes et le meilleur histogramme. représentation de l'histogramme optimal des grands ensembles de données Nous proposons d'utiliser la distance L2 Wasserstein faire la comparaison (Gibbs et su, 2002, Barrio et al., 1999, et Verde et Irpino, 2006). Il peut être considéré comme le prolongement naturel de la distance euclidienne à partir des données de point aux données de distribution, et il a des propriétés intéressantes de decompo- sition. Etant donné deux fonctions de distribution F et G, la distance L2 Wassertein peuvent être calculés selon la formule suivante: () 1 22 1 1 0 () () Wd F t G t dt - - = -∫ où F-1 et G- 1 sont les fonctions de quantiles des deux distributions. La mise à distance Compu est lourd lorsque la distribution est continue, mais Verde et Irpino (2006) est montré sa faisabilité lorsqu'ils traitent avec des histogrammes. En annexe, nous montrons que la distance proposée peut se décomposer comme la somme de la différence des carrés moyens, la différence carré des écarts-types et une partie uel resid- qui peut être pris comme une distance de forme entre les deux distributions. La décompositions tion est résumée comme suit: () () 2 22 2 (1 (,)) W fgfgfg QQ ShapeLocation Taille DCORR F Gμ μ σ σ σ σ = - + - + - Nous considérons comme une erreur maximale autorisée par interpolation des données avec l'histogramme de la distance L2 Wasserstein entre l'histogramme trivial (l'histogramme permettant à un seul godet) et celui optimal, on peut obtenir une qualité relative de l'indice en forme que le rapport entre la distance quadratique du modèle obtenu (M *) et la valeur optimale histogramme (Opt) et la distance quadratique entre le modèle trivial (Tri) et la valeur optimale. Nous appelons cette mesure comme SGFR (S Quare de qualité de l'ajustement Ratio): 2 2 (*,) (*) (,) WW d M Opt SGFR M d Tri Opt = Utilisation de la décomposition de la distance au carré, on peut évaluer la « qualité » de ness Good- de ajuster compte tenu de la quantité de la distance est influencée par un emplacement, une taille ou une différence de forme. 5 application sur un ensemble de données réelles et artificielle Nous testons les techniques proposées sur trois données. Le premier est un ensemble de données artificiel qui dérive de la génération aléatoire de 10.000 valeurs. Il dérive d'un mélange de trois répartition mal nor- f (x) = 0.33N (20,20) + 0.33N (40,10) + 0.34N (70,25). Le deuxième ensemble est constitué des 10.000 observations des dst_bytes variables de la database1 Cup KDD 99. Cet ensemble de données a été choisi pour sa caractéristique d'être un exemple d'une distribution de peaky (discontinue). Le troisième ensemble recueille la première 10,000 observation de la variable d'élévation à partir du type de couverture forestière database2. Cet ensemble de données a été choisi pour être un exemple d'une distribution lisse (continue). 1 2 http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html http://kdd.ics.uci.edu/databases/covertype/covertype.html A. Irpino et E.Romano RNTI - X -. Les valeurs Artificial dataset = 10.000 Obs = 10,000 Godets algorithme mesure 10 25 50 100 200 Temps en sec. 0,55955 0,604214 0,566546 0,58457 0,56958 MD SSE 65,58197 69,49141 75,58185 82,9155 89,14396 SGFR 1,033287 1,06364 1,109271 1,1618 1,20468 Temps en sec. 40,14913 52,06039 65,02176 82,95111 111,3314 Vopt SSE 7,71032 2,902464 0,339665 0,13697 0,02490 SGFR 0,354292 0,217371 0,074347 0,04719 0,02008 Temps en sec. 19,07574 33,5456 55,99695 97,6234 173,8116 FISHER SSE 8,211255 5,167489 0,955339 0,00252 0,00097 0,36562 SGFR 0,290044 0,124707 0,00633 0,00386 Temps en sec. 0.240339 0,23878 0,315012 0,52743 1,11027 PWST SSE 0,512781 0,048029 0,004592 0,00196 0,00052 SGFR 0,091357 0,027947 0,00862 0,00563 0,00289 Temps en sec. 0,19953 0,23803 0,31237 0,52245 1,500763 PWW SSE 0,02326 0,00285 0,00100 0,51278 0,00036 0,01942 0,00676 SGFR 0,09135 0,00396 0,00234 TAB. 2 - synoptiques des performances des cinq algorithmes en utilisant le premier ensemble de données: MD (MaxDiff (F, F)), Vopt (V-optimale (V, F), Fisher, PWST (pièce approximation rationnelle de la fonction de distribution, non pondéré) , PWW (Piece approximation Wise à la tion fonc- de distribution, pondérée par la fréquence du godet). les méthodes de comparaison en gras les meilleurs résultats sont montré. utilisés ici sont les MaxDiff (F, F), le V-optimale (V, F ) l'algorithme Fisher, la norme et la version pondérée de l'algorithme d'interpolation cumulée par morceaux. Nous avons mesuré le temps de fonctionnement en utilisant MATLABTM sur un PC (CPU Intel Cen- Trino 1,77Mhz, RAM 1024 Mo). la précision est calculée en utilisant le classique fonction d'erreur et la nouvelle mesure de précision sur la base L2 Wasserstein métrique entre les distributions. les principaux résultats sont recueillies dans les tableaux 2 à 5. Bien que l'algorithme MaxDiff (F, F) ont les meilleures performances en termes de temps passé pour l'estimation de l'histogramme il est moins précis lorsque le nombre de valeurs de t il domaine de la variable est très grande. Le algo- rithme est piecewise toujours le meilleur en termes de temps CPU et la précision. Pour illustrer l'exacti- tude améliorée du projet approche Figure 1 montre les principaux résultats pour β = 10 du artificielle Dataset. En ce qui concerne la qualité de qualité de l'ajustement (Tab. 5) plus les seaux le meilleur de tous les algorithmes (sauf pour le MaxDiff et le Voptimal (V, F) pour l'ensemble de données KDD99 biaisé) correspondent aux deux premiers moments. algorithme Fisher et les ont ANCE de mieux définies par intervalles perfor- sur les autres. En comparant la qualité de qualité de l'ajustement entre Fisher et les algorithmes de piecewise (Tab. 5), la dernière semble être plus précis dans l'estimation des deux premiers moments où le nombre de seaux augmente. Nous supposons que cela est dû au fait que les algorithmes Fisher, étant basés sur une variance c RITÈRE, permet de regrouper les données en classes sphériques, alors que les méthodes de sont basées sur morceaux le meilleur ajustement linéaire de la fonction de distribution, mettant l'accent sur implicitement la densité uniforme locale des données. représentation de l'histogramme optimal des grands ensembles de données cov Forest. Les valeurs du jeu de données = 360 obs. = 10,000 Godets algorithme mesure 10 25 50 100 200 Temps en sec. 0,00147 0,00144 0,00150 0,00152 0,00165 MD SSE 25,28642 19,70946 0,22061 0,07711 0,00861 0,14530 0,01547 SGFR 0,16422 0,00912 0,00281 Temps en sec. 0,24917 0,44977 0,98323 0,11676 1,37233 Vopt SSE 0,07816 0,07039 0,05140 0,85188 0,02625 0,03025 0,00849 0,00801 SGFR 0,00689 0,00481 Temps en sec. 15,11523 30,80159 51,00386 92,46458 165,41046 FISHER SSE 0,22606 0,08548 0,04502 1,91894 0,02175 0,04506 0,01500 0,00875 SGFR 0,00603 0,00368 Temps en sec. 0,24187 0,32029 0,53857 0,19329 1,47643 0,44978 0,08213 PWST SSE 0,03813 0,01217 0,00153 0,02136 0,00902 0,00596 SGFR 0,00323 0,00105 Temps en sec. 0,24211 0,50761 0,55726 0,19092 1,20335 PWW SSE 0,09306 0,04329 0,01098 0,44978 0,00153 0,00959 0,00628 SGFR 0,02136 0,00306 0,00106 TAB. 3 - synoptiques des performances des cinq algorithmes à l'aide du jeu de données de type de couverture Forest: MD (MaxDiff (F, F)), Vopt (V-optimale (V, F), Fisher, PWST (Piece approximation rationnelle de la fonction de distribution, non pondérée), PWW (Piece approximation rationnelle de la fonction de distribution, pondérée par la fréquence du godet). en gras les meilleurs résultats sont montré. KDD99 dataset Valeurs = 2096 obs. = 10.000 Godets algorithme mesures 10 25 50 100 200 Temps en sec. 0,03753 0,07955 0,02525 0,02647 0,02837 MD SSE 1.2998E + 10 + 09 1.2516E 1.2516E + 09 + 08 2.6199E 1.5893E + 08 SGFR 0,24139 0,24139 0,10965 0,78485 0,08537 Temps sec. 3,63551 6,94830 11,83113 18,52914 33,68945 Vopt SSE 8.3791E + 09 5.7991E + 09 5.7991E + 09 + 09 1.7037E 1.5893E + 08 SGFR 0,52340 0,52340 0,28197 0,62976 0,08537 Temps sec. 29,57838 73,82461 130,39616 232,45977 309,87441 FISHER SSE 1.1452E + 09 + 07 8.0053E 4.6098E + 04 9.8253E + 03 + 03 1.8049E SGFR 0,23301 0,06158 0,00154 0,00068 0,00027 Temps sec. 0,19037 0,24140 0,34018 0 0,56922 1,34964 PWST SSE 2.0296E + 06 + 05 3.0148E 1.9798E + 04 4.5786E + 03 + 02 4.9934E SGFR 0,00965 0,00370 0,00098 0,00046 0,00015 Temps en sec. 0,24546 0,33983 0,56360 0,19179 1,16070 PWW SSE 2.4614E + 05 3.8040E + 04 8.3669E + 03 + 03 1.8198E 221,99776 SGFR 0,00128 0,00052 0,00026 0,00342 0,00009 TAB. 4 - synoptiques des performances des cinq algorithmes utilisant l'ensemble de données KDD 99: MD (MaxDiff (F, F)), Vopt (V-optimale (V, F), Fisher, PWST (Piece approximation rationnelle de la fonction de distribution, non pondéré .), PWW (Piece approximation rationnelle de la fonc- tion de distribution, pondérée par la fréquence du godet) en gras les meilleurs résultats sont montré A. Irpino et E.Romano RNTI - mesures β = 10 Forêt artificielle Kdd 99 Alg -. X. β = β = 200 10 = 200 β β = β = 10 200 d2 (M, Opt) 65,58186 89,14386 25,23811 0,00737 1.296E + 10 1.533E + 08% μ MD,% σ, s% 41,5% 32,5% 26,0% 71,5% 10,5% 18,0% 1,3% 0,8% 97,9% 1,6% 0,2% 98,2% 47,2% 47,0% 5,7% 1,3% 76,2% 22,5% d2 (M, Opt) 7,71016 0,02479 0,85628 0,02167 8.342E + 09 1.533E + 08% μ Vopt, σ% s% 1,1% 27,0% 71,8% 0,0% 2,0% 98,0% 0,1% 1,4% 98,5% 0,2% 0,1% 99,7% 31,8%. 61,5% 6,8% 1,3% 76,2% 22,5% d2 (M, Opt) 8,21110 0,00092 1,90010 0,01265 1.142E + 09 1.565E + 03% μ FISHER,% σ, s% 2,7% 4,7% 92,6% 12,0% 0,8% 87,2% 0,6% 0,3% 99,1% 0,4% 0,0% 99,6% 75,1% 10,6% 14,3% 5,2% 0,9% 93,8% T d2 (M, Opt) 0,51266 0,00051 0,42695 0,00103 1,960 E + 06 5.026E + 02% μ PWST,% σ, s% 1,8% 15,5% 82,7% 0,3% 1,2% 98,4% 14,5% 6,4% 79,1% 0,8% 0,0% 99,1% 38,7% 1,1% 60,2% 3,0% 0,3% 96,7% d2 (M, Opt) 0,51266 0,00034 0,42695 0,00105 2.465E + 05 1.830E + 02% PWW μ, σ%, s% 1,8% 15,5% 82,7% 0,2% 1,0% 98,8% 14,5% 6,4% 79,1% 0,8% 0,2 % 99,0% 5,8% 10,9% 83,3% 0,7% 0,0% 99,2% TAB. 5 - Synopse s: la qualité de la bonté adéquation entre le modèle et le meilleur histogramme, AC- Cording à la décomposition proposée de la distance au carré Wasserstein. FIGUE. 1 - Histogramme de représentation pour l'ensemble de données artificielle et l'illustration de l'approximation de la fonction de répartition empirique pour les différentes méthodes. 6 Conclusions et perspectives Dans le présent document, plusieurs algorithmes bien établis pour la construction de histo- grammes de données ont montré à l'échec de la précision lorsque les données contenues dans la base de données sont quasi-continue, par exemple lorsque les valeurs assumées par le domaine de la une variable ne sont pas si peu MD Vopt Fisher PWW optimale représentation de l'histogramme des grands ensembles de données par rapport aux tuples stockés. Les techniques proposées semble plus capable de faire face à ce problème. En outre, compte tenu de la qualité de l'ajustement au meilleur modèle, la décomposition de la L2 Wasserstein métrique permet de découvrir la qualité de l'approximation d'un histogramme des données, ce qui explique la distance en termes de qualité de l'ajustement des deux premiers instants et la une partie de la distance en raison de seulement un facteur de forme. Dans le présent document la construction de l'histogramme multivariée n'a pas été considéré comme ce sera notre prochaine étape, naturellement par rapport aux techniques existantes qui semble souffrir le problème « malédiction de la dimensionnalité ». A Deeper besoins de perspicacité à donner afin de tester les techniques proposées dans un cadre de flux de données pour l'étude de leurs propriétés dans le cas des fenêtres en mouvement ou dans le histogrammes cas des mises à jour continues des modèles d'histogramme. Références Barrio, E., Matran, C., Rodriguez Rodriguez, J. et Cuesta-Albertos, J.A. (1999). Tests de qualité de l'ajustement en fonction de la distance L2-Wasserstein. Annales de la statistique (1999), 27, 1230-1239. Fisher, W. D., (1958). Le regroupement pour l'homogénéité maximale. Jorn. American Stat. Ass., 53, 789-798. Gibbs, et Su A.L., F.E. (2002). Le choix et de délimitation des mesures de probabilité, Inter- national Statistical Review, 70, 419. Ioannidis, Y., P. V. (mai 1995). Équilibre entre optimalité de l'histogramme et l'aspect pratique pour la requête estimation de la taille du résultat. Proc. de ACMSIGMOD, 233-244. Ioannidis, Y. (1993). Universalité de série Histogrammes. Actes de VLDB, terre Dublin IRE, pages 256-277. Ioannidis, Y. et Poosala, V. (1995). Équilibre entre optimalité de l'histogramme et l'aspect pratique pour la requête estimation de la taille du résultat. Dans SIGMOD, 233-244, San Jose, CA. Konig, A., W. G. (1999). d'ajustement de courbe paramétrique pour la mation de requête entraînée par rétroaction résultat dimension. VLDB Conf., 423-434. Kooi R. (1980). L'optimisation des requêtes dans les bases de données relationnelles. Thèse de doctorat, Université Case Western Reserve Piatetski-Shapiro, G., (1984). Une estimation précise du nombre de tuples satisfaisant une condi- tion. Proc. de SIGMOD, 256-276 Poosala V., Ganti V., Ioannidis Y.E. (1999): Question approximative Réponse à l'aide de grammes histopathologie. Les données IEEE Eng. Taureau. 22 (4): 5-14. Poosala, Y., Ioannidis, Y., Haas, et Shekita P.J., E.J. (1996). histogrammes améliorés pour l'estimation de la sélectivité des prédicats de gamme. Dans SIGMOD, 294-305, Montréal, Canada. A. Irpino et E.Romano RNTI - X - Vert, R., Irpino, A. (2006). Une nouvelle distance sur la base de la Wasserstein ing cluster- hiérarchique de l'histogramme des données symboliques. Données Science et classification (Eds. Batanjeli, Bock, Ferligoj, Žiberna), Springer, Berlin, p. 185-192. Baguette, M. P. (1997). choix de la largeur de bac d'histogramme à base de données. Annexe Preuve de la décomposition de la distance Wasserstein. () () () 1 22 1 1 0 2 2 (() ()): () () 2 (1 (() ())) W ijijijijij QQ ShapeLocation Taille d F x F x F t F t dt Corr Y i Y jμ μ σ σ σ σ - - = - = = - + - + - ∫ (1) observons deux fonctions de densité fi (x) et fj (x) ayant les deux premiers moments finis. Pour chaque fonction de densité peut être associée à la distribution f onctions Fi (x) et Fj (x), les moyens de μi et uJ, les écarts-types σi et σj où: 1 1 0 () () ii ix fx dx F t dtμ + ∞ - = -∞ = ⋅ ∫ ∫ ( 2) En effet () () xf dx x xdf x + ∞ + ∞ -∞ -∞ = ∫ ∫ si t = F (x) et en considérant que 1 1 (()) () x FF F x t- - = = par substitution, on obtient 1 1 0 () F t dtμ - = ∫ (3) où: () 1 22 2 2 1 2 0 xxfx dx F t dtσ μ () () () () () μ + ∞ - -∞ = - = -∫ ∫ (4) pour les mêmes substitutions ci-dessus adoptées maintenant supposons pour centrer les deux distributions en utilisant leurs moyens tels que: 1 1 1 c ci iii iz x F F tz t () () () F tμ μ - - - = - = = - (5) Barrio et al. (1999) est prouvé que () () () () () () () 22 2,:, c cW ijij W i jd F x F xd F x F xμ μ = - + (6) où () ( ) () () 1 22 1 1 0,: () () ccc cW iji jd F x F x F t F t dt - - = -∫ (7) développement du carré, nous obtenons une représentation d'histogramme optimal de grands ensembles de données () () () () () () () 1 1 1 2 22 1 1 1 1 0 0 0 1 1 1 2 21 1 1 1 0 0 0 1 2 2 1 1 0 () () () (), : () () 2 () () () () 2 () () 2 () () cccccc W ijijijiijjiijjijiijjd F x F x F t dt F t dt F t F t dt F t dt F t dt F t F t dt F t F t dt μ μ μ μ σ σ μ μ - - - - - - - - - - = + - = = - + - - - - = = + - - - ∫ ∫ ∫ ∫ ∫ ∫ ∫ (8) considérons la quantité () () () () () () () () 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 2 2 2 21 1 1 1 0 0 0 0 () () () () () () () () () () ccijiijjiijjijcc ijiijj F t F t dt F t F t dt F t F t dt QQ F t dt F t dt F t dt F t dt μ μ μ μ σ σ μ μ ρ - - - - - - - - - - - - - - - - ∫ ∫ ∫ = = = ∫ ∫ ∫ ∫ (9) on peut considérer que la corrélation des deux séries de données où chaque couple de observa- tions est représenté respectivement par la t-ième quantile de la première distribution, et la t-ième tuile quan- du second. Dans ce sens, on peut considérer comme la corrélation entre les fonc- tions quantile représentées par la courbe des points de quantile infinies dans une parcelle de QQ. Il convient de noter que 0 1QQρ <≤ différemment de la gamme classique de variation de l'indice de corrélation de Bravais-Pearson le (-1, + 1). L'équation (8) peut être réécrite sous la forme () () () () () 1 2 2 2 1 1 2 2 0, 2 () () 2c cW ijijiijjij QQ i jd F x F x F t F t dtσ σ μ μ σ σ ρ σ σ - - = + - - - + = -∫ (10) Addition et soustraction de 2 i σ jσ on obtient () () () () () 22 2 2, 2 2 2 2 1c cW ijijijij QQ ijijij QQD F x F x σ σ σ σ σ σ ρ σ σ σ σ σ σ ρ = + - + - = - + - (11) On peut remplacer ce résultat en (6) à obtenir: () () () () () () () () () () 2 2 22 2,:, 2 1c cW ijij W ijijijij QQD F x F xd F x F xμ μ μ μ σ σ σ σ ρ = - + = - + - + - QED La CV d'représentation d'un grand histogramme ensemble de l'Est Une bonne Données Manière et verser visualiseur des résumer et Données is d'fréquemment Optimiseur exécutée l'AFIN luation tion de requests Dans le Système de gestion de bases de données. En this article, nous Mon- trons les performances et les Deux Propriétés de construction juin verser stratégies des histogrammes sur optimale un à facts REELLES descripteur sur la base de d'un apriori du choix de Intervalles Élémentaires Nombre. Le premier sur l'EST basons de Fisher Algorithme, le second Alors Qué is sur un basons procédé Pour L'interpolation géométrique de la fonction de par pirique em- répartition par fonction Une morceaux linéaire. La qualité de l'calculee en is ajustement le Wasserstein Utilisant les distributions Entre métrique. Nous les comparons des methods exécutions contre proposées sur to vary Quelques des Celles ensembles de artificialisation Données et CIELS Réels."
967,Revue des Nouvelles Technologies de l'Information,EGC,2007,SyRQuS - Recherche par combinaison de graphes RDF,"Nous nous intéressons à un mécanisme permettant la construction de réponses combinés à partir de plusieurs graphes RDF. Nous imposons, par souci de cohérence, que cette combinaison soit réalisée uniquement si les graphes RDF ne se contredisent pas. Pour déterminer la non-contradiction entre deux graphes RDF nous utilisons une mesure de similarité, calculée au moment de l'ajout de documents RDF dans la base de documents.",Adrian Tanasescu,http://editions-rnti.fr/render_pdf.php?p1&p=1001347,http://editions-rnti.fr/render_pdf.php?p=1001347,en,"SyRQuS - Recherche par RDF Graphes de combinaison. Adrian Tanasescu * * Université Lyon 1, Villeurbanne, F-69622, France, LIRIS UMR CNRS 5205 43, Bat. Nautibus, 43 Bld. du 11 Novembre 1918, 69622 Villeurbanne atanases@liris.cnrs.fr, http://bat710.univ-lyon1.fr/ atanases / CV. Nous nous intéressons A un mechanism la construction de permettant Combinés à partir Réponses de RDF. Several Graphes Nous imposons, par souci de cohérence, Qué this combinaison si Soit les réalisée Graphes UNIQUEMENT RDF ne se pas contredisent. Pour la Déterminant non-contradiction Entre deux Graphes RDF nous utilisons Une de mesure similarité, calculee au moment où de l'ajout de documents RDF Dans la base de documents de. 1 La SyRQuS Même si plateforme ACDE fait several RDF Que Années became un norme W3C par recommandé, le Développement des langages de RDF was Requête en plus longue. Après l'apparition de RDF, des langages d'Acceder aux permettant triplets RDF Emerge Ontario, TRIPLE Comme (Sintek et al., 2002) ou Encore Ecraser (SquishQL, 2002). De bureaux premiers ministres d'Inspirés Sont Autre RQL Comme Langages, RDQL - LANGAGE D'origine de la Jena plateforme (Jen) - ou Encore SeRQL de la base de langage Sésame (Kampman et Broekstra). Boutique Tous les efforts de la SCÉ vers un de convergence AUJOURD'HUI SQL comme langage Qui est en formation de la future recommendation Devenir W3C: SPARQL (Seaborne et Prud'hommeaux, 2006). En de statut Déjà version candidat recommendation Dans le d'avril 2006, C'est petit à langage petit adopted par les vers le plateformes Web orienté sémantique RDF Utilisant. Pour nous this raison notre effort Avons orienté vers le Développement d'un outil per- mettant d'interprète les requests à l'aide formulées de CE langage. SyRQuS (RDF Syntetizing Système de recherche) was un de Dans Développé environnement PHP / MySQL d'I'assureur un AFIN déploie- ment aux rapports indépendant d'exploitation par Systèmes. Il utilise le parseur ARC RDF / XML d'Extraire les AFIN triplets des documents RDF et l'analyseur de RAP de requests SPARQL (RDF API pour PHP). Les de la Plateforme Fonctionnalités SyRQuS en soi Deux partis décomposent: 1. documents RDF Ajout de. Nouveaux This de la Réalise, d'une partie juin, l'insertion des documents RDF Dans la Base de Données et, d'Autre Part, la mise à jour de la matrice de verser document similarité nouveau each RDF added. 2. Interrogation de la Base de Données. Après la formulation d'un SPARQL en Requête, le moteur de recherche establish Suivantes ÉTAPES: (a) de la Décomposition et récupération des Requête triplets de la clause WHERE; SyRQuS - Recherche par RDF Graphes de combinaison (b) Pour each triplet de la Requête sur le document RDF les y cherche respondent et sur la matrice Construit documents AINSI mais; (C) de la Tri document mais matrice par croissant du ordre Nombre de mais par le document each répondus; (D) Affichage des Complètes ne necessitant Réponses pas de Graphes de combinaison; (E) pour document each Db Qui fournit juin et Réponse partielle Dans la limite du ATTENDUS Nombre de Réponses: - Construction de l'ensemble E de documents Qui ne pas Db et contredisent Qui aux sous-répondent requests Db ne répond auxquelles pas; - des triplés Ajout aux documents Réponses Appartenant aux triplets E Dans de Db Réponses, sous condition la d'équivalence des sujets des Premiers Ë Au MoiNs UNE des Ressource Derniers. (F) Présentation du Format Dans un Résultat HTML et un fichier XML 2 Conclusion et perspectives SyRQuS is demande à la Une Destinée recherche d'informations Dans les documents RDF. Son d'obtain Përmet Implémentation des Plus, de par Réponses la combinaison de several RDF Graphes des fournissant Réponses partielles. Developpee Dans un souci de com- patibilité Avec le langage de SPARQL Requête, le treatment Elle Përmet des requests de select type, type CE de being le seul Requête à notre de Pouvoir Profiter de recherche par algorithme de Graphes combinaison. ACTUELLEMENT, nou s Travaillons sur l'intégration des ontologies Dans les documents référencées RDF, Ce Qui nous permettra de proceder à des aproximations de au moment où de Réponses l'Interrogation de la Base de documents. HP Labs Références Web sémantique de recherche. Jena - Sémantique Web Framework pour Java. http: //jena.sourceforge.net/. Kampman, A. et J. Broekstra. Sésame - Open Source Base de données RDF. http: //www.openrdf.org/. Par voie maritime, A. et E. Prud'hommeaux (2006). langage de requête SPARQL pour RDF. candidat W3C mandation dation, W3C. http: //www.w3.org/TR/2006/WD-rdf-sparql-query-20061004/. Sintek, M., S. Decker, et A. Harth (2002). TRIPLER. http: //triple.semanticweb.org/. SquishQL (2002). Ecraser Query Language. http: //swordfish.rdfweb.org/rdfquery/ql. Résumé De nos jours, RDF est un standard recommandé par le W3C pour décrire les connaissances sur res- sources sur le Web. Dans ce contexte, nous explorons le champ de réponse de requête RDF et regarder de plus près la possibilité de combiner plusieurs graphes RDF tout en recherchant des réponses de la requête. Nous vous proposons un outil prototype qui génère des réponses complètes virtuelles en combinant partiellement répondant à des graphes RDF. La génération de ces réponses est effectuée dans une condition de non-contradiction calculée par une mesure de similarité entre le RDF Graphes être combinés."
1007,Revue des Nouvelles Technologies de l'Information,EGC,2006,Biclustering of Gene Expression Data Based on Local Nearness,"The analysis of gene expression data in DNA chips is an importanttool used in genomic research whose main objectives range from the study ofthe functionality of specific genes and their participation in biological processto the reconstruction of diseases's conditions and their subsequent prognosis.Gene expression data are arranged in matrices where each gene corresponds toone row and every column represents one specific experimental condition. Thebiclustering techniques have the purpose of finding subsets of genes that showsimilar activity patterns under a subset of conditions. Our approach consists ofa biclustering algorithm based on local nearness. The algorithm searches forbiclusters in a greedy fashion, starting with two–genes biclusters and includingas much as possible depending on a distance threshold which guarantees thesimilarity of gene behaviors.","Jesús S. Aguilar-Ruiz, Domingo S. Rodríguez, Dan A. Simovici",http://editions-rnti.fr/render_pdf.php?p1&p=1000427,http://editions-rnti.fr/render_pdf.php?p=1000427,en,"Biclustering d'expression génique à base de données sur Jésus S. Aguilar locale Nearness-Ruiz *, Domingo Savio Rodriguez * Dan A. Simovici ** * BIGS Bioinformatique Séville, Université de Séville, Espagne dsavio@lsi.us.es, ** Univ. du Massachusetts Boston, Massachusetts 02125, États-Unis dsim@cs.umb.edu~~V~~singular~~3rd Résumé. L'analyse des données d'expression des gènes dans l'ADN des puces est un outil important utilisé dans la recherche en génomique dont les principaux objectifs vont de l'étude de la fonctionnalité des gènes spécifiques et de leur participation dans le processus biologique à la reconstruction des conditions de maladies et leur pronostic ultérieur. des données d'expression génique sont disposées dans des matrices où chacun correspond à des gènes d'une rangée et chaque colonne représente une condition expérimentale spécifique. Les techniques biclustering ont pour but de trouver des sous-ensembles de gènes qui montrent les modèles d'activité similaires sous un sous-ensemble de conditions. Notre approche consiste d'un algorithme de biclustering basé sur la proximité locale. L'algorithme recherche pour biclusters d'une manière gourmande, en commençant par deux gènes biclusters et comprenant autant que possible en fonction d'un seuil de distance qui garantit la similitude des comportements de gènes. 1 Introduction La technologie de l'ADN microarray représente une excellente occasion d'étudier l'information génomique dans son ensemble, afin que nous puissions analyser les relations entre les milliers de gènes simul- tanément. Les expériences réalisées sur les gènes dans des conditions différentes produisent les niveaux d'expression de leur ARNm transcrit et ces informations sont stockées dans des puces à ADN. Un bicluster est un sous-ensemble de gènes qui montrent les modèles d'activité similaires sous un sous-ensemble de condi- tions. La recherche sur biclustering a commencé en 1972 avec les travaux de Hartigan, où la façon de diviser une matrice en sous-matrices avec la variance minimale a été étudiée (Hartigan et al., 1972). Dans cette approche, le bicluster parfait était la sous-matrice formée par des valeurs constantes, à savoir, avec une variance égale à zéro. L'algorithme de Hartigan, appelé cluster directe, divise la matrice de données dans un certain nombre de biclusters, avec la valeur de variance minimum, donc le fait de trouver un certain nombre de sous-matrices égal au nombre d'éléments de la matrice est évitée. Une autre façon de rechercher biclusters est de mesurer la cohérence entre leurs gènes et condi- tions. Cheng & Church (Cheng et al., 2000) a introduit une mesure, le résidu quadratique moyenne (MSR), qui calcule la similitude entre les valeurs d'expression au sein du bicluster. Les idées de Cheng et l'église ont été développées par Yang (Yang et al., 2002, 2003) qui a traité les valeurs manquantes dans les matrices. En conséquence de cette approche un algorithme nommé - 681 - RNTI-E-6 Basé sur biclustering Local a été conçu Nearness floc. D'autres travaux (par exemple (Wang et al., 2002)) reposent sur une valeur de qualité ainsi calculé à partir des valeurs d'expression de biclusters, afin de mesurer leur cohérence. D'autres alternatives à la recherche de biclusters ont été étudiés au cours des dernières années. On peut aussi considérer qu'une valeur dans la matrice de données est la somme des contributions des biclusters de dif- férents. Sur la base de l'idée précédente, Lazzeroni (Lazzeroni et al., 2000) présente les modèles carreaux, dans lequel la matrice de données est décrit comme une fonction linéaire de couches correspondant à ses biclusters et montre comment estimer un modèle en utilisant un procédé de maximisation itérative. Shamir (Shamir et al., 2002) propose une nouvelle méthode pour obtenir biclusters à partir d'une combinaison de modélisation graphique théorique et statistique des données. Dans ce modèle, un gène répond à une condition si son niveau d'expression change de manière significative à cet égard aux sorcières condition à son niveau mal nor. Dans un ouvrage récent (Liu et al., 2004), une généralisation du modèle OPSM, présenté par (Ben et al., 2002), est présenté. Le modèle de OPSM est basée sur la recherche des biclusters dans lequel un sous-ensemble de gènes induisent une commande similaire linéaire le long d'un sous-ensemble de conditions. Certaines des techniques de recherche pour struct spécifiques Ures dans la matrice de données pour trouver biclusters: crée une méthode pour les gènes et les conditions regroupement simultanément basées sur la recherche de modèles de « matrices » dans damiers de données d'expression génique (Gerstein et al., 2003). Auparavant, les données sont traitées par la normalisation dans un modèle de cadre spectral (plusieurs schémas tous construits autour de l'idée de mettre les gènes sur la même échelle afin qu'ils aient le même niveau moyen de expres- sion dans des conditions, et même pour les conditions) . techniques de calcul évolutionnistes ont également été utilisés dans ce domaine de recherche. Ces techniques utilisent les aspects de la sélection naturelle dans les sciences informatiques, y compris les algorithmes génétiques, programmation génétique et des stratégies évolutives. In (Aguilar et al., 2005) une technique évolutive, basée sur la recherche des biclusters suivant une stratégie de recouvrement séquentiel et en mesurant le résidu quadratique moyenne, est utilisé. Dans ce travail, nous proposons un algorithme pour obtenir biclusters de haute qualité basée sur ness quasi locale, à savoir, biclusters avec le nombre maximum de gènes et dans laquelle la valeur absolue de la différence entre les deux valeurs d'expression de toute paire de gènes sous la même condition ne dépasse pas une certaine valeur. Par conséquent, nous considérons que deux gènes dont la distance entre eux ENTRE est inférieur à un seuil par rapport à certaines conditions expérimentales, ont un comportement similaire en ce qui concerne ces conditions. Pour trouver le seuil de distance appropriée, nous avons réalisé une étude statistique préliminaire afin de permettre l'algorithme de découvrir biclusters de haute qualité. La qualité est en outre évalué au moyen du résidu quadratique moyenne, de sorte que d'une analyse comparative avec d'autres techniques est possible. Le document est organisé comme suit: dans la section 2, les définitions relatives à biclustering sont présentés; l'algorithme est représenté à la section 3; à la section 4, nous décrivons la méthode utilisée et illustrons par un exemple simple; résultats expérimentaux sont présentés à la section 5, en comparant la qualité de ceux générés par Cheng & Église et les algorithmes de Aguilar & Divina; enfin, les conclusions les plus intéressantes et les orientations de recherche futures sont résumées à la section 6. 2 Définitions Les données d'expression des gènes sont disposés dans des matrices. Une matrice est définie comme une triple M = (G, C, ℓ), où G, C sont deux ensembles finis dénommés l'ensemble des gènes et l'ensemble des conditions respectivement, et ℓ: G × C - → ℜ est la fonction de niveau. Le nombre réel de ℓ (g, c) est également désigné par <g, c> et représente le niveau d'expression du gène G dans la condition spécifique c. - 682 -RNTI-E-6 J. S. Aguilar-Ruiz et al. Ensuite, les principaux concepts de notre approche sont définis et des exemples pour les clarifier sont fournis. Définition 1 Soit M = (G, C, ℓ) une matrice formée par un ensemble de gènes, G, et un ensemble de conditions, C. Nous disons qu'une paire d'ensembles non vides (I, J) est un δ-bicluster, si je ⊆ G, J ⊆ C et J = {c ∈ C | ∀g, g '∈ I, | <G, c> - <g ', c> | ≤ δ} La valeur absolue de la différence entre deux valeurs d'expression de toute paire de gènes dans le bicluster sous la même condition ne soit pas supérieure à un seuil δ. Définition 2 Soit (I, J) un bicluster. Le résidu R d'un élément aij du bicluster (I, J) est R (aij) = aij - aij - aij - Aij où aij est la moyenne de la i-ième ligne dans le bicluster, Aij la moyenne de la jème colonne dans la bicluster et aij est la moyenne de tous les éléments dans le bicluster. Le résidu quadratique moyenne d'un MSR bicluster (I, J) est MSR (I, J) = 1 | I || J | Σ i ∈ I, j∈J R2 (aij) Le MSR est la variance de l'ensemble de toutes les valeurs dans la bicluster, ainsi que la variance de la ligne moyenne et la variance de la colonne moyenne. Cette valeur est indicative de la cohérence des valeurs entre les lignes et les colonnes. Plus le MSR est, plus la cohérence. Exemple 1 Considérons la matrice M = (G, C, ℓ), où G = {g1, g2, g3, g4, g5}, C = {c1, c2, c3, c4} et ℓ est défini par: (ℓ ( gi, cj)) =   9 3 2 1 7 8 1 4 1 6 2 2 5 5 1 7 2 1 3 1   Si le thresho ld δ est 5, certains des biclusters sont B1 = {{g1, g3, g4}, {c1, c3, c4}}, B2 = {{g1, g2, g3}, {c2, c3, c4}}, et B3 = {{g3, g4, g5}, {c1, c2, c3}}, où B1 (gi, cj) =   1 3 2 1 2 2 5 5 7   B2 (gi, cj) =   9 3 2 8 1 4 6 2 2   B3 (gi, cj) =   1 6 2 5 1 5 2 1 3   Dans tous les biclusters, la valeur absolue de la différence entre les valeurs d'expression de chaque paire de des gènes est inférieure ou égale au seuil δ = 5, dans les mêmes conditions. Notre objectif est d'obtenir biclusters avec le nombre maximum de gènes et conditions et avec la valeur minimale de MSR. - 683 - RNTI-E-6 Basé sur biclustering local 3 Nearness algorithme Notre approche, nommée BLN (biclustering par la section locale Proximités), est basée sur la proximité locale, à savoir, dans la définition de δ-bicluster fourni ci-dessus, Def. 1. Son objectif est d'obtenir différents biclusters avec le nombre maximum de gènes de sorte que tous ont les deux propriétés suivantes: • la différence entre les deux valeurs d'expression de toute paire de ces gènes dans les mêmes conditions dans le bicluster ne dépasse pas δ; • le nombre de conditions de chaque bicluster est pas inférieure à λ. L'algorithme illustré sur la figure 1, se compose de deux phases. Dans la première partie, nous obtenons un ensemble de biclusters valides avec seulement deux gènes (2g_Bicluster), de sorte que l'algorithme analyse toutes les paires possibles de gènes dans la matrice de données afin de les trouver, comme on peut le voir dans la figure 1 (ligne 9) . Cette première série de bicluster sera utilisé par l'algorithme dans la deuxième partie. Nous avons conçu une structure de données d'arbre spécial dans lequel ces biclusters sont stockés. Dans cet arbre, les nœuds représentent des conditions et les feuilles sont des groupes de gènes de 2g_Biclusters qui ont un groupe commun de conditions, à savoir., Pour les atteindre, nous devons suivre le même chemin dans l'arbre. La raison pour laquelle nous utilisons cette structure est de minimiser la quantité de mémoire utilisée pour le stockage de ces premiers biclusters (ils peuvent être plusieurs milliers) et de réduire le temps d'exécution. Le but de la seconde partie 1 Mode opératoire: BLN 2 Entrée: 3 M (matrice de données M = (G, C, ℓ)) 4 δ (différence maximale entre deux valeurs d'expression) 5 λ (longueur minimale de l'ensemble des conditions pour chaque bicluster) 6 sortie: 7 B (ensemble final de biclusters) 8 Méthode: 9 Initialize B pour contenir tous les biclusters (i, j) avec | I | = 2 et | J | ≥ λ 10 S = ∅ 11 répétition 12 B '= B - S 13 Useful_Genes = {g ∈ I | ∀ (I, J) ∈ B '} 14 S = B 15 forEach bicluster (I, J) ∈ B' 16 pour chaque g '∈ (Useful_Genes - I) 17 si pour tout g' '∈ I il y a un bicluster ({g ', g' '}, J') avec J J ⊂ '18 Add (I ∪ {g'}, J) B 19 jusqu'à | B | = | S | 20 fin BLN Fig. 1 - L'algorithme de BLN. est de créer de nouveaux biclusters contenant plus de deux gènes. Cette partie est itérative et suit une méthodologie avide d'élaguer l'espace de recherche. A chaque itération un groupe de nouveaux biclusters est - 684 -RNTI-E-6 J. S. Aguilar-Ruiz et al. établi. Les extrémités de processus lorsque aucun nouveau bicluster est obtenu (ligne 19, le nombre de biclusters n'a pas modifié). Dans chaque itération, l'ensemble des biclusters qui a été obtenu dans le précédent est analysé, comme on peut le voir dans la ligne 15 (dans la première itération BLN commence à travailler avec les biclusters obtenus dans la première phase). Pour créer un nouveau bicluster d'un autre, nous devons ajouter de nouveaux gènes. Ces nouveaux gènes sont regroupés dans un ensemble appelé gènes utiles, qui est formé par les gènes de tous les biclusters de l'ensemble en cours d'analyse, moins les gènes de la bicluster qui est en cours de traitement à ce moment (lignes 16 à 18). Pour ajouter un gène, g », à un bicluster (I, J), BLN doit vérifier que pour chaque gène dans le bicluster, g », un 2g_Bicluster formé par g » et g » existe et son groupe de conditions associées doit soit supérieure ou égale aux conditions de J, soit les conditions de la nouvelle bicluster possible J »inclut celle de l'ancien I (ligne 17). De cette façon, que la BLN assure différence entre la valeur d'expression de la nouvelle génération et les autres est toujours inférieure à δ. Un exemple simple basé sur la matrice de données M = (G, C, ℓ) décrit dans l'exemple 1. On applique à BLN M en utilisant δ = 5 et λ = 2. Après la première phase de l'algorithme, nous avons obtenu un groupe de biclusters 2 du gène indiqué dans le tableau 1. La seconde partie analysera tous les bicluster gènes Conditions Conditions Genes {1, 2} {2, 3, 4} {2, 4} {1, 3, 4} {1, 3} {1, 2, 3, 4} {2, 5} {1, 3, 4} {1, 4} {1, 3, 4} {3, 4} {1, 2, 3, 4} {1, 5} {1, 3, 4} {3, 5} {1, 2, 3, 4} {2, 3} {2, 3, 4} {4, 5} {1, 2, 3} TAB. 1 - Exemple de génération 2g_Biclusters. de ce groupe d'essayer de générer de nouveaux. Nous avons choisi une de cette bicluster préliminaire: B1 = {{g1, g5}, {c1, c3, c4}} B1 (gi, cj) = (1 3 2 2 3 1) Dans ce cas, le groupe de Useful_Genes sera { g2, g3, g4}. Pour chaque gène de cet ensemble, nous essayons de créer une nouvelle vérification de bicluster si elle peut faire partie de la bicluster nous traitons, B1. Nous commençons par g2, et comme le biclustering étant considéré est {g1, g5}, nous vérifions si {g2, g1} et {g2, g5} sont biclusters, puis si ces biclusters ont les conditions pour inclure g2 dans {g1 , g5}. Conditions Genes {2} {1, 2, 3, 4} {2, 5} {1, 3, 4} {1,5} {1,3,4} La première combinaison de gènes, (g2, g1), génère un bicluster avec un groupe de conditions iN- compatibles avec les conditions de B1, donc nous ne pouvons pas ajouter g2 au bicluster. La même chose se produit avec le g4 gène. Conditions de gènes {4, 1} {1, 3, 4} {4, 5} {1, 2, 3} {1,5} {1,3,4} - 685 - RNTI-E-6 Basé sur biclustering locale Cependant Proximités, le g3 gène est approprié pour être inclus dans le bicluster. Conditions Genes {3, 1} {1, 2, 3, 4} {3, 5} {1, 2, 3, 4} {1,5} {1,3,4} L'ensemble des conditions {c1, c2 , c3, c4} est compatible avec les conditions de B1: {c1, c3, c4} pour chaque paire de gènes formés par chaque gène de B1 et le g3 génique des Useful_Genes fixés. En conséquence, un nouveau bicluster est créé, Bnew = {{g1, g3, g5}, {c1, c3, c4}}, où (Bnew (gi, cj) =   1 3 2 1 2 2 2 3 1   4 méthode Dans cette section, nous décrivons la méthode utilisée pour obtenir biclusters basé sur la proximité locale en utilisant le BLN algorithme. l'objectif est de générer le plus grand nombre de biclusters avec le nombre maximum de gènes et une faible valeur de MSR. pour ce faire, nous devons d'abord déterminer la valeur correcte des paramètres de l'algorithme, en accordant une attention particulière au seuil de distance entre les valeurs d'expression. δ Comme cette valeur est critique, nous avons soigneusement la méthode afin de fournir biclusters valides et de haute qualité . Nous avons développé nos expériences avec un ensemble de données bien connues:. le monie Saccharomyces ensemble de données d'expression du cycle cellulaire de visiae la levure DataSet consiste d'une matrice de données composée par 2884 gènes (lignes) et 17 conditions expérimentales (colonnes) Nous devons recueillies statistiques. informations sur cette matrice de données pour décider par mètres sont adaptés à nos besoins. Si l'on analyse les données de levure matrix on trouve que la distance maximale entre les valeurs d'expression est 596. Nous considérons que la moitié de cette valeur, qui est, 298, comme la distance maximale que nous allons permettre, δ_max. Nous avons divisé cette valeur par 100, créant ainsi 100 intervalles différents pour δ. L'étape suivante consiste à exécuter une version spéciale de BLN dont le but est d'obtenir des informations statistiques sur le nombre de biclusters générés après les première et deuxième parties. Cette version a procédé à un test pour chaque δ: 2, 98 * i, i allant de 1 à 100 et pour chaque nombre de conditions minimales permis, λ, de 1 à 17. Ces pistes ne sont que la simulation de sorte que le coût de calcul est inférieure à une course de BLN normal. Nous avons enregistré le nombre de biclusters générés après les première et deuxième phases de 100 * 17 situations différentes. De plus, nous informations compilées à la moyenne du nombre de gènes et le nombre maximum de gènes après la deuxième partie. Pour réduire cette quantité de données que nous mettons l'accent sur un certain nombre de conditions minimales allant de 10 à 17. Le nombre de bicluster généré avec ces tests valeurs sont illustrées dans les graphiques de la figure. 2. Le premier graphique (à gauche) montre l'évolution du nombre de différents biclusters c réé pour toutes les valeurs d'essai après la première phase, avec un nombre maximal de biclusters de 364,713 avec δ = 35,76 et λ = 10 et un nombre minimal de biclusters de 10 à δ = 2,98 et λ = 17. Le deuxième graphique (à droite) montre les mêmes informations par rapport à la seconde partie de l'algorithme, générant des prochaines valeurs extrêmes: 10.215.022 avec δ = 35,76 et λ = 10, et 21 avec δ = 2,98 et λ = 17. Il est évident que les conditions restrictives qui rendent le nombre - 686 -RNTI-E-6 JS Aguilar-Ruiz et al. 0 50000 100000 150000 200000 250000 300000 350000 400000 0 5 10 15 20 25 30 35 40 Min Cond. = 10 min Cond. = 11 min Cond. = 12 min Cond. = 13 min Cond. = 14 min Cond. = 15 min Cond . = 16 min Cond. = 17 0 2e + 006 + 006 4E + 6E + 006 8e 006 1e + 007 1.2E + 007 0 5 10 15 20 25 30 35 40 Min Cond. = 10 min Cond. = 11 min Cond. = 12 Cond Min. = 13 min Cond. = 14 Cond Min. = 15 Cond Min. = 16 min Cond. = 17 FIG. 2 - Nombre de biclusters avec différentes valeurs de δ (dans l'axe X) et λ (courbes identifier le nombre minimum de conditions): après la première phase de BLN (à gauche) et après la seconde phase de BLN (à droite), respectivement. Lorsque les augmentations du seuil, le nombre de biclusters fait aussi. Lorsque le nombre minimum de conditions augmente, le nombre de biclusters diminue. - 687 - RNTI-E-6 biclustering basée sur Local Nearness 0 2 4 6 8 10 0 5 10 15 20 25 30 35 40 Min Cond = 10 min Cond = 11 Min Cond = 12 Min Cond = 13 Min Cond.... . = 14 Cond Min. = 15 Cond Min. = 16 Cond Min. = 17 FIG. 3 - Moyenne du nombre de biclusters avec différentes valeurs de δ (axe X) et λ. de biclusters inférieur sont une faible valeur de δ et la forte valeur de λ. En fait, si λ est égal au nombre de conditions expérimentales, alors nous trouverions des grappes au lieu de biclusters, tout en permettant de chevauchement entre les groupes, contrairement aux techniques de clustering traditionnelles. Il vaut la peine de noter que pour chaque paire de valeurs de test, AI et Ài, le nombre de biclusters générés après les première et deuxième parties sont liées, de telle sorte que le dernier est directement proportionnelle à la première dans tous les cas . Nous avons recueilli des informations sur le nombre de gènes, bien que seulement pour la deuxième partie du processus (dans la première partie que nous générons que 2g_Biclusters). Dans la figure 3, nous pouvons observer le nombre moyen de gènes des biclusters pour nos valeurs de test. Il est intéressant de noter que la moyenne des gènes diminue alors que la valeur de δ croît pour tout le nombre minimum de conditions jusqu'à ce que la valeur 25. En effet, le nombre de biclusters augmente avec δ tandis que le nombre de leurs gènes ne change pas. A partir du moment δ a la valeur de 25 le nombre de gènes dans les biclusters se développe et la moyenne ainsi. De δ = 34, le début moyennes de plus en plus. Pour cette raison, nous avons choisi la valeur 35,76 pour nos expériences, car il y a plus de choix pour sélectionner de bons biclusters. Avec δ = 35,76 et λ = la moyenne des 10 gènes atteint sa plus grande valeur. Nous pouvons voir l'évolution du nombre maximum de gènes dans les biclusters de la figure 4. Cette valeur est constante pour toutes les valeurs de test jusqu'à ce que la valeur δ est d'environ 27. A partir de ce moment, nous trouvons biclusters avec plusieurs gènes. Le nombre de gènes atteint sa valeur maximale, 17, avec δ = 35,76 et λ = λ = 11 ou 10 (les deux graphiques sont identiques). La principale conclusion de cette analyse précédente est que le nombre maximum de gènes est atteint avec la valeur δ = 35,76. La valeur correcte pour λ est de 10 ou 11. Nous avons finalement choisi 10 comme le nombre de conditions minimales autorisées car avec δ = 35,76 et λ = 10 l'algorithme génère plus biclusters que avec λ = 11. Une fois les paramètres corrects ont été choisis suivant étape consiste à exécuter BLN et étudier les résultats. 5 Résultats expérimentaux Nous comparons BLN avec l'algorithme Cheng et l'Eglise (CC) et l'algorithme SEBI qui est un algorithme basé sur l'évolution qui extrait biclusters suivant une stratégie de couverture séquentielle (Aguilar et al., 2005). - 688 -RNTI-E-6 J. S. Aguilar-Ruiz et al. 4 6 8 10 12 14 16 18 0 5 10 15 20 25 30 35 40 Min Cond. = 10 min Cond. = 11 min Cond. = 12 min Cond. = 13 min Cond. = 14 min Cond. = 15 Cond Min. = 16 min Cond. = 17 FIG. 4 - Nombre maximum de gènes dans divers biclusters les valeurs de δ (axe X) et λ. Bicluster Genes Conditions MSR Row Variance 433315 17 10 105,87 231,52 462795 15 10 95,69 166,08 548356 15 10 81,56 173,35 682571 16 12 87,10 292,20 755312 14 10 69,85 153,78 847 161 14 10 91,65 355,04 1031000 14 10 89,62 159,25 8604074 14 10 73,84 127,69 9509611 15 10 73,93 182,27 TAB. 2 - Informations sur biclusters générés par BLN. Dans la première colonne chaque bicluster est identifiée par son numéro de génération. Les deuxième et troisième colonnes indiquent le nombre de gènes et conditions, respectivement. La valeur MSR est indiquée dans la colonne quatrième et la dernière colonne est liée à la variance de la ligne. Une fois l'analyse effectuée dans la section précédente, le BLN algorithme est exécuté en utilisant δ = 35,76 et λ = 10. Par conséquent, BLN génère 10.215.022 différents biclusters avec différentes tailles d'ensembles de gènes. Le chevauchement entre biclusters est évidente, et même beaucoup d'entre eux sont inclus dans d'autres, de sorte que seules 9.147 biclusters ont finalement été générés contenant un certain nombre de gènes supérieur ou égal à 14. La première phase de BLN a pris 41 secondes. La deuxième phase, 12.200 secondes (9,684 sans génération graphique et les fichiers génétiques). Le nombre moyen de gènes dans biclusters était 8. Les biclusters générés couvert 93,93% des gènes et 100% des conditions, à savoir, 93,93% des éléments de la matrice d'expression. L'algorithme de BLN a été implémenté en Java et exécuter sur une plate-forme Win XP. Les critères utilisés pour mesurer la qualité de biclusters sont le nombre maximum de gènes, la longueur moyenne minimale résidus au carré, et le nombre maximum de conditions, dans cet ordre. Suite à ces critères que nous avons sélectionné les 100 meilleurs biclusters parmi tous ayant un certain nombre de gènes entre 14 et 17. Les caractéristiques de certains d'entre eux peuvent être observés dans le tableau 2. Dans le tableau 2, les biclusters sélectionnés présente une petite valeur MSR, à savoir, il existe une grande cohé- rence dans les gènes et les conditions. Ce problème similaire peut être observée dans les graphiques - 689 - RNTI-E-6 biclustering basée sur Local Nearness 200 250 300 350 400 1 2 3 4 5 6 7 8 9 10 Bi: 548356 Genes: 15 Conditions: 10 Msr: 81,5601777777778 200 300 350 400 250 1 2 3 4 5 6 7 8 9 10 Bi: 433315 Genes: 17 Conditions: 10 Msr: 105,8735640138408 FIG. 5 - Un exemple de biclusters générés par BLN. Les formes des graphiques montrent la qualité de biclusters. Alg. MSR Vol. Num. des gènes Num. de Cond. CC 204,29 1576,98 166,71 12,09 202,68 204,67 13,20 SEBI 15,44 BLN 70,02 166,84 15,34 10,88 TAB. 3 - Comparaison des performances entre les CC, SEBI et BLN. La première colonne (algorithme), la seconde colonne (résidu quadratique moyenne arithmétique), la troisième colonne (volume moyen), la quatrième colonne (nombre moyen de gènes), cinquième colonne (nombre moyen de gènes) et sixième colonne (nombre moyen de conditions). sur la figure 5. Ces graphiques montrent l'évolution des valeurs d'expression de l'ensemble des gènes dans l'ensemble des conditions. On obtient biclusters avec un nombre élevé de gènes et, étant 17 la valeur maximale. Dans les données de levure déterminer le nombre de gènes et la valeur de MSR varier avec les paramètres, à savoir, lorsque le seuil de distance augmente et le nombre minimum de conditions diminue (conditions expérimentales moins restrictives), puis le nombre de gènes augmente sensiblement. Dans le tableau 3, nous comparons nos 100 meilleurs biclusters et leurs valeurs moyennes avec celles obtenues par les algorithmes de Cheng & Church (CC) et Aguilar & Divina (SEBI). BLN obtient de meilleurs résultats en ce qui concerne la valeur MSR que les deux autres algorithmes. Le volume en moyenne, à savoir, le nombre de gènes se multiplie par le nombre de conditions, et la moyenne des conditions de biclusters sont plus bas. Le nombre moyen de gènes est 15,34, supérieure à celle de SEBI de. La plus intéressante propriété de biclusters trouvée par BLN est qu'il pro- Vides biclusters avec une très faible résidu quadratique moyenne par rapport aux autres techniques tout en maintenant le nombre de gènes entre 14 et 17. La variance ligne montre que l'algorithme est capable de trouver des modèles intéressants, qui sont illustrés à la figure 5. sur la figure 5 sont représentés six biclusters, avec des gènes allant de 14 à 17. les valeurs de niveau d'expression des gènes dans les biclusters sont très proches les uns des autres, en conservant le seuil fixé par la - 690 -RNTI-E-6 JS Aguilar -Ruiz et al. algorithme. Il est particulièrement intéressant de noter que la plage de valeurs est très faible en ce qui concerne la gamme de niveaux d'expression, tel qu'il apparaît dans les graphiques. En outre, certaines formes montrent des modèles significatifs dans les données, comme bicluster dans le coin supérieur droit de la figure 5. 6 Conclusions Notre technique pour trouver biclusters est basée sur la proximité locale, à savoir la distance entre les valeurs d'expression des gènes, sous le même état, dans les biclusters. Nous trouvons des groupes de gènes qui ont un comportement similaire dans un sous-ensemble de conditions. Une étude statistique précédente des données de déterminer les paramètres appropriés pour notre approche, appelée BLN, qui fournit un groupe de différentes biclusters avec des gènes très liés et très faible résidu quadratique moyenne. Par rapport aux algorithmes précédents de notre approche de biclusters avec moins de gènes que CC et très faible résidu quadratique moyenne. Par rapport à SEBI, BLN améliore le nombre moyen de gènes qui maintient son résidu quadratique moyenne très faible. Des exemples graphiques de biclusters sont fournis et ils présentent un comportement similaire pour les gènes dans biclusters. Les recherches futures se concentrera sur l'amélioration de l'efficacité de l'algorithme au moyen de techniques d'élagage et à la validation des biclusters avec des connaissances biologiques, telles que Gene Ontology. Acquittement Cette recherche a été soutenue par l'Agence espagnole de recherche CICYT en bourse TIN2004- 001. Références Aguilar, J. S., F. Divina (2005) évolutionnaire biclustering des biopuces données, 3ème atelier européen sur évolutionnaire bio-informatique. Ben-Dor, A., B. Chor, R. Karp, Z. Yakhini (2002) A la découverte structure locale dans les données de gène sion: L'Ordre Préserver problème sous-matrice, 6 ACM Conférence internationale sur la recherche en biologie moléculaire computationnelle, RECOMB2002. Cheng, Y., G. M. Eglise, (2000) biclustering des données d'expression, Actes de la 8e Conférence nationale inter sur les systèmes intelligents pour la biologie moléculaire, ISMB'00, 93-103. Dougherty, E., J. Barrera, B. Marcel, S. Kim, R. Cesar, Y. Chen, M. Bittner, J. Trent (2002) Inference de clustering avec application à microarrays d'expression génique, Journal of Computational Biology , vol. 9 (1), 105-126. M. Gerstein, J. Chang, R. Basri, Y. Kluger (2003) spectrale biclustering de données microarray: Coclustering gènes et conditions, Journal Genome Research, vol. 13 (4), 703-716. Hartigan, J.A. (1972) le regroupement direct d'une matrice de données, Journal de l'American Statistical Association, 67 (337), 123-129. Lazzeroni, L., A. Owen (2000) modèles Plaid pour les données d'expression génique, Rapport technique Stan- Université Ford. - 691 - RNTI-E-6 biclustering Basé sur Liu locale Proximités, J., J. Yang, W. Wang (2004) biclustering dans l'expression génique des données par Tendency, IEEE Computational Systems bio-informatique Conférence 2004, 183-193. Madère, S., A. Oliveira (2004) biclustering algorithmes d'analyse de données biologiques: Sur- vey, IEEE Transactions / ACM sur la biologie computationnelle et bio-informatique, vol. 1 (1), 24- 45. Shamir, R., R. Sharan, A. Tanay (2002) A la découverte biclusters statistiquement significatives dans les données d'expression génique, bio-informatique, vol. 19, Suppl. 1 2002, 136-144. Wang, H., W. Wang, W. Haixun, P. Yu (2002) Clustering par similarité de modèle dans de grands ensembles de données, SIGMOD Conférence internationale sur la gestion des données, 394-405. Yang, J., W. Wang, W. Haixun, P. Yu (2002) Amélioration des performances des Bicluster Discovery dans un grand ensemble de données, 6e ACM Conférence internationale sur la recherche en Computationa l Molec- Biologie Ular, RECOMB2002, Affiche. Yang, J., W. Wang, W. Haixun, P. Yu (2003) sur des données biclustering amélioré d'expression, 3e Conférence IEEE sur bio-informatique et bio-ingénierie, 321-327. L'analyse des CV d'expression de Données Dans l'ADN Gènes is un outil important de la recherche used Dans les Génomique Do not s'étendent de Objectifs principaux du l'étude des tionnel caractère fonc- et Gènes Spécifiques their participation Dans les Processus biologiques à la recons- truction des conditions de maladies et à their pronostique. Les d'expression de Données Gènes Dans des Sont matrices arrangées where each correspondent à Une gène ligne et each état Une colonne Représente Expérimentale Spécifique. Les techniques de ""biclustering"" mais de verser ONt Pressothérapie des sous-ensembles de les Gènes Qui montrent d'activité Modèles sous sous similaires ONU-ensemble des conditions. Notre approach Consiste en un algorithme de ""biclustering"" basons sur la locale proximité. L'Algorithme des ""biclusters Cherche"" d'une Manière gloutonne. Il ""des AVEC commencent l'biclusters"" qui contiennent deux et gènes de Autant- ensuite inclut Que possible en Gènes un seuil de respectant la distance de garantissant la similarité des Comportements gènes. - 692 -RNTI-E-6"
1045,Revue des Nouvelles Technologies de l'Information,EGC,2006,Finding fragments of orders and total orders from 0-1 data,"High-dimensional collections of 0-1 data occur in many applications. The attributes insuch data sets are typically considered to be unordered. However, in many cases there is anatural total or partial order underlying the variables of the data set. Examples of variablesfor which such orders exist include terms in documents and paleontological sites in fossil datacollections. We describe methods for finding fragments of total orders from such data, basedon finding frequently occurring patterns. We also discuss techniques for finding good totalorderings (seriation) based on spectral ordering and MCMC methods",Heikki Mannila,http://editions-rnti.fr/render_pdf.php?p1&p=1000315,http://editions-rnti.fr/render_pdf.php?p=1000315,en,"Trouver des fragments de commandes et le total des commandes 0-1 données Heikki Mannila HIIT Unité de recherche de base, Université d'Helsinki, Département des sciences informatiques et de l'Université de technologie d'Helsinki, Laboratoire d'informatique et de la Science Heikki.Mannila@cs.helsinki.fi information High- collections de dimensions 0-1 données se produisent dans de nombreuses applications. Les attributs de ces ensembles de données sont généralement considérés comme non ordonnée. Cependant, dans de nombreux cas, il y a un ordre total ou partiel naturel sous-jacent aux variables de l'ensemble de données. Des exemples de variables pour lesquelles ces ordres existent dans les documents comprennent des termes et des sites paléontologiques dans les collections de données fossiles. Nous décrivons des méthodes pour trouver des fragments de commandes totales de ces données, en fonction de trouver des modèles qui se posent. Nous discutons également des techniques pour trouver de bonnes ordonnancements totales (sériation) en fonction de commande spectrale et des méthodes MCMC. CV sur collections aux de haute de 0-1 Données dimension Que l'sur les applications de la DANS Rencontre Nombreuses. Bien Que les attributes de tells Dans Soient ensembles de Données non typiquement considérés Comme ordonnés, un total de sous-UO ont tendance partiel SOUVENT les des variables ordre. Par exemple, il exists de tells Entre les Ordres Annoter Dans un ensemble utilisés de documents, les sites de paléontologiques ous les collections Dans banque de points Fossiles. Nous DE- crivons des methods, sur la recherche fondées de motifs Fréquents, de Qui permettent des fragments d'Retrouver au total à partir ordre de Données Telles. Nous Discutons des techniques also sur l'ordre fondées spectrale et les MCMC Qui Modèles de Pressothérapie de permettent bures Totaux (sériations Ordres). - 1 - RNTI-E-6 - 2 -RNTI-E-6"
1055,Revue des Nouvelles Technologies de l'Information,EGC,2006,Maintaining an Online Bibliographical Database: The Problem of Data Quality,"CiteSeer and Google-Scholar are huge digital libraries which provideaccess to (computer-)science publications. Both collections are operated likespecialized search engines, they crawl the web with little human interventionand analyse the documents to classify them and to extract some metadata fromthe full texts. On the other hand there are traditional bibliographic data baseslike INSPEC for engineering and PubMed for medicine. For the field of computerscience the DBLP service evolved from a small specialized bibliographyto a digital library covering most subfields of computer science. The collectionsof the second group are maintained with massive human effort. On the longterm this investment is only justified if data quality of the manually maintainedcollections remains much higher than that of the search engine style collections.In this paper we discuss management and algorithmic issues of data quality. Wefocus on the special problem of person names","Michael Ley, Patrick Reuther",http://editions-rnti.fr/render_pdf.php?p1&p=1000317,http://editions-rnti.fr/render_pdf.php?p=1000317,en,"Le maintien d'une base de données en ligne Bibliographical: Le problème de la qualité des données Michael Ley *, Patrick Reuther * * Département des bases de données et des systèmes d'information, Université de Trèves, Allemagne {ley, Reuther} @ uni-trier.de http: //dbis.uni- trier.de http://dblp.uni-trier.de Résumé. CiteSeer et Google Scholar sont énormes bibliothèques numériques, qui permettent d'accéder aux publications scientifiques (ordinateur-). Les deux collections fonctionnent comme les moteurs de recherche spécialisés, ils explorent le web avec peu d'intervention humaine et d'analyser les documents de les classer et d'extraire des métadonnées des textes intégraux. D'autre part, il y a des bases de données bibliographiques traditionnelles comme INSPEC pour l'ingénierie et PubMed pour la médecine. Pour le domaine de l'informatique au service de DBLP a évolué d'une petite bibliographie spécialisée dans une bibliothèque numérique couvrant la plupart des sous-domaines de la science informatique. Les collections du second groupe sont maintenus avec un effort humain massif. À long terme cet investissement ne se justifie que si la qualité des données des collections maintenues manuellement reste beaucoup plus élevé que celui des collections de style du moteur de recherche. Dans cet article, nous discutons de gestion et de problèmes algorithmiques de la qualité des données. Nous nous concentrons sur le problème particulier des noms de personnes. 1 Introduction Dans les domaines les plus scientifiques la quantité de publications est en croissance exponentielle. L'objectif principal des publications scientifiques est de documenter et de communiquer de nouvelles idées et de nouvelles ré- sultats. Sur la publication de niveau personnel est une sorte de points de collecte de crédit pour le CV. Sur le plan institutionnel, il y a une demande croissante d'évaluer les scientifiques et les départements par des mesures bibliométriques, qui nous l'espérons considèrent la qualité du travail. Tous les aspects exigent une collecte fiable, l'organisation et l'accès aux publications. À l'ère de papier cette infrastruc- ture a été fournie par les éditeurs et les bibliothèques. Internet, cependant, a permis à de nouveaux joueurs d'offrir des services. Par conséquent, de nombreux portails Internet spécialisés sont devenus importants pour les communautés fiques fiques. Les moteurs de recherche comme Google (-Scholar) ou CiteSeer, des archives centralisées comme arXic.org/CoRR et un grand nombre de serveurs Web personnels et / ou service, il est très facile de communiquer du matériel scientifique. Les anciens joueurs - éditeurs, sociétés savantes, les bibliothèques, les producteurs de bases de données, etc. - face à ces nouveaux concurrents en construisant de grandes bibliothèques numériques comme ScienceDirect (Elsevier), SpringerLink, ACM Digital Library ou Xplore (IEEE) dans le domaine de la science informatique. DBLP (Digital Bibliographie et projet de bibliothèque) (Ley, 2002) est un Internet « nouveau venu » que le service a commencé en 1993. Le service DBLP a évolué à partir d'une petite bibliographie spécia- isé aux systèmes de bases de données et la programmation logique à une bibliothèque numérique couvrant la plupart des sous-champs - 5 - RNTI-E-6 Le maintien d'une base de données en ligne de Bibliographical informatique. Aujourd'hui (Octobre 2005) des indices de DBLP plus de 675.000 publications publiées par plus de 400,000 auteurs et est accessible plus de deux millions de fois par mois sur le site principal maintenu à notre service. Pour construire une base de données bibliographique nécessite toujours des décisions entre qualité et quantité. Vous pouvez décrire chaque publication par un très riche ensemble de métadonnées et comprennent des classifications, des liens de citations, résumés, etc. - ou de la limiter au minimum: les auteurs, le titre et le lieu de publication (journal, livre, adresse Web). Pour DBLP, nous avons décidé de l'approche minimaliste, nous empêchent nos ressources très limitées pour produire des métadonnées détaillées d'un grand nombre. Pour chaque attribut des métadonnées du degré de cohérence qui fait la différence: Il est facile de produire un grand nombre de notices bibliographiques sans normalisation des noms de revues, les noms de conférence, noms de personnes, etc. Dès que vous essayez de garantie qu'une entité ( Journal, conférence, personne, ...) est toujours représenté par la même chaîne de caractères et pas tities en- partager la même représentation, l'entretien des données soient vient très cher. Traditionnellement, ce processus est appelé le contrôle de l'autorité. En DBLP le nombre de différents journaux est quelques taines, le dreds nombre de différentes séries de conférences quelques milliers. Pour la cohérence de garantie à cette échelle nécessite des soins, mais il est est pas un vrai problème. Même pour une base de données graphique biblio- taille moyenne comme DBLP, le contrôle de l'autorité pour les noms de personnes est beaucoup plus difficile: l'ampleur est> 400K et les informations disponibles sont souvent incomplètes et contradictoires. 2 Processus de gestion de la qualité des données basée sur les données de la qualité comprend de nombreuses dimensions et aspects. présente Redman une ETY de Vari de dimension tels que l'exhaustivité, l'exactitude, l'exactitude, la monnaie et la cohérence des données, pour ne citer que quelques-uns (Redman, 1996). D'autres aspects sont les univocité, la crédibilité, l'actualité, signifiance. Une bonne vue d'ensemble sur les différentes dimensions de la qualité des données peuvent être obtenues à partir (Dasus et Johnson, 2003) (Scannapieco et al., 2005). l'acquisition de l'information est une phase critique pour la gestion de la qualité des données. Pour DBLP il existe un large éventail de sources d'information primaires. En général, nous obtenons des documents électroniques, mais parfois toutes les informations doivent être tapé. Certaines sources importantes comme SpringerLink pour les notes de conférence en série sciences informatiques fournissent des informations de base dans un format très structuré qui est facile à transformer en nos formats internes. Pour de nombreuses sources très diversement formatée, il est pas rentable de développer des programmes wrapper, nous devons utiliser un éditeur de texte standard et / ou des scripts adhoc pour transformer l'entrée à un format adapté à notre logiciel. Dans certains cas, nous avons seulement les premières pages (pages de titre, table des matières) d'un volume de journal ou d'une procédure. La table des matières contient souvent des informations inférieure à la tête de l'article lui-même: Parfois, les noms donnés des auteurs sont abrégés. Les informations d'affiliation pour les auteurs est souvent absent. De nombreuses tables des matières contiennent des erreurs, surtout si elles ont été produites sous la pression du temps comme de nombreuses procédures. Même dans la tête de l'article lui-même, vous trouverez peut-être des erreurs typographiques. Une politique très simple mais important est d'entrer tous les articles d'un numéro de volume ou de la revue des procédures en une seule étape. En DBLP nous ne faisons que très peu exception de ce tout ou rien politique. Pour la qualité des données ce qui a plusieurs avantages par rapport à l'entrée CV des scientifiques ou des listes de référence des documents: Il est plus facile de garantir une couverture complète d'une série de journal ou d'une conférence. Il y a moins de danger pour devenir biaisé en faveur d'une personne (s). La rapidité est seulement pour atteindre, si de nouveaux numéros de revues ou de procédures sont complètement entrés dès qu'ils sont publiés. - 6 -RNTI-E-6 M. Ley et P. Reuther Une décision de conception très tôt était de générer des pages de l'auteur: Pour chaque personne qui a (co) auteur (ou modifié) une publication indexées dans DBLP notre logiciel génère un HTML- la page qui lui énumère / ses publications et fournit des hyperliens vers les co-auteurs et aux tables des matières pages l'article est paru dans. du point de vue de la base de données de ces sont simples vues matérialisées, pour les utilisateurs, ils le rendent très pratique pour naviguer dans la personne -personne et graphiques personne-publication. Le graphique implicite par la relation de co-auteur est une instance d'un réseau social (Watts, 2004) (Staab, 2005), le filet de co-auteur DBLP a récemment été utilisé pour analyser la structure de plusieurs sous-communautés de l'informatique (Hassan et Holt, 2004 ) (Elmacioglu et Lee, 2005) (Liu et al., 2005). Nous interprétons une nouvelle publication comme un ensemble de nouvelles arêtes dans le graphe de co-auteur - ou comme une incrémentation des poids des bords existants. Pour chaque nouvelle publication, nous essayons de trouver tous les auteurs de la collection existante. Nous utilisons plusieurs outils de recherche simple avec une variété d'algorithmes correspondant, dans la plupart des cas des expressions régulières traditionnelles sont plus utiles que toutes les fonctions de distance délicate. La recherche est essentiellement un processus manuel entraîné par l'intuition et l'expérience comment trouver plus efficacement perso n noms qui pourraient être mal orthographiés ou incomplets. Habituellement, ce processus de recherche manuelle est organisée en deux niveaux: Nous recrutons des étudiants pour effectuer la mise en forme (si nécessaire) et une première passe de recherche. Ils annoter des articles ou des noms qui nécessitent une enquête plus approfondie ou plus de connaissances de fond. Souvent, les étudiants trouvent incomplètes ou sans erreur dans la base de données. Dans un second passage sur la table des matières les cas problématiques sont traités et erreurs dans la base de données sont corrigées (ce qui est fait par M. Ley). Enfin les nouvelles informations sont entrées dans la base de données. Au cours de cette étape beaucoup de simples conventions de mise en forme sont vérifiés par des scripts, par exemple, nous sommes avertis s'il y a des caractères majuscules consécutifs dans un nom de personne. Lors d'une journée de travail, nous ajoutons -500 notices bibliographiques à DBLP. Il est irréaliste de croire que cela est possible sans introduire de nouvelles erreurs et sans oublier les anciens. Il est inévitable que les soins au cours du processus d'entrée varie. Le rêve évident est d'avoir un outil qui fait le travail - ou plus réaliste - ce qui nous permet de le faire. Pour approcher cet objectif, nous avons essayé de comprendre comment nous trouvons des erreurs et des incohérences plus efficacement. Souvent, il est très utile de regarder le voisinage d'une personne dans le graphe de co-auteur. Parce que la plupart des publications scientifiques sont produites par des groupes, de nombreuses erreurs apparaissent localement. Une première étape importante pour faciliter l'inspection manuelle a été le développement du navigateur DBL- (Klink et al., 2004) dans le cadre du projet SemiPort (Fankhauser et al., 2005). Le DBL-Browser fournit une interface utilisateur visuelle dans l'esprit de Microsoft Explorer: Un mélange de visualisations d'arbre avec dossier et documents icônes et de l'hypertexte de style web, il est très facile de naviguer à l'intérieur et entre les pages de l'auteur. Pour les personnes ayant de longues listes de publications la liste chronologique fournie par notre interface web devient insuffisante, des sélections par le co-auteur, revue / conférence, année, etc. sont très utiles. La DB mémoire principale sous-jacente du DBL-Browser garantit des temps de latence courts. Ceci est un facteur très important pour la facilité d'utilisation du système: une réaction rapide rend pratique « fouiner » et trouver des entrées suspectes. Le processus de la stratégie axée sur ce qui tente d'erreurs Avert d'entrer dans la base de données en contrôlant et en améliorant le processus d'acquisition de l'information devrait être complétée par une plus de données stratégie axée sur ce qui tente de détecter et corriger les erreurs dans les données existantes (Redman, 1996). - 7 - RNTI-E-6 Le maintien d'une base de données en ligne 3 Bibliographical guidée par les données des stratégies basées sur les données de gestion de la qualité peut être divisé en bashing base de données et les modifications de données. L'idée clé derrière dénigrement de base de données est de comparer ou recouper les données stockées dans une base de données de différentes sources de données comme une autre base de données ou d'informations des personnes du monde réel afin de trouver des erreurs ou pour confirmer la qualité des données d'origine. dénigrement de base de données est utile pour la détection d'erreur, mais la correction des erreurs est gênant. S'il existe des différences entre les deux enregistrements - qui sont supposés sous forme d'enregistrements décrivant la même entité de différentes sources - la question se pose laquelle des deux dossiers est correct, ou si l'un de ces événements est à cent pour cent correct. les modifications de données ne se concentrent pas sur la comparaison des enregistrements provenant de différentes sources, mais utilisent des règles métier. Ces règles métier sont spécifiques au domaine de la base de données. Pour le domaine des notices bibliographiques une telle règle est par exemple: « Alertez-nous s'il y a des auteurs dans l'ensemble de données qui varient légèrement dans leur orthographe, mais ont exactement les mêmes co-auteurs ». Exactement cette règle a été mise en œuvre par un logiciel simple: Nous construisons une structure de données qui représente le graphe de co-auteur. Nos algorithme vérifie toutes les paires (A1, A2) des noeuds d'auteurs qui ont la distance 2 sur le graphique. Si les noms de ces noeuds sont très similaires, nous les soupçonnons de représenter la même personne: si StringDistance (nom (a1), nom (a2)) < t avertissement alors la fonction de StringDistance et la valeur de seuil t requis une certaine expérimentation. À l'heure actuelle une version modifiée de la distance Levenshtein classique est utilisé, il met en œuvre des règles spéciales pour les caractères diacritiques (accents, trémas de, etc.) et pour les pièces de nom abrégé. Le programme produit une liste de plusieurs milliers d'avertissements. Le principal problème ne sont pas les fausses gouttes, mais les paires suspectes qui ne peuvent être résolus en raison du manque d'information. Dans de nombreux cas, nous sommes en mesure de trouver la partie manquante du casse-tête - par exemple sur les « pages d'accueil personnelles » des scientifiques eux-mêmes, mais souvent les informations ne sont pas disponibles avec un effort raisonnable. Nous avons vite découvert qu'il est plus économique de ne regarder que les personnes dont la liste de publications a été modifié récemment. Pour ces personnes, il est plus susceptible de résoudre orthographes contradictoires ou pour remplir les parties de nom abrégé. Le logiciel simple esquissée ci-dessus est utilisé quotidiennement depuis 2 ans. Il nous a aidés à trouver un grand nombre d'erreurs, mais il devrait être remplacé par un système amélioré pour deux raisons: Nous trouvons encore des erreurs trop plus ou moins par hasard et non par un processus de recherche bien compris. La précision des avertissements est encore trop faible - nous passons trop de temps sur des paires suspectes de noms que nous ne pouvons pas résoudre. Parce que le temps nous pouvons investir pour des corrections d'erreur est très limitée, nous avons besoin d'un outil qui nous montre les cas les plus prometteurs. 4 Cadre pour personne Nom Matching / Outlook Nous expérimentent actuellement un cadre logiciel beaucoup plus flexible pour la correspondance nom de la personne. Les idées principales sont les suivantes: • Il n'a pas de sens d'appliquer des fonctions à distance à toutes les paires de noms de personnes dans notre col- lection parce que cet espace de produit est trop grand (O (n2) algorithmes pour n> 400000) et parce que la comparaison des noms totalement sans rapport avec produit trop de gouttes fausses. Notre distance - 8 -RNTI-E-6 M. Ley et P. Reuther 2 dans l'heuristique du graphe de co-auteur est un (très réussie) par exemple d'une fonction de blocage. Un bloc est un ensemble de noms de personnes qui sont quelque part « en rapport », le blocage est défini comme un ensemble de blocs. Un nom de personne peut être membre de plusieurs blocs à l'intérieur d'un blocage. fonctions de distance sont appliquées uniquement à tous les tuples tirés d'un bloc et non de l'ensemble beaucoup plus grand de tous les noms - la complexité est désormais dominé par la taille du plus grand bloc. Une fonction de blocage est un algorithme qui produit un blocage. • Un ensemble très riche de fonctions de distance est décrite dans la littérature. Un excellent point de départ pour les explorer est le projet SecondString (Bilenko et al., 2003). Notre logiciel permet de brancher facilement dans de nouvelles fonctions à distance et de les combiner. Pour les noms de domaine très personne fonctions spécifiques semblent être utiles, par exemple pour correspondre à des transcriptions de noms chinois. • Le système est mis en œuvre en tant que données l'architecture en streaming très similaire à un processeur de requête dans un système de gestion de base de données. Cette architecture bien comprise donne beaucoup de flexibilité pour ajouter des opérateurs tels que l'union, intersection, la matérialisation, le chargement des résultats plus anciens, sélection, etc. Le point de départ du nouveau logiciel a été le reimplementation Java de nos algorithmes éprouvés dans le nouveau cadre. L'étape suivante a été l'ajout de plusieurs fonc- tions à distance et les opérateurs de flux. Pour la liste des avertissements plus utiles, chaque bloc à l'intérieur d'un blocage a une étiquette résultant - par exemple le nom de la personne qui construit la connexion tween les deux BE- suspects pour le blocage ou la distance 2, le nom de la conférence / revue à la fois ont publié, ou le mot de titre à la fois utilisé dans certaines de leurs publications. Cette annotation se propage à travers le flux. Une sortie typique de nos regards système comme celui-ci: Brian T. Bennett (2) - (Peter A. Franaszek) et (journaux / ibmrd) - Brian T. Bennet (2) Il y a 2 occurrences du nom Brian T. Bennett et 2 autres avec un « t ». Ils partagent le co-auteur Peter A. Franaszek et ont tous deux publié au Journal IBM de la Recherche et Dev eloppement. Pour le mouvement des logiciels open source a produit une variété fascinante de systèmes qui sont souvent compétitifs aux logiciels commerciaux. Pour le champ étroit des métadonnées pour les publications de l'informatique une telle culture « base de données ouverte » est presque absente. La seule exception est le partage des fichiers BibTeX dans la collection des sciences informatiques bibliographies fondée par Achille Christian. Depuis quelques années, l'ensemble de données DBLP est disponible en XML (http: //dblp.uni- trier.de/xml). A notre grande surprise cela a eu un impact très intéressant: (1) Nous sommes conscients de> 100 publications qui utilisent les données de DBLP comme un ensemble de données de test pour une très large gamme d'expériences, la plupart dans le domaine du traitement XML. (2) Plusieurs groupes ont publié des articles sur le problème de nom homonymie et utilisé les données DBLP comme exemple principal (Lee et al., 2004) (Le et al., 2005) (Han et al., 2005) .... Notre prochaines étapes consisteront à comprendre les détails de ces articles, à réimplémenter les méthodes proposées dans notre cadre, et de les tester dans notre travail quotidien. Remerciements: Le type le plus encourageant du contrôle de la qualité est la rétroaction des utilisateurs. Nous AP- précier tous les e-mails par les utilisateurs, nous espérons qu'aucun courrier électronique ne deviennent graves victimes de nos filtres anti-spam rigides. Nous essayons de corriger toutes les erreurs que nous sont pointés immédiatement. Malheureusement, il est bien être- yond nos ressources pour inclure toutes les publications que nous sommes invités à considérer. À l'heure actuelle DBLP est pris en charge par la zone Microsoft Bay Research Center et par le Max-Planck-Institut für Informatik. Nous espérons trouver plus de sponsors ... - 9 - RNTI-E-6 Le maintien d'une base de données en ligne Références bibliographiques Bilenko, M., R. J. Mooney, W. W. Cohen, P. ravikumar et S. E. Fienberg (2003). correspondance des noms d'adaptation dans l'intégration de l'information. IEEE Intell. Syst. 18 (5), 16-23. Dasu, T. et T. Johnson (2003). Data Mining d'exploration et de nettoyage des données. John Wiley. Elmacioglu, E. et D. Lee (2005). Sur six degrés de séparation dans DBLP-DB et plus. SIGMOD de la fiche 34 (2), 33-40. Fankhauser, P. et al. (2005). Fachinformationssystem Informatik (FIS-I) und Semantische Technologien für Informationsportale (SemIPort). En Informatik 2005, Bd. 2, pp. 698-712. Han, H., W. Xu, H. Zha et C. L. Giles (2005). Un modèle de mélange hiérarchique de Bayes naïfs pour le nom homonymie dans les citations de l'auteur. Dans SAC 2005, p. 1065-1069. ACM. Hassan, A. E. et R. C. Holt (2004). Le petit monde de logiciels d'ingénierie inverse. En WCRE, pp. 278-283. Klink, S., M. Ley, E. Rabbidge, P. Reuther, B. Walter et A. Weber (2004). La navigation et la visualisation des données bibliographiques numériques. En VisSym 2004, pp. 237-242. Lee, M.-L., W. Hsu, et V. Kothari (2004). Nettoyage des liens parasites dans les données. IEEE intelli- gent Systems 19 (2), 28-33. Ley, M. (2002). La bibliographie scientifique informatique dblp: Evolution, la recherche, les questions perspec- tives. En 2002 SPIRE, Lisbonne, Portugal, 11-13 Septembre, 2002, p. 1-10. Springer. Liu, X., J. Bollen, M. L. Nelson, et H. V. de Sompel (2005). réseaux co-auteur dans la communauté de la recherche en bibliothèque numérique. CoRR cs.DL / 0502056. Sur, B.-W., D. Lee, J. Kang et P. Mitra (2005). Etude comparative du problème de nom homonymie en utilisant un cadre à base de blocage évolutive. En JCDL 2005, p. 344-353. Redman, T. C. (1996). Qualité des données pour l'ère de l'information. Artech House. Scannapieco, M., P. Missier et C. Batini (2005). la qualité des données à un coup d'oeil. Datenbank- Spektrum 14, 6-14. Staab, S. (2005). Les réseaux sociaux appliqués. IEEE Intell. Syst. 20 (1), 80-93. Watts, D. J. (2004). Six Degrees: la science d'un âge connecté. NY: W. W. Norton. CiteSeer et CV Google Scholar des bibliothèques Sont Électroniques ac- CÉS gigantesques Ë Donnant des publications scientifiques (en informatique). Deux collections CÉS Sont des machines gérées de COMME recherche Spécialisées Qui le web Avec parcourent d'interventions hu- peu Maines et les Documents pour analysent les et versez classer des Méta-Extraire des Données textes complets. D'une partie de autre, il y a des also Bases de données bibliog raphiques PEC en INS- Comme ingéniérie et PubMed en médecine. En informatique, le service de DBLP un evolue d'en Une petite bibliographie Une bibliothèque électronique la Plupart des Couvrant Domaines de l'informatique. Les collections du deuxième groupe Avec effort Sont gérées de un dérables humain consi-. À longue terme, un tel Ne est justifié Investissement Que Si la Qualité des Donnees RESTE Très une Supérieure de collections de Celle machines de type de recherche. Dans this article, nous dis- aspects cutons les et gestion Algorithmique de la qualité des Données. Nous nous concentrons sur le des NOMs Problème de Particulier personnes. - 10 -RNTI-E-6"
1118,Revue des Nouvelles Technologies de l'Information,EGC,2005,Classifying XML Materialized Views for their maintenance on distributed Web sources,"Ces dernières années ont mis en évidence la croissance et la diversité des informations électroniques accessibles sur le web. C'est ainsi que les systèmes d'intégration de données tels que des médiateurs ont été conçus pour intégrer ces données distribuées et hétérogènes dans une vue uniforme. Pour faciliter l'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'interrogation pour XML qui s'est imposé pour les systèmes basés sur XML. Ainsi XQuery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources. Pour optimiser l'évaluation de requêtes, les vues sont matérialisées lors de la mise à jour des sources, car dans le contexte de sources web, très peu d'informations sont fournies par les sources. Les méthodes habituellement proposées ne peuvent pas être appliquées. Cet article étudie comment mettre à jour des vues matérialisées XML sur des sources web, au sein d'une architecture de médiation.","Tuyet-Tram Dang-Ngoc, Virginie Sans, Dominique Laurent",http://editions-rnti.fr/render_pdf.php?p1&p=1000331,http://editions-rnti.fr/render_pdf.php?p=1000331,en,"! ""$ #% & '(*) +, .- / 01/2 3) 4657819;: <1 « > = /9@?A)4:.2!/ B8CEDGFHIKJ BLMFNPORQSNPTVU * JXWUZY \ [P] \ ^ / _ L UZ_`TV_aH + bcNPTEd] VQSYPOe_aTV_gf CH H8N CEL HTZI hjilknmoh8NPpqY LN * ily LRD] cs / TV_`tZH L dK_`IDY * uvkwH LUD JXM YPTxIlYZ_adlH y] VNMtPHTCHzS {CYP | a} V ~ VH4kw ~ qN C t \ _A ZP pxy kwH LUD JXM YPTxIrYP_gdKHkwH {cH. LN * Tq [H * NLG+ * + * {} CH VIKJ_`TcuYq CJ [u HLUD L x1 G kwHde {L cH TV_ HLHd4N * TVT ¢ ¡HHD + YZTZI0Oe_adeHT £ ¡Htx_g {cHTE [H <| aN¤ [L YP_gdldrN * Tq [H ¥ HIE | gN UL NPTE {{cH| c_`tZH L dK_`IV¡H¤ {cHd_`TVuYL Oen * Il_aYPTEd§¡H | aH [iL YZTV_gf CHdNP [[ HdrdK_apV | AHD; d CVL | aH¤ © nHp8 kSª HdKI ¢ NP_`TqdK_f CH «{d chd ¢ D dKIHOeHdw {ª _aTxIV¡ HULNIr_`YZT ¥ {{cH cYPTVT ¢ ¡HHd.IlH |! F ad C H «{{cHdwO¬¡H c_gNJ IRH CVL d ¢ YPTxI¡HIV¡H «[YZT. [C d} qY CVL _aTZIV¡HULHL [Hd {VYPTVT ¢ ¡HHD {V_adKI L _ `p C ¡HHdnHI / ~ ¢ ¡HIV¡HL YPUEHTVHd {VNPTEd C TEHt CHC TV_`uYL OeHP ¢ m Y CVL u®NZ [ _A | `_`IlH L | ª _aTZIV¡HULN * Il_aYPT {CHD / {cYPTVT ¢ ¡HHdNeI L NMtPH L d {c_`u¯uM¡HLHTZI ° d d D ± dKIH OeHd] ²S³ ± h'n, ¡HIV¡H'NP {} VYP cIV¡Hμ [YZO0OeH'uYLON * I¶d · I ° N * TE {EN L {ļ } qY CVL ¡H [° ~ * EN TVUZH L {cHd_`TcuYLON * Il_aYPTEd4²S¹ CH LRD HdKI CT| | aNPTVUZNPUPH4 {jª _aTxIlH lRL YPUZN * Il_aYPT¶} qY CVL ²³ ± hf C _Vdª HdKI._`Oe}} GYZdc¡Hw GY CEL | `Hdd D dKIHO0Hd8pqNPdc¡Hd d CVL ²S³ ± hvPz _aTEdl_c²S¹ CH LRD HdKI HO0} E | `¡H + YD d CVL {CHDD D d · IHOeHdS {{cH0Oº¡H V_aN * Il_aYPT»} GY CEL [YPTE [HtZYP_ L {cHdSt CHd «{} w¡H¼qTV_`Hdd CVL E | C dl_aH CVL d4dlY CEL [Hd¶m Y CEL YP} VIl_aO0_gdlH L | ª · ¡HtNP | CN * Il_aYPTμ {cH LHF CjFHIlHd] 1 | `Hd4t CHd dlYPTxI4ON * IV¡HL _gN * | a_adV¡HHd» h8N¶ {c_¾½ ¥ [ C | ¾IV¡HAHd · I {* Chon _aTZIrHTV_ L _`TE [L ¡HOeHTxIrNP | `HOeHTxI4 {CHDT CHd ¢ Oen * IV ¡HL _gN * | a_gdc¡HHd |. aY L d {cH «|! gN4Oe_gdKHANS¿ · Y CVL {chd DLy CEL [HD] \ [NL {VNPTEd ¢ | `H [YPTxIlH \ ilh {cH« DLy CVL [Hd © nHpj] xI L Hdv} GH C {ª _`TcuYLON * Il_aYPTEdndKYZTxIvuY CEL TV_aHdn} FR L | AHD ¢ DLy CVL [Hd hjHd O¬¡HIl ~ VYC {~ chd ENPpV_¾I CH | a | aHOeHTxI « } L YZ}} qYxdc¡HHdTVH + GH C tZHTxI «} enpd FHIL H + NP}} V V | a_af C ¡HHD« kwHINL Ir_a [ | `H¡HIC {c_`H« [L YZOeO0HTxInOeHIlI HeN / ¿· Y CEL {cHdnt CHdwOeN * IV¡HL _gN * | a_gdc¡HHd .²³ ± h¤d CVL {chd DLy CVL [Hdn © wHpj] VN C dKH_`T; {ª C TVHNL [° ~ E_¾IrH [Í CVL H {{cHO¬¡H c_gNIl_aYPT8 À Á 0) ¥ ""±: 2n) 4 iXTÂ_`Tq {C DKI LRD © Npd nH | a | / NZde_`TLHdlHNL [° ~ j]. n {VNI ° N¶_aTcuY L ONIr_`YZTÂd D dKIlHOde © nH L HY * u¯IlHTÂp C _A | ¾IAdlH} qNJ L N * ilh | D B ~ V_gd L Hd C | ¾I ° d_aT ± ~ ~ V_aUP V | D ~ VHIrH L YPUZHTVHY C d! NPTE {{± c_gd · I L de C IRH {± d D d · IrHOd] E ©! ~ V_g [° ~ ± UPHTVH L NIRHD u L NPUPOeHTxIrH {¥ t \ _ah © DWY * u {FRIrN0N * Tq {¥ _aTcuY L ONIl_aYPT8iXT¤Ã ZZY] ZÄÂ_`H {L ~ cH VYZ | a {A} L YZ} {qYxdKH ¥ N4O0H {c_gNN Il_aYPT L [° ~ V_`IlH [I CVL H4Å®ÄÂ_`H {L ~ cH VYZ | a {¤Ã ZZYÆ uY L [YZO + pV_aTV_aTVU + _aTcuY L ONIr_`YZT ¥ u L YPOO C | `} V Il_a | aH {VNI ° N + Tpo CVL [Hd B ~ VHUPYxN * | nY * uSN¶OeH {c_gNIr_`YZT¬NL [° ~ V_`IlH [I CEL H <* _ad4IlY¤_aTZIrHULN ILH ¥ Ir ~ VH; DKY CVL [Hd4_aTºY L {cH L IrY¶} LHdlHTxI NC TV_`uYL EAO \ _`H © ÈIrYIl ~ VH¼qTEN * | C DLH L Y L N *} V} E | `_g [N * N * L Il_aYPTjAÉHTVH | a | D dK} GHNPÊx_aTVUq] 8NO0H {c_gNIl_aYPT'NLJ [° ~ V_`IlH [I CVL H ¥ _gd [YZOe} {qYxdKH ¶YPunOeH {* c_aN Ily L dNPTE {¤ © L NP} V} GH L d ! z £ © L NP} V} GH L _gd + NPdrdlY \ [_ * aN ilh {± Iry; HNP [° ~ DKY CVL [H0 © _¾Ir ~ ~ ¶Ir VH L YP | AHD Y * UNH \ IL NP [Il_aTVUNPTE {¶ILN * TEdlOe_¾IlIl_aTVU_`TVuYL Oen * Il_aYPT »u L YZOËIl ~ VH ¥ DLy CEL [Il HdSIlY ~ VH_aTZIrHULN * ily L ÌnHuYLH! ILN * TEdlOe_adrdl_`YZTj] PIr ~ VH «{VNI ° NN L HI L NPTEd · L uY OEH {} e_`TxIlY0N L H {{VH¼ETVH euy L ONI C DLH {p ~ D Il EHO0H {c_gNIly L B ~ EHO0H {c_gNIly L L N _`TxIrHUIRHD de _aTcuY L ONIr_`YZT <} L Yt \ _a {{VH <p D ~ Il VH © L N E *}} qH L d N * TE {} L HdlHTxI ° C DNNF H LLD N * pV | aH C TV_`uY L ORT \ _`H © ÍIrY4Ir ~ VH C DLH L NP} V} V | une_une [NIl_aYPT8 B Y / u®NP [_` | a_¾I ° NIrHvIr ~ VH de _`TxIlHU L NIr_`YZTY * UQ {VN * IrNNP [L Yxdld {c_¾ÎÏH L HTxI.d D dKIlHOed]²S³ »hA ~ ENZd8pGHHT0NZ {} CYP VIlH {~ NPdSIl EHedKIrNPTE {{VN L ± UYL Oen * ISuYL _`TcuYLON * Il_aYPT¤HV [° ~ EN * TEUPH4NPTE {± ²S¹ CH LRD] JND · I ° N * TE {{VN L ¶f CH LRD | aNPTcJ UC NPUPHeuYL f CH LRD _`TEU ± ²S³ ± hv] 8 ~ ENZdpGH [YPOeH <N; ON * ¿· YLLHf C _ LHOeHTxIuYL ²S³ ± hjJÐpENZdKH {D dKIlHOed'D Ñ H [HTxIl | D] SOeH {c_gNIr_`YZTÍN L [° ~ V_`IlH [I CVL HdApENPdlH {oYZT²S³ »hÒNPTE {CH Â²S¹ LLD ~ ENMtPH» pqHHTÍ} L YZ} {qYxdKH N * TE {<_ AO0 } E | `HO0HTxIlH {¤Å®QN * TUE * JXWUPYc [« N * Tq {É «NL {EN L _`T y * PZ * Æ å® QL NP} qH L Ó! ÔO` y * Z Ã Æ .Ä¶HN * | gdlY4TVYPIlH RNTI-E-3433 kw | aNZdldl_`u D _aTVUe²S³ »h'³ ± NIRH L _aNP | `_H {A ^ S_`H © UYL Ir ~ VH_ L ³ ± NP_`TxIrHTENPTE [H« YPTÄ¤Hp ± b \ Y CEL [Il Hd ~ EN * I0} LHtx_aY C + d © nY L + ECd NPDA É C} CIRN ONO * O` Ã PZ * Æ C dKHY * Ir ~ VH L uYLONIrd + uY lel H} L HdlHTxIr_`TVU¶dKHO0_`J d · I LLC [I CEL H {{VNI ° NÅHUEw³ Æ uY L f C H LRD _`TEU'Il ~ VHdKH¶ {VNI ° NÍÅHUEÈh8Y LH | Ð] ¢ ²S³ ± HJJ · Æ © _`Il RMN ~ în dK_aOe_` |! GN L NP} V} L YZNZ [° ~ j iXTμd C [° ~ uN *} V} V | a_g [NIr_`YZTEd] 8IR ~ VH C DLH ¥ ypu ¢ t \ _ah © d + _ad4 [.lrc [_gN * | ÐiXTE {{CHH] dK_aOe_` |. GN L | D Ily »Il ~ VH <[NPdlH ¥ ypu LH | aN * Il_aYPTENP | {VN * IrNPpENPdlHd] qNPT ± ²S³ ± h¬t \ _`H © [TNP; pqHedKHHT »Nzd / N <[YP | a | aH [¨Ir_`YZT ± {{cH¼qTVH» p D NPT ± ²S¹ CH LRD LHF CHdKI ²S¹ CH LLD _gd C dKH { IlY4¼E | `ilh L {FRIrN4N * TE {A_`TxIrHU L NIrHIr ~ VHOIrY4} ~ LHdlHTxInIl VH «{{c_`ÎGH LHTxI ¢ VN * IRN DKY CVL [Hd ¢ Nzd NdK_aTVUP |! AHdKY CVL [ HSIlY0Il ~ VH C dKH L ^ / _ aH © ÉonIRH L _aNP | `_N * Il_aYPTÂ [YZTEdl_adKIrd_aT} L Yc [Hdldl_`TEU|Il ~ VH¶tx_aH © (_aTÍN' | AYC [N * | [Nouvelle-Zélande [° ~ VHuYL pqHIKIlH LLHdK} {GYPTqdKH cH | D {aN CVL _`TVUHcH [C Il_aYPTIl_aOeHPY© nHtPH L] * cONP_`TxIrNP_`TE_`TVUON ilh L _gN * |! A_h {<tx_aH © d © ~ ~ Il VHT VHdKY CVL [H ¥ {VN * IRN ± NLHC} {G VN * ilh {'_ad + TVY * ien * T'HNPd D IrNZdKÊ¤_aTμUPHTVH LN * | Ð B ~ V_ad +} L YPpV | aH O% ~ + enpd pqHHT d · IC {{c_aH'_aTÅ®hN CVL HTxI ONOOa Ypp Ã Æ uY L {FRIRN ± © nN LH ~ VY C ~ dlHdAiXT|Ir VHA [YPTxIlHxI4Y * u! ²S³ ± htx_aH © d] Il ~ VH ± NZ {{V c_`Il_aYPTENP | ¢ UYP | a | `Y©! _ATVU|} GYP_aTZI ° de ~ ENMtPHIrY¤pGHIrNPÊPHT¬_aTxIlY'NP [[YC TZI'Å®N Æ iXTÂN| {c_gd · IL C de {IRH HT \ t \ _ L YPTEO0HTxIA_`TVuYL Oen * Il_aYPToHV [° ~ ENPTVUPHdOC d · IApGH »YZ} {cIl_aOe_H 'áp Æ iXTN' © wH p [YZTZIrH \ I] ¢ Ir ~ VH DKY CVL [hdaN LH »NC IrYPTVYZOeY C d ¥ NPTE {oir ~ C dA {cYμTEY * I <TEY * D Il_`u Il ~ VH _ L [° ~ * EN TEUPHd] N * Tq {{Â cYμTVYPI <} LYt \ _A {cH _`TxIrH L TEN * |} L YP} GH L Il_aHdÅ d C [° ~ Nzd ¢ YP_g {Vd Æ aA ~ VHT Nº {VN * IrNμdlY CEL [ H »_gdAOeYc {{c_`¼EH] SONIlh L _gN * | A_h {o²³ ± hÒtx_aH © d {{cH¼ETVH YPTÂIr ~ V_ad {VN * IRN DKY CVL [H¤ ~ ENMtPH »Ily HQI C} Ï {VNIRH {np [[Y L {c_aTVUZ | D _`T Y L {cH L Iry L HONP_`T [YPTqdK_gd · IrHTxI'z / dTVYPIl_g [H {_`T Å®z / pV_¾IrHpGY C | Ó ¢ OO` Z * Æ _¾Ie_gd4TEY * ILHNZdKYZTEN * pE | `HIrY LH [YPOe} C IRH ¥ Il ~ EH <© ~ VYZ |!` Ht \ _`H © RNI0HNP [° ~ DKY CVL [H C} {VN Ï! IRHP] xp C Iv_`Iv_gd.pqHIKIrH L Iry C DLH _`TE [L HOeHTxIrNP | \ ilh [° ~ ETV_af C HdNPd / Å®zpE_¾IrHpGY C | Ó ¢ OO` Z * ae Av | ¾J · D H {Bcn Ó ¢ O * O` y * PxyÆ pA zpV_`IlHpqY C | ONO * P * O` Ã Æ ~ ENZd} L {YP} GYZdlH'N * T|_aTE [LHOeHTZI ° N * | wOeNP_`TxIrHJ TEN * Tq [ H »NP |` UZY L _¾Ir ~ VO uYLONIlh L _gN * | A_h {t \ _ah © deuY L dlHOe_¾JXdKI .lrc [Í CVL H {{o VNJe N|pENPdlH ° {YPToIr ~ VH »{VN * IRN Oey \ {VH | V³ {å® V_¾ÎÏH LHTxInIl ~ ENPT²S³ ± h Æ NPTE {~ eil EHSf CH LRD | aNPTVUZNPUPH Hn VHY© nHtZH L]Il ~ VHf C H LRD YP} GH L NIr_`YZTEd] JIL ~ VH} GYZdrdK_apV | aH C} {VN ÏIrHYP} GH L NIl_aYPTqdN * TE { »Il ~ VH C} {G FRIlHOeHIl ~ VYC {¶uY L OYZTV | DNtPH LRD | `_aOe_¾IrH {d C pEdlHIY * Ui} GYZdrdK_apV | aH LHF CHd · I ° d YPTedKHOe_¾JXd · I LLC [I CEL H {4 {* VN Irne 1³; YLHYTPH L _A + Ir ~ V_gdN *} V} L YxNP [° ~ j] Il ~ VH <_`TxIrH L TEN * | _g {cHTZIr_¾¼qH L dY * u ¢ Ir ~ VH ¥ YPpV¿ · H [¨ I ° C dO d · I4pqH <ÊxTEY©! T¶IrY ± _aO0} GY L IIr ~ VH <{VN * IrN_aT'Il ~ EH ¥ ONJ ilh L _gN * |! A_h {t \ _ah ©] G © ~ V_A [° ~ »_AD / TVY * I« [YPTq [H_atN * pV | aH_aT; Il ~ EH4 [NZdKHYPuvN C IrYPTVYZOeY C ddlY CEL [Hd Ñ H [HTxIl | D _`T Ir ~ VH Ñ z / i · W / i! Ä} L Y * ¿· H [i] «Åv | ¾J · D H {Bcn ONO * y * O` PxyÆ ~ enpd ¥} L YZ} {qYxdKH ¬N¤OeHIr ~ VYC {{pENPdlH YPT Il ~ VH; {cH [YZO0} GYZdl_`Il_aYPT Y * uSNPT¬²³ ± h¸dKY CVL [H {VY \ [C OeHTxI C} i {VNIrH_aTxIlY¤pENPdl_g [<} L _aOe_¾Ir_`tZH C} {VN Ï * ilh YP} GH L NIr_`YZTEdÅ_aTEdlH L I{VH | aHIlH[° ~ * ENPTVUPHY u.NL IlI de C IlHY L H | aHOeHTxI Æ © ~ V_A [° ~;! NP}} V V | D Ily ¥ NPT_a {{} cHTxIl_`¼EH qYxdK_`Il_aYPT¶_`T ± Il ~ VH0²S³ ± hºI L HHZ / Y© wHtPH L Il ~ VHeNPdrd C Oe} cIl_aYPT Ir ~ FRIsil ~} VH0 GYZdl_¾Ir_`YZT ± Y * uwNPT C} {VN ÏIrH4_gd æ \ TVY! © Tº [NPTVTVYPI4N *} V} E | D IlY¶N C IrYPTVYZO0Y C d © nHp¬dKY CVL [HD] ©! ~ E_a [° ~ ° {VY »TVYPI +} L Ytx_g {~ Chir V_gd + æ \ _aTE {μYPu _`TVuY L Oen * Il_aYPTj iXTIl ~} E_adv FR *} GH L] Z © nH / [YPTEdl_a {VH L ONIRH L _aNP | `_H {et \ _`H © dYPT © NHP ¥ DLy CEL [Hd {{p cH¼ETVH D NPT ¥ ²S¹ CH LRD LHF CHdKIYPTeN «OEH {c_gNIry L M © L N *}} V GH L N L [~ ° V_`IlH [CEL HZ8Ä¤HNPdrd C ¨I OEH ¢ Ir ~ FRI.Ir ~ VH [YZTZIrHTxIrdY * uGIl ~ VH t \ _ah © d NLHd · Iry LH {¤_`T'N |! AY \ [N * | ² ³ ± VN * {HÅ IRN * pqNPdlHP B ~ V_gddlYP | C Il_aYPT| | PNAH {EdIlYI · © nYONP_`T'f C HdKIl_aYPTqd ¥ AKA Æ © ~ EN!I_`TVuY L Oen * Il_aYPT; _gd / TEHH {{VH uY L / C} {VN ÏIr_`TEU ¥ Il ~ EH + t \ _`H © d¬N * TE {'a yPÆ ~ VY© ilya} L Yc [HDLD C} {G FRIlHd YPTIl ~ EHOeN * ilh L _gN * |. A_h {Atx_aH © d iXT'Il ~ E_ad4} FR *} GH L] © nH <N * TqdK © nH L Ir ~ VHdlH <f CHd · Ir_`YZTEd0NPduYZ | `| aY© d AKA Æ © wH [| gNPdrdK_`u D Ir ~ VH <{V_¾ÎÏH L HTxI æ \ _`TE {C} EdSYPu {VN ÏIRHD «N * Tq {±}} L YP GYZdlH + NdKYZ | C Ir_`YZT; IlYON * _aTxIrN * _A ± Ir ~ VH0OeN * ilh L _gN * | A_h {; t \ _ah ©] ÏpENZdKH {± YPT Il ~ VHTVYPIl_aYPT ¥ YPuu L NPUPOeHTxI] \ N * Tq {± Å yZÆ dKIrN L Il_aTVU ©! _`Il ~ ~ AIl VH [NZdKHYPujt \ _`H © {{cH¼ETEH ¥ p DÏ YZ} qH LN * L ille] © wHdKI C {D Il ~ EHUPHTEH LN * |! J [NZdKHY * u t \ _`H © d {cH¼qTVH {p D Il ~ EH [YPO4pV_aTEN * _gdKYZTAYPu dlHtZH LN * |} GH LN IYPIry L d B ~ VH L Hd · I ¢ Y * UIL ~ V_gd ¢}} PEV qH L _AD ¢ Y L UXN * TV_H {¥ NPdwuYP | a | aY© d B ~ VH «TVHxIdlH [Il_aYPT ¥ UYC [C dldlHdnYZT C} {VN Ï * ilh TVY * Ir_¾¼G [NIr_`YZTe_`TcuY L ON * Il_aYPTeN * TE {b \ H [¨Il_aYPT {CHDL [L _apqHd1Il ~ VH! TN L _aY C d C} {G VN * ilh [NPdlHdÄ¤H [YZTE [| C {cH _`T ± b \ H [¨Ir_`YZTep D d C OeONL __`TVU0Ir ~ VH [L YZTZI de C Ir_`YZTEd! N * Tq {< {c_gdr [C drdK_aTVU0u CI CVL HLHdlHNL [° ~ 8N WB IWJ / Ã RNTI-E-3 434 QN * TVUPJW / UPYc [*] * EbVN TEd! NPTE {<hN CVL HTxI « ; 1 A *) 4) 4 B ~ VH _aTZIrH L u®NP [HwpGHI · © nH.! HT4Ir ~ VH! OEH {c_gNIry LN * Tq {Ir ~ VH! © L NP} V} GH L UYL f CH LLD _aTVUdKY CVL [Hd1 ©! _`Il ~ e²S¹ CH LRD »²S³ ± HA® QN * TUE * JXWUPYc [ «N * Tq {É« NL {VN L _A yPPZ * Æ ~ qNPdepGHHTH \ IrHTE {{cH μIrY'd C} E} qY L IeONIlh L _gN * | A_h {t \ _`H © ONP_`TxIlHTEN * Tq [HP BYIr ~ V_ad +}} CVL qYxdKHZ] 8²³ ± ^ HÅ / _ aH © LHUP_gdKI LNIl_aYPT|N * Tq {C} {G VN * ilh ¥ TEY * Il_`¼q [NIl_aYPT u L YZOIr ~ VHA © L N * E}} qH L ~ ENMtPHpGHHTμ_aTE [Y L} GY L NIRH {¶_`T'Ir ~ VH ¥ © LN *} V} GH L u C TE [Il_aYPTVTENP | `_`Il_aHd B ~ VH ¥ © NHP © L NP} V} GH L | `YxNP {Vdw © NHP <DLy CEL [HdnN * TE {} AÊPHH EdnTVH [HdrdlN LLD _aTcuYLONIr_`YZT »ÅHdldlHTxIl_gN * | a | D [° ~ VH [° C ECd ORN * TE {NQ Æ IlY4 [° ~ VH [° Ê0_¾u8dlY CEL [Hd. ~ ENMtZH [° ~ * EN TEUPH {edl_`Tq [ H | gNPdKIvIr_`OeH_`Iw ~ ENZdpGHHT ¥ | `YxNP {{cH] ZNPTE {{¥ dlHTE 0Ir ~ VH TVH [HdrdlN LLD {OEYC c_ `¼q [N * Il_aYPTIlY0Ir ~ VHOeH {c_gNIry L ³; Y L HYTZH L] E © nH0NZdld C OeHP] _A ± Ir ~ V_gddKH [¨Il_aYPT¤N * TE { »_` T|b \ H [Il_aYPTEd `aen * TE {y Ir ~ FRJe «Il ~ EH0t \ _ah © _gd {{cH¼qTVH + p D YZTVHvYZ} qH LN * Il_aYPT j} L YP¿ · H [¨Ir_`YZTj] LHD · IL _g [¨Ir_` YZTj][N L IrHdl_aNPT} L {Yc C [Iy L ¿· YZ_`T8 B ~ VHnUPHTVH L N * | [NZdKHYPujNt \ _`H © o {VH¼ETV_`Il_aYPTA_`T \ tPYZ | `t \ L _aTVUdKHtPH NP | VYP} GH L NIry L d _advY C Ir | `_aTVH {¥ _`TAb \ H [¨Ir_`YZT 1Ä¶HLHuHL Ily ± Å QSNPTVU * JXWUZY \ [Ó ¢ O * O` Ypp ZN Æ uY L Oey L H «{N ° Chỉ * _A | ad! » # B YE \ TVY© Il ~ VH_aTcuY L ONIr_`YZT <TVHH {{cH <Ily C} {VN ÏIRH «Il ~ VHT \ _ah ©] c © nH [YZTEdK_g {L cH Ir ~ VH} FR L NPO0HIlH L dwIl ~ qNI [° ~ EN L NIRH L _H / Ir ~ VHT \ _ah © ļ [YPOe} C IRNIr_`YZT; N * TE {<Il ~ EH + DKY CVL [H C} {VN ÏIRHP PAR {cY ¥ dlYE] V © nH «¼ L d · I_aTxI LY \ {C [H ~ Il VHTEY * Il_aYPTqd ¢ Y * u [YP |. A | aH [¨Ir_ `YZTEd! N * Tq {¥ u LN * UZO0HTxIrd $ &% q &% « ) (+ *, * ®.-0/21 (+ 31) 453! 6798: 45; qº <3 = / ZT?> A @ CBEDÕ¯Õ` D & Ó F® * _gdNdlHIY * u.²³ ± {h'CYC [C OeHTZI de d la LH} ~ LHdlHTxIr_`TVU0Ir VH dlNPOeHHTxIl_`ID niXTY * Il ~ EH L © wY L {NDLR] / N [YZ | `| aH [Il_aYPT_gdN¬dlHIY * u4YPpc¿ · H [Ird <~ ENMt \ _aTVU¬N dK_aOe_` | gN L DKI .lrc [Í CVL HZRVY LHcNPOe} V | `HZ] SN [YZ |` | aH [ Il_aYPT ¥ YPu8 | `_AP LNL _`Hdvdl ~ VY C | a {YZTV | D ~ qNMtPH | a_`p L N LLD YPpc¿ · H [Í ° d ÄÂ_`Il ~ <dlHOe_`JdKI .lrc [I CVL H {{EN ¥Irne] ~ PIr VH d · I llc [CEL Hd ¢ ¨I Y * U1i · © wYeYPpV¿ · H [Í ° d _aTNe [YP | a | `H [¨Il_aYPT;! [ N * TpqHHf CN * | YL TVY * I + Å®HP U ¥ N * TNC Ir ~ VY L YZpc¿ · H [I / [N * T ¥ HQI [YPOe} {GYZdlH ¶Y * u NA¼ L dKIlTENPOeH0YZTV! | ! D] 8 © ~ EH LHNPd «NPTVY * Ir ~ VH L _gd [YZO0} {GYZdlH ¶Y * u N |! · ANZd IrTEN * OeHZ] 8N * TE {|N ¼ L d · IrTEN * OEH Æ ZTG> A @ HBJI rÔLKPeEÓ _gd4NPTμ_a {{VHTxIl_`¼EH ºd C pVI LHHAYPu / N * Tμ²³ ± hai LHHP¤z » [YP | a | `H [¨Il_aYPTμ [N * TμpGH H_`Il ~ VH LN;! u C |` | © ¢ nHpμdl_¾IrHA © ~ VH L HAHNP [° ~ '} qN * UPHA_ad0N ± {CYC [C O0HTxI LH | gNIl_atPHeIrY ± Ir ~ VH <[YP | a | `H [¨Il_aYPT8] LN 1Y dK_aTVUZ |` H0} FR * UZHP]! Ï © ~ VH L H4HNP [° ~ »²S³ ± hºu L NPUPOeHTxI «NIIl ~ VHedlNPOeH + | aHtPH | L H} L HdKHTxIrd / NPT »YZpc¿ · H [* ISY uvIl ~ V_gd [YZ |` | aH [Il_aYPTj D Ì NPTEN * | D ~ dK_aTVUIl VHS²S¹ CH LRD + LHF CHd · IVILE ~ FR * Iw {cH¼qTVHdIl ~ VHStx_aH ©] x © nH / [VH NPTE {{C [H Ir ~ VH} FR * Il ~ ~ Il Edv_`T VH «{VN * IrNdlY CVL [HC dKH {p ~ D Ir VH LHF CHdKI1Ä¶H [ N * | a | QIL ~} V_gdv qNIl ~ <dKHI # ~ MμIr VH) NO IN Õ> A @ HBCI rÔLKPeEÓ B ~ V_gd ¢ u L NPUPOeHTxI! _Gd! N0d C Pci LHHY * UIL ~ VH «ILHH« [YP | a | `H [¨Ir_`YZT <YPuIr ~ VHdKY CVL [HP VY LHcNPOe} V |` HZ] \ | `HIC dnNP}} V V | D Il ~ VHuYP | a | aY! © _`TVU4²S¹ CH LrDeL Hf CHdKInNP}} V V | a_`H {¥ YPTAIl ~ VH²S³ »h¤ {VY \ [CJ OeHTxIY * u.1_`U CVL HeÃÅp Æ PRQTSJU: VWVTXZY [] \: VT ^ `_Ra2_0V2b: c + d & EtF \ [2gLh` \: V]. ^: _ Ra2_`i: h`aLj: kmlRnm_0cmh o`pmq`S` QGU: VmhmaLj: kml0nL_Rh2i: b: aL_: r <s] t`umv S`q`w2x: S`yGz2_`bRcTj0 \ 2k: {} | `U: Vmh`aTj: kml0n2_: h2 ~ 0VL_0cTkmX0af <b `|` U: V2h`aLj: kmlRn2_RhL ^ 0nmnT <^ CMHL Rn`nTh2kRVLkR \ mb:zmh2_: bRcTjR \ {2KR! PAR <{cHIrH L O0_aTEN * ~ IlHIr VH} V | pnb [HY * u ~ Il EHU LN * UPOeHTZI / LH} Nzd [ _adlH | D} Nzd GYZdrdK_apV |! AH + N * TE {Iry ¥ Oe_`TE_`Oe_H Il ~ VHeTVH [HdldrN LRD _`TcuYLON * Il_aYPT ± UYL «C} G {FRIlHd] © NHE [N * | g [C |! * AN IlH0 © ~ ENI © NH0 [N * | a | Ir ~ VH eÔ2F¯eÔ * + Gl & Õ B ~ VHONc_aON * | E} LH¼V4_gd.Ir ~ VH / | aYPTVU CHdKI} LH¼V [YZOeO0YZT0IlYIr ~ VH /} FR * Il ~ Edy * ~ uÏIl VH dKHIM / B ~ VHTj] ~ PIr VH Oe_`TV_aON * | u LN * UZOeHTxIwIrHOe} V | gNIrH_gd [L HN * ilh {¥ p D ¥ C dl_`TEU0YZTV! | D Il ~ VH C dlHu C |} FRIr ~ Ed! N * TE {<_ gd! N * TVTVYPIrN * ilh {~ Npd _aTÅkw VHT ONO * O` y * PZ * Æ Å®QN * TVUPJW / UPYc [Ó! ÔO` y * Z * Zp Æ IrYIrN ÊZHw_aTxIlYNP [[Y C TXI ONPTE {VNIry LRD N * TE {Nw B ivJÃ RNTI-E-3435 kw | aNZdldl_`u D _aTVUe²S³ »h'³ ± NIRH L _aNP | `_H {A ^ S_`H © UYL Ir ~ VH_ L ³ ± NP_`TxIrHTENPTE [H« YPTÄ¤Hp ± b \ Y CEL [Hd (a) (b) XML Fragment nameaddress 3, Adam str.Old Days adresse auteurs bibliothèque auteur LASTNAME livres année 1973 John catégorie de prix classique $ 7.4Only Vous titre Le titre commençant BookBook William firstname auteurs de la bibliothèque de l'auteur Paris XQuery Livres prenom titre de lastname préfixe maximal fragment minimal _aU CEL HEA .sSdKHu C | u LN * UPOeHTZI ypu NPT <²S¹ CH LlDAL Hf CHdKI YP} cIr_`YZTEN * |! c} qNIl ~ qd Å LHDL} j_aTdKYZ |! `_G {0N * TE {{0 cY * IlIlH {4 | a_aTVÊcd _`T1_`U CVL HSÃÅ®N ÆKÆ 1iXT4Il ~ EH} LHT \ _`YC d ²S¹ CH LRD HVN * Oe} V |! AHP] cnp | `| GIL ~ EH} qNIl ~ C qd dKH {¥ p D Il ~ VH L Hf C HdKIw ~ ~ qNMtPHIr VH «} FRIr ~_A <[YZOeO0YZTj] N * TE {<Ir ~ C d] \ Ir ~ V_A d} FRIr ~ _gd ¢ Ir ~ VHONc_`ONP |} LH¼V.-P \ M48 1 & 37 (8MJ4 / 21 & (3 B ~ VH0TVH [HdrdlN PDSL} {VN ÏIrH4_aTcuY L ONIr_`YZT ±} L Yt \ _A {{cH, p D Il ~ VHe © L N *}} cJ qH L Diry <TEY * Il_`u D NPT C} {VN ÏIrH0IlYIl ~ VHeONIRH L _aNP | `_H {; t \ _`H © È_gd 0Å_ Æ Il ~ VHe²³ ± h O0_aTV_aON * | 1u L NPUPOeHTxI ÅpGHuYLH ¥ NPTE {|Nu¯IlH LC} {VN ÏIRH Æ] ¢ Å_a_ Æ Il ~ VH} GYZdl_¾Ir_`YZT|Y * UVIR ~ VHU LN * UZO0HTxI_aT¤Il ~ VH ¥ [YP | a | `H [¨Il_aYPT'NPTE {± Ir ~ VH IL _`} E | `H ÔoxoÓ E N O * Ó ELOÓ9F®« ! ©! ~ VH L H $ # &% w_ad8Il ~ VH ¢ {VN * IRNI D} Ghw [YPTE [H L TVH {p ~ D Il EHndlY CVL [H C} i {VNIrH¶Å® [YP | a | `H [¨Ir_`YZTj] L 1u NPUPOeHTxI Æ] » $ & (% ¥ Ir ~} vhid GHAY * udlY CVL [ HC} {G VN * IlH¤Å_aTEdKH L Ir_`YZTj] {VA | aHIr_`YZTj] Oey \ {V_¾¼q [NIr_`YZT Æ] {8NPTE) * $% +, μIr ~} VHeYP GH L NIr_`YZT¶ {~ cH¼ETV_aTVUAIr VHet \ _`H © 'Å} L YP¿ · H [¨Ir_`YZTj] LHD · IL _g [¨Ir_`YZTj] [NL IrHdl_aNPT <} L {Yc C [I NPTE {4¿ · YP_aTj!] VdlHH + b \ H [¨Il_aYPT UYL Ir ~ VHUPHTVH LN * | [NZdKH Æ ³; YLHYTZH L] * N * T ¥ NPTEN * | D dK_gd.Y * UIR ~ VH / YZ} qH LN * L ille d.NP}} V qHNL _`TEU_`TIr ~ VH / t \ ​​_ah © Â {cH¼ETV_` Il_aYPT ¥ ~ ENZdHTEN * pV | aH {C + d ily ± p C _` | g {ºN ± [| aNZdldl_`¼q [N * Il_aYPTÂÅ®dlHH <TVH \ I4dlH [Il_aYPT Æ Y * u ¢ Il ~ VH {c_`ÎGH LHTZI0ON * _aTZIrHTENPTE [H <[NPdlHd4N * TE {~ ENPdNP | Adlyn * | a | ` Y© nH {C Dily; [° ~ EN L NZ [ilh L _H + Il ~ VHNZ {{V c_`Il_aYPTENP | ._`TcuYLON * Il_aYPT »TEH [HdldrN LRD UYL Ir ~ VHT \ _`H © ON * _aTZIrHTENPTE [HZZ / d © wH0dK ~ qN * | a | Il dlHH4_`T de ~ VH0TVHxIdlH [¨Ir_`YZTj] Iil ~ E_ad_aTcuY L ONIl_aYPT¶ [N * T »pqHe [YZO0} C {ilh] H_`Il ~ VH L u L YPO £ Ir ~ VHT \ _`H © Í_`IrdlH |` u YL u L YZO £ Il ~ VHdlY CEL [HP &.-? 0 / = '1 1 / NMT \ _aTVU ¥ [° ~ EN L NP [ilh L _H {Il ~ EH TVH [HdrdlN LLD _aTcuYLONIr_`YZT <} L Yt \ _g {{cH <p D Il ~ EH © L NP} V} GH L d] © c nHUP_atPHN * T YTPH L t \ _ah © YPuIr ~ VH © nN D t \ _ah © dNLH «ON * _aTxIrNP_`TVH {B ~ VH«} L Yc [HdrdnIlY ONP_`TxIrNP_`T <Il ~ EHtx_aH © ļ [TNP <TEY! © HQI {VH [YZOe} {qYxdKH A_aTxIlYeI · © nY0Y L Il ~ L HHdKIlH} Ed NPdnuYZ | `| aY© d az2 ÔOX OX D Ó9F®<Ï O *Ó9F D ÔÓ9F®4365SGÔ * O7 OFO I Ó98E! Od] F GÕ` aA ~ EHT4NPT C} {VN Ï * IlHnYPTIr ~ VH DLy CEL [H / ~ ENZdwpGHHT {VHIlH [IRH {<p D Il ~ VH «© LN *} V} GH L] Xil ~ VH« t \ _`H © ONIRH L _aNP | `_H L _gd ¢ TVY * Ir_¾¼qH {N * PGy C I Ir ~ VH C} {VN Ï * IlHP B ~ VHYZTV | D _`TVuY L NISO * Il_aYPTUP_atPHT; NJe! Il ~ qNIIr_`OeH_gd ¢ Il ~ EHI L _`} E | `H: O * Oo * Ó; E N = <ÔOx. ÉroÓ9F®! .I ¢ NPdlH {+ YPT4Il ~ V_gd IL _A} V | `HZ] * _` I_ad} GYZdrdK_apV |! AHnIlY {cHIrH L Oe_`TVH ¢ © ~ V_g [° ~ {V {ENP c_`Il_aYPTqN * | _aTcuY L ONIr_`YZT <_gd ¢ TVHH {{cH <_ aTYL {L cH IrYOeNP_`TxI ° N * _aTAIr ~ VHT \ _`H © y 5 ""F¯ Ó & F® * GO * I ° FöÓ9F®GÓ9F®iOÕ! ± CTNSP CHdKI0p C _` | g {L cH f CHL _aHd4Ir ~ VH; DKY CVL [L HAY Ir ~ VH t \ _ah © Ë_`IrdlH | `u + N * PGy CI < Il ~ VH¤HtZHTxI CN * | SNZ {{V c_`Il_aYPTENP | / _aTcuYLONIr_`YZTÂTVHH {{cH IlYμON * _aTxIrNP_`TÂIr ~ VH t \ _ah © d «Ä¶H0TVY * IrHIr ~ ENJe «_`T» Il ~ VH [YPTxIlHxIY * uv © nHp¶dKY CVL [HD] q_`I «_ad_aO0} GY L IrNPTxI / IlYOe_` TE_`Oe_H Ir ~ VHANZ [[HdldIrYIl ~ VH <DKY CVL [Hd B ~ C d] 1 ©! ~ ~ VHT¶Ir VHANZ {{V c_`Il_aYPTENP | _`TVuYL Oen * Il_aYPT¤_ad +} LHdlHTxI_`T Ir ~ VHT \ _`H ©] c © wH {VY0TEY * I f CH LRD Il ~ EHdKY CVL [Hd Ñ WB IWJ / a RNTI-E-3 436 QN * TVUPJW / UPYc [*] * EbVN TEd! NPTE {<hN CVL HTxI nfo Ó 8VmF® N O * N O O03 Öl nOOxo 8VmF® <znI0Il ~ V_gded · I ° N * UPH <Y * u ~ Ir VH} L Yc [DRPH] © nH! Æ \ TVY© rir ~ VHIL _A} V | `HZ] N * TE {¬Il ~ EH; _`TcuYLON * Il_aYPT TVH [HdrdlN LLD UYL} LY \ [H! drdK_aTVU¤Il ~ EH C} {G FRIlHZ Ì ¢ NZdKH {|YPT'Y CEL N * TQN * | D dl_gdY * uHNP [° ~ 'Ï {VN [NZdKHZ] Ir ~ VHAt \ _ah © C}IRH LHF CHd · I4 [N * TμpGHAUZHTVH LN * ilh {{NPTE <} L Yc [HdrdKH {~ iXTIr VH¤NL [° ~ E_¾IrH [¨ Je CVL HP] © nH »} L YP} GYZdlH Ir ~ VH» UYZ | `| aY©! _ATVU¬ [YPOe} qYZTVHTxI ° d ¥ uY L Ir ~ VH¶} L Yc [Hdldl_aTVUºYPu C} {VN ÏIRHD z r N O o8o * OK * o7] * _ ATY L {cH L Ilyse \ TVY © |! © ~ V_A [° ~ + YP} GH L NIly L Å®Y L dlHf C HTE [hvy * uVYP} GH L NIry L d Æ _AD C dKH {~ ¤_`T¶Il VHAtx_aH © {cH¼qTV_¾Ir_`YZTj B ~ VHeYPIl ~ VH L [YPOe} GYPTVHTxI_adN l N O Ó NFO] 1_`T|YL {cH L ily f CH LRD Ir ~ VHwdlY CVL [H YL Il ~ VHvtx_aH © © AA®! ~ VHTTVHH {{cH Æ]NPTE {Ily [YPOe} C ~ IrH.Il VHv²S¹ CH LRD / C} {VN Ï * ilh LHF CHd · I Il ~ FR * I_ad / d · Iry LH {{ANPTE } LY \ [{HdrdKH ANZ [[YL {V_`TVU4IlY ¥ NEDL} qH [_`¼EH {} GYP | une_une [D Å®dlHHHTE {< Y * ub \ H [Il_aYPT y aÃ ® « ! £ 1 11 ± 2) 0} Ä¶HA LHdKHTxIY CVL OeHIl ~ VYC {|y * u t \ _ah © OeNP_`TxI ° N * _aTEN * TE [HP] C dK_aTVU »NPT|HVN * Oe!} V | AHP PAR» {VY ± dKYq] © nHe¼ L d · ILH [NP | `| iu L Ypoá QSNPTVU * JXWUZYc [NPTE {É« NL {VN L _`T y * ZP * Æ Ir ~ VHTVY * Y * Ir_`YZTEd ¢ u1² nh | * aN Il_aYPT;! N * Tq {A² B8c} E | `HZ 5"" Å®QN * TVUPJW / UPYc [ «N * TE {É« NL {VN L _A yPPP * Æ}} LHdlHTxIrdIl ~ VHS V ~ D dl_g [N * | qN * | aUPHp LN de la ²z | aUPHp LNpqNPdlH {~ eYPTeIr VH LH | aN * Il_aYPTENP | .YP} GH LNIry L dS {{cHdK_aUPTEH »UYL ²³ ± h B ~} EHdlH0YZ qH LN * Ily L d« NL Il [N * | a | aH {}> VLObœufOb ~ V_gd N * | aUPHp L NeNP_`Od ¢ Iry ¥ [YPTqd · Je llc [IHcH [C Il_aYPT} V | gN * TQD ¢ u YL Ir ~ VHHtMNP | C NIl_aYPTYPu1²S¹ CH LLD N * TE {} L Yc [HdldlHd IC} E | `HdAÅ® [N * | a | H` {> N O ÏÕ` Æ Y * UVI LHH s d · I llc [CEL HdiXT'N * ¨I T|² Ñ H | gNIr_`YZTj] {cYZON * _aTEdN L He²S³ ± h I L HHD * Y u.UP_atPHT <} qNIl ~ ± dlHI ° d] qNIKI L de C IrHdÅ [NP | `| aH {²znIKI L _AP C ilh Æ NLH²Sm.N * Il ~ Ed LHuHLHTE [_aTVUeTVYc {chd _A <Ir ~ VH ²S³ çais, ILHHd VNP [° ~ e²znIKI L de C IlHS [N * TepGHOC | ¾Ir_¾JtN * | CH {;! © ~ å® VHTLHuHLHTE [L _aTVUdKHtPH NP | Vd C PVJ ILHHd Æ] YLHOe} D cI © ~ VH å®! TLHuHLHTE [_`TEU / TVY «C d Pci LHH Æ 1² Ñ H | aN * Il_aYPTEd NL hvy L {VH LH {[YP | a |` H [¨Il_aYPTqd8YPuE² BC} V | AHD] © ~ VH L HAHNP [° ~'² B8c} E |! `H_ad0 [YZOe} {qYxdKH'y * u²mNIl ~ ºTEN * OEH {ºNIKI L de C IRHD]. ©! ~ VYZdlHAtNP | CHd LHuHLHTE [H d C pCI LHHd_aT; Il ~ VHe [YZ | `| aH [Il_aYPT; YPu1I LHHd / z / dNLHd C | `I] QIL ~ VH0dl [° ~ * VHONY uvNPT; ² Ñ H | * aN Il_aYPT ± _gd / ypu ID} GH Å « ! #% $ Æ]©! ~ VH L H & L 8ª d.N HnIl ~ EHNIKI L de C IrHd.NPTE { » 8ª d. [YPOe} GYZdlHnIl ~ VH!} FR * Il ~ ~ dKHISYPu1Ir VH²³ ± h'ILHHd ¢ 1_aU CVL H y dl ~ VY© DNPT;! ² Ñ H | aN * Il_aYPT ± NPTE {I · © wYYPu._¾I ° d / m² B8c} E | `Hd Ä¤H + TVYPIlH Il ~ EN * Isir ~ VHedKH [YPTq { »NL IlI de C IlH4_adO C | ¾Ir_`tN * | C N * ilh { »_` T »Il ~ VH4¼ L d · I« ² BC} V |! AHP] © ~ EH L HNPdIl ~ V_gd «NL IlI de C ilh _adHOe} D cI _aTIl ~ VHdlH [YPTE {<} ² BC V | AHP XT uplet personne ville nom de numéro de voiture Mary Berlin 3710 personne XT uplet / voiture / personne couleur / nom personne / numéro personne / numéro ville personne voiture cityname colorcolor John Paris rouge vert 4242 XAttributes Partie arbres Partie 1_aU CVL H y .² BC} V | AHP Ñ WB ivJÃ RNTI-E -3437 kw | aNZdldl_`u D _aTVUe²S³ de la h'³ ± NIRH L _aNP | `_H {A ^ S_`H © UYL Ir ~ VH_ L ³ ± NP_`TxIrHTENPTE [H« YPTÄ¤Hp ± b \ Y CEL [Hd VY L Il ~ EH <DRN * ÊPHYPu | `_gdl_`pV_a | a_¾I D] 1 © nHAdl_`Oe} V | D © L _`IlH <² BC} V |! AHd4NPdY L {VH L H {'dlHIrd + Y * u tN * | CHd0dlHIrd] _`UZTVY L _aTVU LHuHLHTE [Hd|iI_ad0_aOe} qY LI ° N * TxI + IrY¶TVYPIlH <Il ~ EN ~ * IEIL V_gdedK_aOe} V | `_`¼q [NIl_aYPT¬_`Oe} V | a_aHd4Il ~ FR * I © wH [YZTEdK_g {~ cH L Il EHSTN * | CHd_aT ¥ Il ~ EH / | aHNMtPHdY * uILHHd Ä¶HLHuHL Ily <Å QSNPTVU * JXWUZYc [Ó ¢ * O` Ô y * P ZN Æ UYL Ir ~ VH UPHTVH LN * | V [NPdlHP] P_aTE [| C {V_`TVU_aTxIlH L TENP | VTVYc {chd hjHI pGH / N * Te² nh | gNIl_aYPT {{cH¼ETVH ep D N * TeHc} L Hdrdl_`YZT Y * u1Il ~ VH²z | aUPHp L N0YZTdlY CVL [Hd WY©] © nH_`TxI L Yc {C [H> N ÏÕ` Qo & F n OA² B i · Q ® wz Tº² B i · QË_gd0N¶} ENP_ L 9O N D N> N 5N GÕ`! VNP [° ~ ² B8c} V | `H / u L YZO Il ~ VHdKY CVL [Hd._aT \ tPYZ |` TZH {~ e_aTIl VH [YZTEd · Je llc [¨Il_aYPTYPujYPTVH / ² BCJ} V |! ahy * u _gdNPdrdKYc [_Une * ilh {4 © _`Il ~ ¥ N «{c_gd · Ir_`TE [Iv² B i · QOA &> N ÏÕ` N NLH ¢ UPHTVH LNIRH {4_`TE [L Hoeh TxIrNP | `| D Æ B ~ V_gd! ² B i · QÈ_adNZdldlYc [_gNIRH {IrYHNZ [° ~ <²znIKI L de C ilh + Y * u1Il ~ VH² B8c} E | `H + dlY0Il ~ EN * I / N * | a | j_`Ird! L ²znIKI de C ~ IlHd ENMtPHIr ~ VH + DRN * OEH «² B i · STE T4 [H« Il ~ V_gd! ² B i · QÈ_ad! ¼VcH {<u YLHNP [° ~ ²SznIlI L de C IlHYPu1HNP [° ~ ² BC} V | AHP] E_`I [NPTVTVY * IwpqH / [° ~ * qN TVUZH {4_` T ¥ NPT D ​​YP} GH LNIr_`YZTeNP}} V V | a_`H {4IlYIr ~ VH² nh | gNQ Il_aYPT8 C} V | a_g [NIrH² BC} V | aHdv ~ ENMtPH {c_gd · Ir_`TE [I / B i ² · Q4 Ñ Huh LLL _aTVU0IlY0Il ~ VH² nh | gNIr_`YZT_`T1_`U CVL H y] \ Ir ~ VH «I · © wYe² B8c} E |` HdNLH «© L _`IKIrHTNPd Å,! % (!%% +! Y y! Æ N * TE {|Ã ! P! Ã! % +, Æ INH | aY© L H în «HVN * Oe} V | AHD ¢ YPu1² nh | gNIr_`YZTEd! [YZO0} C {ilh Au L YPORI · © wYdlY CVL [Hd N * TE {* ! # ""$% &"" & ') (+ $ * ! &, .- 0/132 « ) 4 65.787 1) 9;:! 0 <3 65787 1 => => 5787 1 !? « ! @ $ ""57871) A, B & CD"" D (E5 7GF 1 5 1 7GF! H3I JK 5 7GF 1! L ') # 9'! M5 7GF132 « ) 4 657BN 1) 9;: 0 <3 657BN 1 => => 1 57 milliards !?! « ! $ @"" 57BN1) A ""D! / 5 1 5 7BO 7BO 1! H3I JK 5 7BO 1! P) (Q, E 65 7BO R"" D ( . + * $ R ""D (.) RS"" -1HIJK 5 F 7 1T « DCD5 F 71U3U) U3U 5! F8F 1 <) '! et R5) F8F V m L YP¿ · H [Il_aYPTAYPuW YPT $ &% +,%! $ % + X X; + $ &%, 6¶; 3! # ""$%; 3 &!"" D3 & '3 (3; # *;! 1 $) 2'!) 4 65787 1 39;: G <3 65787 1 => => 57871 A, E & CD ""D (E5 7GF 1 5 7GF 1H IJK 5 7GF1) 2) 4 1 657BN 39;:! G <3 657BN 1 => => 57BN1 A "" ! / 5 7BO 1 5 7BO 1HIJK 5 7BO Vn HdKI L _A [Il_aYPTY * uY YPT $ &% +, 6 [Z PP; 3! # ""$% &"" & ') (+ * $; 3 ! &, ES / 1) 2 « ) 4 65787 1) 9; 0 <3 65787 1 => => 5787 1! « ! @ $"" 57871) 2 ') 4 65 7BN 1) 9;: 0 <3 65 7BN 1 => => 5 7BN 1'! ! $ @ ""5 7BN V Y \ ZYP_aT <YPuW NPTE {YPT $ &% +"", 6CN * TE {+, 6¤ ! # ""$% &"" & ') (; 3 ! # *; $ ^]; R "". D (+ * $; 3 &, ES / ""D () - 0"" -1. ) A, B & CD ""D (E57GF 1 57GF 1HIJK 57GF _ N7 1 L '! # 9') 657GF 1T ""DCD5! F 71) A"" D! / 5 7BO 1 5 7BO 1HIJK 5 7BO_ N7 1P) (Q, E 65 7BO 1T « DCD5 F 7 Va` nknN L IlHdK_gN * T} LY \ {C [¨IYPub NPTE {~ Å_aTIl VH LH d C | `Il_aTVU² n H | gNIl_aYPTYPu1N¿ · YP_aTYL ypu N [NL IlHdK_gN * T <} L {Yc C [] qN * L T²SznIKI de C + IRH [TNP <pGHNPdrdlY \ [_ aN * ilh {¥ ©! _`Il ~ Oey L HSiR ~ EN * TYZTVH «² B i · Q Æ` 3! # ""$%; 3 &!"" D3 & « 3 (;! 3 # *; $ 3! et, .- 0 / R 2 '34 M5787 1 39 ""(D () RS"" -1 + D * $ R""..);: 0 <) M5787 1 => => 5787 1 !?) $ c »! 5787 1HIJK 5 F 7 1T ""DCD5) F 71! A, E & RC"" D (E5 7GF 1 5 7GF 1HIJK 5 7GF 1! L '! 9'! 65 7GF 1 H IJK 5 F 7 1T « DCD5 F 71) 2 '34 M57BN 1 39; 0 <) 1 M57BN => => 1 57 milliards !? « ) $ c "";! 57 milliards 1HIJK 5 F 7 1T""! DCD5) F 71 A ""/;!!! 57BO 1 57BO 1HIJK 57BO 1 P 3 (d, B 657BO 1HIJK 5 F 7 1T « DCD5) F 71) 2 '34 M5 787 39 1; 0 <) M5 787 1 => => 5 787 1 !? ') $ c »; 5 787 1U3U) U3U 5 F8F 1 <)'! & R5 F8F1 A, E & RC"" D (E57GF 1 57GF 1 H IJK 57GF 1! L '! +9'! 657GF 1U3U) U3U 5! F8F 1 <) 'Et R5) de F8F1) 2 '34 M5 7BN 1 39;:! 0 <) M5 7BN 1 => => 5 7BN 1 !?) $ c ""; 5 7BN 1U3U) U3U 5 F8F 1 <) 'et R5 F8F1 A!"" /;! 57BO 1 57BO 1HI JK 57BO 1! P 3 (d, B 657BO 1U3U) U3U 5! F8F 1 <) '! & R5) F8F Ñ WB IWJ / A RNTI-E -3 438 QN * TVUPJW / UPYc [*] * EbVN TEd! NPTE {<hN CVL HTxI & 1 1. ""/ =!"" # Ä¶HdKI C {DN * | a | C} i {VN * ILH «[NPdlHdw_aTZI L Yc {C [H {A_`Tb \ H [Il_aYPT y aÃPaÃP ³; YLH} LH [_adlH | D] \ © Nhs [YZTEdl_a {L cH Ir ~ VH UYP | a | `Y© _aTVU [NZdKHd UYL Il ~ VH4I D} qHeY * UWH |!! AHOeHTxI «IlYOeY \ {V_¾u D êà_ Æ N <© ~ VYZ |` H0²S³ ± hºu LN * UPOeHTZI] Å_a_ Æ N} EN LI / YPu.NPTHc_adKIl_aTVUAdl_aTVUP | Ahh | aHOeHTxI] Å_a_a_ Æ NeO C | `Il_atN * | CH {; H | `HOeHTxI] GN * TE {'Å®_`t Æ NTEYPTcJHc_gd · Ir_`TVU H | aHOeHTxI B ~ VH C} {G VN *} IlHID QHD ¢ © wH «[YZTEdl_a {cH LNL HS_`TqdKH L Ir_`YZTj] c {cH | aHIr_`YZTNPTE {{¥ OEYC c_` ¼q [N * Il_aYPT¶Å1HtPHT Il ~ VY C UP ~ N »{OEYC c_`¼q [N * Il_aYPT [N * T¬pqH; dlHHT¬NPdeN¤ {VA | aHIr_`YZTμuYZ | `| aY© wH {uP D N * L T¬_`TqdKH Ir_`YZT Æ μ1_aTEN * | a | D] HtZH LRD H | aHOeHTxI ° N LRD tx_aH © YP} GH L NIr_`YZTA_gddKI C {{c_aH DLH} FR L N * ilh | D & $% T61.3 = / 21 & 7; 1 & 3;} 43n * ® B ~ ~ VHOeHIr VYC {AuYL _g {cH TxIl_`u D _`TEU ¥ N * T² BC} V | AHP] EHtPHTN * C u¯IlH InDo [[Hdrdl_`YZTAY * u ² S} GH LNIl_aYPTqd ¢ ~ ENMt \ _`TVU [+ YZO pV_aTVH {¤ [YZ | C O0Tqd] j {cH | `HIlH {» Y L {C} V | une_une [NIlh { »² B8c} V |` Hd «_adSIr ~ VH0uYP | a |` Y! © _aTVU 0Å®N Æ u L YPO (Ir ~ VHD C [J [Hdldl_`YZT; Y * u²S} GH L NIr_`YZTEd] q © WH4 [N * T ± {{C VH [HIR ~ VH + _aTV_`Il_gN * | L NPTVUPHYPu.HNP [° ~ ± [YP | C OeTj] u L Æ áp YZO Ir ~ VH ² B i · Q4] j © NH0 [N * T¤ÊxTEY! © Ily © ~ V_g [° ~ ¶_aTV_`Il_gN * | .² BC} V | aH ¥ N * L T¶²SznIlI de C IlHpGH | aYPTVUxd0_aTENP | ` | D] © nH [N * T _A {VHTxIl_`u D Ir ~ VH «YL _`UZ_`TENP | I² BC} V | aH_aTdK} V_`IlHYPujIl ~ EH O0_`A_aOe} V | `_ah {<p D Il ~ EH²S} qH LN * L ille dWYPIlHIl ~ qNI p DH \ IlHTE {~ c_aTVUIr V_ad ¢ OeHIl ~ VYC {] \ © nH [YC | g {¥ N * | gdlY4 {{cH C [H ©! ~ V_g [° ~ ¥} EN LI ¢ Y * ujHNP [° ~ ¥ YL _aUP_aTEN * | paresseusement CVL [H ~ qNPd pqHHTC DLH {_A <Ir ~ VHT \ _` H © d $ T318` $ / m1, (+ 3 (4 798`4 7; qμ.3 = / b C} V} GYZdlHeNATVH © u LN * UPOeHTZI LH} {LHdlHTxIlH; p ~ D Il VH0TVH © E² BC} V | aH _AD «Nouvelle-Zélande {V {{cH ± _`T|dKY CVL [Hc zT'² B i · QR_gd [YZO0} C {ilh ¤uYL Il ~ EH ¥ TVH © ² BC} V | aHANPTE {{¤_ad4NP V {{cH ¶IlY; HtZH LRD ²SznIlI L de C ilh ¥ YPuwIr ~ ​​VH ²IC} V | AHP 8 :( .-R / m1, (+ 3 s / TV} LY * ¿· H [ilh {4 [YP | C OeTEd1Y * uEIr ~ VH ¢ © ¬² BC TVH} V | aHNLHLHOeYTPH {+ NPTE {Ir ~ VH ¢ © ¬² BC TVH J} V | aH_adNZ {V {{cH ¥ IrY4Ir ~ VH L Hd C | ¾I² Ñ H | gNIr_`YZTj VY L HVN * Oe} V | aH Ir ~ VH ± _aTEdlH L Il_aYPT Y * u ¥ Å! P! ! , (, Æ _aTZIrYa UP_atPHdnIl ~ VH «UYZ |` | aY! © _`TEU4² nh | gNIr_`YZT 3! # ""D $ 3 @! &"" 3! & « 3 ( ! 3 # *; 132 '34 65787 1 39 $;: 0 <) 65787 1 => => 57871) A, ! E & RC « D (. 57GF 1 57GF 1HI JRK 57GF132 '34 65 7BN 1 39;: 0 <) 65 7BO 1 = >! => 5 7BN1) A "") / 57BO 1 57BO 1HI JRK 57BO1 4' ! @ $""! 5 7 1 5 7 1 I » 5 7 \ 2 / L8: 1, -0 / 21, (+ 3 # B ~ EH» ² BC V} | aH »_gdNP { V {{cH oIlYμIl ~ VH LHD C | ¾Ir_`TVUº² nh | * aN Il_aYPT_`uIl ~ VH¶} {LH c_g [N * HLI »_gd dLn * Il_gd · ¼qH {VY LHVN * Oe} V |! AHP] _aTEdlH L Il_aYPTμYPu0Å% +%% $% $ P Ã y * & $ ! & $ {Æ cY \ Hd0TVY * I ¥ [° ~ ENPTVUPH] N * TE {¤Ir ~ VH ¥ _aTEdKH L Ir_`YZT|Y * ua! P! ! , (, Æ _aTxIlY UP_atPHdIr ~ VH UYP | a | `Y! © _aTVUe² nh | gNIl_aYPT P 3! # ""D $ @ &"" & "") ( + * $ &, .- 0/132 '34 65 787 1) 9;: 0 <3 65 787 1 =>! => 5 787 1 !? « $ @"" 5 787132 '34 657BN 1) 9;: 0 <3 657BN 1 => => 1 !? 57 milliards!! $ @ ""57BN1 4' ! @ $ ""! 57 1 57 1! I!"" 57 1! L ') # 9'! M57 ') 48` / \ 21 et 453' 8 :( 61 R / iXTAIl ~ V_gd [NZdKHZ] cN * |! A | G² B8c} V | `H d (  L uY NP | `| I² BC} V | AHD _`T ~ ENMtPHSIlY pqH4_`TqdKH L IRH {~ _aTZIrYIl EHT \ _`H © B YA [L HNIlh «Ir ~ VHdlH² B8c} E |` Hd] G © wH + _A {D cHTxIl_`u ² B8c} V | `HdpGH | aYPTVUZ_`TEUIlY _A <Ir ~ VHT \ _`H © Í_`IrdlH | `u ·] C dl_aTVU0Il ~ VH² B i · QSD Ñ WB ivJÃ RNTI-E-3439 kw | aNZdldl_`u D _aTVUe²S³ »h'³ ± NIRH L _aNP | `_H {A ^ S_`H © UYL Ir ~ VH_ L ³ ± NP_`TxIrHTENPTE [H« YPTÄ¤Hp ± b \ Y CEL [Hd VY LHcNPOe} V | `HZ]! Il ~ EH¶_aTEdlH L Il_aYPTYPu <Å! P! ! , (, Æ _`TxIlY UP_atPHd / Ir ~ VH0uYP | a | `Y! © _aTVU² nh | aN * Il_aYPT `BY; [YZO0} C ilh`] j © NH0 ~ ENMtPHe [YZTEdK_g {{cH LH ± N * T¤² BC} V | aHe_`T Il ~ VH} _`Il ~ eN L Iw © <² B i · Q ÃPÃSN * TE {¥ DLH | aH [ilh {AN * | a | G² BC} V |! aHdw © ~ VH L H + Azas N * E}} QHn L d_aTj B ~ VH [YP | C OeTEd NPdrdKYc [_Une * ilh {Aily «YPu8Ir ~ VHdlH² BC} V | AHD [YPTEdKIl_`IC ilh« Il ~ EH [YPTxIrHTxI! Y * uY * `3! # ""$%; 3 &!"" D3 & « 3 (;! 3 # *; $ 3! et, .- S / R ""D (+ * $ R"". (D) RS ""-1.) 2 '34 M5 787 39 1; 0 < ) M5 787 1 => => 5 787 1?) $ c ""5 787 1HIJK 5 F 7 1T"" DCD 5 F 71! A, E & RC « D (E57GF 1 57GF 1HIJK 57GF 1 L '! # 9') 657GF 1HI ! JK 5 F 7 1T « DCD5) F 71) 2 '34 M57BN 1 39; 0 <) 1 M57BN => => 1 57 milliards?) $ c ""5 1HIJK 57 milliards! F 7 1T"" DCD5) F 71! A ""/;! 5 7BO 1 5 7BO 1HIJK 5 7BO 1P) (Q, E M5 7BO 1HIJK 5 F 7 1T"" DCD5 F 71) 2 '34 M5787 1 39; 0 <) M5787 1 => => 5787 1? ') $ c ""5787 1U3U) U3U 5! F8F 1 <)'! et R5) F8F1! A, E & RC"" D (E5 7GF 1 5 7GF 1HI JK 5 7GF 1 L '! # 9') 65 7GF 1U3U) U 3U 5 F8F 1 <) 'et R5 F8F1) 2 '34 M5 7BN 1 39;:! 0 <) M5 7BN 1 => => 5 7BN 1 ') $ c ""5 7BN 1U3U) U3U 5 F8F 1 <)' et R5 F8F1 A!"" /;?! 57BO 1 57BO 1H IJK 57BO 1P) (Q, E M57BO 1U3U) U3U 5! F8F 1 <) '! & R5) F8F1 4') $ c ""! 5 7 1 5 7 1! 3I! ""5 7 1L""! N ° 9) 65 7 1HIJK 5 F 7 1T"" DCD5 F 71 4' ) $ c ""! 57 1 57 1! 3I!"" 57 1 L « ! # 9 ') 657 1U3U) U3U 5! F8F 1 <)'! & R5) F8F (1 et 3 Ä¤H¼ L dKIS [YZO0} C ilh + u L YZO'Ir ~ VH + t \ _ah ©] QIL ~ VH0dKHISYPuvN * | a | 8² BC} V | aHdSu L YPO [Ir ~ FRIO C dKISpGH ¿· YP_aTVH {A ©! _¾Ir ~'Ä¤HS {C c_gdKIl_aTVU _gdl ~ ¥ I · © nY + [NZdKHd ¢ Å · Ã Æ iuIr ~ VH LHD C | `Il_aTVU4dlHIn_adwTVYPInHO0} VI D] Xil ~ VHSTVH © ² BC} V | aHdwN LH ¢ [LHN! IRH {0NPd _aTeIl ~ VH [NL IlHdl_gN * T0} L {Yc C [¢ Å yZÆ / ~ Il VH L © _gdlHP] * | aYZNZ {c_`TEU _AD. TEH [HdldrN LRD ily ¥ [YPOe} C IRH «Il ~ VHn¿ · YP_aT ©! _¾Ir ~'L VY HcNPOe} V | `HZ] GIL ~ VH0_`TqdKH L Ir_`YZTEd / YPuSÅ%% +,! Z! Ã! , (, Æ å® [NPdlH <AKA ÆKÆ] N * TE {μY * u + Å%%% $! P% $ ! ZPP & $, (,% $ Æ Å [NPdlH »Å yPÆKÆ _aTZIrY UP_atPHdIl ~ VH <UYP! | a | aY! © _`TVU ² Ñ H | * aN Il_aYPT P ³ ± Y L HYTPH L] Il ~ VH ± _aTEdlH L Il_aYPToY * ua! Z! ! Æ {cY \ Hd TVY * I / [° ~ * qN TVUZH ZWYPIlHIl ~ qNJe «~ ENZdnIlYepGH |` YxNP {{cH ¥ Iry L HNP [° ~ ¥ Ir ~ V_gd [YZTE [| C dl_`YZTj; 3! ! # »! $% & ""& ') (3 ​​# *; $] R"" D (.! # *; $ 3 &, .- 0 / R ""D RS (.)"" -1 A, E & CD ""D (E5 7GF 1 5! 7GF 1HI JRK 5 7GF _ N7 1! L '! 9'! 65 7GF 1T ""DCD5 F 71! A""! / 57BO 1 5 7BO 1HI JRK 57BO_ N7 1! P 3 (d, B 657BO 1T « DCD5) F 71 (M57 1 57 57 1HI JRK _ N7 1! L '! +9'! 657 1T « DCD5) F 71 -8) 5 7 1 5 7 1U3U3U) U 5 7_ NF 1! L « ! +9' ! ! 65 7 1 <) 'et R5 F8F $ * ® </ 21 & (3 (4 798 7:. 45; Gμ.3 = / b C}} V GYZdlH; N; u LN * UZO0HTxIË_gde {cH | aHIrH {ºu L YZO DKY CVL [H] vNPTE {~'Ir FRJe Ë_gd L H} L {HdKHTxIlH uP D Ir ~ VH ² BC} V | aH ! © _`Il ~ |² B i · Q G B ~ * EHT¤IlYON _aTxIrN * _aT¶Il ~ VHet \ _`H ©] NP | `| .² B8c } E | `Hdd C [° ~ ¶Il ~ qNI + NI | aHNZd · Je YPTVHYPu8Ir ~ VH_ L ²SznIlI L de C ilh ~ ENZdnIl ~ V_gd ² B i · Q +] EN LH {VA |! `HIlH {Au L YPORIl ~ VH² nh | gNIr_`YZTj VY LHVN * Oe} V | AHP] c | AHIC d / [YPTqdK_g {L cH Ir ~ VH {VA | aHIr_`YZTu L YZO Y * UIL ~ VH «u LN * D UZOeHTxI 3Â,! % (!%% +! Y y! Æ '1_ L DKI] n © wH_g {cHTxIr_¾u DN * | a |! ² BC} V | aHde_aT¬Ir ~ VH t \ _`H © ¬ONI ° [° ~ V_aTVU «Il ~ VH ¢ u LN * UZOeHTxI] fMD] Xil ~ VH ¢ I · © nY« ² BC} V |! AHdv © _¾Ir ~ 0² B i · QSdÃPÃN * Tq {{AÃ kwYPTEdl_a VH L _`TVU YPTVHYPuIr ~ VHdlH² B i · QSD] VuYLHVN * Oe} V | aHÃPÃZ] V © wH LHOeYtPHNP | `| j² B8c} V |` Hd_aTIl ~ VHT \ _ah © ~ ENMt \ _`TEU0² B i · Q ÃPÃ_aTeIl ~ EH_ L ²znIKI L _AP C IlHd B ~ VH LHD C | ¾Ir_`TVU + ² Ñ H | aN * Il_aYPTEdvY * uÏIl ~ {V_gdn cH | aHIr_`YZTeuYLHNP [° ~ ² S} GH LNIr_`YZT [! YZTEdK_g {{H cH L A_aTIl ~ V_gd HVNPO0} E | `HZ] VN L ~ Hdl VY! © TpqH | `Y© V Ir ~ VH L Hd C | ¾Ir_`TEUedKIrN * IlHYPu Nu¯IrH L {cH | `u HIl_aTVU L YPO ; 3! # » $%; 3 &! ""D3 & « 3 (;! 3 # *;! 1 $ A, E & CD"" D (E57GF 1 57GF 1HIJK 57GF1) 2 « ) 4 65 39 1 7BN;: G <3 65 1 = 7BN > => 5 7BN1 A ""/ 57BO 1 57BO 1HIJK 57BO Vir ~ VH LHD C |! ¾Ir_`TEUedKIrN * ilh ypu Nu¯IlH L {cH | aHIr_`TEU u L YPO; 3! # ""$% &"" & ') (+ * $; 3 ! &, ES / 1) 2 « ) 4 65 7BN 1) 9;: 0 <3 65 7BN 1 => => 5 7BN 1!? « ! @ $"" 5 7BN Ñ WB IWJ / Ã RNTI-E-3 440 QN * TVUPJW / UPYc [*] * EbVN TEd! NPTE {<hN CVL HTxI Va` Il ~ VH LHD C | `Il_aTVUd · I ° NIlHYPu `N * u¯IlH L {VH | aHIl_aTVU u L` YZO 3! # ""$%; 3 &!"" D3 & « 3 (;! 3 # *; $ 3! et, .- 0 / R ""D (. + * $ R"" D (.) RS ""-1! A, E & RC"" D (1 E57GF 57GF 1 HIJK 57GF 1! L '! 9'! 657GF 1HIJK 5! 7 F 1T « DCD5) F 71 ) 2 '34 M57BN 1 39; 0 <) M57BN 1 => => 1 57 milliards !? « ) $ c»; 5 ! 7BN 1HIJK 5 F 7 1T ""DCD5) F 71 A!"" /;! 5 7BO 1 5 7BO 1HIJK 5 7BO 1! P 3 (d, B 65 7BO 1HIJK 5 F 7 1T ""DCD5 F 71! A, E & RC"" D (E57GF 1 57GF 1HIJK 57GF 1! L '! +9'! 657GF 1U3U) U3U 5! F8F 1 <) '! & R5) F8F1 ) 2 '34 M5 7BN 1 39; 0 <) M5 7BN 1 => => 5 7BN 1 !? ') $ C ""; 5 7BN 1U3U) U3U 5 F8F 1 <)' et R5 F8F1 A!"" /;! 57BO 1 57BO 1HIJ K 57BO 1! P 3 (d, B 657BO 1U3U) U3U 5! F8F 1 <) '! & R5) F8F V ~ Il EH LHD C | `Il_aTVUdKIrNIrHY * u Nu¯IrH L {cH | `HIl_aTVU u L YPO! # ""$% &"" & "") (;! 3 # *; $ ^]; R"" D (. + * $; 3 &, ES / ""D - 0 (.)"" 1) A, B & CD ""D (E57GF! 1 57GF 1HIJK 57GF _ N7 1 L '! # 9') 657GF 1T « DCD5! F 7 Ä¶H ¢ TVYPIlHnIl ~ qNI.OeY \ {V_¾¼q [NIr_`YZTEd1Y * UEU L N * N UZOeHTxIrd L IOE L HNIRH {+ p {D cH | aHIr_`YZTEduYP | a | `Y© nH {+ p D _aTEdlH L Il_aYPTEd $ T318` / m1,3 !;. * / M1,3 !; 436 (61 et 7 1,3 !; 453 * ® \ μ.3 = / T31 <8` / 21 & (3 # Ä¶H » [YPTEdl_g {L cH Ir ~ VH »[NZdKH;! YPu_aTEdlH L Il_aTVUμN|TVYc {~ cHμÅ® © V_A [° ~ [N * TopGHIl ~ VH LY \ Y * I ¥ ypu« N © ~ VYP |! AHD C Pci LHH Æ3_`T|N * T¤Hc_gd · Ir_`TEUAu LN * UZO0HTxI Y * uir ~ VHdKY CVL [H% C dKH { »p ~ D Il EH0² B8c } V | `H ! © _¾Ir ~ |² B i · Q GeiXT »Ir ~ V_gd [NZdKHZ] © wH¿ C DKI ~ ENMtPH4IlY, Nouvelle-Zélande {{V ± Ir ~ VHTVH © ÈTVYc {cHe_aT¶ Ir ~ VHdKHIY * u ¢ TVYc {chd «ypu Il ~ VH²SznIlI L de C IlHdw_g {{cHTZIr_¾¼qH ¥ p ~ D Il EHS² B i · Q G .WYPIlHSIl ~ EN * Dans} LH¼Vd C ½e0O C d · I ¢ PGHS [YZTEdl_a {{cH LH] HQI [NC dKHZ] FR * | a | ²SznIlI L _`p C IlHd LHuHLHTE [_`TEU4Ir ~ VH} FR * Il ~ OC dKIpqH C} {G VN * ilh {} VY LHVNPO0 E | `HZ] Z_aT ^] c [YPTEdl_a {VH L Ir ~ VH / L _aTEdlH Il_aYPTY * uH | `HO0HTxIn [YZ |` YL% / _ `~ tîr VHU LN * UZOeH TxI] PuYL © ~ V_g [° ~ <~ Ir VHOe_`TE_`ON * |! ju L NPUPOeHTxI Nu¯IlH L} L Y * ¿· H [¨Il_aYPT <UP_atPHd Å D,! % (!% +% +! Y y! Æ 1i D {cHTZIr_¾u D _aTVUNSOeN * Ir [° ~ V_aTVU « ² BC} V | aHÅ®dlHH / b \ H [¨Ir_`YZT y `a Æ]© nH ¢ dlHHwIl ~ qNI.Il ~ EHn_aTEdlH L Il_aYPTeNIih [Il Ird ~ VHe² B8c} E | `Hd« _`T¶Il ~ VHet \ _ah © ~ ENMt \ _`TVU <i ² B · QdeÃPÃeN * TE {ºã + z / DSIL ~ EHdlH0² / IC} E | `HDNL H + Hf C NP | Il _`T ~ VH4t \ _`H ©] E © nH «IRN * ÊZHYPTEHY * u ~ Il VHO] EuYLHVNPO0} E |` H «Ir ~ VH YPTEH ~ ENMt \ _`TEU² B i · QRÃZÃP] GN * TE {{V © NHNP {Ir ~ VH tN * | CH =% «_ aT <Ir ~ VHdlHIÅ% (!% +% +, Æ .Ä¤HYPpcI ° N * _aTAIr ~ VH «UYP | a |` Y! © _aTVUe² nh | gNIl_aYPT8 3! # ""D $ 3 @! &"" 3! & « 3 ( ! # *; 132 '34 65787 1 39 $;: 0 <):; 5787 1 => => 57871) A, E & RC « D (1 57GF 57GF 1 HIJK 57GF132 '34!.! 65 7BN 1 39;:! 0 <) 65 7BN 1 => => 5 7BN1) A "") / 57BO 1 57BO 1 HI JK 57BO Ä¶H0TVYPIlHIr ~ FRI_`T »Il ~ VH0 [NZdKH4Y * uH | aHOeHTZI ° d] ÏOeYc {c_`¼q [N * Il_aYPTEdNLHILHN * ilh {; p D _g {D cHTZIr_¾u _aTVU ¥ Ir ~ VH H | aHOeHTxI_aTIl ~ VHTN * | C HdlHI / N * Tq {L} H V | pnb [D _`TEU0_`Ip Il ~ EHTVH © tN * | C * HP / m1, (+ 3 Ä¤H / [YZTEdl_a {L cH Il ~ EHS {cH |. `HIl_aYPT ¥ YPuNTVYc {+ Å © cH ! ~ E_a [° ~ A [N * TepGHIl ~ EH L YxYPIY * u8Nd C Pci LHH Æ u L YZO N * THc_gd · Ir_`TEU + u * L N UZO0H TxI Ey * u8Ir ~ VHdKY CVL [H C dKH {Ap D Ir ~ VH «² BC} V | aH! © _`Il ~ ² B i · Q q ~ ixT Il V_gdv [NPdlHP] * © nH ¢ ~ ENMtPHnIrY {VA |! AHIrH ¢ Il ~ VH TVYc {cH ¢ u L YPOÈIl ~ VHdlHIYPuGTEY \ {VHD Y * ~ uGIl EH²SznIlI L de C IlHd_a {cHTxIl_`¼EH {p ~ D Il VH0² B i · Q _A ± Il ~ EH L _`UZ ~ xI [YZ | C O0T8iuIl ~ {VH0TVYc cH0_ad «NP |` YZTVH_aT ± Il ~ EHedlHI_`T¤N <[YP | C OET C DLH {} NPdSN LH {c_g [N * IlH_aTIl ~} VH4²S qH LN * Il_aYPTj] GN * | a | j¼ETqN * | 8² B8c} V | `Hd / ~ qNMtx_aTVU ¥ NPT;! ²SznIKI L de C + IRH © _`Il ~ ± ² B i · Q OC dKISpGH LHO0YTZH {WYPIlH4Il ~ EN * I] ~ VH L H0N * UZNP_`T8] q} LH¼E * d C ½0OC d · I «pqHe [YZTEdl_a {cH LH {] GpqH [NC dKH N * | a | j²SznIlI L _`p C IlHd LHuHLHTE [_`TEU + Ir ~ VH} FRIr ~ O C dKIpqH C} {G VN * ilh {VY L HVN * Oe} V | AHP]_A @] * © wH ¢ [YZTEdl_a {L cH Ir ~ VH {VA | [YZ aHOeHTZI | |! aHIr_`YZT + YPuqH. `YL%%, A_`T4YPTVHnYc [[CEL HTE [HY * u ¢ Il ~ EHU L NPUPOeHTxI ¥ Å,! !% (%% +, Y + y Æ ixT `] ~ 8IR V_ad4 {VA |! AHIr_`YZTμNIih [Ird YPTVHSY * UIR ~ VH / m² B8c} E | `Hdw ~ qNMtx_aTVU² B i · QÈÃZÃ / YL Ã © SIJ Nhs [° ~ EYxYxdKH + APAP] Pil ~ VHTA © {Nhs cH | `HIlH / Il ~ VHSH | aHOeHTxI Ñ WB ivJÃ RNTI-E-3441 kw | aNZdldl_`u D _aTVUe²S³» h'³ ± NIRH L _aNP | `_H {A ^ S_`H © UYL Ir ~ VH_ L ³ ± NP_`TxIrHTENPTE [H« YPTÄ¤Hp ± b \ Y CEL [Hd [YZ | `YL% +% +, §_`T¬HNP [° ~ ¬²IC} V | Ahl © ~ EH LH <Ir ~ VH H | aHOeHTxIX »~ ENPd4Il ~ EH² B i · Q7ÃPÃZ] * TE vN {¬UPHI4Ir ~ VH UYP | a |` Y! © _aTVUe² nh | gNIl_aYPT `` Z 3! # ""$%; 3 &!"" D3 & « 3 (;! 3 # *; $ 3! et, .- S / R ""D (. + * $ R"" D (.) RS ""-1) 2 '34 M5787 1 395 787 1 => => 5787 1?) $ c ""5787 1HIJK 5! F 7 1T"" DCD5) F 71! A, E & RC « D (E5 7GF 1 5 7GF 1HIJK 5 7GF 1L""! n ° 9) 65 7GF 1HIJK 5 F 7 1T « DCD5 F 71) 2 '34 M57BN 1 39; 0 <) M57BN 1 => => 5 7BN 1) $ c ""1HIJK 5 F 57 milliards 7 1T! ""DCD5) F 71 A""/;?! 57BO 1 57BO 1HIJK 57BO 1P) (Q, E M57BO 1HIJK 5! 7 F 1T « DCD 5) F 71) 2 '34 M5 787 1 395 787 1 => => 5 787 1?) $ c ""5 787 1U3U) U3U 5 F8F 1 <) '! et R5 F8F1! A, E & RC « D (E57GF 1 57GF 1HIJK 57GF 1 L '! # 9' ) 657GF 1U3U) U3U 5! F8F 1 <) '! & R 5) F8F1) 2 '34 M5 7BN 1 39; 0 <) M5 7BN 1 => => 5 7BN 1 « ) $ c"" 5 7BN? 1U3U) U3U 5 F8F 1 <) 'et R5 F8F1 A ""/;!! 5 7BO 1 5 7BO 1HIJK 5 7BO 1P) (Q, E M5 7BO 1U3U) U3U 5 F8F 1 <) '! & # R5 F8F ""!"" # # A & 1 iXToIl ~ V_gdAdlH [Il_aYPTj] ¢ © nH YC Ir | `_aTVH Ir ~ VH ± [NZdKH, © ~ EH LHIL ~! VH ± t \ _ah ©, _AD <{{cH¼qTVH op DN' [YPO4pV_`TqNIl_aYPToYPu ²s} qH LN * L ille d¸iXTÂY L {L cH IrYμON * _aTxIrN * _aTÂ_`I <©! ~ VHTÍdKY CVL [Hd [° ~ * qN TVUZHP] ! © nH ± OC dKIA_aTxI LY \ {C [H Ir ~ VH TVY * u * Ir_`YZT|Y 8F ""DN 0 Õ ¥ Ó z £ [YZ | C O0T|_gd «ON L EPH {~ ¶ V_g {{V cHT¤ © ~ EHT'N} L YP¿ · H [Il_aYPT¤H |! A_`Oe_aTENIRHD _¾I] j © ~ V_A |! `He_`I_ad C DLH {¶_`T¤NA} L {H c_a [NIrH4_aT¶²S} GH L NIry L D4A L Npd HdKI L _A [Il_aYPTj] IY L ¿· YZ_`T Æ 4W / Y * Il_g [H4Il ~ qNJe _¾uNe ~ V_g {{V CHT ± [YZ | C OET; {cY \ HdTVY * E / NP}} V QHn L _aTIl ~ VH4 [| a_`HTZI / NP}} V V | une_une [NIr_`YZT C dK_aTVUeIl ~ EHtx_aH ©] Vil ~ ~ VHTIl V_gd [YZ | C ¢ OeT_gd OeNPTV_a} C | gN! Ilh {N * Npd | a | Y * Ir ~ VH L [YP | C OeTEd! ©! ~ VHT <~ ONP_`TxIrNP_`TE_`TVU0Ir VHT \ _`H © 1_`U CVL H Å®N Æ dK ~ EY© d0N' {VHIrNP_` | aH {¬HVNPO0} E | `HY * T * uN ²Sz / |` UZHp LN¶ILHHLH} LHdlHTxIl_aTVU |N|t \ _`H © {{cH¼qTVH »YPTI · © nYAdKY CVL [Hd NPTE {ON *} V} GH {Iry ¥ Ir ~ VH + ² Ñ H | gNIr_`YZTEd NPTE {L} Hdl qH [¨Ir_`tZH | D VY L Il ~ EH »dlNPÊPHY * u | a_adl_apV_` |! A_`I D] YPTE | D Il ~ VH} FR LI <Y * u «² Ñ H | aN * Il_aYPTEdA ~ ENPdpGHHTLH} {LHdlHTxIlH] p ¢ CI 1_`U CVL H Å®p Æ NPTE {± 1_aU CVL H Å [Æ dK ~ EY© ~ EY© ~ iL VHedKH [YZTE {}'A LHDL jSIl ~ V_ L {Æ ² BC} V | aH0Y * u LHDL} j Æ _AD LHN * | a | D + L H L} HdlHTxIlH {1_aU CVL H Å {Æ dl ~ VY© DIL ~ VH / DLH [YPTE {} E² B8c V | `HSY * uÏIl ~ EH¼ETENP | Q² nh | gNIr_`YZTj TZH LRD | aY© nH L [NPdlH + | aHIlIlH L «L H} L HdlHTxI ° d / N {c_adKIl_aTE [IdlHIY * vtN P | CHd LHuHLHTE [Hd] ÏNPTE {; TC O4pqH L dYPT Il ~ VH L _`UZ ~ xI «dl_a {vhen L H4² B i · Qd0s }} V GH L [NPdlH4 | aHIlIlH L d LH} LHdlHTxISTqN * OeHd «YPuw [YZ | C OeTEdeÅ fMD # * VÕ O98 O TIL OO F®O Æ RWYPIlH¶Il ~ EN * I <_aTIr ~} V_adHcNPOe V | `HZ] © nH¶ ~ ENMtZH» {C} E | `_g [N * ilh L Y© d¶Å y NPTE {{YPT Æ NPTE eO C | `Il_atN * | C NIRH {TEY \ {VHD / æ Æ .Q CVL _aTVU²S} GH L NIr_`YZTEdÅ L Hd · I L _g [¨Ir_`YZTj] ¿· YZ_`Tj] x} L Y * ¿· H [¨Il_aYPT Æ] [YZ | C * TE OeTEdvN {L Y© d N L H [YZO + pV_aTVH {] P {VA |! `HIlH {0y L [L HNIRH {] Pp C I_`I_adNP | `© ¢ N D} GYZdrdK_apV | aHwIrY [YPOe} C ~ Il ilh VH N.O IN ö [YP | C OeTEd iXTAY CEL HVNPO0} E | `HZ] x_aTIr ~ VH / ¼qTEN * | G² Ñ H | aN * Il_aYPT] \ © Nhs [TNP LH [YZTEdKI .lrc [¨ IIr ~ VH C dlHu C |} eN L IAYPuIr ~ VH Ir ~ V_ L {Â_aTV_`Il_gN * | / ² B8c} V | `Hd 1_aTEN * | a | D] ~ V_g {{V chti [YZ! | C OeTEd <[NPTN *}} V NGH L Nu¯IrH L} LY * ¿· H [Il_aYPTj B ~ VH DNLHLH} {LHdlHTxIlH A_aTULHD YPT <Il ~ EH «¼EU CVL HP iXTºÅ®QN * EVP * JXW UPYc [Ó! ÔOa Ypp xN Æ] Ï © WH4 {CHDR [L _`pGH ~ VY© IrYAd C} V} qY L I «[YZO + pV_aTENIr_`YZTEd! Ypu ²s} GH L J N! Ir_`YZTEd _A <Ir ~ VHT \ _`H © {{VH¼ETV_`Il_aYPTpENZdKH YPT <Il ~ VH «¼ETqN * | jt \ _ah ©] ENPTE {<Il ~ VH dl_`Oe} V | aHYP} GH LNIr_`YZTEd ¢ _A <Ir ~ VH t \ _`H © o {VH¼ETV_`Il_aYPT8 B Y + {cYdlYE] Z © wH LH} V | ANZ [H ¢ _aTIl ~ VH IL _`} V | aH / {Nzd cH¼qTVH {e_aTAb \ H [Il_aYPT y `ÃZ`ÃZ]"" * $ &% +, p D %% V ©! ~ VH L H%% V_gd ¢ Ir ~ VH²S³ ± h LHF CHd · Je ixty CVL HVNPO0} E | `HZ] Vd C} E} qYxdKH« © nH _`TEdlH LIN0TVH © iu L NPUPOeHTxI! _`TIr ~ VHdKY CVL [H LH} {LHdlHTxIlH ¥ p ~ D Il VHS² BC} V | aH4Å; & $ ​​& $% $!! % $ Æ .z} V} E | D _aTVUIr ~ VH /} L Y * ¿· H [¨Ir_`YZT0YZT! !! `RUZ_`tZHdÅ & $ & $ * & $ Æ B ~ VHTμNP}} V V | D _`TVUIr ~ VH¿ · YZ_`T|OeHNPTEdIr ~ ENI + u L YZOÇIl ~ EH¼ETENP | V² nh | gNIl_aYPT8] © Nhan L HN * pV | aHIrY L Hp C _A | a {Il ~ VH ! [YP | C OeTEdY * u ² de B8c} E |! `Hd + ~ © VH L H 8EY L HVN * Oe} V | AHP] 1 © wH <dKH | `H [IIR ~ VH N * TE { [YZ | C OeTEdnYPu² BC} V |! AHDN © _`Il ~ ² B i · Qa y] cN * TE {A © wHUPHIwIr ~ ​​VH} FR * _ L d « Å! Æ N * TE {¶Å;! Æ Il ~ qNI N L HS [+ YPO pE_`TVH {¥ ©! _`Il ~ ¤Å & $!% $! % $ Æ vb \ Y + ¼ETEN * | a | D] Xil ~ VH «² B8c} E |` HdvIrY + pGHNP {V {{cH eIlY4Il ~ EH / ¼ETqN * | C \ _`H © NLHÅ & $! % $! % $! Æ N * TE {¤Å% $! & $! & $ Æ Å bcHH_aTEdKH L Ir_`YZT <_A;! 1_`U CVL H Å®N ÆKÆ Ñ WB IWJ / Ã RNTI-E 3 442 QN * TVUPJW / UPYc [*] * EbVN TEd! NPTE {<hN CVL HTxI 12 12 12 12 pe rs sur / n um r pe rs être sur / c ar pe rs sur / un ge pe rs sur / n am e personne nom de la couleur de voiture de couleur voiture numéro John rouge vert 4242 (b) pe rs sur / a ge pe rs sur / c ar pe rs sur / n um être rw ou k / em pl oy ee w ou k / sa la ire 12 12 2412,24 personne travail couleur de voiture couleur de voiture rouge vert 30000 $ salarynumber employé 4242 (d) R4 P21 D21 Q21 r22 q22 t22 u23 h23 V23 W24 H24 x24 MNO B11 C11 D11 f12 G12 H12 J13 k13 L13 f14 G14 H14 N15 O15 p15 CB D R3 b11 c11 d11,21 p21 f12 f12 f14 h12,23 g12 g14 g12 h14,23 h12,24 f14 g14 x24 q21 v23 u23 u23 w24 w24h14,24 x24 v23 OMD, NCB R5 b11 c11 d11,21 p21 f12 G12 h12,23 f12 f14 G14 h14,23 G12 h12,24 F14 G14 u23 u23 W24 w24h14,24 BCD, Nouveau-Mexique Vue: finale XRelation RF a11 b1 1 c11 d11 e12 f12 G12 H12 I13 J13 k13 l13 e14 f14 G14 H14 M15 N15 O15 p15 ABCD Source S1 R1 MNO p21 d21 q21 r22 q22 t22 u23 h23 V23 Y25 S25 Z25 W24 H24 x24 Source S2 R2 23 23 23 (a) D = NO <42 B, C, DB, C, D, M Y16 G16 h16,23 u23 Y16 G16 h16,24 W24 w ou k / TIT le w ou k / sa la ire w ou k / em pl oy ee salaire 30000 $ titre principal des employés de travail 4242 (c) insertion d'insertion A16 Y16 G16 h16 1_aU CVL H .zT;! HCH [C Il_aYPT} V | ANPT <© _`Il ~ ² Ñ H | gNIr_`YZTEdN * Tq {A² · B i + Q) 42:) 0' "";: <.: = 0,7) 4 ¥ iXTxIlHULN * Il_aTVU © NHP ± _aTcuYLONIr_`YZT; ~ enpd / HQI [YPOeH + NPT CVL UPHTZITEHH {j] ÏNPTE {IrNPÊ \ _`TVUA_`Tx Ily <NP [[Y C TxIIr ~ VH dK} GH ​​[_¾¼G [_`Il_aHd.YPuG © nHpe_aTcuY L ONIl_aYPT0_gdN «[° ~ EN * | a |` HTVUPHZ8iXTE {{VHH j]! P © nHpdKY CVL [Hd NLH dlHOe_¾JXdKI .lrc [Í CVL H {] Il ~ VH; DLy CVL [Hd + [N * TºpGH CTLHNP [° ~ * qN pV | aH <NPTE {~'Ir VH D u LHF CHTZIr | D [° ~ ENPTVUPH <p C I ¥ {cY¤TVY *} I0 L Ytx_g {CHN * TD _`TVuYL Oen * Il_aYPTN * PGy CI! Ir ~ VH_ L [~ ° ENPTVUPHd ixT ± Ir ~ V_ad}} PEV qH L] Ï © nH + ~ ENMtZH +} L {YP} GYZdlH »NAdlYP | C Il_aYPT; uY L L N _`TxIlHUIr_`TVUA © wHp¶dlY CVL [Hd_aTZIrY <N <OeHJ {c_gNIl_aYPT'N L [° ~ V_`IlH [Í CVL H0pENZdKH { »YZT¶ONIRH L _aNP | `_H {± t \ _ah © s} {VN ÏIrH_`TVuY L Oen * Il_aYPT|N L HdKHTZI «Iry <Ir ~ VH OEH {c_gNIry L NPd.NPT4²³ ± hOe_`TE_`ON * | Vu L NPUPOeHTxI B ~ VHT0 © nH ¢ ~ ENMtPHnUP_atPHT0N «[| aNZdldl_¾¼G [NIr_`YZT4YPuGNP | `| E [NPdlHd Y * uGOeNP_`TxIrHTENPTE [hny * uEON * ilh L _gN * | A_h {t \ _ah © D1P D [ YZTEdK_g {L cH _aTVUIr ~ VH C} {VN ÏIrHvI D} GHP]Ir ~ VH {VNI ° NI D} qH ¢ [YZTcJ [H L TVH {Ap ~ D Il VH C} i {VN * IlHN * Tq {¥ Ir ~} VHYP GH L NIr_`YZTEd ¢ {~ cH¼ETE_`TVUeIr VHT \ _`H © Ä¶H0N L H + [Celll HTxIl | D _aOe} V | aHOeHTxIl_aTVU <Y CVL N *} V} L YZNZ [° ~ Il ~ ~ LYC UZ ± NOeYc {C | AH4 [NP | `| aH {> A @ HB F® I l O 8E IrY0pGH_aTZIrHULN * ilh {<_ ~ ATAIR VH «u C | a | 8²S³» h|O0H {c_gNIl_aYPTNL [° ~ V_`IlH [CEL H²Sh8_`tZHP ixT ¨I »Ir ~ V_gd«} FR *} GH L] © Wh0 ~ ENMtZH0 [YPTEdl_a {VH LH {± ON * L ilh _gN * | A_h {;! t \ _ah © d «© ~ {VYxdKHe cH¼qTV_¾Ir_`YZTEd« {* cYTVY I_aTcJ [YL} GY LNIrHnNPT D4L H [YZTEd · Je llc [¨Il_aYPT4} V ~ ENZdKHZ / Y© wHtPH L Ir ~ V_ad.uHNJe CVL H de _AD [YPOeOeYPTV! | DC DLH {0_aT²S¹ CH LRD LHF CHdKIrd_aT¶YL {L cH Iry <YC Il} CI «Il ~ VH LHD C | ¾Ir_`TEU {VN * IrNNP [ [YL {c_aTVUIrYNAuYLONIdk} GH [_¾¼qH {± _aT¶Il ~} V_gd V ~ ENZdKHZzSdIr ~ V_ad +} + qYZ_`TxI _gd_aOe} qY LI ° N * TxIuYL tx_aH © £ {cH¼ETV_`Il_aYPTj] © Nhan LH [CVLrL HTxIl | D DKI C {D _aTVU Ir ~ VH _`Oe} ENZ [¨IYPuIr ~ VH LH [YZTEd · Je llc [¨Il_aYPT <} V ~ ENZdKH «_aTY CVL N * E}} L YxNP [° ~ j * .S2 z / pV_¾IrHpGY C |! Ð] 1b`] 8³ »[UP C ~ j] \ q ! a] Ñ D d] ³'`] 8 ^ NPdrdlNP | `YXD] Ï ^ + a] NPTE {± ÄÂ_aHTEH L] \ EjhvÅ · Ã P Xae iXTq [LHJ OeHTxI ° N * | W³ »N * _aTZIrHTENPTE [H ¥ UYL ³» NIlh L _gN * | A_h ¤ {^ S_`H © d + YTZH L b \ HOe_adKI LRC [I CVL H {'QNIrne; ixT nl D / iO 8V vqÓ; <I N ^ O98 ¨EÓÐÕI = B] Z Nw B ivJÃ RNTI-E-3443 kw | aNZdldl_`u D _aTVUe²S³ »h'³ ± NIRH L _aNP | `_H {A ^ S_`H © UYL Ir ~ VH_ L ³ ± NP_`TxIrHTENPTE [H« YPTÄ¤Hp ± b \ Y CEL [Hd kw ~ VHTj] 1a] + \ xN * UxNP {c_gdK ~ 8] V ^ 4`] Gh8N * ECDL ~ * VON TENPTj] ch V ^ 4`] GN * TE {} m.NP FR L _YZd] \ B8a yPPPxÆ L YPO B ¢ LHH mwnIKIrH L TEdwIlY ¥ ÉHTEH L N * | A_h {B ¢ L HHmwNIKIrH L TEd ST .½ ¥ [_`HTZIvtMNP | C NIl_aYPT <Y * u1²S¹ C H LRD 1iXT nl DTO I Ó 8 ¨EÓÐÕI = B] y * ZP QN * TVUPJW / UPYc [*] B JB qNPTE {± É «NL {VN L _aTj] GÉ01Å y * ZPxÆ! 1H {cH LN * Il_aTVU ~ VHIrH L YPUZHTVHYC dqnI ° NAb \ Y CVL [Hd ©! _`Il ~ ²S³ »h.iXT nl D. I 5I] yPPP QN * TVUPJW / UPYc [*] B JB a] jh8N CVL HTxI] Q4`] jNPTE {¶bcNPTEd] ^ + Å Æ ³ Ypp ZN »N * _aTZIrHTENPTE [H4Y * u ¢ ³ ± NIRH L _aNP | `_H {^ S_`H © d4YZT¬QS_adKI L de C ilh {¬b \ Y CVL [HdÅ¯IrY¤NP}} V GHNL Æ BH [° ~ ETV_a [N * | LH} qY LI] hjilknmÈJ «de / TV_atPH L dl_¾I D ypu kwH LUD JXM YPTxIrYP_gdKHZ QN * TVUPJW / UPYc [*] B JB a] Wẽ« NL {VN L _aTj] e0 a] * TE vN {B8L NMtPH L d] W! Å Æ B Ypp Zp ¢ LHHÉ LN *} ~ V ¬ ^ / _ aH © ST ½ [_`HTxI tNP | C N * Il_aYPT ¥ YPu8²S¹ C H LLD _A <N * T <²S³ ± h¤³; H {c_aN * ille L jiXT Ô O O4 * EO 5ÔD O 5 ÓXAÔ V ° O *! ] Y * Z E QL NP} qH L] * Q4a] / NP | `Ht D] Pz + 4a] PN * TE {4Ä¤H | g { ] ZQ4PbÏEÅ y * P Ã Æ B ~ EH ¢ W / _`O4pV | aHniXTxIrHULNIr_`YZT vTVUP_aTVHP \ ixT @ D * 5 @ RE DF®Ô * a + ¨EÓXl O Ó ¢ ¤ l N * @ ÔGÔ2KZqÓw iQOxo *! ] \ TPYP | C OEH Z] Ypp až v | ¾J · DH {Bcn] ³μ`] 1Ä¤NPTVUE] ha] .Q / _aTVUq] hv`] N * Tq { ! nC TE {cHTEdKIlH_aTVH L] 1zwÅ y * ZZyPÆ <z / Tºz | aUPH p L NP_a [z /} V} L YZNP [° ~ ¤uY L iXTE [L HOeHTZI ° N * | V³ »N * _aTZIrHTENPTE [HAY * u³ ± NIRH L _aNP | `_H {¤²S¹ CH LRD ^ S_`H © diXT nl D / iO 8V N o 8 QO O;O 8V; C IöÓ9F®¬Ô * ÔOxo @ ÔGÔ2KZqÓ% A @ <! ] YPPZy É C} VIrNV] Iza] h {C RDNP [° ~ VH L] EÌ`] ïI ¢ N .lrc] Iksa] G ^ .H | a_`Ê \ ~ VYtI] `iM] i³ ± NL [_gN * Teye] Ñ a] * TE {în ± m.NP} * EN ÊZYPTEdKIrNPTxIl_`J TVY C] 4! AKA PZZÆ» ²S³ »hJpENPdlH {'_aTcuYLONIr_`YZTμOeH {c_gN! Ir_`YZTº © _`Il ~ ³; iX² + |iXT @ nr D 5 @; @ EÓÐÕ. * I * @ ÔÏÔLKxEÓ! IQOxo] Le juge PZ hN CVL HTxI] SQ + a] Sh8H [° ~ xIlHTxpY L UPH L] \ Ea] Sb \} D \ L NIlYxd] W + `] N * TE {^ YZdrdKHTj] SÉ0Å Ypp Ã Æ e³ ± YPTVYPIlYPTE_a [kwYZO0.} E |` HO0HTxIrd UYL iXTE { } {cH qHTE VNPTZInQSN * IrNÄNLH ~ VY C dlHd EÓÐÕ! c N iOOi I B]% # » Å Æ] * y pÂp ÄÂ_aH {L ~ cH EYP | g {] ZÉ0EÅKÃ PxyPÆ V³;! H {c_gNIry L d ~ _`T4Il EHz L [° ~ V_¾IrH [I CVL H ¢ Y * u C I H CVL! IXTcuY L ONIr_`YZTeb D dKIlHOed! N ox] $ $ Å Xae] Ã PZY <:! IXTÂIl ~ EH ± | aNZd · IDHNL d] n ~ © _¾Ir AIL ~ VH »ULY! © _`TVU'NPTE {{Â c_atPH L dl_¾I DY * u_`TVuYL Oen * Il_aYPTo_aTH | aH [IL YPTV_g [uYLO NP [[Hdrdl_`pV |! Ahnu L YZO Il ~ EH Ä¶Hpj] x {FRIrN_aTxIlHU L N * D Il_aYPT0d dKIlHOd1JXd C [° ~ 0NZd O0H {c_gNIly L dKJj de nH L Hn {{cHdK_aUPTEH 4IlY _`TxIrHU L NIRH! Il ~ EHdlH / {c_gd · IL C de {IRH ¥ NPTE {e ~ VHIrH L YPUZHTVHYC d. {VN * IrN_aTAN C TV_¾uYLO t \ _`H © BYu®NZ [_A |! `_`IrN * ilh Ir ~ VH _`TxIrHULNIr_`YZT¶YPu ¢ {FRIrNNZ [L YZdrd {V_¾ÎÏH LHTxI + d D dKIlHOd] j²³ ± ho ~ ENZdpqHHT'NP {} VYP CIRH {¤NPdIl ~ EH ¥ d · I N ° * TE {{L VN uYLONIeuY L _aTcuY L ONIr_`YZT¬Hc [° ~ * qN TVUZHPo²S¹ C H LRD] nN¤} qY© nH L u C |! | GN * UTV C N * L UZHAuY f C H LRD _aTVU¤²S³ ± hv] ~ ENPd0pGH [YPOeH <N¶ON¿· Y L4L Hf C _ LHOeHTZI4uYL ²S³ »hJpENPdlH {pd D d · IrHOd] N * TE {º_ad C dlh {º_`T¬ O0H {c_gNIl_aYPT d D dKIlHOedIlYA {VHdl_`UZT; tx_aH © d / YPT ± dlHtZH LN * | 8 {VN * IRN ¥ DLy CEL [Hd PAR ¥ YZ} cIl_aOe_ H + f CH LRD HtN * | C NIr_`YZTj] Et \ _ah © d N L HONIRH L _aNP | `_H {B ~ VH + {V_¾½ ¥ [C | ¾I D _gdIrYAON * _aTxIrN * _A; _aTE [L HOeHTxI ° N * | a | D ONIlh L _gN * | A_h {t \ _ah © d / © ~ V_` | Ahl DKY CVL [HdvN LHC} {G VN * ilh {iXTeIl ~ VH / [YPTxIlH \ IVY * ui © PVT ¥ DKY CVL [Hd] * TZH LRD uH © o_aTcuYLONIl_aYPTqdN L H!} L Ytx_g {{p cH D DLy CVL [HD] 1 N * TE {{¤OeHIr ~ VYC Vd C d C N * | a | D} L {YP} GYZdlH ¤ {cY; TVYPI + N *} V} V | D _aT|Il ~ E_ad [YPTxIlH \ I B ~ E_ad} qN *} GH L d · I C {c_aHde ~ VY© Ily C} {VN ÏIrHOeN * ilh L _gN * | A_h {º²S³ ± Ht \ _ah © d0YPT © PVT DLy CEL [Hd0_`T¬Il ~ EH [YPTxIlH \ IeYPuN OEH {c_gNIr_`YZTN L [° ~ E_¾IrH [i CVL HP n W B IWJ / A RNTI-E-3 444"
1149,Revue des Nouvelles Technologies de l'Information,EGC,2005,Microarray data mining : recent advances,,Gregory Piatetsky-Shapiro,http://editions-rnti.fr/render_pdf.php?p1&p=1000443,http://editions-rnti.fr/render_pdf.php?p=1000443,en,
1170,Revue des Nouvelles Technologies de l'Information,EGC,2005,Semi-supervised incremental clustering of categorical data,"Le clustering semi-supervisé combine l'apprentissage supervisé et non-supervisé pour produire meilleurs clusterings. Dans la phase initiale supervisée de l'algorithme, un échantillon d'apprentissage est produit par sélection aléatoire. On suppose que les exemples de l'échantillon d'apprentissage sont étiquetés par un attribut de classe. Puis, un algorithme incrémentiel développé pour les données catégoriques est utilisé pour produire un ensemble de clusters pur (tels que les exemple de chaque cluster ont la même étiquette), qui servent de ""seeding clusters"" pour la deuxième phase non-supervisée de l'algorithme. Dans cette phase, l'algorithme incrémentiel est appliqué aux données non étiquetées. La qualité du clustering est évaluée par l'index de Gini moyen des clusters. Les expériences démontrent que des très bons clusterings peuvent être obtenus avec des petits échantillons d'apprentissage.","Dan A. Simovici, Natima Singla",http://editions-rnti.fr/render_pdf.php?p1&p=1000246,http://editions-rnti.fr/render_pdf.php?p=1000246,en,"Semi-Supervisé incrémental Clustering de données catégorielles Dan Simovici * Namita Singla ** * Université du Massachusetts Department of Computer Science de Boston, Boston, MA 02125, USA dsim@cs.umb.edu ** Université du Massachusetts Department of Computer Science Boston, Boston , MA 02125, Etats-Unis de CV. Le semi-supervisez la mise en grappes combinent l'apprentissage et supervisez non supervisez verser clusterings PRODUIRE Meilleurs. Dans la phase de ini- tiale supervisee de l'algorithme, un échantillon d'apprentissage pro- is Duit de sélection par aléatoire. On suppose Que les exemples de l'échantillon d'apprentissage par un Sont étiquetés de classe attribut. Puis, un algo- rithme incrémentiel Développé répandrai les Données catégoriques is used Ensemble pour un atelier Produire de grappes pur (tells Que les exemple de each groupe have the same étiquette), Qui Servent de « grappes d'ensemencement » pour la phase deuxiéme non supervisee de l'algorithme. Dans la phase this, l'al gorithme incrémentiel is aux Appliqué non Données étiquetées. La qua- lité du regroupement par l'EST évaluée indice de Gini des grappes des Moyen. Les expériences des très Que démontrent bures clusterings PEUVENT Être des petits Avec obtenus d'apprentissage Échantillons. 1 Introduction Le regroupement est un procédé qui vise à des données de partition en groupes qui se compose d'objets similaires. Similarité entre objets est mesurée en utilisant une métrique définie sur l'ensemble des objets ou, lorsque cela est possible, en utilisant classifications préexistantes des objets. En général, le regroupement est une activité non supervisée. En d'autres termes, le regroupement a lieu sans aucune intervention d'un opérateur extérieur qui attribue des objets classes. En supposant que la classe d'un objet est déterminé par les autres caractéristiques de l'objet, un bon algorithme de regroupement devrait générer des grappes aussi homogènes que possible. Le noyau de l'algorithme de regroupement est la construction progressive d'une partition de regroupement de l'ensemble des objets de telle sorte que que la distance totale à partir de cette partition pour les partitions déterminées par les attributs est minime. Un défi particulier du regroupement des données catégoriques découle du fait qu'aucun ordre naturel existe sur les domaines des attributs des objets. Cela ne laisse que la distance de Hamming comme une mesure de dissemblance, un mauvais choix pour établir une discrimination entre les attributs à valeurs multiples d'objets. regroupement semi-supervisée de données catégoriques comporte deux phases: la première phase consiste en un procédé supervisé qui est appliqué à un ensemble d'apprentissage obtenu échantillonnage aléatoire du jeu de données. Les clusters sont formés en utilisant un algorithme de classification incrémentale RNTI-E-3189 Clustering semi-supervisée qui est appropriée pour les données catégoriques. Ensuite, ces grappes sont réparties en groupes homogènes, qui forment les pôles d'ensemencement pour la deuxième phase de l'algorithme. Dans la deuxième phase sans supervision, les objets sont progressivement ajoutés aux clusters existants sans utiliser l'étiquette de classe. Enfin, clusterings sont évaluées en utilisant l'indice de Gini. regroupement incrémentale peut être attribuée à (Hartigan 1975) et (Carpenter et al., 1990). Cela a été suivi d'un article fondateur de Fisher (Fisher 1987) qui a créé COBWEB, un algorithme de clustering incrémental que les restructurations impliqués des groupes en plus des ajouts supplémentaires d'objets. clusters supplémentaires liés aux aspects dynamiques de bases de données ont été discutées dans (Can 1993) et (Can et al., 1995). Il est également à noter que le regroupement progressif a été utilisé dans une variété d'applications (Langford et al., 2001), (Lin et al., 2004), (Charikar et al., 2997), (Ester et al., 1998) . L'autre paradigme principal appliqué ici, le regroupement semi-supervisé, a récemment reçu beaucoup d'attention (Cheung et Yeung 2004), (Bilenko et al., 2004), (Cohn et al., 2003), (Zhu et al., 2002 ), principalement liés à des données numériques. Nous mettons l'accent est mis ici sur les données catégoriques qui nécessite une ap spécifique proach. regroupement supplémentaire assure que la principale utilisation de la mémoire est minime, car il n'y a pas besoin de garder en mémoire les distances mutuelles entre les objets; Par conséquent, les algorithmes sont très évolutive par rapport à la taille de l'ensemble des objets et le nombre d'attributs. regroupement semi-supervisé, agissant comme une enveloppe pour le regroupement progressif sous-jacente améliore la qualité de la mise en grappes. 2 partitions et Clusterings Soit S un ensemble. Une partition sur S est une collection non vide de sous-ensembles non vides de S indexé par un ensemble I, π = {Bi | i ∈ I} tel que ⋃ i∈I Bi = S i et 6 = j implique Bi ∩ Bj = ∅. Les ensembles Bi sont les blocs de la partition π. L'ensemble des partitions sur S est notée par une partie (S). Pour π, σ ∈ PARTIE (S) nous écrivons π ≤ σ si chaque bloc B de π est inclus dans un bloc de σ, ou de façon équivalente, si chaque bloc de σ est une union exacte des blocs de π. Cette commande partielle génère une structure en treillis sur une partie (S); Cela signifie que pour tous les deux partitions n, π '∈ PARTIE (S), il existe au moins une partition π1 telle que π ≤ π1 et π' ≤ π1 et il y a une plus grande partition π2 de telle sorte que π2 ≤ π et π2 ≤ π. La première partition est désignée par π ∨ π ', tandis que la seconde est notée π ∧ π. Pour introduire une métrique sur l'ensemble des partitions d'un ensemble fini, nous définissons l'application v: PARTIE (S) - → R v (π) = Σni = 1 | Bi | 2, où π = {B1,. . . , Bn}. L'application v est une valeur plus faible sur une partie (S), qui est, v (π ∨ σ) + v (π ∧ σ) ≥ v (π) + v (σ) (1) pour π, σ ∈ PARTIE (S ) (voir l'annexe 5 pour une preuve). Pour chaque évaluation inférieur v la cartographie d: (PARTIE (S)) 2 - → R définie par d (π, σ) = v (π) + v (σ) - 2v (π ∧ σ) est une mesure sur une partie ( S) (voir (JP et B. Leclerc Barthélemy, 1995), (JP Barthélemy, 1978), (Monjardet, 1981) Une propriété particulière de cette mesure RNTI -. 1 RNTI-E-3 190 Dan Simovici et Namita Singla permet formulation d'un algorithme de classification supplémentaire qui est utilisée comme une partie de la grappe semi-supervisée. système d'objet An est une paire S = (S, H), où S est défini appelé l'ensemble des objets de S, H = {A1, ..., Am} est un ensemble de correspondances définies sur S. pour chaque mappage Ai (désigné comme attribut de S), il existe un ensemble non vide Ei appelé le domaine de telle sorte que Ai Ai: S - → Ei pour 1 ≤ . i ≤ m la valeur d'un attribut Ai sur un t objet est désigné par t [Ai] Ceci est cohérent avec la terminologie utilisée dans les bases de données relationnelles, où un tableau peut être considéré comme un système d'objet;. cependant, la notion d'objet système est plus général parce que obje CTS ont une identité en tant que membres de l'ensemble S, au lieu d'être considérée comme seulement m-uplets de valeurs. Dans cet esprit, nous appellerons t [Ai] comme projection de t sur Ai. Un attribut A d'un système d'objet S = (S, H) génère une partition πA de l'ensemble des objets S, où deux objets appartiennent au même bloc de πA si elles ont la même projection sur A. On note par BAA le bloc de π a qui se compose de l'ensemble des tuples de S dont l'un est a-composant. Notez que pour les bases de données relationnelles, πA est la partition de l'ensemble des lignes d'une table qui est obtenue en utilisant le groupe par une option de sélection dans la norme SQL. Un regroupement d'un système d'objet S = (S, H) est définie comme étant une partition de κ de S. Les blocs de la partition κ sont les grappes de κ. 3 A semi-supervisée incrémental Clustering algo- rithme Un regroupement semi-supervisée d'un système d'objet S = (S, H) commence par la consommation as- qu'un oracle fournit la valeur d'un attribut spécial K d'objets appelée classe de l'objet pour un sous-ensemble T de l'objet mis en S. Dans la première phase de l'algorithme un algorithme de classification incrémentale a est appliqué à l'objet ensemble T qui produit une agrégation initiale σ de cet ensemble. En général, ces groupes ne sont pas pures par rapport à la classe K, qui est, nous pouvons trouver dans les mêmes objets de classe qui ont des valeurs distinctes de l'attribut K. Ensuite, chacun des groupes de T est divisé en grappes pures. La cloison κ0 de T obtenue de la manière contient les grappes d'ensemencement pour le regroupement de l'ensemble des objets. le deuxième phase, sans surveillance de l'algorithme commence avec la partition de l'ensemble κ0 T. En utilisant l'algorithme de clustering incrémental, objets de l'ensemble S - T sont ajoutés aux clusters existants ou former de nouveaux groupes. L'attribut class (si existant) ne joue aucun rôle dans cette phase. La classification finale prolonge la cloison κ0 de T à une κ de séparation de regroupement de l'ensemble des objets. Nous commençons par discuter de notre algorithme de classification incrémentale. Pour un système d'objet S = (S, H) nous cherchons un groupement κ = {C1,. . . , Cn} ∈ partie (S) de telle sorte que la distance totale de κ aux cloisons des attributs: D (κ) = nΣ i = 1 d (κ, πAi) RNTI - 1 RNTI-E-3191 Clustering semi-supervisée a un minimum local. La définition de d nous permet d'écrire: D (κ) = nΣ i = 1 | Ci | 2 + mAΣ j = 1 | BAAJ | 2 - 2 nΣ i = 1 mAΣ j = 1 | Ci ∩BAaj | 2, Soit t un nouvel objet, t 6∈ S, et nous allons laisser Z = S ∪ {t}. Pour former un regroupement de l'ensemble Z l'objet t peut ajouter à un cluster Ck existant ou un nouveau cluster + 1 Cn, peut être créé qui se compose uniquement de t. Si t est ajouté à un cluster Ck existant, le nouveau cluster est κ (k) = {C1,. . . , Ck-1, Ck ∪ {t}, Ck + 1,. . . ,} Cn, et la nouvelle partition d'attribut est πA '= {Baa1,. . . , BAt [A] ∪ {t},. . . , BAamA} Maintenant, nous avons: d (κ (k), πA ') - d (κ, πA) = (| Ck | + 1) 2 - | Ck | 2 + (| BAt [A] | + 1) 2 - | BAt [A] | 2 - 2 (2 | Ck ∩BAt [A] | + 1) = 2 | Ck | + 1 + 2 | BAt [A] | + 1- 4 | Ck ∩BAt [A] | - 2 = 2 | Ck ⊕BAt [A] |, où ⊕ est la différence symétrique des ensembles donnés par X ⊕ Y = (X ∪ Y) - (X ∩ Y) pour tous ensembles X, Y. Lorsque t est la formation d'un nouveau cluster, nous avons les partitions de '= {C1,. . . ,. . . , Cn, {t}} πA '= {Baa1,. . . , BAt [A] ∪ {t},. . . , BAamA} qui rendement d (κ ', πA') - d (κ, πA) = 2 | BAt [A] |. Par conséquent, D (κ ') - D (κ) = {Σ A 2 · | Ck ⊕BAt [A] | dans l'affaire 1Σ A 2 · | BAt [A] | dans l'affaire 2. Ainsi, le choix entre ajouter un objet à un cluster existant et la création d'un nouveau cluster est basée sur la comparaison des nombres min k Σ A | Ck ⊕BAt [A] | et Σ A | BAt [A] |. Si le premier nombre est plus petit, on ajoute t à un cluster Ck pour lequel Σ A | Ck ⊕ BAt [A] | est minime; autrement, nous créons un nouveau groupe d'un objet. RNTI - 1 RNTI-E-3 192 Dan Simovici et Namita Singla entrée: ensemble de données S, fraction de p de consigne surveillée, « « ne sont pas encore » » seuil α sortie: regroupement C1,. . . , Cn Méthode: obtenir un échantillon aléatoire d'objets T de l'ensemble des objets S telle que | T || S | = P; calculer le regroupement des semences de l'ensemble T κ0 = {D1,. . . , D`} = A (T, α) calculer la classification finale κ = C (S, T, κ0, α) Fig. 1 - Pseudocode de l'algorithme de classification semi-supervisée Pour les algorithmes de classification incrémentielles certains ordres d'objets peut entraîner plutôt pauvres clusterings. Pour diminuer le problème de l'effet de commande nous élargissons la rithme algo- initiale en adoptant la technique « non encore » introduit dans (Roure et Talavera, 1998). Un nouveau cluster est créé uniquement lorsque l'effet d'ajouter l'objet t sur la distance totale est assez importante. Tel est le cas lorsque P A | BAt [A] | vison P A | Ck⊕BAt [A] | <Α, où α ≤ 1 est un paramètre fourni par l'utilisateur. Dans le cas contraire, le t objet placé dans un tampon non encore. Toutes les expériences décrites à la section 4 utilisées α = 0,95. Lorsque P A | BAt [A] | vison P A | Ck⊕BAt [A] | > 1, le t objet est placé dans un cluster Ck existant qui minimise Σ A | Ck ⊕BAt [A] |. Cette approche limite le nombre de nouveaux clusters singleton qui seraient autrement créés. Après que tous les objets de l'ensemble S ont été examinés, les objets contenus par les α = 1 avec un tampon non encore sont traités Cela empêche de nouvelles insertions dans la mémoire tampon et les résultats dans les deux placer ces objets dans des groupes existants ou de créer de nouveaux clusters. Ainsi, la construction de la classification finale de κ S commence par une cloison de classification initiale κ0 d'un sous-ensemble T et avec un paramètre α. On note la classification finale par κ C (S, T, κ0, α). La partition créée sur le premier ensemble d'objets T est désigné par κ0 = A (T, α), et il utilise le même algorithme que ci-dessus. L'algorithme est donnée suivante: 4 Résultats expérimentaux Nous avons appliqué le regroupement semi-supervisé à plusieurs bases de données catégoriques obtenues à partir de l'ensemble de données UCI (C. L. Blake et C. J. Merz, 1998). Chaque expérience a été appliquée à l'aide d'une série de pourcentages de plus en plus pour l'ensemble de données semi-supervisé, en moyenne sur cinq échantillons aléatoires. La qualité du regroupement des données catégoriques nécessite un spécialiste Treatmet RNTI - 1 RNTI-E-3193 clustering semi-supervisé car les distances entre les objets ne peuvent pas être définis naturellement. Nous avons évalué clusterings en utilisant l'indice de Gini moyenne des grappes (Demiriz et al., 1999). Soit K l'attribut class et laisser {BKK1,. . . , BKkp} la partition de l'ensemble d'objets S. La classe-impureté d'un ensemble d'objets U est défini comme l'indice de Gini de la « partition de trace » {U ∩BKkj | 1 ≤ j ≤ p}: giniK (U) = 1- j = 1 pΣ (| U ∩BKkj | | U |) 2. Notez que si un groupe U est pur, qui est, il contient des objets qui appartiennent à une seule classe, puis giniK (U) = 0. Pour un regroupement κ = {U1,. . . , U`} de l'ensemble des objets S l'indice de Gini est donnée par impK (κ) = Σ i = 1 | Ui | | S | giniK (Ui). De toute évidence, les faibles valeurs de impK (κ) indiquent une bonne clusterings. L'algorithme a été appliqué à l'ensemble de données CHAMPIGNONS. Cet ensemble de données contient 8124 enregistrements de champignons et est généralement utilisé comme ensemble de test pour les algorithmes de classification, où la tâche est de construire un classificateur qui est capable de prédire le caractère toxique / comestible des champignons sur la base des valeurs des attributs des champignons. Les groupes montrent un degré assez remarquable de pureté. Par exemple, pour une partie semi-supervisée de 10%, nous avons obtenu les groupes suivants: Cluster instances edib./pois. Pour cent du numéro de groupe dominant 1 4225 3575/650 84,615 2 165 0/165 100 3 3055 0/3055 100 4 394 393/1 99,746 5 2 0/2 100 6 55 48/7 87,273 7 36 0/36 100 8 192 192 / 0 100 Il est tout à fait remarquable que cinq des huit groupes obtenus de cette manière sont pures et les grappes restantes ont un degré élevé de pureté. Pour d'autres dimensions de l'échantillon sous surveillance, nous avons obtenu les résultats suivants: RNTI - 1 RNTI-E-3 194 Dan Simovici et Namita Singla 0,08 0,09 0,1 0,11 0,12 0,13 0,14 0,15 0,16 5 10 15 20 25 30 pourcentage d'impuretés de l'échantillon Impureté par rapport à l'échantillon Taille des données champignons? ? ? ? ? ? ? Fig. 2 - Impureté Diminuer Taille de l'échantillon pour les champignons base de données de champignons Pourcentage Nombre de grappes Temps Impureté supervisé (ms) 5% 8 0,15362536 2443 10% 8 0,1371508 2454 15% 8 0,12705285 2444 20% 8 0,10735634 2374 25% 9 0,09911141 3545 30% 9 0.0816238 3415 la dépendance de la mesure d'impureté sur la fraction de l'échantillon surveillée est représenté sur la figure 2. Un analogue, mais une amélioration plus lente de la qualité de la grappe peut être observée pour ZOO, un autre ensemble de données catégoriques de UCI (CL Blake et CJ Merz, 1998): RNTI - 1 RNTI-E-3195 pourcentage semi-supervisée Clustering 0,11 0,115 0,12 0,105 0,125 0,13 0,135 0,14 0,145 0,15 5 10 15 20 25 30 impuretés de l'échantillon par rapport à l'impureté Taille de l'échantillon solaire Fusées? ? ? ? ? ? ? Fig. 3 - Impuretés Diminuer Taille de l'échantillon pour SOLAR FLARES base de données Zoo Pourcentage Nombre de grappes Temps Impureté supervisé (ms) 5% 3 0,39802246 110 10% 4 0,37841779 90 15% 4 0,37841779 110 20% 6 0,28431165 130 25% 7 0,33374854 141 30 % 7 0.33981398 114 la variation des impuretés moyennes pour cinq expériences avec chaque taille de l'échantillon pour la base de données SOLAR_FLARES est représenté sur la figure 3. 5 Conclusion et travaux futurs semi-supervisée regroupement incrémentale est un algorithme de classification efficace des données tegorical ca- qui génère presque grappes homogènes par rapport aux classifications basées sur des valeurs d'attribut. Une idée naturelle pour le développement de l'approche semi-supervisée serait d'utiliser un modèle renforcé (Freund, 1995) du regroupement progressif semi-supervisé où plusieurs petits échantillons de formation seraient utilisés pour générer clusterings; un objet serait alors classés en fonction de ses positions par rapport à l'ensemble des clust ers. RNTI - 1 RNTI-E-3 196 Dan Simovici et Namita Singla Nous explorerons le regroupement progressif semi-supervisé dans le contexte de flux regroupement des objets, qui est un type de données importantes dans le secteur minier Internet et la sécurité du réseau. L'ordre des objets est hors de propos dans ce domaine puisque les objets doivent être traités à leur arrivée. Références J.P. et B. Leclerc Barthélemy, la procédure médiane pour les partitions, dans Cloisonnement ensembles de données, pages 3-34, Providence, 1995, American Mathematical Society. J.P. Barthélemy, sur les Remarques Métriques des ensembles Propriétés ordonnés, Math. Sci. . Hum, 61: 39-60, 1978. M. Bilenko, S. Basu, et RJ Mooney, intégration des contraintes et de l'apprentissage métrique dans le regroupement semi-supervisé, à la Conférence internationale sur l'apprentissage machine, Banff, Canada, 2004. G. Birkhoff , Théorie Lattice, American Mathematical Society, Providence, 1973. CL Blake et CJ Merz, UCI Repository des bases de données d'apprentissage machine, Université de Californie, Irvine, Département des sciences de l'information et de l'ordinateur, http: //www.ics.uci.edu /~mlearn/MLRepository.html, 1998. F. Can, EA Fox, CD Snavely, et RK France, regroupement incrémentiel pour les bases de données de documents très importants: Une première expérience de MARIAN, Inf. Science, 84:. 101-114, 1995. F. Can, regroupement incrémental pour le traitement de l'information dynamique, ACM Transaction pour les systèmes d'information, 11: 143-164, 1993. G. Carpenter et S. Grossberg, Art3: recherche hiérarchique utilisant chimique émetteurs dans les architectures de reconnaissance des formes d'auto-organisation, réseaux de neurones, 3: 129-152, 1990. M. Charikar, C. Chekuri, T. Feder, et R. Motwani, regroupement incrémental et la recherche d'information dynamique, en STOC, pages 626- 635, 1997. H. Cheung et DY Yeung, localement linéaire adaptation métrique pour semi-supervisé Tering assorti-, à la Conférence internationale sur l'apprentissage machine, Banff, Canada, 2004. D. Cohn, R. Caruana, et A. McCallum, semi le regroupement -supervised avec les commentaires des utilisateurs, rapport technique, 2003. A. Demiriz, KP Bennett, et ME Embrechts, semi-supervisé le regroupement en utilisant des algorithmes gé- Netic, rapport technique Math 9901, Rensselaer Institut polytechnique, Troy, New York, 1999. M . Ester, HP Kriegel, J. Sander, M. Wimmer, et X. Xu, Incre regroupement mentale pour l'exploitation minière dans un environnement d'entreposage de données, VLDB, pages 323-333, 1998. D. Fisher, l'acquisition des connaissances par le regroupement conceptuel incrémental, machine Mémorisation en, 2: 139-172, 1987. Y. Freund, un Dynamiser faible algorithme d'apprentissage à la majorité, l'information et la mise Compu, 121: 256-285, 1995. JA Hartigan, Clustering algorithmes, John Wiley, New York, 1975. RNTI - 1 RNTI-E-3197 semi-Supervisé Clustering T. Langford, CG Giraud-Carrier, et J. Magee, détection des éclosions de maladies infectieuses dans les hôpitaux à travers le regroupement progressif, dans les Actes de la 8e Conférence sur la grippe aviaire en médecine (AIME), pages 30-39. Springer, 2001. J. Lin, M. Vlachos, EJ Keogh, et D. Gunopulos, regroupement progressif de séries itératives de temps, dans EDBT, pages 106-122, 2004. B. MONJARDET, métriques sur des ensembles ordonnés - une enquête, Mathématiques discrètes, 35: 173-184, 1981. J. Roure et Luis Talavera, regroupement progressif robuste avec de mauvais exemple ordonnancements: Une nouvelle stratégie, en IBERAMIA, pages 136-147, 1998. Z. Zhu, Y. Pilpel, et GM Eglise, l'identification numérique des sites de liaison de facteur de transcription via un algorithme en cluster centré sur la transcription du facteur (TFCC), Journal of Molecular Biology, 318: 71-81, 2002. Une preuve de l'inégalité (1) Soit π, σ deux partitions de l'ensemble fini S, de telle sorte que π = {B1,. . . , Bm} et σ = {C1,. . . ,} Cn. Il est connu (voir (Birkhoff 1973), par exemple) que π ∧ σ est constitué par tous les ensembles de la forme Bi ∩ Cj tel que Bi ∩ Cj 6 = ∅. D'un autre côté, π ∨ σ a une description plus compliquée; à savoir, x, y ∈ S appartiennent au même bloc D de π ∨ σ s'il existe une suite d'éléments de S, z0,. . . , Zk tel que x = z0, zk = y et pour chaque paire (zp, zp + 1) il y a un bloc Bi de π ou un bloc Cj de σ telle que les deux zp et ZP + 1 appartiennent à Bi ou à Cj pour 1 ≤ p ≤ k - 1. Considérons le graphe biparti Gπ, σ dont l'ensemble des sommets est constitué par les blocs de π et les blocs de σ . Un bord (Bi, Cj) existe uniquement si Bi ∩ Cj 6 = ∅. Si K est une composante connexe de ce graphique, il est facile de voir que ⋃ {Bi ∈ π | Bi ∈ K} = {⋃ Cj ∈ σ | Cj ∈ K}. En outre, chaque bloc D de π ∨ σ est égal à l'union des blocs de π (ou les blocs de σ) qui appartiennent à une composante connexe K de Gπ, σ. Exemple 5.1 Soit S = {ai | 1 ≤ i ≤ 12} et laisser π = {Bi | 1 ≤ i ≤ 5} et {σ = Cj | 1 ≤ j ≤ 4}, où B1 = {a1, a2}, C1 = {a2, a4}, B2 = {a3, a4, a5}, C2 = {a1, a3, a5, a6, a7}, B3 = {a6, a7}, C3 = {a8, a11}, B4 = {a8, a9, a10}, C4 = {a9, a10, a12}, B5 = {a11, a12}. Le graphique Gπ, σ représenté sur la figure 4 comporte deux éléments connectés qui correspondent aux blocs D1 = {a1, a2, a3, a4, a5, a6, a7} = B1 ∪B2 ∪B3 = C1 ∪ C2, D2 = {a8 , a9, a10, a11, a12} = B4 = ∪B5 C3 ∪ C4. RNTI - 1 RNTI-E-3 198 Dan Simovici et Namita Singla a11, a12 a8, a9, a10 a6, a7 a3, a4, a5 a1, a2 a9, a10, a12 a8, a11 a1, a3, a5, a6, a7 a2, a4 B1 B2 B3 B4 B5 C1 C2 C3 C4 Fig. 4 - le graphique Gπ, σ de la partition π ∨ σ. La partition π ∧ σ est composé de 9 blocs qui correspondent aux arêtes du graphe: B1 ∩ C1 = {a2}, B1 ∩ C2 = {a1}, B2 ∩ C1 = {a4}, B2 ∩ C2 = {a3, a5 }, B3 ∩ C2 = {a6, a7}, B4 ∩ C3 = {} a8, B4 ∩ C4 = {a9, a10}, B5 ∩ C3 = {} a11, B5 ∩ C4 = {a12}. Laissez D1,. . . , Le Dr être les blocs de la partition π ∨ σ. Pour un bloc Dk définir les ensembles Ik ⊆ {1,. . . , M} et {1 Jk ⊆,. . . , N} où Ik = {i | Bi ∩ Dk 6 = ∅} et {Jk = j | RNTI - 1 RNTI-E-3199 Semi-Clustering Bi ∩Dk supervisé 6 = ∅}. Notez que v (π ∨ σ) = rΣ k = 1 | Dk | 2, v (π ∧ σ) = rΣ k = 1 Σ Σ i∈Ik j∈Jk | Bi ∩ Cj | 2, v (π) = rΣ k = 1 Σ i∈Ik | Bi | 2 = rΣ k = 1 Σ i∈Ik   Σ j∈Jk | Bi ∩ Cj |   2, v (σ) = rΣ k = 1 Σ j∈Jk | Cj | 2 = rΣ k = 1 Σ j∈Jk (Σ i∈Ik | Bi ∩ Cj |) 2. Il est immédiat de vérifier l'inégalité: (Σ i∈Ik Σ j∈Jk | Bi ∩ Cj |) 2 + Σ Σ i∈Ik j∈Jk | Bi ∩ Cj | 2 ≥ Σi∈Ik (Σ j∈Jk | Bi ∩ Cj |) 2 + Σ j∈Jk (Σ i∈Ik | Bi ∩ Cj |) 2 Ceci équivaut à: | Dk | 2 + Σ Σ i∈Ik j∈Jk | Bi ∩ Cj | 2 ≥ Σi ∈Ik (Σ j∈Jk | Bi ∩ Cj |) 2 + Σ j∈Jk (Σ i∈Ik | Bi ∩ Cj |) 2 Ajout les inégalités similaires pour 1 ≤ k ≤ r on a l'inégalité voulue: v ( π ∨ σ) + v (π ∧ σ) ≥ v (π) + v (σ)., Résumé regroupement semi-supervisé combinats supervisé et apprentissage non supervisé aux pro- Duce mieux clusterings. Dans la phase initiale surveillée de l'algorithme proposé un ensemble d'entraînement est généré par l'échantillonnage. On suppose que les exemples de l'ensemble de la formation sont marquées par un attribut de classe. Ensuite, un algorithme incrémental développé pour les données catégorielles est utilisé pour produire un ensemble de clusters purs (tels que les instances de chaque groupe ont la même étiquette) qui servent de « grappes d'ensemencement » pour la deuxième phase unsu- pervised. Dans cette phase, l'algorithme incrémental est appliqué aux données non marqué. La qualité du regroupement est évaluée par l'indice de Gini moyen des grappes. Les expériences montrent que de très bonnes clusterings peuvent être obtenus avec des ensembles de formation relativement faible. RNTI - 1 RNTI-E-3 200"
1172,Revue des Nouvelles Technologies de l'Information,EGC,2005,SSC : Statistical Subspace Clustering,"Cet article se place dans le cadre du subspace clustering, dont la problématique est double : identifier simultanément les clusters et le sous-espace spécifique dans lequel chacun est défini, et caractériser chaque cluster par un nombre minimal de dimensions, permettant ainsi une présentation des résultats compréhensible par un expert du domaine d'application. Les méthodes proposées jusqu'à présent pour cette tâche ont le défaut de se restreindre à un cadre numérique. L'objectif de cet article est de proposer un algorithme de subspace clustering capable de traiter des données décrites à la fois par des attributs continus et des attributs catégoriels. Nous présentons une méthode basée sur l'algorithme classique EM mais opérant sur un modelé simplifié des données et suivi d'une technique originale de sélection d'attributs pour ne garder que les dimensions pertinentes de chaque cluster. Les expérimentations présentées ensuite, menées sur des bases de données aussi bien artificielles que réelles, montrent que notre algorithme présente des résultats robustes en termes de qualité de la classification et de compréhensibilité des clusters obtenus.","Laurent Candillier, Isabelle Tellier, Fabien Torre, Olivier Bousquet",http://editions-rnti.fr/render_pdf.php?p1&p=1000241,http://editions-rnti.fr/render_pdf.php?p=1000241,en,"SSC: statistique subspatial Clustering Laurent Candillier1,2, Isabelle Tellier1, Fabien Torre1, Olivier Bousquet2 1 grappa - Université Charles de Gaulle - Lille 3 candillier@grappa.univ-lille3.fr http: //www.grappa.univ-lille3.fr 2 Pertinence - 32 rue des Jeûneurs -75002 Paris olivier.bousquet@pertinence.com http: //www.pertinence.com CV. Cet article se lieu le cadre du Dans subspatial clustering, la Do not is à double problématique: identifier les grappes et simultanément le Sous- espace Spécifique Dans each is defini Lequel, et each assorti- ment caractériser par un minimum de dimensions Nombre de postes, permettant AINSI Une présentation des experts par de compréhensible un de du domaine d'application d'. Les methods Jusqu'a présent répandrai proposées this le défaut Tâche de Ontario se cadre un à restreindre numérique. L'article Objectif de l'Est de CET pro- un poseur de sous-espace cluster algorithme capable de des Données Traiter à la Fois décrites par des attributes Continus et des attributes catégoriels. Nous Présentons Une sur l'méthode basée EM algorithme classique Opérant sur un Mais modèle des Données et simplifié D'une technique Suivi ori- Ginale de sélection d'attributes verser ne keep les dimensions Que perti- nentes de each cluster. Les Expérimentations ensuite présentées, sur des bases menées de bien also Données Que REELLES artificielles, Que notre montrent des Résultats Algorithme Présente en Annoter de robustes qualité de la classification et de compréhensibilité des grappes obtenus. Introduction Face aux informations d'Quantités Qui ne d'increase Dans cessent les Bases de données du monde entier, l'extraction de connaissances heuristiques à automatique partir de bureaux bases et les techniques de visualisation des Indispensables Résultats Sont devenues. C'est la raison d'être de la fouille de Données. Dans un cadre de CE, l'apprentissage non supervisez (clustering OU) is used Depuis longtemps les répandrai identifiant Groupes (ou groupes) d'éléments similaires (enquête de voir Berkhin 2002). Une problèmatique à visage Supplémentaire des apparait Bases de données de grande dimensionnalité: Dans CE CAS, les Groupes PEUVENT Être caractérisés sous UNIQUEMENT PAR-ensembles CERTAINS de dimensions et dimensions SCÉ pertinentes d'PEUVENT Être un groupe Différentes à l'Autre. Sur de tells Problèmes, les techniques de regroupement classiques mal voiture fonctionnent, sur la distance Une fondées Entre definie objets dans l'Espace globalement de description, ne PEUVENT pas ELLES le fait appréhender la notion de Qué Varie d'un similarité groupe à l'Autre . Une nouvelle une problématique Emerge Fait recemment, du sous-espace cluster Celle, l'ENJEU Do not is de les cibler d'objets et Groupes, verser each, le sous-RNTI-espace E Spécifique-3177 SSC: subspatial statistique Clustering il is Dans defini Lequel 1. Et s'accompagne d'Objectif this un second: de provide Description Celui Une des Groupes iDENTIFIE compréhensible. Les methods proposées se verser this Tâche sur le focalisées Sont premier et Objectif le deuxième déshabillé en Ontario. De plus, la partie Expérimentale de bureaux porte travaux sur des Exclusivement Données Numériques. L'de cet article Objectif de l'Est un proposeur algorithme de regroupement sous-espace CA- pable de des Données Traiter à la Fois décrites par des attributes Continus et des attributes catégoriels, à l'très demandé de regler le Utilisateur Moins de paramêtres possible, et sortie en juin fournissant simple représentation des groupes iDENTIFIE. Nous coupes sur ACDE Call for l'EM adapted au Algorithme en cluster (Ye et al., 2003). Nous en proposons la version en juin Simplifiée l'hypothèse Que ajoutant les Données des Sont générées distributions Indépendantes SELON sur each dimension. Nous Céci d'en Përmet Une théorie dériver sous forme de compréhensible rules PUISQUE each dimension is characterized indépendem ment des Autres. La suite de l'article is costume Comme Organisée: Dans la section 1, nous Présentons notre méthode de regroupement sous-espace; les Expérimentations Dans la salle de bains présentées section 2, des bases Sur menées de bien artificialisation also Données cielles Que REELLES, presentent les de notre algorithme Résultats; et nous terminons Dans la section 3 conclusions par les CE par Quelques perspectives Ouvertes et travail. Algorithme 1 Dans SSC (Parsons et al., 2004), les auteurs Ont Etudie et comparé les methods to vary de sous-espace clustering. Toutes Sont capables de Retrouver EFFICACEMENT les grappes et their sous-espace Spécifique, Mais ELLES nécessitent Souvent des paramêtres Difficiles à regler Par l'Utilisateur ET influant Sur performances their (seuil de densité, clusters, la distance MINIMALE clusters Entre Nombre de postes Moyen de dimensions characteristics des , etc.) de plus, tous les essais sur were des bases effectués de Données Exclusivement Numériques. Enfin, la proposition de aboutie Aucune simple présentation des Résultats n'à éffectuée Été. Ce point le plus POURTANT voiture cruciale si la same dimensionnalité des grappes was Réduite les sous-Dans espaces Qui their Propres Sont, ci-may Celle encore trop élévée Être expert verser du domaine d'Qu'un demande le puisse appréhender Résultat. Ou il possible d'EST Souvent de bureaux Ignorer les dimensions CERTAINES, tout en conservant le same des objets partitionnement. 1.1 Modèle de provide probabiliste en AFIN sortie de notre algorithme de sous-espace simple, des grappes en cluster Une description trouvés, de les NOUS REPRESENTER choisissons sous forme de rules (hy- percubes Dans des sous-espaces de l'espace d'origine), la représentation Comme association reconnue Facilement interprétable. Pour integrer this Dans l'algorithme contrainte EM classique, nous proposons d'ajouter l'hypothèse Que les Données des Sont générées distribu- tions SELON Indépendantes sur each dimension. This cav hypothèse d'effet le modèle classique affaiblir, en Prenant also les compte di- Entre corrélations possibles mensions, Mais AINSI, la modélisation à la is Adaptée sous forme de présentation rules 1la la différence Avec de la problématique d'attributes sélection ( ous sélection de fonction) is that the sous-espace à locale is cible pôle each, et non globale à tous (Parsons et al., 2004). EGC 2005 RNTI-E-3 178 Candillier et al. des grappes de voiture each dimension is found characterized indépendemment des Autres. De Plus, L'Est Algorithme, plus ALORS Que l'algorithme Rapide classique, voiture le nouveau modèle de paramêtres Nécessite Moins (O (M) Au Lieu du O (M 2) classique, le M répandrai de dimensions Nombre), et les opérations Matricielles Sont évitées. Dans notre modèle, nous supposons Que les Données were des distributions générées gaussiennes SELON sur les dimensions et continue des distributions mul- SELON tinomiales sur les dimensions discrètes. Le modèle des EST Fait paramêtres Suivants composé verser cluster Ck each: fils Wk poids; verser dimension d each continuer, sa moyenne et sa variance μkd σkd; et verser each dimension d discrète, les Fréquences de each modalité Freqskd. Il suppose La Donnee du Nombre K de REcherches clusters. 1.2 Nous l'utilisons Algorithme EM Algorithme sur notre modèle classique. Les paramêtres du modèle correspondant cachés aux Probabilités d'appartenance de each à each groupe objet. Dans Notre CAS, les dimensions being Indépendantes supposées, la P Probabilité d'appartenance (- → Xi | Ck) d'un objet - → Xi à un groupe Ck correspond au produit des Probabilités P (Xid | CKD) sur each dimension d: 1√ 2πσkd exp (- 12 (Xid-μkd σkd) 2) si d is continue, et Freqskd (Xid) si d is discrète. Et versez EVITER sur Qu'une Une Probabilité dimension nulle n'annule la Probabilité globale, nous utilisons très positif Une constante Qui Faible Constitué Une borne minimale sur les Probabilités P (Xid | CKD). L'EM is Algorithme verser convergeur Connu Dans Lentement CERTAINS CAS. Pour l'accelerer, nous proposons d'ajouter l'heuristique suivante: s'arrêter when les attributions de groupes aux objets ne pas changent. Ce critère d'Arret Au FORTEMENT ressemblent à celles critere ALORS d'Arret de K-means. À each iteration, il Faut EVALUER les also Fait at- tributions de groupes aux objets costume Comme: Cluster (- → Xi) = ArgMaxkP (- → Xi | Ck). Nalement Fi-, l'algorithme is RELANCE un certains Nombre de solutions des AVEC FOIS Initiales Aléatoires. PUI la partition la fonction E maximisant = Σ i log (P (- → Xi)) is conservée. 1.3 Présentation du Résultat Que Le Résultat AFIN Le plus Compréhensible Soit possible, nous nous souhaitons Don- ner Une vue sur each seconde grappe, à sa représentation correspondant Simplifiée sous forme de règle, par le décrite Chacune de dimensions possibles Moins. Dans mes premiers temps de l'ONU, each groupe Représenté par l'EST minimum intervalle l'en- Semble Contenant des facts des objets Inclus dans le groupe sur les dimensions continue, et par la probable ainsi que la modalité sur les dimensions discrètes. Salle de bains, le soutien de la Calculated is règle (l'ensemble des objets compris Dans la règle). Un Wkd Puis weight is Attribué à des dimensions d Chacune du groupe Ck, en fonction de la dispersion relative des objets sur la dimension. Pour les dimensions se poursuit, il s'agit du rapport Entre la variance locale et la variance par rapport à globale μkd (N correspond au d'objets de Nombre la base). Et versez les dimensions Discrets il s'agit de la frequency relatif de la probable ainsi que la modalité (correspondent Modalitesd à l'ensemble des sur la Modalités dimension possibles d, et Frequencesd à l'ensemble des Fréquences de bureaux de Chacune sur l Modalités « ensemble de la base). EGC 2005 RNTI-E-3179 SSC: Subspace Clustering statistique Wkd =        1- σ2 kd σ2 d, with σ2d = Σ i (Xid-μkd) 2 N si d continuer Freqskd (mod) -Frequencesd ( mod) 1-Frequencesd (mod) SI d mod = Discrete AVEC arg {} m∈Modalitesd Freqskd (m) la sélection des Puis dimensions s'effectue Comme costume pertinentes: pour les dimensions Toutes, Dans l'ordre présentées croissant de poids Leur , la dimension si supprimer sa suppression ne pas le soutien modifie de la règle. Enfin, de Visualiseur AFIN les Résultats obtenus graphiquement, nous proposons de l'ONU Calculer à each Associe poids Couple de dimensions PRESENTES Dans la description des groupes: Vij = Σ k max (wki, WKJ). Plus ce poids important de l'Est, ainsi que les rules sur SCÉ deux projetées dimensions Sont Spécifiques. 2 Expérimentations 2.1 Essais sur Données de nous artificielles comparateur AFIN aux methods to vary de sous-espace regroupement, nous pro- posons de MENER SCÉ des bases EXPÉRIENCES Sur UNIQUEMENT Numériques. Les plus parmi Récentes, LAC de (Domeniconi et al., 2004) is Une méthode Qui Efficace, est comme la Nôtre, Un seul parametre Nécessite: Le Utilisateur Nombre de grappes REcherches. Nous proposons de nous comparateur à et Algorithme this des bases artificielles utilisons EVALUER les verser d'Taux de notre algorithme Erreurs et de LAC en classification. À partition each is la Associée des grappes pureté moyenne Produits (la correspondent à au pureté maximale d'idée Pourcentage du groupe objets au same Qui appartiennent initial). K des points d'ancrage (- → O1, ..., - → OK) Tirés Sont dans l'Espace aléatoirement de la description à M dimensions, groupes et centroïdes Sont utilisés des Comme (C1, ..., CK) Ë Générer. À chacun de bureaux grappes is Une partie des Associée N objets, et un sous-ensemble des dimensions M dimensions constituant des ses characteristics. Les des coordonnées Puis objets à un groupe Appartenant Ck Une Sont générées SELON centre de loi normalien OKD et d'écart de type sur ek dimension d characteristic Toute de Ck; Elles genere Sont es Une loi uniforme SELON Dans l'espace de la description des dimensions non characteristics. Les expériences en menées Varier les paramêtres Faisant de des bases artificielles génération mis en avant Ontario la de notre méthode robustesse. En Particulier, verser faire Elle se revele Efficace visage au bruit existant Dans les Données (la pureté moyenne des grappes is for de 90% à 20% de bruit Dans la base contre 70% for BAC). Concerning le temps d'exécution de la méthode, l'heuristique nous Que d'Përmet proposed Avons obtain, verser des Résultats de qualité Similaire, des temps de calcul, plus de proches de K Ceux moyens (verser sa rapidité Connu) Que de Ceux considérer. Concerning le seul de l'algorithme Paramètre, le Nombre de grappes, si REcherches ci-is Celui au Inférieur de grappes Nombre réel, concepts de ALORS QUELQUES le Mais Sont fusionnés ne s'éloigne pas Résultat de la solution Complètement réelle. Is au upper Se il Nombre réel, les concepts se recouvrent ALORS. Enfin, les Que notons de notre algorithme Résultats tout also are si les robustes Données were des Lois générées SELON Dans les Intervalles uniformes de dimensions their de définition characteristics, au lieu de gaussiennes Loïs. EGC 2005 RNTI-E-3 180 Candillier et al. 2.2 Essais sur REELLES Des Données expériences also have sur des Été menées Bases de données REELLES. , La Parmi la base ELLES Automobile, des bases de question de l'UCI Données (Blake et Merz 1998), contains Description la (et catégorielle numérique) d'un ensemble de Voitures. Sur la base de this, les visualisations Graphiques correspondant à deux couples de dimensions maximales de poids SONT FOURNIES figure 1. L'algorithme se réunit en AINSI Avant que le prix des Voitures AUGMENTE FORTEMENT when their longueur DEPASSE les 170 (figure 1 (a)), Qc Les Voitures Ayant UNE traction arrière (RWD) Ont ONU poids à vide supérieur aux tractions avant et 4 roues motrices (figure 1 (b)), et that the majority des Voitures Les plus Chères Sont à traction arrière (figures correspondance Entre les deux concerning le groupe C2). Pour plus de détails sur les de Expérimentations, voir (Candillier et al., 2005). 5118 8111 11104 14097 17090 20083 23076 26069 29062 32055 35048 141 147 153 165 171 177 159 183 189 195 201 pr ic e longueur Objets C0 C1 C2 (a) des projections sur longueur et prix. 1488 1745 2002 2259 2516 2773 3030 3287 3544 3801 4058 rwd 4WD fwd cu rb -w t ei gh roues motrices Objets C0 C1 C2 (b) les projections Sur traction ET poids. Figure 1 -. De SSC sur Résultats la base de l'automobile, versez K = 3. 3 Conclusions et perspectives Nous presented Dans Avons cet article Une nouvelle méthode de sous-espace sur l'agrégation basée en EM Algorithme l'hypothèse Que ajoutant les Données were générées sé- des distributions Indépendantes LON sur each dimension. This une idée déjà-Été étudiée Dans (Pelleg et Moore 2000). Il exists several Entre notre méthode des différences et la their. La première Dans la modélisation apparait: au lieu de Supposer la répartition gaussienne Sur une dimension continue, les auteurs la à l'uniforme supposent d'un intérieur Donné intervalle, et de file d'attente Une utilisent la distribution aux bords de CET intervalle, d'un personne à charge Qui σ evolue Paramètre au cours de l'algorithme. This Variation entre Retrouvé Dans la salle de bains finale de regroupement méthode. En Particulier, their pas capable Ne est méthode de fils à jour Mettre Modèle de Façon incrémentale, la Nôtre Que Alors may s'adapter à la présentation de Nouveaux exemples. De plus, nous effi- tivement integrated Avons la catégorielle Qui ne etait problématique QU'A titre de évoquée perspectives Dans l'article et nous Avons Une méthode originale Proposé de sélection d'attributes per- mettant de provide en sortie un et Résultat compréhensible des grappes iDENTIFIE visuel. Nous defini Une also Avons originale heuristique verser accelerer l'algorithme. Pour la recherche Poursuivre Dans CE sens, il Semble de s'inspirer de Intéressant l'article EGC 2005 RNTI-E-3181 SSC: subspatial statistique Clustering de (. Bradley et al 1998) Qui Traité de l'accélération de l'EM Algorithme Dans le général CAS. Une piste possible Autre is d'EVITER de considerer les dimensions Toutes au cours de l'algorithme, en ne les dimensions Que sélectionnant de poids maximale. Notons enfin Que notre méthode la Nécessite d'un Donnée de la partie Paramètre de l'Utilisateur: K, le Nombre de grappes REcherches. Une improvement possible à identifier automatiquement consisterait CE Paramètre. Pour ACDE, il EST le classique d'UTILISER BIC critère (Ye et al., 2003). Dans Notre CAS, enchainee piste d'originale Serait le fait Qué UTILISER when K is au upper de groupes Nombre réel REcherches, les rules Associées Alors aux groupes se chevauchent. P. Enquête Berkhin Références du regroupement des techniques d'exploration de données. Rapport technique, logiciel Accrue, San Jose, Californie, 2002. Blake C.L. et Merz C. J. (1998). UCI référentiel de bases de données d'apprentissage de la machine [http: //www.ics.uci.edu/~mlearn/MLRepository.html]. Bradley P., U. Fayyad, et Reina C. Mise à l'échelle EM (Expectation-Maximisation) Clustering aux grandes bases de données. Microsoft Research Report, MSR-TR-98-35, août 1998. Candillier L., Tellier I., Torre F. Bousquet et O. SSC: statistique subspatial Clustering. Rapport technique grappa, 2005. [http: //grappa.univ-lille3.fr/~candillier/publis]. Domeniconi C, Papadopoulos D., D. et Gunopolos Ma S. Subspace Clustering de données de grande dimension. Dans SIAM Int. Conf. sur Data Mining, 2004. L. Parsons, E. Haque et Liu H. L'évaluation des algorithmes de regroupement de sous-espace. Dans l'atelier sur le clustering de haute dimension de données et ses applications, SIAM Int. Conf. sur Data Mining, pp 48-56, 2004. Pelleg D. et A. Moore Mélanges de rectangles: regroupement doux interprétable. Dans C. Brodley et A. éditeurs Danyluk, ICML 2001, pp 401-408. Ye L. et Spetsakis M.E. Clustering sur les données non observées en utilisant un mélange de gaussiennes. Rapport technique, Université York, octobre 2003. Résumé Dans cet article, nous nous concentrons sur la tâche de regroupement subspatiale, qui a deux objectifs: en même temps d'identifier les groupes et les sous-espaces où chacun d'eux est défini et décrire chaque groupe avec comme quelques dimensions que possible, de sorte que les résultats sont facilement interprétable par un utilisateur humain. Un défaut de méthodes existantes est qu'elles ne tiennent compte que des bases de données numériques. Le but de cet article est de proposer un nouvel algorithme de clustering subspatiale, capable d'aborder les bases de données qui peuvent contenir en continu ainsi que des attributs discrets. Nous présentons une méthode basée sur l'algorithme EM classique, mais appliqué à un modèle mod'ele simplifié et suivi par une technique originale de sélection de fonction qui ne conserve que les dimensions qui sont pertinentes pour chaque groupe. Des expériences, menées sur artifical ainsi que des bases de données réelles, montrent que notre algo- rithme donne des résultats robustes, en termes de classement et de l'intelligibilité de la sortie. EGC 2005 RNTI-E-3 182"
1187,Revue des Nouvelles Technologies de l'Information,EGC,2004,A Galois connecion semantics-based approach for deriving generic bases of association rules,"L'augmentation vertigineuse de la taille des données (textuelles ou transactionnelles) est un défi constant pour la ""scalabilité"" des techniques d'extraction des connaissances. Dans ce papier, on présente une approche pour la dérivation des bases génériques de règles associatives. Les principales caract éristiques de cette approches sont les suivantes. D'une part, l'introduction d'une structure de données appelée ""Trie-itemset"" pour le stockage de la relation en entrée. D'autre part, on utilise une méthode ""Diviser pour régner"" pour réduire le coût de construction de structures partiellement ordonnées, à partir desquelles les bases génériques de règles sont directement extraites.","Sadok Ben Yahia, Narjes Doggaz, Yahya Slimani, Jihem Rezgui",http://editions-rnti.fr/render_pdf.php?p1&p=1001033,http://editions-rnti.fr/render_pdf.php?p=1001033,en,"Une approche basée sur la sémantique de connexion Galois pour obtenir des bases génériques de règles d'association S. Ben Yahia, N. Doggaz Y. Slimani, J. Rezgui Département des Sciences de l'Informatique Faculté des Sciences de Tunis Campus Universitaire, 1060 Tunis, Tunisie. sadok.benyahia; yahya.slimani; narjes.doggaz@fst.rnu.tn, jihen rezgui@yahoo.fr CV. L'augmentation de la taille vertigineuse des Données (transactionnelles OU textuelles) is a Constante de Défi pour la « Scalabilité » des techniques d'ex- traction des connaissances heuristiques. Dans papier CE, sur approach Une Présente la verser des bases Génériques dérivation de rules associatives. Les ractéristiques de CA- Principales approaches this les Suivantes are. D'une partie juin, l'instauration d'structure de juin appelee Données « Trie-itemset » pour le stockage de la rela- tion en entrée. D'une partie de autre, utiliser le juin méthode « verser Diviser régner » pour le Réduire de construction de Coût des structures ordonnées Partiellement, à partir les bases Génériques desquelles de rules are Directement extraites. 1 Introduction De nombreuses recherches dans l'exploration de données de grandes bases de données a mis l'accent sur la découverte des règles de sociation as- [Agrawal et Skirant, 1994, Brin et al., 1997, Manilla et al., 1994]. Association génération de règles est réalisée à partir d'un ensemble F de itemsets fréquents dans un contexte d'extraction D, pour une minsupp de soutien minimal. Une association règle r est une relation entre itemsets de la forme r: X ⇒ (Y -X), dans laquelle X et Y sont des motifs fréquents, et X ⊂ Y. Jeux d'éléments X et (Y-X) sont appelés, respectivement, antécédent et conclusion de la règle r. Les règles d'association valides sont celles dont la mesure de confiance Conf (r) = support (Y) support (X) 1 est supérieur ou égal au seuil minimal de confiance, minconf nommé. Si Conf (r) = 1 alors r est appelée règle d'association exacte (ER), sinon elle est appelée règle d'association approximative (AR). Et la visualisation de l'exploitation des règles d'association est loin d'être une tâche triviale, surtout à cause du grand nombre de règles potentiellement intéressantes qui peuvent être tirées à partir d'un ensemble de données. Diverses techniques sont utilisées pour limiter le nombre de règles déclarées, en commençant par des techniques d'élagage de base sur la base de thre- sholds à la fois la fréquence du motif représenté (appelé le support) et la force de la dépendance entre l'antécédent et la conclusion (appelée confiance) . Des techniques plus avancées qui produisent un nombre limité de l'ensemble de règles reposent sur les fermetures et les connexions Galois [Bastide et al., 2000, Stumme et al., 2001, Zaki, 2000], qui sont à leur tour dérivé de la théorie de Galois et l'analyse de concepts formels (CAF) [Ganter et Wille, 1999]. Enfin, les travaux sur la FCA ont donné une série de résultats sur les représentations compactes des familles ensemble fermées, également appelées bases, dont l'impact sur l'association réduction de la règle est actuellement à l'étude intensive au sein de la communauté [Bastide et al., 2000, Stumme et al., 2001]. Dans cet article, nous proposons une nouvelle structure de données à base de Trie l'on appelle l'arbre « itemset-Trie ». arbre itemset-Trie étend l'idée revendiquée par les auteurs de FPTree [Han et al., 2000] et CATS [Cheung et Zaiane, 2003], visant à améliorer la compression de stockage et permettre (fermé) extraction de motifs fréquents sans candidat « explicite » itemsets génération. Ensuite, nous proposons un algorithme, qui tombe dans la caractérisation « Divide and Conquer » pour extraire les itemsets fermés fréquents avec leurs générateurs minimaux associés. Il est à noter que la dérivation de la base Luxemburger est basée sur l'exploration de ces jeux d'éléments fermés organisés sur leur ordre naturel partielle (également appelée relation de priorité). Voilà pourquoi nous construisons sur l'approche basée sur la sémantique de connexion Galois pour obtenir générique ... mouche, en même temps que le processus de découverte des itemsets fermés, le « iceberg réseau » local. Ces sous-structures locales commandées peuvent être tirées tout naturellement d'une manière parallèle. Ensuite, ces sous-structures ordonnées sont analysés pour dériver, de façon simple, l'association locale des bases génériques. Enfin, les bases locales sont fusionnées pour générer le global. Un tel processus peut être récapitulé comme suit: - Construire le itemset-Trie - Construire les structures locales ordonnées - Fusionner les structures locales ordonnées pour déduire des bases de règles d'association Le reste du papier est organisé comme suit: Dans la section 2, nous motiver le choix de le jeu d'éléments d'arborescence et présente un algorithme pour sa construction. La section 3 présente un algorithme pour la construction des structures ordonnées. L'article 4 conclut le papier et les points les orientations futures à suivre. 2 structure de données itemset-Trie Dans le contexte de l'exploitation minière motifs fréquents (fermés) dans les bases de données de transaction ou de nombreux autres types de bases de données, un nombre important d'études se fondent sur Apriori comme « générer - et test » approche [Agrawal et Skirant, 1994 ]. Cependant, cette approche souffre d'une étape de génération de jeu de candidats très coûteux, en particulier avec des motifs longs ou lorsque nous réduisons les besoins par l'utilisateur. Cet inconvénient est renforcé par des analyses de base de données de disque stockées tediously répétées. Pour éviter le goulot d'étranglement d'approche, des études récentes (par exemple, les travaux d'avant-garde de Han et al. Et sa structure FP-tree [Han et al., 2000]) a proposé d'adopter une structure de données de pointe, où la base de données est compressé afin de réaliser l'extraction de modèle. L'idée derrière la structure de données compacte FP-arbre est que lorsque plusieurs transactions partagent une fréquence identique itemset, ils peuvent être fusionnés en un seul avec un certain nombre d'occurrences enregistré. À côté d'une étape de tri coûteux, la structure FP-Tree proposée est malheureusement pas adapté à un processus d'extraction interactive, dans lequel un utilisateur peut être intéressé à faire varier la valeur de support. Dans ce cas, le FP-arbre doit être reconstruit depuis sa construction dépend soutien. Bien que le travail présenté dans [Cheung et Zaiane, 2003] aborde cette insuffisance, la structure de pro- posé appelé CATS dans lequel un seul élément est représenté dans un nœud. Voilà pourquoi nous introduisons une, indépendante de soutien, structure plus compacte appelée itemset-Trie, dans lequel chaque noeud est composé par un jeu d'éléments. Pour illustrer cette compacité, considérons la base de données de transac- tions données par la figure 1. La figure 1 (a) représente le FP-arbre associé, tandis que la figure 1 (b) représente la itemset-Trie associée. Exemple 1 Considérons la base de données de transaction donnée par la figure 1 (gauche). Chaque nœud a la structure suivante: <itemset / support>. Dans un premier temps, le vide Trie est et il est composé par un seul nœud racine. Nous commençons par le traitement de la première transaction « acfmp ». Nous dérivons un nœud de la racine et nous ajoutons un nouveau nœud contenant la chaîne « acfmp / 1 ». Ensuite, nous traitons la transaction « ABCFM ». Cette transaction et le précédent ont en commun (ou préfixé) {a} élément. Par conséquent, le premier noeud est divisé: on garde le noeud avec « / 2 » et deux noeuds sont dérivés contenant respectivement « BCFM / 1 » et « PIE / 1 ». Traitement de la troisième transaction « bf » conduira à la création d'un nouveau noeud « bf / 1 », directement dérivé du nœud racine, car aucun élément sont préfixés en commun. Le processus décrit ci-dessous est respecté jusqu'à ce que le traitement de toutes les transactions. 3 Dérivation des structures ordonnées En raison du manque d'espace disponible, lecteur intéressé par les résultats clés de la base RNTI-réseau Galois - E - 2 Ben Yahia et al. Articles TID 1 acfmp 2 ABCFM 3 bf 4 BCP 5 acfmp / 1/ 2 (a ) (b) figure 1 - gauche:. base de données de transaction droite: FP-tree et l'itemset-Trie associée à la base de données de transaction. paradigme CAF est appelé [Ganter et Wille, 1999]. En sortie de la première étape, nous avons construit le jeu d'éléments d'arborescence. Afin d'effectuer une extraction de règles asso- ciation (spécialement la base de règles approchées), nous avons besoin de construire des structures ordonnées en fonction de la relation de priorité. 3. 1 Principes Comme nous ne travaillons qu'avec itemsets fermés, les besoins de construction de commande pour récupérer la relation de la préséance famille de itemsets fermés. L'objectif principal (et contribution également) de notre approche est de découvrir les itemsets fermés et de les commander à la volée. Nous ne voulons pas construire une seule structure ordonnée de la relation d'entrée (qui tourne pour construire le diagramme de Hasse), mais au contraire, nous cherchons la construction de plusieurs structures ordonnées. Bien sûr, une certaine redondance apparaît à-dire une donnée itemset fermé peut apparaître dans plus d'une structure ordonnée, mais nous éviter le coût élevé de la construction du diagramme de Hasse. Ceci est effectué dans un processus graduel à-dire en reliant une notion à la fois à une structure qui est partiellement terminée. Une fois l'arbre itemset-Trie est construit, il peut être utilisé pour fermer la mine et leurs jeux d'éléments générateurs minimaux ted ciations à plusieurs reprises pour différents réglages de seuils de soutien sans qu'il soit nécessaire de reconstruire l'arbre. Comme FP-croissance [Han et al., 2000] et Féline [Cheung et Zaiane, 2003], l'algorithme proposé tombe dans les règles d'association Algorithmes d'exploration de caractérisation « Diviser pour mieux régner ». Le premier itemset-Trie est fragmenté en sous-essais conditionnels. En effet, étant donné un modèle appelé p, un arbre itemset-conditionnelle de Trie de p est construit, ce qui représente fidèlement toutes tions qui contiennent transac- motif p. Par exemple, compte tenu de la base de données de transaction de la figure 1, l'ensemble des 1-itemsets, avec leurs supports associés, est la suivante: <a / 3; b / 3; c / 4; f / 4; m / 3; p / 3>. Nous avons donc de tirer les années A, B et ainsi de suite itemset-essais conditionnels. Il est à noter que, contrairement à FP-croissance [Han et al., 2000] et placard [Pei et al., 2002] algorithmes, nous considérons que l'ordre lexicographique et nous considérons que dans une donnée conditionnelle Trie toutes les 1 itemset restants devraient être compris. Par exemple, dans les algorithmes mentionnés ci-dessus (à savoir, FP-croissance et Closet), la condition de Trie b ne comprend pas le 1-itemset {a} et de c exclura les deux {a} et {b}. Les auteurs, dans le but de découvrir que itemsets fermés, affirment qu'il n'y a pas besoin d'inclure le 1-itemset {a} dans celui de la b, puisque tous les itemsets fermés contenant {a} ont déjà été extraites pour Trie conditionnelle de l'un. Dans notre approche, nous cherchons à extraire itemsets fermés et leurs générateurs minimaux associés, à RNTI - E - 2 Une approche basée sur la sémantique de connexion Galois pour dériver générique ... construire leur structure ordonnée associée (à savoir, diagramme de Hasse). Étant donné que nous prévoyons de mener le processus d'extraction de manière parallèle, en attribuant à chaque processeur un sous-ensemble de l'ensemble des essais sous condition, chaque sous-Trie doit contenir une description exhaustive pour assurer la justesse de la découverte des itemsets fermée et pour réduire les coûts de communication inter-processeurs à vérifier itemsets inclusions. (A) (b) (c) acfm acfmp ABCFM (d) ABCFM b bf bc BCP (f) Roota / 3 bc fm / 1 c fm p / 2 Roota / 3 bc fm / 1 Roota / 3 c fm p / 2 b / 2 Roota / 3 bc fm / 1 f / 1 cp / 1 (e) Fig. 2 - (a) {a} est conditionnelle jeu d'éléments d'arborescence. (B) {ab} » s conditionnelle itemset-Trie. (C) {ap} » s conditionnelle itemset-Trie. (D) structure ordonnée associée à la 1-itemset {a}. (E) {b} est conditionnelle itemset-Trie. (F) structure ordonnée associée à la 1-itemset {b}. Exemple 2 Considérons la base de données de transaction donnée par la figure 1 (gauche). Ci-dessous, nous DES- CRIBE les structures ordonnées pour la construction minsup = 1. L'ensemble de 1-itemsets, avec leurs supports sociated as-, est définie comme suit: <a / 3; b / 3; c / 4; f / 4; m / 3; p / 3>. Ensuite, en commençant par itemset-Trie conditionnelle de l'un, représenté sur la figure 2 (a), nous pouvons trouver les associés itemset La liste: <b / 1; c / 3; f / 3; m / 3; p / 2>. De cette liste, on remarque que le 1-itemsets c, f et m sont aussi fréquentes que le 1-itemset a. Ils constituent donc un itemset fermé {} de ACFM avec un support égal à 2 et le 1-itemset {a} comme générateur minimal. Le 1-itemsets c, f et m ar e retiré de La. Puisqu'il est pas vide, nous devons aller récursive plus en profondeur et de construire les sous-tente, comme le montre la figure 2 (b et c), respectivement pour les 2-itemsets {ab} et {ap }. De Lab on découvre le itemset fermé {} ABCFM avec un support égal à 1 et le 2-itemset {ab} comme générateur minimal. Alors que de tour, on découvre le itemset fermé {} de acfmp avec le soutien égal à 1 et le 2-itemset {ap} comme générateur minimal. Le traitement de La fin car il n'y a pas plus d'éléments à manipuler. En sortie, le diagramme de Hasse local (associé à itemset-Trie conditionnelle du a) peut être établie progressivement. En effet, la première, les jeux d'éléments fermés {ACFM} et {} ABCFM en profondeur de la liste La permet de connecter, et la seconde pour connecter {acfm} et {} acfmp, comme représenté sur la figure 2 (d). L'algorithme doit traiter suivant la liste Lb: <a / 1; c / 2; f / 2; m / 1; p / 1>, extraite de la trie conditionnelle représentée sur la figure 2 (e). Nous pouvons vérifier facilement qu'aucun 1-itemset est si fréquent que b et {b} RNTI - E - 2 Ben Yahia et al. est un jeu d'éléments de fermeture. Étant donné que la liste reste à développer est pas vide, nous allons plus loin en profondeur et nous commençons par le 2-itemset {ab}. Lab est définie comme suit: <c / 1; f / 1; m / 1> et dont on découvre le itemset fermé {} ABCFM avec un support égal à 1 et le 2-itemset {ab} comme générateur minimal. Il n'y a pas plus d'exploration de cette liste car il est vide. Le itemset fermé {b} est connecté au itemset fermé {} ABCFM. Ensuite, nous devons nous attaquer Lbc qui est égale à <a / 1; f / 1; m / 1; p / 1>. Tout 1-itemset dans cette liste est si fréquente que {bc} et nous pouvons conclure que {bc} est un itemset fermé avec un support égal à 2 et ayant {bc} comme générateur minimal. La liste avec laquelle d'aller plus loin en profondeur reste inchangé. Nous avons respectivement à gérer CSLA, LBCF et LBCM, tout ce qui donne le itemset fermé {} ABCFM. Le itemset fermé {bc} est relié à celui de {} ABCFM. Ensuite, nous devons connecter {b} à {bc}. Ceci est effectué après un contrôle systématique si elles partagent un successeur immédiat commun, ce qui est le cas dans cet exemple. En fait, {bc} et {b} sont connectés respectivement à leur successeur immédiat qui est {} ABCFM. Voilà pourquoi nous devons supprimer le lien entre {b} et {} ABCFM. Le traitement des extrémités de la liste Lbc en lançant la liste LBCP, qui donne l'itemset fermé {bcp} avec un support égal à 1 et {} bcp comme générateur minimal. La structure ordonnée associée est illustrée sur la figure 2 (f). 3.2 Dérivation des bases génériques de règles d'association Le problème de la pertinence et de l'utilité des règles d'association extraites est d'impor- tance primaire. En effet, dans la plupart des bases de données réelles, des milliers et des millions même de règles de haute confiance sont générés parmi lesquels beaucoup sont redondantes. Ce problème a encouragé le dévelop- pement des outils pour la classification des règles en fonction de leurs propriétés, pour la règle de sélection selon des critères définis par l'utilisateur, et pour la visualisation de la règle. En ce qui concerne [Luxemburger, 1991] et [Guigues et Duquenne, 1986], nous considérons que, compte tenu d'une structure locale ordonnée, relation basée préséance ting représenta- ordonné itemsets fermés, bases génériques de règles d'association peuvent être dérivées d'une manière simple. Dans cette structure chaque itemset fermée est « décorée » avec sa liste associée des générateurs minimaux. En effet, AR représente « inter-noeud » implications, assorti d'une information statistique, à savoir, la confiance, à partir d'un sous-jeu d'éléments de fermeture-super-fermé-jeu d'éléments en commençant à partir d'un noeud donné dans une structure ordonnée. Inversement, ER sont implications « intra-noeud » extraites de chaque noeud dans la structure ordonnée. 4 Conclusion Nous avons présenté dans cet article une nouvelle structure de données pour extraire les itemsets fermés fréquents afin de générer des bases génériques de règles d'association. Les principales caractéristiques de cette structure sont. Tout d'abord une représentation compacte, puisque dans notre approche du nœud représente un itemset alors que dans d'autres approches, comme FP-Gr owth et CATS, un noeud ne représente qu'un seul attribut. En second lieu, un adapté pour une « Divide and Conquer » fermé itemsets approche d'extraction. Ensuite, nous avons proposé un algorithme pour construire des structures locales commandées à partir de laquelle il est possible d'obtenir des bases génériques de règles d'association. Maintenant, l'approche proposée est en cours d'expérimentation. Dans un proche avenir, nous prévoyons d'examiner les avantages potentiels de la mise en œuvre de l'approche proposée sur une machine MIMD (IBM SP2). En effet, les fils de la méthode de construction à un parallélisation naturel, en ce sens que chaque processeur d'une architecture parallèle peut construire localement sa structure ordonnée. Une fois que les structures locales sont construits, un processeur maître peut les fusionner pour dériver un ensemble de bases génériques de règles d'association. RNTI - E - 2 Une approche basée sur la sémantique de connexion Galois pour obtenir générique ... Références [Agrawal et Skirant, 1994] R. Agrawal et R. Skirant. algorithmes rapides pour les règles d'association minière. Dans Actes de la 20e Conférence internationale sur les très grandes bases de données, pages 478-499, Juin 1994. [Bastide et al., 2000] Y. Bastide, N. Pasquier, R. Taouil, L. Lakhal, et G. Stumme. Exploitation minière règles d'association non redondantes minimales en utilisant itemsets fermés fréquents. Dans Actes de la Conférence internationale DOOD'2000, Notes de cours en sciences informatiques, Springer-, pages 972-986 verlag, juillet 2000. [Brin et al., 1997] S. Brin, R. Motwani, et J. Ullman. Dynamique itemset comptage et les règles de cations impli-. Dans Actes SIGMOD, Conférence internationale sur la gestion des données, Tucson, Arizona, Etats-Unis, pages 255-264, 1997. [Cheung et Zaiane, 2003] W. Cheung et O.R. Zaiane. l'extraction progressive des motifs fréquents sans génération de candidat ou contrainte de soutien. Dans Actes du génie national septième inter-bases de données et applications Symposium (IDEAS 2003), Hong Kong, Chine, 16-18, Juillet 2003. [Ganter et Wille, 1999] B. Ganter et R. Wille. Analyse formelle de concepts. Springer-Verlag, Heidelberg, 1999. [Guigues et Duquenne, 1986] J.L. Guigues et V. Duquenne. Minimales d'families implica- tions d'un informatives tableau résultant de Données binaires. Mathématiques et Sciences Humaines, (95): 5-18, 1986. J. Han, J. Pei, et Y. Yin [Han et al., 2000]. Exploitation minière motifs fréquents sans génération de candidats. Dans Proceedings of the ACM-SIGMOD Intl. Conférence sur la gestion des données (SIGMOD'00), Dallas, Texas, pages 1-12, mai 2000. [Luxemburger, 1991] M. Luxemburger. de Implication un partielles de Contexte. Mathématiques et Sciences Humaines, 29 (113): 35-55, 1991. H. Manilla, H. Toinoven, et I. Verkamo [Manilla et al., 1994]. Des algorithmes efficaces pour les règles d'association disco-Vering. Dans AAAI sur Worshop Knowledge Discovery dans les bases de données, pages 181-192, Juillet 1994. [Pei et al., 2002] J. Pei, J. Han, R. Mao, S. Nishio, S. Tang, et D. Yang. Closet: Un algorithme effi- cace pour l'exploitation itemsets fermés fréquents. Dans Actes du SIGMOD DMKD'00, Dallas, TX, pages 21-30, 2002. [Stumme et al., 2001] G. Stumme, R. Taouil, Y. Bastide, N. Pasquier, et L. Lakhal. structuration intelligente et la réduction des règles d'association avec l'analyse formelle de concepts. Dans Actes de la Conférence KI'2001, Lecture Notes in Intelligence artificielle 2174, Springer-Verlag, pages 335-350, septembre 2001. [Zaki, 2000] M. J. Zaki. Génération de règles d'association non redondants. Dans Actes du 6e SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données, Boston, MA, pages 34-43, Août 2000. Résumé. La croissance constante de la taille des données textuelles (ou transactionnelles) est un progrès-facteur clé pour plus des techniques d'extraction de connaissances aiguës, dont sont constamment mis au défi l'efficacité et de l'efficacité. Dans cet article, nous présentons une approche pour obtenir des bases génériques de règles d'association. L'approche proposée est la base sémantique de connexion Galois. Les principales caractéristiques de notre approche sont les suivants: d'abord, pour éviter les opérations d'E / S intensives, nous présentons une structure de données à base de Trie avancé pour stocker les relation d'entrée. En second lieu, nous utilisons une méthode « Divide and Conquer » pour réduire le coût de la construction de petites sous-structures partiellement ordonnées, dont nous retirons dans une association de façon simple des bases génériques. RNTI - E - 2"
1188,Revue des Nouvelles Technologies de l'Information,EGC,2004,A metric approach to supervised discretization,"Nous présentons une nouvelle approche à la discrétisation supervisée des attributs continues qui se sert de l'espace métrique des partitions d'un ensemble fini. Nous discutons deux nouvelles idées fondamentales : une généralisation des techniques de discrétisation de Fayyad-Irani basée sur une distance sur des partitions, dérivée de l'entropie généralisée de Daroczy, et un nouveau critère géométrique pour arrêter l'algorithme de discrétisation. Les arbres de décision résultants sont plus petits, ont moins de feuilles, et montrent des niveaux plus élevés d'exactitude établis par la validation croisée stratifiée.","Dan A. Simovici, Richard Butterworth",http://editions-rnti.fr/render_pdf.php?p1&p=1000966,http://editions-rnti.fr/render_pdf.php?p=1000966,en,"Microsoft Word - EGC2004_Khiops.doc une méthode robuste pour partitionner les valeurs des attributs catégorielles Marc Boullé * * France Télécom R & D, 2, avenue Pierre Marzin, 22300 Lannion, France marc.boulle@francetelecom.com CV. Dans le domaine de l'apprentissage supervisez, les methods de groupages des d'un attribut Modalités symbolique de construct un permettent nouvel au attribut synthetic au maximum la conservant Valeur informationnelle de l'initiale et attribut le diminuant Nombre de Modalités. Nous proposons ici juin de l'algorithme généralisation de discrétisation Khiops1 for the du groupages des Problème Modalités. L'algorithme de CONTROLER Përmet Proposé a priori le osée de sur-et d'Improving apprentissage la robustesse des significativement groupages Produits. This characteristic was de robustesse available in la statistique des étudiant les variations du Khi2 du critère de LORs de regroupements d'un tableau lignes de Contingence et en modélisant le comportement de l'algorithme statistique Khiops. Des Expérimentations de Intensifs Ontario permis et approach this valider Montré Que la Ontario méthode de groupages Khiops aboutIt à des groupages Performants, à la Fois en term et de qualité prédictive Nombre de Faible Groupes. 1. Introduction Alors que le problème de discrétisation a été largement étudié dans le passé, le problème de regroupement n'a pas été exploré si profondément dans la littérature. Cependant, dans la réalité des ensembles de données d'exploration de données, il existe de nombreux cas où le regroupement des valeurs des attributs catégoriques est une étape de pré-traitement obligatoire. Le problème de regroupement consiste à diviser l'ensemble des valeurs d'un attribut catégorique en un nombre fini de groupes. Par exemple, la plupart des arbres de décision exploitent une méthode de regroupement pour gérer les attributs catégoriques, afin d'augmenter le nombre de cas dans chaque nœud de l'arbre [Zighed et Rakotomalala, 2000]. les réseaux de neurones sont basées sur des attributs numériques et souvent utiliser un codage binaire 1-à-N pour prétraiter catégorique attributs. Lorsque les catégories sont trop nombreuses, ce schéma de codage peut être remplacé par une méthode de regroupement. Ce problème se pose dans de nombreux autres algorithmes de classification, tels que les réseaux bayésiens, la régression linéaire ou régression logistique. En outre, le regroupement est un procédé d'usage général qui est intrinsèquement utiles dans l'étape de préparation des données du procédé d'extraction de données [Pyle, 1999]. Les méthodes de regroupement peuvent être regroupés en fonction de la stratégie de recherche de la meilleure partition et au critère de regroupement utilisé pour évaluer les partitions. L'algorithme simple essaie de trouver les meilleurs bipartition avec une catégorie contre tous les autres. Une approche plus intéressante consiste à rechercher une bipartition de toutes les catégories. Le procédé de l'avant séquentiel de sélection dérivé de la [. Cestnik et al, 1987] et évalués par [Berckman, 1995] est un 1 les brevets français N ° 01 07006 et N ° 02 16733 une méthode robuste pour diviser les valeurs de Categorical Attributs RNTI - 1 algorithme glouton qui initialise un groupe avec la meilleure catégorie (contre les autres), et ajoute de nouvelles catégories itérativement à ce premier groupe. Lorsque l'attribut de classe a deux valeurs, [Breiman et al., 1984] ont proposé une méthode d'CART optimale pour regrouper les catégories en deux groupes pour le critère de Gini. Cet algorithme trie d'abord les catégories en fonction de la probabilité de la première valeur de classe, et puis recherche les meilleurs éclatés dans cette liste triée. Cet algorithme a une complexité temporelle de I.log (I), où I est le nombre de catégories. Sur la base des idées présentées dans [Lechevallier, 1990; Fulton et al., 1995], ce résultat peut probablement être étendue à trouver la partition optimale des catégories en groupes K dans le cas de deux valeurs de classe, avec l'utilisation d'un algorithme de programmation dynamique de la complexité du temps I2. Dans le cas général de plus de deux valeurs de classe, la re pas d'algorithme pour trouver le regroupement optimal avec des groupes K, en dehors de la recherche exhaustive. Cependant, [Chou, 1991] a proposé une approche basée sur K-means qui permet de trouver une partition localement optimale des catégories en K- groupes. algorithmes d'arbres de décision gèrent souvent le problème de regroupement avec une heuristique gloutonne basée sur une classification ascendante des catégories. L'algorithme commence par un seul groupe de catégorie, puis recherche la meilleure fusion entre les groupes. Le processus est répété jusqu'à ce qu'aucune autre fusion peut améliorer le critère de regroupement. L'algorithme CHAID [Kass, 1980] utilise cette approche gourmande avec un proche critère de ChiMerge [Kerber, 1991]. Les meilleures fusions sont recherchées en réduisant au minimum le niveau de confiance du critère du chi carré appliqué localement à deux catégories: elles sont fusionnées si elles sont statistiquement similaires. L'algorithme ID3 [Quinlan, 1986] utilise le critère de gain d'information pour évaluer les attributs catégoriques, sans regroupement. Ce critère tend à favoriser les attributs avec de nombreuses catégories et [Quinlan, 1993] a proposé en C4.5 pour exploiter le critère de rapport de gain, en divisant le gain d'information par l'entropie des catégories. Le critère du chi carré a également été appliquée globalement sur l'ensemble des catégories, avec une version normalisée de la valeur du chi carré tels que V ou T du Tschuprow de Cramer [Ritschard et al., 2001] afin de comparer deux différents partitions -SIZE. La méthode de regroupement Khiops est une généralisation directe de la méthode de discrétisation Khiops [Boullé, 2003a]. Au lieu de fusionner des valeurs numériques adjacentes afin d'intervalles de construction, le procédé de groupement fusionne les valeurs nominales en groupes de valeurs. Dans les deux cas, l'algorithme de recherche est une heuristique gloutonne ascendante qui optimise le critère du chi carré appliqué à l'ensemble des intervalles ou des groupes. La règle d'arrêt est basé sur le niveau de confiance calculé avec les statistiques du chi carré. La méthode arrête automatiquement le processus de fusion dès que le niveau de confiance, liée à l'épreuve de l'indépendance entre l'attribut partitionné et l'attribut de classe, ne diminue pas plus. L'ensemble des groupes résultant d'une méthode de regroupement fournit un classificateur univariée élémentaire, qui prévoit la distribution des valeurs de classe dans chaque groupe a appris. Une méthode de regroupement peut être considéré comme un algorithme inductif, donc soumis à overfitting. Nous appliquons une méthode similaire à celle développée pour la méthode de discrétisation Khiops afin d'apporter un véritable contrôle de surajustement. Le principe est d'analyser le comportement de l'algorithme lors du regroupement d'un organisme indépendant d'attribut explicatif de l'attribut de classe. Nous étudions les statistiques des variations des valeurs du chi carré lors de la fusion des catégories et proposons de modéliser le maximum de ces variations dans un processus de regroupement complet. L'algorithme est ensuite modifié afin de forcer toute fusion dont la variation de la valeur du chi carré est inférieure à la variation maximale prédite par notre modélisation statistique. Ce changement dans l'algorithme donne la garantie probabiliste intéressant que tout attribut indépendant seront regroupés au sein d'un seul groupe terminal et que tout attribut dont Boullé RNTI - 1 groupe se compose d'au moins deux groupes contient vraiment des informations prédictives sur l'attribut de classe. Ceci est confirmé expérimentalement. Le reste du document est organisé comme suit. La section 2 présente brièvement les initiales de Khiops de regroupement algorithme. La section 3 présente la modélisation statistique de l'algorithme et son réglage fin pour éviter surajustement. L'article 4 procède à une vaste évaluation expérimentale. 2. Le regroupement Khiops Méthode Dans cette section, nous rappelons les principes du test du chi carré et présente l'algorithme de regroupement Khiops, dont la description détaillée et l'analyse peut être trouvée dans [Boullé, 2003b]. 2.1 Le test du chi carré: Principes et Notat ions Considérons un attribut explicatif et un attribut de classe et déterminer si elles sont indépendantes. Tout d'abord, tous les cas sont résumés dans un tableau de contingence, où les cas sont comptés pour chaque paire de valeurs d'attributs explicatifs et de classe. La valeur de chi carré est calculée à partir du tableau de contingence, d'après le tableau 1 notations. nij: fréquence observée pour les i-ième valeur explicative A B C Total et une valeur de classe d'un jième n11 n12 n13 n1. ni .: Fréquence totale observée pour i e valeur explicative b n21 n22 n23 n2. n.j: Total fréquence observée pour les j-ième valeur de la classe c n31 n32 n33 n3. N: Nombre total observé fréquence d n41 n42 n43 n4. I: Nombre de valeurs d'attributs explicatifs e N51 N52 N53 n5. J: Nombre de valeurs classe totale n.1 n.2 n.3 N TAB 1 - Table de contingence utilisée pour calculer la valeur du chi carré. Laissez eij = ni..n.j / N, représentent la fréquence attendue pour la cellule (i, j) si les attributs explicatifs et de classe sont indépendants. La valeur de chi carré est une mesure sur l'ensemble de tableau de contingence de la différence entre les fréquences observées et les fréquences attendues. Il peut être interprété comme une distance par rapport à l'hypothèse de l'indépendance entre les attributs. () ΣΣ - = i j ij ijij e en Chi 2 2. (1) Dans l'hypothèse nulle d'indépendance, la valeur du chi carré est soumis à des statistiques du chi carré avec (I-1). (J-1) degrés de liberté. Ceci est la base d'un test statistique qui permet de rejeter l'hypothèse de l'indépendance; plus la valeur du chi carré est, plus le niveau de confiance est. 2.2 La valeur initiale algorithme de chi carré dépend des fréquences observées locales dans chaque ligne individuelle et sur les fréquences mondiales observées dans l'ensemble de tableau de contingence. Ceci est une bonne une méthode robuste pour partitionner les valeurs des attributs catégorielles RNTI - critère candidat 1 pour une méthode de regroupement. Les statistiques du chi carré est paramétrées par le nombre de valeurs explicatives (en rapport avec les degrés de liberté). Afin de comparer deux groupes avec différents numéros de groupe, nous utilisons le niveau de confiance au lieu de la valeur carrée khi. Le principe de l'algorithme Khiops est de minimiser le niveau de confiance entre l'attribut explicatif groupés et l'attribut de classe par le biais de statistiques du chi carré. La valeur du chi carré n'est pas fiable pour tester l'hypothèse de l'indépendance si la fréquence attendue dans une cellule du tableau de contingence tombe en dessous une valeur minimale. Les Copes algorithme avec cette contrainte dans un pré-traitement: une catégorie initiale qui ne remplit pas la contrainte de fréquence minimale est inconditionnellement fusionnées en un groupe spécial. Le procédé Khiops est basé sur un algorithme de bas en haut gourmand. Il commence par catégories initiales et cherche ensuite la meilleure fusion entre les catégories. L'algorithme est répété jusqu'à ce qu'aucune autre fusion peut diminuer le niveau de confiance. La complexité de calcul de l'algorithme peut être réduite à O (n.log (N) + I2.log (I)) avec certaines optimisations [Boullé, 2003b]. Il existe deux différences principales entre l'algorithme Khiops initial et l'algorithme CHAID similaire [Kass, 1980]. Tout d'abord, le critère de chi-carré est appliquée globalement à la partition entière, dans le cas de l'algorithme Khiops, alors qu'il est appliqué localement à deux groupes adjacents dans le cas de l'algorithme CHAID. En second lieu, l'algorithme Khiops arrête le processus de fusion lorsque le niveau de confiance augmente après la meilleure fusion candidat, alors que l'algorithme CHAID arrête lorsque le niveau de confiance est au-delà d'un seuil fixé par l'utilisateur. 3. Analyse statistique de l'algorithme L'algorithme Khiops choisit la meilleure fusion entre toutes les fusions possibles des catégories et itère ce processus jusqu'à ce que la règle d'arrêt est remplie. Lorsque l'attribut explicatif et l'attribut de classe sont indépendants, l'ensemble résultant des groupes devrait être composé d'un seul groupe, ce qui signifie qu'il n'y a pas d'information prédictive l'attribut explicatif. Dans ce qui suit, nous étudions le comportement statistique de l'algorithme Khiops initial. Dans le cas de deux attributs indépendants, la valeur du chi carré est soumis à des statistiques du chi carré, avec espérance et de la variance connue. Nous étudions la loi DeltaChi2 (variation de la valeur du chi carré après la fusion des deux catégories) dans le cas de deux attributs indépendants. Au cours d'un processus de regroupement, un grand nombre de fusions sont évaluées, et, à chaque étape, l'algorithme Khiops choisit la fusion qui maximise la valeur du chi carré; à savoir la fusion qui minimise la valeur DeltaChi2 puisque la valeur de chi carré, avant l'opération de fusion est fixe. La règle d'arrêt est remplie lorsque la meilleure valeur DeltaChi2 est trop grande. Toutefois, dans le cas de deux attributs indépendants, le processus de fusion devrait se poursuivre jusqu'à ce que l'algorithme de regroupement atteint un seul groupe terminal. La plus grande valeur de DeltaChi2 rencontrées au cours de l'algorithme de fusion des étapes de décision doit alors être acceptée. Nous allons essayer d'estimer cette valeur MaxDeltaChi2 dans le cas de deux attributs indépendants et de modifier l'algorithme afin de forcer les fusions tant que cette limite n'est pas atteint. 3.1 Statistiques des valeurs MaxDeltaChi2 de l'algorithme Khiops Concentrons-nous sur deux lignes r et r « de la table de contingence, avec des fréquences n et n », et les probabilités de ligne des valeurs de classe p1, p2, ... pj et P'1, P'2, ... P'j. Laissez P1, P2, ... Pj la Boullé RNTI - 1 probabilités des valeurs de la classe sur toute la table de contingence. La valeur du chi carré ne peut que diminuer lorsque les deux lignes sont fusionnées. Définissons la valeur DeltaChi2 comme la variation de la valeur du chi carré lors d'une fusion. () Σ = - + = J j j jj P pp nn nn DeltaChi 1 2' '' 2. (2) Nous avons prouvé que, dans le cas d'un indépendant de l'attribut explicatif d'un attribut de classe avec des valeurs de J, la valeur DeltaChi2 résultant de la fusion de deux lignes avec les mêmes fréquences est asymptotiquement distribuée comme les statistiques du chi carré avec J-1 degrés de liberté [Boullé, 2003b]. La valeur MaxDeltaChi2 est égale au maximum des valeurs de DeltaChi2 rencontrées au cours du processus de regroupement complet vers le bas d'un seul groupe terminal, lorsque l'attribut groupé est indépendant de l'attribut de classe. Dans le cas d'un processus de discrétisation, où les fusions sont contraintes à être adjacentes dans la table de contingence, nous avons proposé dans [Boullé, 2003a] une formule analytique de rapprocher les statistiques des MaxDeltaChi2. Dans le cas d'un processus de regroupement, nous n'avons pas pu rapprocher les statistiques de la MaxDeltaChi2 analytiquement. Cependant, nous avons montré dans [Boullé, 2003b] que les statistiques de la MaxDeltaChi2 ne dépend que de deux paramètres: le nombre de catégories initiales I et le nombre de valeurs de classe J. Plus précisément, les propositions suivantes sont des conjectures qui ont été vérifiés par une vaste expériences sur des données synthétiques: - les statistiques du MaxDeltaChi2 est indépendante de la taille de l'échantillon, - les statistiques du MaxDeltaChi2 est indépendante de la répartition des catégories, - les statistiques du MaxDeltaChi2 est indépendant de la distribution de la classe. Par exemple, on a évalué la première conjecture dans le cas des ensembles de données aléatoires avec 50 catégories initiales et 2 classes équiréparties équiréparties. Nous avons recueilli les valeurs MaxDeltaChi2 résultant d'un processus de regroupement complet, pour 1000 jeux de données générés au hasard. Cette expérience a été répétée pour un grand nombre de tailles d'échantillons allant de 1000 à 200000 cas et a montré que les fonctions de répartition des valeurs MaxDeltaChi2 sont indépendantes de la taille de l'échantillon. Le même genre d'expériences a été effectuée pour vérifier les autres conjectures. Nous avons également prouvé les propositions suivantes dans les cas où il n'y a que deux catégories ou deux classes. Proposition 1. Dans le cas de deux catégories et classes J, les statistiques de la valeur MaxDeltaChi2 est le chi-s statistiques Quare avec (J-1) degrés de liberté. Proposition 2. Dans le cas de I équidistribuée catégories et deux classes équiréparties, la moyenne de la valeur MaxDeltaChi2 est asymptotiquement égale à 2I / π. Dans le cas général, les statistiques de la valeur MaxDeltaChi2 n'a pas pu modéliser avec une expression mathématique, comme celle de la méthode de discrétisation Khiops. Nous avons choisi de calculer expérimentalement la moyenne et l'écart-type de la MaxDeltaChi2, pour un grand nombre de paires de paramètres (I, J). L'analyse des résultats montre un comportement linéaire avec une méthode robuste pour diviser les valeurs de Categorical Attributs RNTI - 1 rapport à deux paramètres I et J, qui est conforme aux propositions 1 et 2. Cette observation permet d'utiliser une table de valeurs à environ la moyenne et l'écart type des valeurs MaxDeltaChi2 et de compter sur une interpolation linéaire entre les valeurs pré-calculées. Enfin, nous faisons une dernière hypothèse, confirmée par l'évaluation expérimentale: la fonction de répartition des valeurs MaxDeltaChi2 peut être approchée par une loi normale avec le même écart moyen et standard. Tous les détails de la simulation sont donnés dans [Boullé, 2003b]. Pour conclure, la valeur MaxDeltaChi2 utilisée par l'algorithme de regroupement Khiops est calculé en raison d'une interpolation linéaire de la moyenne et l'écart-type trouvé dans un tableau de valeur calculée avant pour un nombre donné de catégories et de valeurs de classe. En utilisant la loi normale inverse, la valeur MaxDeltaChi2 est déterminée de sorte qu'elle sera plus grande que les valeurs de DeltaChi2 observées avec une probabilité p (p = 0,95 par exemple). 3.2 Le robuste Khiops algorithme de groupement algorithme robuste Khiops 1. Initialisation 1.1 Trier les valeurs d'attributs explicatifs 1.2 Créer un groupe élémentaire pour chaque valeur 1.3 Création d'un groupe spécial pour traiter toutes les catégories initiales qui ne remplissent pas la contrainte de fréquence minimale; si nécessaire, fusionner ce groupe spécial avec la catégorie reste moins fréquente 1.4 Calculer la valeur MaxDeltaChi2 liée au nombre initial de groupes et de valeurs de classe 2. Optimisation du groupement: répéter les étapes suivantes 2.1 Évaluer toutes les fusions possibles entre des paires de groupes 2.2 Rechercher la meilleure fusion 2.3 fusion et continuer aussi longtemps que l'une des conditions suivantes est pertinente - le niveau de confiance du groupement diminue après la fusion - la valeur DeltaChi2 de la meilleure fusion est inférieure à la valeur MaxDeltaChi2 Dans le cas de deux attributs indépendants , le regroupement devrait se traduire par un seul groupe terminal. Pour une p probabilité donnée, la modélisation statistique des algorithmes Khiops fournit une valeur théorique MaxDeltaChi2 (p) qui sera supérieure à toutes les valeurs DeltaChi2 des fusions réalisées au cours du processus de regroupement, avec une probabilité p. Le premier algorithme de regroupement Khiops est ensuite modifié afin de forcer toutes les fusions dont la valeur est inférieure à DeltaChi2 MaxDeltaChi2 (p). Cela garantit le comportement attendu de l'algorithme avec une probabilité p. Dans le cas de deux attributs à la relation de dépendance inconnue, cette amélioration des garanties de l'algorithme que lorsque l'attribut groupé est constitué d'au moins deux groupes, l'attribut explicatif détient réellement des informations concernant l'attribut de classe avec une probabilité supérieure à p. Nous vous conseillons de laisser p = 0,95, afin d'assurer des résultats fiables de regroupement. L'impact sur l'algorithme Khiops initial est limité à l'évaluation de la règle d'arrêt et conserve la complexité de calcul supra-linéaire de l'algorithme. Boullé RNTI - 1 4. Expériences Dataset continue Taille nominale Classe majorité Attributs Attributs Valeurs Précision adulte 7 8 48842 2 76,07 Australian 6 8 690 2 55,51 sein 10 0 699 2 65,52 Crx 6 9 690 2 55,51 Coeur 10 3 270 2 55,56 HorseColic 7 20 368 2 63,04 ionosphère 34 0 351 2 0 22 64,10 Champignon 8416 2 53,33 TicTacToe 0 9 958 2 65,34 véhicule 18 0 846 4 25,77 4 Waveform 0 0 5000 3 33,84 Vin 13 0 178 3 39,89 TAB 2 - datasets. Dans notre étude expérimentale, nous comparons la méthode Khiops regroupement avec d'autres algorithmes de regroupement supervisé sur deux critères: la performance prédictive et le nombre de groupes. Afin d'évaluer la performance intrinsèque des méthodes de regroupement et d'éliminer le biais du choix d'un algorithme d'induction spécifique, nous utilisons un protocole similaire méthode [Zighed et Rakotomalala, 2000], où chaque groupe est considéré comme une méthode inductive élémentaire prédit la distribution des valeurs de classe dans chaque groupe appris. Nous avons choisi de ne pas utiliser le critère de précision, car il se concentre uniquement sur la valeur de la classe majoritaire et ne peut différencier les prédictions correctes faites avec une probabilité 1 de prédictions correctes fait avec une probabilité légèrement supérieure à 0,5. En outre, de nombreuses applications, notamment dans le domaine de la commercialisation, se fondent sur la notation des instances et doivent évaluer la probabilité de chaque valeur de classe. Pour évaluer la qualité prédictive des groupes, nous utilisons la divergence Kullback-Leibler [Kullback, 1968] appliqué à comparer la distribution des valeurs de la classe estimée de l'ensemble d'apprentissage (basé sur les groupes ont appris) avec la distribution des valeurs de la classe observée sur l'ensemble de test (sur la base des valeurs initiales: le même pour toutes les méthodes testées). Pour une catégorie donnée, laissez pj la probabilité de la j-ième valeur de classe estimée sur l'ensemble d'apprentissage (avec l'utilisation du groupe contenant la catégorie), et QJ la probabilité de la valeur de la classe jeme observée sur l'ensemble de test (à l'aide seule la catégorie). La divergence de Kullback-Leibler entre la distribution estimée et la distribution observée est: () Σ = = J j j j j q p pqpD 1 log || . (3) L'évaluation globale de la qualité prédictive est calculée comme la moyenne de la divergence Kullback- Leibler sur l'ensemble de test. Afin de lisser les distributions empiriques et de traiter des probabilités zéro, nous utilisons l'estimateur de Laplace. Pour d'autres approches pour la définition des mesures de qualité d'ajustement, voir par exemple [Ritschard et Zighed, 2003]. Une méthode robuste pour partitionner les valeurs des attributs catégorielles RNTI - 1 Le problème de regroupement est un problème bi-critères qui tente de compromis entre la qualité prédictive et le nombre de groupes. Le classificateur optimal est le classificateur Bayes: dans le cas d'un classificateur univariée basé sur un seul attribut catégorique, le regroupement optimal est de ne rien faire. Dans les expériences, nous recueillons à la fois les résultats de la qualité prédictive en utilisant la divergence Kullback-Leibler et le nombre de groupes. Nous avons recueilli 12 ensembles de données de U.C. dépôt Irvine [Blake et Merz, 1998], chaque ensemble de données a au moins quelques dixièmes de cas pour chaque valeur de classe et des attributs catégoriques avec plus de deux valeurs. Afin d'augmenter le nombre de candidats attributs catégoriques pour le regroupement, les attributs continus ont été discrétisés dans un pré-traitement avec un 10 discrétisation sans supervision égale largeur. Le tableau 2 décrit les ensembles de données; la dernière colonne correspond à la précision de la classe majoritaire. Les méthodes de regroupement étudiés dans la comparaison sont les suivants: - Khiops: la méthode décrite dans le présent document, - Initial Khiops: la version initiale de la méthode, décrite à l'article 2, - CHAID: la méthode de regroupement utilisé dans la méthode CHAID [Kass, 1980 ], - Tschuprow: la méthode de regroupement décrit par exemple dans [Ritschard et al, 2001], - Ratio Gain:. la méthode de regroupement utilisé dans la méthode C4.5 [Quinlan, 1993]. Toutes ces méthodes sont basées sur un algorithme de bas en haut avide qui se raccorde de manière itérative les catégories en groupes, et détermine automatiquement le nombre de groupes dans la partition finale des catégories. La méthode du rapport de gain est la seule méthode basée sur l'entropie; les autres méthodes utilisent la base chi critères carrés. La première méthode Khiops applique le critère chi-carré sur toute la table de contingence et évalue la partition avec le niveau de confiance lié. La méthode Khiops robuste améliore l'algorithme Khiops initial en fournissant des garanties contre surapprentissage. La méthode Tschuprow est également basée sur une évaluation globale de la table de contingence, mais il utilise T la normalisation de la valeur du carré chi- Tschuprow au lieu du niveau de confiance pour évaluer les partitions. La méthode de CHAID applique le critère de chi-carré localement à deux rangées de la table de contingence. Pour la méthode CHAID, le niveau de signification est fixé à 0,95 pour le seuil de chi-carré, et la correction Bonferroni est pas appliquée. Nous avons réimplémenté ces groupement approches alternatives afin d'éliminer tout écart résultant de différentes divisions de validation croisée. Les groupements sont effectuées sur les 230 attributs des ensembles de données, en utilisant un stratifié dix fois la validation croisée. Afin de déterminer si les performances sont significativement différentes entre la méthode Khiops et les méthodes alternatives, les statistiques t de la différence des résultats est calculée. Sous l'hypothèse nulle, cette valeur a une loi de Student avec 9 degrés de liberté. Le niveau de confiance est fixé à 5% et un test bilatéral est réalisé pour rejeter l'hypothèse nulle. 4.1 Qualité des Groupements Les tables ensemble de résultats sont trop grandes pour être imprimées dans le présent document. Les résultats de la qualité prédictive sont résumés dans le tableau 3, des rapports pour chaque ensemble de données de la moyenne des divergences Kullback- Leibler et le nombre de victoires Khiops significatives (+) et les pertes (-) pour chaque comparaison de la méthode. Les résultats ont été normalisés en utilisant la divergence Kullback-Leibler évalué lorsque aucun regroupement est fait. Les moyens sont des moyens géométriques afin de se concentrer sur les rapports des performances entre les méthodes testées. Boullé RNTI - 1 Les résultats montrent des différences significatives entre les méthodes qui permettent de classer les méthodes testées. Dans un premier groupe de la méthode, la méthode de regroupement Khiops obtient les meilleurs résultats, suivi par la méthode initiale Khiops groupement et puis par la méthode CHAID. La méthode Khiops obtient significativement meilleurs résultats que la méthode CHAID pour 24% des attributs groupés, et nettement moins bons résultats pour 7% des attributs. Dans un deuxième groupe de méthodes, les méthodes Ratio Tchuprow et le gain sont clairement surperformé par les trois principales méthodes. Par exemple, la méthode Khiops dépasse la méthode du ratio de gain pour 35% des attributs, et est battu pour seulement 3,5% des attributs. Dataset Khiops Ini. Khiops CHAID Tschuprow Gain Ratio + - + - + - + - Adult 1,05 1,13 3 2 1,07 4 4 3,76 10 0 4,16 10 0 australien 1,04 1,06 0 0 1,10 2 0 1,10 1 0 1,24 3 0 sein 1,24 1,24 1 0 1,36 4 0 1,45 2 0 1,66 5 0 Crx 1,06 1,07 0 1 1,08 0 1 1,10 1 0 1,23 3 0 Coeur 0,98 1,02 1 0 1,02 0 0 1,03 2 0 1,07 3 0 HorseColic 1,02 1,01 1 4 1,07 3 2 1,08 3 0 1,04 3 2 ionosphère 1,07 1,03 1 3 1,13 7 1 1,06 2 2 1,08 3 4 champignons 1,10 1,24 4 2 1,21 6 2 2,29 11 1 2,60 11 1 TicTacToe 0,97 0,97 0 0 0,91 0 1 0,95 0 0 0,95 0 0 véhicule 1,10 1,10 5 1 1,11 4 4 1,12 2 2 1,30 9 0 Waveform 0,92 0,99 13 0 1,01 19 0 1,48 30 0 1,47 30 0 vin 1,23 1,20 0 1 1,37 6 1 1,24 1 1 1,23 0 1 Synthèse 1,04 1,07 29 14 1,10 55 16 1,35 65 6 1,42 80 8 TAB 3 - Moyens de la qualité prédictive des groupes, le nombre de victoires significatives (+) et des pertes (-) par jeu de données pour la méthode Khiops par rapport aux méthodes alternatives. En résumé, le critère de la qualité prédictive suggère le classement suivant des méthodes éprouvées: Khiops, initiale Khiops, CHAID, Tschuprow, Ratio Gain. 4.2 Taille du groupement Les résultats du numéro de groupe sont résumées dans le tableau 4. Les différences sont très importantes entre les méthodes testées. Les méthodes Ratio Tschuprow et le gain produisent les plus petits groupes de taille moyenne, au détriment d'une faible qualité prédictive. Parmi les méthodes de regroupement de haute qualité, la méthode Khiops est un gagnant pour le numéro du groupe criteri sur, suivie de la méthode Khiops initiale et la méthode CHAID. Les groupes produits par la méthode Khiops sont toujours plus petit que ceux produits par la méthode CHAID, et les différences sont importantes pour 60% des attributs. Bien que les méthodes de Ratio Tschuprow et le gain obtiennent des groupes plus petits en moyenne, les résultats sont contrastés entre les ensembles de données. Pour près d'un quart des attributs, la méthode Khiops obtient des groupements nettement plus petite que la méthode du ratio de gain. Il est intéressant d'analyser plus en profondeur les résultats de l'ensemble de données de forme d'onde, où environ la moitié des attributs sont des attributs du bruit. Une inspection des groupements révèle que l'une méthode robuste pour diviser les valeurs de Categorical Attributs RNTI - Méthode 1 Khiops robustes de regroupement est la seule méthode qui identifie correctement les attributs de bruit avec des groupements réduits à un seul groupe. Dataset Khiops Ini. Khiops CHAID Tschuprow Gain Ratio + - + - + - + - Adult 3,67 3,99 5 0 4,83 11 0 2,05 2 10 2,33 2 10 australien 1,91 2,19 6 1 2,19 4 0 2,19 3 1 2,36 7 1 Sein 2,60 2,83 3 0 4,16 9 0 1,98 1 7 1,98 1 7 Crx 1,93 2,16 5 1 2,18 3 0 2,15 3 1 2,42 8 2 Coeur 1,91 2,27 5 0 2,14 4 0 2,11 3 1 2,08 3 1 HorseColic 1,87 2,20 11 0 2,24 10 0 2,03 7 4 2,03 8 4 ionosphère 2,47 2,94 17 1 3,18 25 0 2,09 0 15 2,05 0 17 Champignon 3,06 3,11 3 3 3,57 10 0 2,00 0 13 2,19 1 13 TicTacToe 2,03 2,03 0 0 2,11 1 0 2,00 0 0 2,00 0 0 véhicule 3,50 3,90 7 0 4,84 17 0 2,58 0 11 2,85 3 11 Waveform 2,67 3,56 30 0 3,76 35 0 2,73 21 19 3,18 21 18 vin 2,60 2,95 5 0 3,56 11 0 2,10 0 6 2,05 1 7 Synthèse 2,54 2,95 97 6 3,28 140 0 2,22 40 88 2,38 55 91 TAB 4 - Moyens de la taille des groupes, le nombre de victoires significatives (+) et des pertes (-) par jeu de données pour la méthode Khiops par rapport aux méthodes alternatives. En résumé, le critère numéro du groupe suggère le classement suivant des méthodes éprouvées: Tschuprow, Ratio Gain, Khiops, initiale Khiops, CHAID. 4.3 Bi-critères d'analyse des résultats afin de mieux comprendre les relations entre la qualité prédictive et la taille des groupes, nous tirons à la figure 1, les moyens globaux des résultats sur un plan de deux critères avec le numéro de groupe sur la coordonnée x et la qualité prédictive sur la coordonnée y. A titre de comparaison, nous présentons également les résultats obtenus par trois méthodes de groupage alternative simple: - Mode: bipartition sans supervision des catégories avec un groupe contenant le mode, soit la catégorie la plus fréquente, - Chi Valeur unique: bipartition des catégories avec une catégorie contre tous les autres, sélectionnés à l'aide du critère de chi-carré (une fusion finale est toujours possible), - exhaustive CHAID: bipartition des catégories obtenues avec l'algorithme CHAID en forçant les fusions jusqu'à ce que la partition contient au plus deux groupes. Les trois méthodes de regroupement de bipartition sont classés comme prévu pour le critère de la qualité prédictive. Les Tschuprow et le gain de méthodes Ratio qui sont autorisés à construire une partition avec plus de deux groupes n'obtiennent pas de meilleurs résultats sur la qualité prédictive que la méthode Exhaustive CHAID. Le groupe des méthodes efficaces (Khiops, initiale Khiops et CHAID) prend clairement avantage des partitions à plusieurs groupes. Parmi ces méthodes principales, la méthode Khiops domine les autres méthodes sur les deux critères. Enfin, compte tenu de la complexité de calcul des algorithmes, celle de l'algorithme optimisé Khiops est O (n.log (N) + Boullé RNTI - 1 I2.log (I)), tandis que d'autres méthodes est O (n.log (N) + I3). Cependant, la différence de temps d'exécution est mineur dans de nombreux cas, lorsque le nombre de valeurs catégoriques I est très faible. 1,10 1,20 1,30 1,00 1,40 1,50 1,60 1,70 1,80 1,90 2,00 1,0 1,5 2,0 2,5 3,0 3,5 4,0 Groupe Nombre K u llb ac k- L ei b le r D iv er g en CE Khiops initiale Khiops CHAID Chi Valeur unique mode Exhaustive CHAID Tschuprow Ratio Gain FIGUE. 1 - évaluation Bi-critères du groupement m éthodes pour le numéro de groupe et les critères de qualité prédictive 5. Conclusion Le principe de la méthode Khiops de regroupement est de minimiser le niveau de confiance lié à l'épreuve de l'indépendance entre l'attribut groupé et l'attribut de classe. Au cours du processus bottom-up de l'algorithme, de nombreuses fusions entre les catégories sont effectuées que les variations produisent de la valeur chi-carré de la table de contingence. En raison d'une modélisation statistique de ces variations lorsque l'attribut explicatif est indépendant de l'attribut de classe, nous avons amélioré l'algorithme de regroupement initial Khiops afin de garantir que les groupes d'attributs indépendants sont réduits à un seul groupe. Cette résistance attestés à surapprentissage est une alternative intéressante à l'approche de validation croisée classique. De nombreuses expériences comparatives montrent que la méthode Khiops surclasse les autres méthodes de regroupement testées. Il permet de réduire considérablement le nombre de valeurs d'attributs catégoriques dans l'étape de pré-traitement exploration de données, tout en gardant la plupart de leur performance prédictive monothétique. [Berckman Références, 1995]. Berckman N. C Valeur de regroupement pour les arbres de décision binaires. Rapport technique. Département Informatique - Université du Massachusetts, 1995. [Blake et Merz, 1998] C.L. Blake et Merz C.J.. UCI référentiel de bases de données d'apprentissage machine URL Web http://www.ics.uci.edu/~mlearn/MLRepository.html, Irvine, CA: Université de Californie, Département de l'information et de l'informatique, 1998. [Boullé, 2003a] M . Boullé. Khiops: une discrétisation Méthode des attributs continus avec une résistance garantie au bruit. Actes de la troisième Conférence internationale sur l'apprentissage et fouille de données dans la reconnaissance des formes, 50-64, 2003. Une méthode robuste pour les valeurs de Partitionnement catégorielles Attributs RNTI - 1 [Boullé, 2003b] M. Boullé. Groupage des facts d'Robuste un par la symbolique attribut Khiops méthode. Note technique NT / FTR & D / 8028, France Télécom R & D, 2003. [Breiman et al., 1984] L. Breiman, J. H. Friedman R.A. Olshen et Pierre C.J.. Et régression des arbres. Californie: [. Cestnik et al, 1987] Wadsworth International, 1984. B. Cestnik, I. Kononenko et I. Bratko. ASSISTANT 86: Un outil pour la connaissance explicitation utilisateurs sophistiqués. Dans Bratko & Lavrac, Les progrès dans l'apprentissage machine, Wilmslow, Royaume-Uni (Eds.): Sigma Press, 1987. [Chou, 1991] P.A. Chou. Optimal Partitionnement pour la classification et la régression des arbres. IEEE Transactions sur le modèle d'analyse et de l'intelligence artificielle, 13 (4): 340-354, 1991. [. Fulton et al, 1995] T. Fulton, S. Kasif et S. Salzberg. Des algorithmes efficaces pour trouver des fractionnements façon multi pour les arbres de décision. Actes de la Conférence mixte internationale Treizième sur l'intelligence artificielle, San Francisco, CA: Morgan Kaufmann, 244-255, 1995. [Kass, 1980] G.V. Kass. Une technique d'exploration pour étudier de grandes quantités de données catégoriques. Statistique appliquée, 29 (2): 119-127, 1980. [Kerber, 1991] R. Kerber. ChiMerge discrétisation des attributs numériques. Actes de la 10e Conférence internationale sur l'intelligence artificielle, 123-128, 1991. [Kullback, 1968] S. Kullback. Théorie de l'information et de la statistique. New York: Wiley, (1959); réédité par Dover, 1968. [Lechevallier, 1990] Y. Lechevallier. Recherche d'une partition sous Une contrainte d'optimale totale ordre. Rapport technique N ° 1247, INRIA, 1990. [Pyle, 1999] D. Pyle. Préparation des données pour l'exploration de données, Morgan Kaufmann, 1999. [Quinlan, 1986] J.R Quinlan. Induction d'arbres de décision. Machine Learning, 1: 81-106, 1986. [Quinlan, 1993] J.R Quinlan. C4.5: Programmes d'apprentissage machine. Morgan Kaufmann, 1993. [Ritschard et al., 2001] G. Ritschard D.A. Zighed et N. Nicoloyannis. Maximisation de l'association par Regroupement de lignes Ou de tableau d'un Colonnes Croisé. Math. & Sci. Hum, n ° 154-155:. 81-98, 2001. [Ritschard et Zighed, 2003] G. Ritschard et D.A. Zighed. Modelisa tion de tables de Contingence par induction d'arbre. Extraction et Gestion des Connaissances, 381-392, 2003. [Zighed et Rakotomalala, 2000] D.A. Zighed et R. Rakotomalala. Graphes d'induction. Hermes Science Publications, 327-359, 2000. Résumé Dans l'apprentissage machine supervisé, le partage des valeurs (également appelées regroupement) d'un objectif d'attributs catégoriques à la construction d'un nouvel attribut synthétique qui conserve les informations de l'attribut initial et réduit le nombre de ses valeurs. Dans cet article, nous proposons une nouvelle méthode de regroupement Khiops, basé sur une généralisation de l'algorithme de discrétisation Khiops. Cette méthode de regroupement fournit des garanties contre surapprentissage et conduit ainsi à des groupes robustes. Cette propriété découle d'une modélisation statistique de la méthode Khiops qui permet d'affiner l'algorithme. De nombreuses expériences démontrent la validité de cette approche et montrent que la méthode de regroupement Khiops construit des groupes de grande qualité, tant en termes de qualité prédictive et petit nombre de groupes."
1189,Revue des Nouvelles Technologies de l'Information,EGC,2004,A robust method for partitioning the values of categorical attributes,"Dans le domaine de l'apprentissage supervisé, les méthodes de groupage des modalités d'un attribut symbolique permettent de construire un nouvel attribut synthétique conservant au maximum la valeur informationnelle de l'attribut initial et diminuant le nombre de modalités. Nous proposons ici une généralisation de l'algorithme de discrétisation Khiops pour le problème du groupage des modalités. L'algorithme proposé permet de contrôler a priori le risque de sur-apprentissage et d'améliorer significativement la robustesse des groupages produits. Cette caractéristique de robustesse a été obtenue en étudiant la statistique des variations du critère du Khi2 lors de regroupements de lignes d'un tableau de contingence et en modélisant le comportement statistique de l'algorithme Khiops. Des expérimentations intensives ont permis de valider cette approche et ont montré que la méthode de groupage Khiops aboutit à des groupages performants, à la fois en terme de qualité prédictive et de faible nombre de groupes.",Marc Boullé,http://editions-rnti.fr/render_pdf.php?p1&p=1000950,http://editions-rnti.fr/render_pdf.php?p=1000950,en,"Microsoft Word - EGC2004_Khiops.doc une méthode robuste pour partitionner les valeurs des attributs catégorielles Marc Boullé * * France Télécom R & D, 2, avenue Pierre Marzin, 22300 Lannion, France marc.boulle@francetelecom.com CV. Dans le domaine de l'apprentissage supervisez, les methods de groupages des d'un attribut Modalités symbolique de construct un permettent nouvel au attribut synthetic au maximum la conservant Valeur informationnelle de l'initiale et attribut le diminuant Nombre de Modalités. Nous proposons ici juin de l'algorithme généralisation de discrétisation Khiops1 for the du groupages des Problème Modalités. L'algorithme de CONTROLER Përmet Proposé a priori le osée de sur-et d'Improving apprentissage la robustesse des significativement groupages Produits. This characteristic was de robustesse available in la statistique des étudiant les variations du Khi2 du critère de LORs de regroupements d'un tableau lignes de Contingence et en modélisant le comportement de l'algorithme statistique Khiops. Des Expérimentations de Intensifs Ontario permis et approach this valider Montré Que la Ontario méthode de groupages Khiops aboutIt à des groupages Performants, à la Fois en term et de qualité prédictive Nombre de Faible Groupes. 1. Introduction Alors que le problème de discrétisation a été largement étudié dans le passé, le problème de regroupement n'a pas été exploré si profondément dans la littérature. Cependant, dans la réalité des ensembles de données d'exploration de données, il existe de nombreux cas où le regroupement des valeurs des attributs catégoriques est une étape de pré-traitement obligatoire. Le problème de regroupement consiste à diviser l'ensemble des valeurs d'un attribut catégorique en un nombre fini de groupes. Par exemple, la plupart des arbres de décision exploitent une méthode de regroupement pour gérer les attributs catégoriques, afin d'augmenter le nombre de cas dans chaque nœud de l'arbre [Zighed et Rakotomalala, 2000]. les réseaux de neurones sont basées sur des attributs numériques et souvent utiliser un codage binaire 1-à-N pour prétraiter catégorique attributs. Lorsque les catégories sont trop nombreuses, ce schéma de codage peut être remplacé par une méthode de regroupement. Ce problème se pose dans de nombreux autres algorithmes de classification, tels que les réseaux bayésiens, la régression linéaire ou régression logistique. En outre, le regroupement est un procédé d'usage général qui est intrinsèquement utiles dans l'étape de préparation des données du procédé d'extraction de données [Pyle, 1999]. Les méthodes de regroupement peuvent être regroupés en fonction de la stratégie de recherche de la meilleure partition et au critère de regroupement utilisé pour évaluer les partitions. L'algorithme simple essaie de trouver les meilleurs bipartition avec une catégorie contre tous les autres. Une approche plus intéressante consiste à rechercher une bipartition de toutes les catégories. Le procédé de l'avant séquentiel de sélection dérivé de la [. Cestnik et al, 1987] et évalués par [Berckman, 1995] est un 1 les brevets français N ° 01 07006 et N ° 02 16733 une méthode robuste pour diviser les valeurs de Categorical Attributs RNTI - 1 algorithme glouton qui initialise un groupe avec la meilleure catégorie (contre les autres), et ajoute de nouvelles catégories itérativement à ce premier groupe. Lorsque l'attribut de classe a deux valeurs, [Breiman et al., 1984] ont proposé une méthode d'CART optimale pour regrouper les catégories en deux groupes pour le critère de Gini. Cet algorithme trie d'abord les catégories en fonction de la probabilité de la première valeur de classe, et puis recherche les meilleurs éclatés dans cette liste triée. Cet algorithme a une complexité temporelle de I.log (I), où I est le nombre de catégories. Sur la base des idées présentées dans [Lechevallier, 1990; Fulton et al., 1995], ce résultat peut probablement être étendue à trouver la partition optimale des catégories en groupes K dans le cas de deux valeurs de classe, avec l'utilisation d'un algorithme de programmation dynamique de la complexité du temps I2. Dans le cas général de plus de deux valeurs de classe, la re pas d'algorithme pour trouver le regroupement optimal avec des groupes K, en dehors de la recherche exhaustive. Cependant, [Chou, 1991] a proposé une approche basée sur K-means qui permet de trouver une partition localement optimale des catégories en K- groupes. algorithmes d'arbres de décision gèrent souvent le problème de regroupement avec une heuristique gloutonne basée sur une classification ascendante des catégories. L'algorithme commence par un seul groupe de catégorie, puis recherche la meilleure fusion entre les groupes. Le processus est répété jusqu'à ce qu'aucune autre fusion peut améliorer le critère de regroupement. L'algorithme CHAID [Kass, 1980] utilise cette approche gourmande avec un proche critère de ChiMerge [Kerber, 1991]. Les meilleures fusions sont recherchées en réduisant au minimum le niveau de confiance du critère du chi carré appliqué localement à deux catégories: elles sont fusionnées si elles sont statistiquement similaires. L'algorithme ID3 [Quinlan, 1986] utilise le critère de gain d'information pour évaluer les attributs catégoriques, sans regroupement. Ce critère tend à favoriser les attributs avec de nombreuses catégories et [Quinlan, 1993] a proposé en C4.5 pour exploiter le critère de rapport de gain, en divisant le gain d'information par l'entropie des catégories. Le critère du chi carré a également été appliquée globalement sur l'ensemble des catégories, avec une version normalisée de la valeur du chi carré tels que V ou T du Tschuprow de Cramer [Ritschard et al., 2001] afin de comparer deux différents partitions -SIZE. La méthode de regroupement Khiops est une généralisation directe de la méthode de discrétisation Khiops [Boullé, 2003a]. Au lieu de fusionner des valeurs numériques adjacentes afin d'intervalles de construction, le procédé de groupement fusionne les valeurs nominales en groupes de valeurs. Dans les deux cas, l'algorithme de recherche est une heuristique gloutonne ascendante qui optimise le critère du chi carré appliqué à l'ensemble des intervalles ou des groupes. La règle d'arrêt est basé sur le niveau de confiance calculé avec les statistiques du chi carré. La méthode arrête automatiquement le processus de fusion dès que le niveau de confiance, liée à l'épreuve de l'indépendance entre l'attribut partitionné et l'attribut de classe, ne diminue pas plus. L'ensemble des groupes résultant d'une méthode de regroupement fournit un classificateur univariée élémentaire, qui prévoit la distribution des valeurs de classe dans chaque groupe a appris. Une méthode de regroupement peut être considéré comme un algorithme inductif, donc soumis à overfitting. Nous appliquons une méthode similaire à celle développée pour la méthode de discrétisation Khiops afin d'apporter un véritable contrôle de surajustement. Le principe est d'analyser le comportement de l'algorithme lors du regroupement d'un organisme indépendant d'attribut explicatif de l'attribut de classe. Nous étudions les statistiques des variations des valeurs du chi carré lors de la fusion des catégories et proposons de modéliser le maximum de ces variations dans un processus de regroupement complet. L'algorithme est ensuite modifié afin de forcer toute fusion dont la variation de la valeur du chi carré est inférieure à la variation maximale prédite par notre modélisation statistique. Ce changement dans l'algorithme donne la garantie probabiliste intéressant que tout attribut indépendant seront regroupés au sein d'un seul groupe terminal et que tout attribut dont Boullé RNTI - 1 groupe se compose d'au moins deux groupes contient vraiment des informations prédictives sur l'attribut de classe. Ceci est confirmé expérimentalement. Le reste du document est organisé comme suit. La section 2 présente brièvement les initiales de Khiops de regroupement algorithme. La section 3 présente la modélisation statistique de l'algorithme et son réglage fin pour éviter surajustement. L'article 4 procède à une vaste évaluation expérimentale. 2. Le regroupement Khiops Méthode Dans cette section, nous rappelons les principes du test du chi carré et présente l'algorithme de regroupement Khiops, dont la description détaillée et l'analyse peut être trouvée dans [Boullé, 2003b]. 2.1 Le test du chi carré: Principes et Notat ions Considérons un attribut explicatif et un attribut de classe et déterminer si elles sont indépendantes. Tout d'abord, tous les cas sont résumés dans un tableau de contingence, où les cas sont comptés pour chaque paire de valeurs d'attributs explicatifs et de classe. La valeur de chi carré est calculée à partir du tableau de contingence, d'après le tableau 1 notations. nij: fréquence observée pour les i-ième valeur explicative A B C Total et une valeur de classe d'un jième n11 n12 n13 n1. ni .: Fréquence totale observée pour i e valeur explicative b n21 n22 n23 n2. n.j: Total fréquence observée pour les j-ième valeur de la classe c n31 n32 n33 n3. N: Nombre total observé fréquence d n41 n42 n43 n4. I: Nombre de valeurs d'attributs explicatifs e N51 N52 N53 n5. J: Nombre de valeurs classe totale n.1 n.2 n.3 N TAB 1 - Table de contingence utilisée pour calculer la valeur du chi carré. Laissez eij = ni..n.j / N, représentent la fréquence attendue pour la cellule (i, j) si les attributs explicatifs et de classe sont indépendants. La valeur de chi carré est une mesure sur l'ensemble de tableau de contingence de la différence entre les fréquences observées et les fréquences attendues. Il peut être interprété comme une distance par rapport à l'hypothèse de l'indépendance entre les attributs. () ΣΣ - = i j ij ijij e en Chi 2 2. (1) Dans l'hypothèse nulle d'indépendance, la valeur du chi carré est soumis à des statistiques du chi carré avec (I-1). (J-1) degrés de liberté. Ceci est la base d'un test statistique qui permet de rejeter l'hypothèse de l'indépendance; plus la valeur du chi carré est, plus le niveau de confiance est. 2.2 La valeur initiale algorithme de chi carré dépend des fréquences observées locales dans chaque ligne individuelle et sur les fréquences mondiales observées dans l'ensemble de tableau de contingence. Ceci est une bonne une méthode robuste pour partitionner les valeurs des attributs catégorielles RNTI - critère candidat 1 pour une méthode de regroupement. Les statistiques du chi carré est paramétrées par le nombre de valeurs explicatives (en rapport avec les degrés de liberté). Afin de comparer deux groupes avec différents numéros de groupe, nous utilisons le niveau de confiance au lieu de la valeur carrée khi. Le principe de l'algorithme Khiops est de minimiser le niveau de confiance entre l'attribut explicatif groupés et l'attribut de classe par le biais de statistiques du chi carré. La valeur du chi carré n'est pas fiable pour tester l'hypothèse de l'indépendance si la fréquence attendue dans une cellule du tableau de contingence tombe en dessous une valeur minimale. Les Copes algorithme avec cette contrainte dans un pré-traitement: une catégorie initiale qui ne remplit pas la contrainte de fréquence minimale est inconditionnellement fusionnées en un groupe spécial. Le procédé Khiops est basé sur un algorithme de bas en haut gourmand. Il commence par catégories initiales et cherche ensuite la meilleure fusion entre les catégories. L'algorithme est répété jusqu'à ce qu'aucune autre fusion peut diminuer le niveau de confiance. La complexité de calcul de l'algorithme peut être réduite à O (n.log (N) + I2.log (I)) avec certaines optimisations [Boullé, 2003b]. Il existe deux différences principales entre l'algorithme Khiops initial et l'algorithme CHAID similaire [Kass, 1980]. Tout d'abord, le critère de chi-carré est appliquée globalement à la partition entière, dans le cas de l'algorithme Khiops, alors qu'il est appliqué localement à deux groupes adjacents dans le cas de l'algorithme CHAID. En second lieu, l'algorithme Khiops arrête le processus de fusion lorsque le niveau de confiance augmente après la meilleure fusion candidat, alors que l'algorithme CHAID arrête lorsque le niveau de confiance est au-delà d'un seuil fixé par l'utilisateur. 3. Analyse statistique de l'algorithme L'algorithme Khiops choisit la meilleure fusion entre toutes les fusions possibles des catégories et itère ce processus jusqu'à ce que la règle d'arrêt est remplie. Lorsque l'attribut explicatif et l'attribut de classe sont indépendants, l'ensemble résultant des groupes devrait être composé d'un seul groupe, ce qui signifie qu'il n'y a pas d'information prédictive l'attribut explicatif. Dans ce qui suit, nous étudions le comportement statistique de l'algorithme Khiops initial. Dans le cas de deux attributs indépendants, la valeur du chi carré est soumis à des statistiques du chi carré, avec espérance et de la variance connue. Nous étudions la loi DeltaChi2 (variation de la valeur du chi carré après la fusion des deux catégories) dans le cas de deux attributs indépendants. Au cours d'un processus de regroupement, un grand nombre de fusions sont évaluées, et, à chaque étape, l'algorithme Khiops choisit la fusion qui maximise la valeur du chi carré; à savoir la fusion qui minimise la valeur DeltaChi2 puisque la valeur de chi carré, avant l'opération de fusion est fixe. La règle d'arrêt est remplie lorsque la meilleure valeur DeltaChi2 est trop grande. Toutefois, dans le cas de deux attributs indépendants, le processus de fusion devrait se poursuivre jusqu'à ce que l'algorithme de regroupement atteint un seul groupe terminal. La plus grande valeur de DeltaChi2 rencontrées au cours de l'algorithme de fusion des étapes de décision doit alors être acceptée. Nous allons essayer d'estimer cette valeur MaxDeltaChi2 dans le cas de deux attributs indépendants et de modifier l'algorithme afin de forcer les fusions tant que cette limite n'est pas atteint. 3.1 Statistiques des valeurs MaxDeltaChi2 de l'algorithme Khiops Concentrons-nous sur deux lignes r et r « de la table de contingence, avec des fréquences n et n », et les probabilités de ligne des valeurs de classe p1, p2, ... pj et P'1, P'2, ... P'j. Laissez P1, P2, ... Pj la Boullé RNTI - 1 probabilités des valeurs de la classe sur toute la table de contingence. La valeur du chi carré ne peut que diminuer lorsque les deux lignes sont fusionnées. Définissons la valeur DeltaChi2 comme la variation de la valeur du chi carré lors d'une fusion. () Σ = - + = J j j jj P pp nn nn DeltaChi 1 2' '' 2. (2) Nous avons prouvé que, dans le cas d'un indépendant de l'attribut explicatif d'un attribut de classe avec des valeurs de J, la valeur DeltaChi2 résultant de la fusion de deux lignes avec les mêmes fréquences est asymptotiquement distribuée comme les statistiques du chi carré avec J-1 degrés de liberté [Boullé, 2003b]. La valeur MaxDeltaChi2 est égale au maximum des valeurs de DeltaChi2 rencontrées au cours du processus de regroupement complet vers le bas d'un seul groupe terminal, lorsque l'attribut groupé est indépendant de l'attribut de classe. Dans le cas d'un processus de discrétisation, où les fusions sont contraintes à être adjacentes dans la table de contingence, nous avons proposé dans [Boullé, 2003a] une formule analytique de rapprocher les statistiques des MaxDeltaChi2. Dans le cas d'un processus de regroupement, nous n'avons pas pu rapprocher les statistiques de la MaxDeltaChi2 analytiquement. Cependant, nous avons montré dans [Boullé, 2003b] que les statistiques de la MaxDeltaChi2 ne dépend que de deux paramètres: le nombre de catégories initiales I et le nombre de valeurs de classe J. Plus précisément, les propositions suivantes sont des conjectures qui ont été vérifiés par une vaste expériences sur des données synthétiques: - les statistiques du MaxDeltaChi2 est indépendante de la taille de l'échantillon, - les statistiques du MaxDeltaChi2 est indépendante de la répartition des catégories, - les statistiques du MaxDeltaChi2 est indépendant de la distribution de la classe. Par exemple, on a évalué la première conjecture dans le cas des ensembles de données aléatoires avec 50 catégories initiales et 2 classes équiréparties équiréparties. Nous avons recueilli les valeurs MaxDeltaChi2 résultant d'un processus de regroupement complet, pour 1000 jeux de données générés au hasard. Cette expérience a été répétée pour un grand nombre de tailles d'échantillons allant de 1000 à 200000 cas et a montré que les fonctions de répartition des valeurs MaxDeltaChi2 sont indépendantes de la taille de l'échantillon. Le même genre d'expériences a été effectuée pour vérifier les autres conjectures. Nous avons également prouvé les propositions suivantes dans les cas où il n'y a que deux catégories ou deux classes. Proposition 1. Dans le cas de deux catégories et classes J, les statistiques de la valeur MaxDeltaChi2 est le chi-s statistiques Quare avec (J-1) degrés de liberté. Proposition 2. Dans le cas de I équidistribuée catégories et deux classes équiréparties, la moyenne de la valeur MaxDeltaChi2 est asymptotiquement égale à 2I / π. Dans le cas général, les statistiques de la valeur MaxDeltaChi2 n'a pas pu modéliser avec une expression mathématique, comme celle de la méthode de discrétisation Khiops. Nous avons choisi de calculer expérimentalement la moyenne et l'écart-type de la MaxDeltaChi2, pour un grand nombre de paires de paramètres (I, J). L'analyse des résultats montre un comportement linéaire avec une méthode robuste pour diviser les valeurs de Categorical Attributs RNTI - 1 rapport à deux paramètres I et J, qui est conforme aux propositions 1 et 2. Cette observation permet d'utiliser une table de valeurs à environ la moyenne et l'écart type des valeurs MaxDeltaChi2 et de compter sur une interpolation linéaire entre les valeurs pré-calculées. Enfin, nous faisons une dernière hypothèse, confirmée par l'évaluation expérimentale: la fonction de répartition des valeurs MaxDeltaChi2 peut être approchée par une loi normale avec le même écart moyen et standard. Tous les détails de la simulation sont donnés dans [Boullé, 2003b]. Pour conclure, la valeur MaxDeltaChi2 utilisée par l'algorithme de regroupement Khiops est calculé en raison d'une interpolation linéaire de la moyenne et l'écart-type trouvé dans un tableau de valeur calculée avant pour un nombre donné de catégories et de valeurs de classe. En utilisant la loi normale inverse, la valeur MaxDeltaChi2 est déterminée de sorte qu'elle sera plus grande que les valeurs de DeltaChi2 observées avec une probabilité p (p = 0,95 par exemple). 3.2 Le robuste Khiops algorithme de groupement algorithme robuste Khiops 1. Initialisation 1.1 Trier les valeurs d'attributs explicatifs 1.2 Créer un groupe élémentaire pour chaque valeur 1.3 Création d'un groupe spécial pour traiter toutes les catégories initiales qui ne remplissent pas la contrainte de fréquence minimale; si nécessaire, fusionner ce groupe spécial avec la catégorie reste moins fréquente 1.4 Calculer la valeur MaxDeltaChi2 liée au nombre initial de groupes et de valeurs de classe 2. Optimisation du groupement: répéter les étapes suivantes 2.1 Évaluer toutes les fusions possibles entre des paires de groupes 2.2 Rechercher la meilleure fusion 2.3 fusion et continuer aussi longtemps que l'une des conditions suivantes est pertinente - le niveau de confiance du groupement diminue après la fusion - la valeur DeltaChi2 de la meilleure fusion est inférieure à la valeur MaxDeltaChi2 Dans le cas de deux attributs indépendants , le regroupement devrait se traduire par un seul groupe terminal. Pour une p probabilité donnée, la modélisation statistique des algorithmes Khiops fournit une valeur théorique MaxDeltaChi2 (p) qui sera supérieure à toutes les valeurs DeltaChi2 des fusions réalisées au cours du processus de regroupement, avec une probabilité p. Le premier algorithme de regroupement Khiops est ensuite modifié afin de forcer toutes les fusions dont la valeur est inférieure à DeltaChi2 MaxDeltaChi2 (p). Cela garantit le comportement attendu de l'algorithme avec une probabilité p. Dans le cas de deux attributs à la relation de dépendance inconnue, cette amélioration des garanties de l'algorithme que lorsque l'attribut groupé est constitué d'au moins deux groupes, l'attribut explicatif détient réellement des informations concernant l'attribut de classe avec une probabilité supérieure à p. Nous vous conseillons de laisser p = 0,95, afin d'assurer des résultats fiables de regroupement. L'impact sur l'algorithme Khiops initial est limité à l'évaluation de la règle d'arrêt et conserve la complexité de calcul supra-linéaire de l'algorithme. Boullé RNTI - 1 4. Expériences Dataset continue Taille nominale Classe majorité Attributs Attributs Valeurs Précision adulte 7 8 48842 2 76,07 Australian 6 8 690 2 55,51 sein 10 0 699 2 65,52 Crx 6 9 690 2 55,51 Coeur 10 3 270 2 55,56 HorseColic 7 20 368 2 63,04 ionosphère 34 0 351 2 0 22 64,10 Champignon 8416 2 53,33 TicTacToe 0 9 958 2 65,34 véhicule 18 0 846 4 25,77 4 Waveform 0 0 5000 3 33,84 Vin 13 0 178 3 39,89 TAB 2 - datasets. Dans notre étude expérimentale, nous comparons la méthode Khiops regroupement avec d'autres algorithmes de regroupement supervisé sur deux critères: la performance prédictive et le nombre de groupes. Afin d'évaluer la performance intrinsèque des méthodes de regroupement et d'éliminer le biais du choix d'un algorithme d'induction spécifique, nous utilisons un protocole similaire méthode [Zighed et Rakotomalala, 2000], où chaque groupe est considéré comme une méthode inductive élémentaire prédit la distribution des valeurs de classe dans chaque groupe appris. Nous avons choisi de ne pas utiliser le critère de précision, car il se concentre uniquement sur la valeur de la classe majoritaire et ne peut différencier les prédictions correctes faites avec une probabilité 1 de prédictions correctes fait avec une probabilité légèrement supérieure à 0,5. En outre, de nombreuses applications, notamment dans le domaine de la commercialisation, se fondent sur la notation des instances et doivent évaluer la probabilité de chaque valeur de classe. Pour évaluer la qualité prédictive des groupes, nous utilisons la divergence Kullback-Leibler [Kullback, 1968] appliqué à comparer la distribution des valeurs de la classe estimée de l'ensemble d'apprentissage (basé sur les groupes ont appris) avec la distribution des valeurs de la classe observée sur l'ensemble de test (sur la base des valeurs initiales: le même pour toutes les méthodes testées). Pour une catégorie donnée, laissez pj la probabilité de la j-ième valeur de classe estimée sur l'ensemble d'apprentissage (avec l'utilisation du groupe contenant la catégorie), et QJ la probabilité de la valeur de la classe jeme observée sur l'ensemble de test (à l'aide seule la catégorie). La divergence de Kullback-Leibler entre la distribution estimée et la distribution observée est: () Σ = = J j j j j q p pqpD 1 log || . (3) L'évaluation globale de la qualité prédictive est calculée comme la moyenne de la divergence Kullback- Leibler sur l'ensemble de test. Afin de lisser les distributions empiriques et de traiter des probabilités zéro, nous utilisons l'estimateur de Laplace. Pour d'autres approches pour la définition des mesures de qualité d'ajustement, voir par exemple [Ritschard et Zighed, 2003]. Une méthode robuste pour partitionner les valeurs des attributs catégorielles RNTI - 1 Le problème de regroupement est un problème bi-critères qui tente de compromis entre la qualité prédictive et le nombre de groupes. Le classificateur optimal est le classificateur Bayes: dans le cas d'un classificateur univariée basé sur un seul attribut catégorique, le regroupement optimal est de ne rien faire. Dans les expériences, nous recueillons à la fois les résultats de la qualité prédictive en utilisant la divergence Kullback-Leibler et le nombre de groupes. Nous avons recueilli 12 ensembles de données de U.C. dépôt Irvine [Blake et Merz, 1998], chaque ensemble de données a au moins quelques dixièmes de cas pour chaque valeur de classe et des attributs catégoriques avec plus de deux valeurs. Afin d'augmenter le nombre de candidats attributs catégoriques pour le regroupement, les attributs continus ont été discrétisés dans un pré-traitement avec un 10 discrétisation sans supervision égale largeur. Le tableau 2 décrit les ensembles de données; la dernière colonne correspond à la précision de la classe majoritaire. Les méthodes de regroupement étudiés dans la comparaison sont les suivants: - Khiops: la méthode décrite dans le présent document, - Initial Khiops: la version initiale de la méthode, décrite à l'article 2, - CHAID: la méthode de regroupement utilisé dans la méthode CHAID [Kass, 1980 ], - Tschuprow: la méthode de regroupement décrit par exemple dans [Ritschard et al, 2001], - Ratio Gain:. la méthode de regroupement utilisé dans la méthode C4.5 [Quinlan, 1993]. Toutes ces méthodes sont basées sur un algorithme de bas en haut avide qui se raccorde de manière itérative les catégories en groupes, et détermine automatiquement le nombre de groupes dans la partition finale des catégories. La méthode du rapport de gain est la seule méthode basée sur l'entropie; les autres méthodes utilisent la base chi critères carrés. La première méthode Khiops applique le critère chi-carré sur toute la table de contingence et évalue la partition avec le niveau de confiance lié. La méthode Khiops robuste améliore l'algorithme Khiops initial en fournissant des garanties contre surapprentissage. La méthode Tschuprow est également basée sur une évaluation globale de la table de contingence, mais il utilise T la normalisation de la valeur du carré chi- Tschuprow au lieu du niveau de confiance pour évaluer les partitions. La méthode de CHAID applique le critère de chi-carré localement à deux rangées de la table de contingence. Pour la méthode CHAID, le niveau de signification est fixé à 0,95 pour le seuil de chi-carré, et la correction Bonferroni est pas appliquée. Nous avons réimplémenté ces groupement approches alternatives afin d'éliminer tout écart résultant de différentes divisions de validation croisée. Les groupements sont effectuées sur les 230 attributs des ensembles de données, en utilisant un stratifié dix fois la validation croisée. Afin de déterminer si les performances sont significativement différentes entre la méthode Khiops et les méthodes alternatives, les statistiques t de la différence des résultats est calculée. Sous l'hypothèse nulle, cette valeur a une loi de Student avec 9 degrés de liberté. Le niveau de confiance est fixé à 5% et un test bilatéral est réalisé pour rejeter l'hypothèse nulle. 4.1 Qualité des Groupements Les tables ensemble de résultats sont trop grandes pour être imprimées dans le présent document. Les résultats de la qualité prédictive sont résumés dans le tableau 3, des rapports pour chaque ensemble de données de la moyenne des divergences Kullback- Leibler et le nombre de victoires Khiops significatives (+) et les pertes (-) pour chaque comparaison de la méthode. Les résultats ont été normalisés en utilisant la divergence Kullback-Leibler évalué lorsque aucun regroupement est fait. Les moyens sont des moyens géométriques afin de se concentrer sur les rapports des performances entre les méthodes testées. Boullé RNTI - 1 Les résultats montrent des différences significatives entre les méthodes qui permettent de classer les méthodes testées. Dans un premier groupe de la méthode, la méthode de regroupement Khiops obtient les meilleurs résultats, suivi par la méthode initiale Khiops groupement et puis par la méthode CHAID. La méthode Khiops obtient significativement meilleurs résultats que la méthode CHAID pour 24% des attributs groupés, et nettement moins bons résultats pour 7% des attributs. Dans un deuxième groupe de méthodes, les méthodes Ratio Tchuprow et le gain sont clairement surperformé par les trois principales méthodes. Par exemple, la méthode Khiops dépasse la méthode du ratio de gain pour 35% des attributs, et est battu pour seulement 3,5% des attributs. Dataset Khiops Ini. Khiops CHAID Tschuprow Gain Ratio + - + - + - + - Adult 1,05 1,13 3 2 1,07 4 4 3,76 10 0 4,16 10 0 australien 1,04 1,06 0 0 1,10 2 0 1,10 1 0 1,24 3 0 sein 1,24 1,24 1 0 1,36 4 0 1,45 2 0 1,66 5 0 Crx 1,06 1,07 0 1 1,08 0 1 1,10 1 0 1,23 3 0 Coeur 0,98 1,02 1 0 1,02 0 0 1,03 2 0 1,07 3 0 HorseColic 1,02 1,01 1 4 1,07 3 2 1,08 3 0 1,04 3 2 ionosphère 1,07 1,03 1 3 1,13 7 1 1,06 2 2 1,08 3 4 champignons 1,10 1,24 4 2 1,21 6 2 2,29 11 1 2,60 11 1 TicTacToe 0,97 0,97 0 0 0,91 0 1 0,95 0 0 0,95 0 0 véhicule 1,10 1,10 5 1 1,11 4 4 1,12 2 2 1,30 9 0 Waveform 0,92 0,99 13 0 1,01 19 0 1,48 30 0 1,47 30 0 vin 1,23 1,20 0 1 1,37 6 1 1,24 1 1 1,23 0 1 Synthèse 1,04 1,07 29 14 1,10 55 16 1,35 65 6 1,42 80 8 TAB 3 - Moyens de la qualité prédictive des groupes, le nombre de victoires significatives (+) et des pertes (-) par jeu de données pour la méthode Khiops par rapport aux méthodes alternatives. En résumé, le critère de la qualité prédictive suggère le classement suivant des méthodes éprouvées: Khiops, initiale Khiops, CHAID, Tschuprow, Ratio Gain. 4.2 Taille du groupement Les résultats du numéro de groupe sont résumées dans le tableau 4. Les différences sont très importantes entre les méthodes testées. Les méthodes Ratio Tschuprow et le gain produisent les plus petits groupes de taille moyenne, au détriment d'une faible qualité prédictive. Parmi les méthodes de regroupement de haute qualité, la méthode Khiops est un gagnant pour le numéro du groupe criteri sur, suivie de la méthode Khiops initiale et la méthode CHAID. Les groupes produits par la méthode Khiops sont toujours plus petit que ceux produits par la méthode CHAID, et les différences sont importantes pour 60% des attributs. Bien que les méthodes de Ratio Tschuprow et le gain obtiennent des groupes plus petits en moyenne, les résultats sont contrastés entre les ensembles de données. Pour près d'un quart des attributs, la méthode Khiops obtient des groupements nettement plus petite que la méthode du ratio de gain. Il est intéressant d'analyser plus en profondeur les résultats de l'ensemble de données de forme d'onde, où environ la moitié des attributs sont des attributs du bruit. Une inspection des groupements révèle que l'une méthode robuste pour diviser les valeurs de Categorical Attributs RNTI - Méthode 1 Khiops robustes de regroupement est la seule méthode qui identifie correctement les attributs de bruit avec des groupements réduits à un seul groupe. Dataset Khiops Ini. Khiops CHAID Tschuprow Gain Ratio + - + - + - + - Adult 3,67 3,99 5 0 4,83 11 0 2,05 2 10 2,33 2 10 australien 1,91 2,19 6 1 2,19 4 0 2,19 3 1 2,36 7 1 Sein 2,60 2,83 3 0 4,16 9 0 1,98 1 7 1,98 1 7 Crx 1,93 2,16 5 1 2,18 3 0 2,15 3 1 2,42 8 2 Coeur 1,91 2,27 5 0 2,14 4 0 2,11 3 1 2,08 3 1 HorseColic 1,87 2,20 11 0 2,24 10 0 2,03 7 4 2,03 8 4 ionosphère 2,47 2,94 17 1 3,18 25 0 2,09 0 15 2,05 0 17 Champignon 3,06 3,11 3 3 3,57 10 0 2,00 0 13 2,19 1 13 TicTacToe 2,03 2,03 0 0 2,11 1 0 2,00 0 0 2,00 0 0 véhicule 3,50 3,90 7 0 4,84 17 0 2,58 0 11 2,85 3 11 Waveform 2,67 3,56 30 0 3,76 35 0 2,73 21 19 3,18 21 18 vin 2,60 2,95 5 0 3,56 11 0 2,10 0 6 2,05 1 7 Synthèse 2,54 2,95 97 6 3,28 140 0 2,22 40 88 2,38 55 91 TAB 4 - Moyens de la taille des groupes, le nombre de victoires significatives (+) et des pertes (-) par jeu de données pour la méthode Khiops par rapport aux méthodes alternatives. En résumé, le critère numéro du groupe suggère le classement suivant des méthodes éprouvées: Tschuprow, Ratio Gain, Khiops, initiale Khiops, CHAID. 4.3 Bi-critères d'analyse des résultats afin de mieux comprendre les relations entre la qualité prédictive et la taille des groupes, nous tirons à la figure 1, les moyens globaux des résultats sur un plan de deux critères avec le numéro de groupe sur la coordonnée x et la qualité prédictive sur la coordonnée y. A titre de comparaison, nous présentons également les résultats obtenus par trois méthodes de groupage alternative simple: - Mode: bipartition sans supervision des catégories avec un groupe contenant le mode, soit la catégorie la plus fréquente, - Chi Valeur unique: bipartition des catégories avec une catégorie contre tous les autres, sélectionnés à l'aide du critère de chi-carré (une fusion finale est toujours possible), - exhaustive CHAID: bipartition des catégories obtenues avec l'algorithme CHAID en forçant les fusions jusqu'à ce que la partition contient au plus deux groupes. Les trois méthodes de regroupement de bipartition sont classés comme prévu pour le critère de la qualité prédictive. Les Tschuprow et le gain de méthodes Ratio qui sont autorisés à construire une partition avec plus de deux groupes n'obtiennent pas de meilleurs résultats sur la qualité prédictive que la méthode Exhaustive CHAID. Le groupe des méthodes efficaces (Khiops, initiale Khiops et CHAID) prend clairement avantage des partitions à plusieurs groupes. Parmi ces méthodes principales, la méthode Khiops domine les autres méthodes sur les deux critères. Enfin, compte tenu de la complexité de calcul des algorithmes, celle de l'algorithme optimisé Khiops est O (n.log (N) + Boullé RNTI - 1 I2.log (I)), tandis que d'autres méthodes est O (n.log (N) + I3). Cependant, la différence de temps d'exécution est mineur dans de nombreux cas, lorsque le nombre de valeurs catégoriques I est très faible. 1,10 1,20 1,30 1,00 1,40 1,50 1,60 1,70 1,80 1,90 2,00 1,0 1,5 2,0 2,5 3,0 3,5 4,0 Groupe Nombre K u llb ac k- L ei b le r D iv er g en CE Khiops initiale Khiops CHAID Chi Valeur unique mode Exhaustive CHAID Tschuprow Ratio Gain FIGUE. 1 - évaluation Bi-critères du groupement m éthodes pour le numéro de groupe et les critères de qualité prédictive 5. Conclusion Le principe de la méthode Khiops de regroupement est de minimiser le niveau de confiance lié à l'épreuve de l'indépendance entre l'attribut groupé et l'attribut de classe. Au cours du processus bottom-up de l'algorithme, de nombreuses fusions entre les catégories sont effectuées que les variations produisent de la valeur chi-carré de la table de contingence. En raison d'une modélisation statistique de ces variations lorsque l'attribut explicatif est indépendant de l'attribut de classe, nous avons amélioré l'algorithme de regroupement initial Khiops afin de garantir que les groupes d'attributs indépendants sont réduits à un seul groupe. Cette résistance attestés à surapprentissage est une alternative intéressante à l'approche de validation croisée classique. De nombreuses expériences comparatives montrent que la méthode Khiops surclasse les autres méthodes de regroupement testées. Il permet de réduire considérablement le nombre de valeurs d'attributs catégoriques dans l'étape de pré-traitement exploration de données, tout en gardant la plupart de leur performance prédictive monothétique. [Berckman Références, 1995]. Berckman N. C Valeur de regroupement pour les arbres de décision binaires. Rapport technique. Département Informatique - Université du Massachusetts, 1995. [Blake et Merz, 1998] C.L. Blake et Merz C.J.. UCI référentiel de bases de données d'apprentissage machine URL Web http://www.ics.uci.edu/~mlearn/MLRepository.html, Irvine, CA: Université de Californie, Département de l'information et de l'informatique, 1998. [Boullé, 2003a] M . Boullé. Khiops: une discrétisation Méthode des attributs continus avec une résistance garantie au bruit. Actes de la troisième Conférence internationale sur l'apprentissage et fouille de données dans la reconnaissance des formes, 50-64, 2003. Une méthode robuste pour les valeurs de Partitionnement catégorielles Attributs RNTI - 1 [Boullé, 2003b] M. Boullé. Groupage des facts d'Robuste un par la symbolique attribut Khiops méthode. Note technique NT / FTR & D / 8028, France Télécom R & D, 2003. [Breiman et al., 1984] L. Breiman, J. H. Friedman R.A. Olshen et Pierre C.J.. Et régression des arbres. Californie: [. Cestnik et al, 1987] Wadsworth International, 1984. B. Cestnik, I. Kononenko et I. Bratko. ASSISTANT 86: Un outil pour la connaissance explicitation utilisateurs sophistiqués. Dans Bratko & Lavrac, Les progrès dans l'apprentissage machine, Wilmslow, Royaume-Uni (Eds.): Sigma Press, 1987. [Chou, 1991] P.A. Chou. Optimal Partitionnement pour la classification et la régression des arbres. IEEE Transactions sur le modèle d'analyse et de l'intelligence artificielle, 13 (4): 340-354, 1991. [. Fulton et al, 1995] T. Fulton, S. Kasif et S. Salzberg. Des algorithmes efficaces pour trouver des fractionnements façon multi pour les arbres de décision. Actes de la Conférence mixte internationale Treizième sur l'intelligence artificielle, San Francisco, CA: Morgan Kaufmann, 244-255, 1995. [Kass, 1980] G.V. Kass. Une technique d'exploration pour étudier de grandes quantités de données catégoriques. Statistique appliquée, 29 (2): 119-127, 1980. [Kerber, 1991] R. Kerber. ChiMerge discrétisation des attributs numériques. Actes de la 10e Conférence internationale sur l'intelligence artificielle, 123-128, 1991. [Kullback, 1968] S. Kullback. Théorie de l'information et de la statistique. New York: Wiley, (1959); réédité par Dover, 1968. [Lechevallier, 1990] Y. Lechevallier. Recherche d'une partition sous Une contrainte d'optimale totale ordre. Rapport technique N ° 1247, INRIA, 1990. [Pyle, 1999] D. Pyle. Préparation des données pour l'exploration de données, Morgan Kaufmann, 1999. [Quinlan, 1986] J.R Quinlan. Induction d'arbres de décision. Machine Learning, 1: 81-106, 1986. [Quinlan, 1993] J.R Quinlan. C4.5: Programmes d'apprentissage machine. Morgan Kaufmann, 1993. [Ritschard et al., 2001] G. Ritschard D.A. Zighed et N. Nicoloyannis. Maximisation de l'association par Regroupement de lignes Ou de tableau d'un Colonnes Croisé. Math. & Sci. Hum, n ° 154-155:. 81-98, 2001. [Ritschard et Zighed, 2003] G. Ritschard et D.A. Zighed. Modelisa tion de tables de Contingence par induction d'arbre. Extraction et Gestion des Connaissances, 381-392, 2003. [Zighed et Rakotomalala, 2000] D.A. Zighed et R. Rakotomalala. Graphes d'induction. Hermes Science Publications, 327-359, 2000. Résumé Dans l'apprentissage machine supervisé, le partage des valeurs (également appelées regroupement) d'un objectif d'attributs catégoriques à la construction d'un nouvel attribut synthétique qui conserve les informations de l'attribut initial et réduit le nombre de ses valeurs. Dans cet article, nous proposons une nouvelle méthode de regroupement Khiops, basé sur une généralisation de l'algorithme de discrétisation Khiops. Cette méthode de regroupement fournit des garanties contre surapprentissage et conduit ainsi à des groupes robustes. Cette propriété découle d'une modélisation statistique de la méthode Khiops qui permet d'affiner l'algorithme. De nombreuses expériences démontrent la validité de cette approche et montrent que la méthode de regroupement Khiops construit des groupes de grande qualité, tant en termes de qualité prédictive et petit nombre de groupes."
1216,Revue des Nouvelles Technologies de l'Information,EGC,2004,Extraction of text summary using latent semantic indexing and information retrieval technique : comparison of four strategies,"In this paper, we present four generic text summarization techniques. Each technique extracts a text summary by ranking and extracting sentences from an original document. The first method, SUMMARIZER 1, uses standard information retrieval (IR) methods to rank sentences. The second method, SUMMARIZER 2, uses the Latent Semantic Analysis (LSA) technique to identify semantically important sentences, for summary creations. The third method, SUMMARIZER 3, uses a combination of the latent semantic analysis technique, reduction and relevance measure. The fourth method simply uses the TF*IDF (Term frequency * Inverse Document frequency) weighting scheme. Evaluations of the four methods are conducted using Document Understanding Conferences (DUC) datasets from NIST. We have compared the summary of each method with the manual summaries. Summarizer 4, with its lowest overhead, has comparable performance to summarizer 1. Analysis shows that a combination of LSA technique and the relevance measure (Summarizer 3) has the best performance on an average.","Abdelghani Bellaachia, Anand Mahajan",http://editions-rnti.fr/render_pdf.php?p1&p=1001111,http://editions-rnti.fr/render_pdf.php?p=1001111,en,"final_paper_france.PDF Extraction du texte Résumé Utilisation de l'indexation sémantique et Latent Information Retrieval Technique: Comparaison des quatre stratégies Bellaachia * Abdelghani, Anand Mahajan * * Département Informatique Université George Washington Washington DC, 20052 bell@gwu.edu http: //seas.gwu .edu / ~ cloche Résumé. Dans cet article, nous présentons quatre techniques de résumé de texte générique. Chaque technique extrait un résumé de texte par le classement et l'extraction des phrases à partir d'un document original. La première méthode, SUMMARIZER 1, utilise recherche d'information des méthodes standard (IR) à des peines de rang. La deuxième méthode, SUMMARIZER 2, utilise la technique d'analyse sémantique latente (LSA) pour identifier les phrases sémantiquement importantes, pour les créations sommaires. La troisième méthode, SUMMARIZER 3, utilise une combinaison de la technique d'analyse sémantique latente, la réduction et la mesure de pertinence. La quatrième méthode utilise simplement la TF * IDF (fréquence de durée * de fréquence de document inverse) schéma de pondération. Les évaluations des quatre méthodes est utilisé dans des conférences Comprendre le document (CIC) des ensembles de données du NIST. Nous avons comparé le résumé de chaque méthode avec les résumés manuels. Summarizer 4, avec ses frais généraux plus bas, a des performances comparables à summarizer 1. L'analyse montre qu'une combinaison de technique LSA et la mesure de pertinence (summarizer 3) a la meilleure performance moyenne. 1. Introduction La vitesse et l'ampleur de la diffusion de l'information ont considérablement augmenté avec la croissance explosive du web dans le monde entier. En utilisant des techniques de récupération d'informations classiques (IR) pour trouver des informations pertinentes efficacement dans une vaste mer de documents texte accessible sur Internet, est devenu de plus en plus insuffisante. Les moteurs de recherche de texte servent de filtres d'information qui passer au crible une première série de documents pertinents. Leur approche par mots-clés récupère des millions de visites par lequel l'utilisateur est submergé. Par conséquent, il est nécessaire de techniques d'identifier rapidement les documents les plus pertinents. Summarizers texte peuvent être utilisés pour aider les utilisateurs à identifier ensemble final des documents pertinents. Recherche de texte et deux technologies sont summarization essentielles qui se complètent mutuellement. Présenter à l'utilisateur un résumé de chaque document facilite grandement la tâche de trouver les documents souhaités. Les objectifs de Summarizers texte peuvent être classés selon leur intention, et de la couverture mise au point [MCDONALD ET AL.]. Intention fait référence à l'utilisation potentielle du résumé. Firmin et texte Résumé Utilisation de l'indexation sémantique et Latent recherche d'information technique RNTI - E - 2 Chrzanowski diviser l'intention un résumé en trois catégories principales [ET FIRMIN CHRZANOWSKI, 1999]: - indicatif -, informatif et - évaluative. résumés indicatifs donnent une indication sur le thème central du texte original ou suffisamment d'informations pour juger la pertinence du texte. des résumés d'information peuvent servir de substituts pour les documents complets. résumés évaluatives expriment le point de vue de l'auteur sur un sujet donné. Mise au point fait référence à la portée du résumé, si générique ou requête pertinente. Enfin, la couverture fait référence au nombre de documents qui contribuent au résumé, si le résumé est fondé sur un seul document ou plusieurs documents. des résumés de texte peuvent être des résumés aux requêtes ou des résumés génériques [Gong et Liu, 2001]. Création d'un résumé de requête pertinente est un processus de récupération des phrases pertinentes de requête à partir du document et peut être facilement atteint par l'extension des technologies classiques IR. Comme ils sont « requête biaisées », ils ne fournissent pas un sens global du contenu du document. Cependant, un résumé générique donne un sens global du contenu du document et détermine dans quelle catégorie il appartient. Un bon résumé générique doit contenir les principaux thèmes du document tout en gardant la redondance au minimum. Puisque ni requête, ni le sujet est fourni au summarizatio n processus, il est tout un défi de développer une méthode de générique de haute summarization qualité. Dans cet article, nous présentons quatre méthodes de résumé de texte générique. Chaque méthode crée un résumé de texte par le classement et l'extraction des phrases à partir d'un document original. La première méthode, SUMMARIZER 1, utilise recherche d'information des méthodes standard (IR) à des peines de rang. La deuxième méthode, SUMMARIZER 2, utilise la technique d'analyse sémantique latente (LSA) pour identifier les phrases sémantiquement importantes, pour les créations sommaires. La troisième méthode, SUMMARIZER 3, utilise une combinaison de la technique d'analyse sémantique latente, la réduction et la mesure de pertinence. La quatrième méthode utilise simplement le TF * schéma de pondération IDF. Nous avons également comparé les performances de toutes ces méthodes à l'aide des conférences Comprendre le document (CIC) des ensembles de données du NIST. L'article est organisé de la manière suivante. La section suivante décrit les travaux connexes. La section 3 présente les quatre méthodes de compression. Les évaluations de rendement sont présentés à la section 4. Enfin, la section 5 conclut le document. 2. Travaux connexes Summarization est une tâche difficile traitement du langage naturel. Il nécessite une analyse sémantique, le traitement et l'interprétation du discours déductif (regroupement du contenu en utilisant les connaissances du monde). Les tentatives d'exécution véritable abstraction - création de résumés comme des résumés - n'ont pas été très réussie. programmes d'abstraction texte produisent des phrases grammaticales qui résument les concepts d'un document. Les concepts dans un résumé sont souvent considérés comme ayant été comprimé. Alors que la formation d'un résumé peut mieux répondre à l'idée d'un résumé, sa création implique une plus grande complexité et la difficulté [Hovy et Lin, 1998]. Heureusement, cependant, une approximation appelée extraction est plus réalisable aujourd'hui. Pour créer un extrait, un système a besoin simplement d'identifier les plus importants / actualité / thème central (s) du texte Résumé Utilisation de l'indexation sémantique et Latent recherche d'information technique RNTI - E - 2, le texte et les retourner au lecteur. Un reste sommaire extrait plus proche du document original, en utilisant des phrases du texte, limitant ainsi le biais qui pourrait autrement apparaître dans un résumé [LUHN, 1958]. Bien que le résumé est pas nécessairement cohérente, le lecteur peut se faire une opinion du contenu de l'original. La plupart des systèmes automatisés de récapitulation extraits produisent aujourd'hui seulement. La majorité des études de recherche ont été mis l'accent sur la création de résumés de texte requête pertinente. SUMMARIST est une tentative de développer une technologie d'extraction robuste [Hovy et Lin, 1998]. Elle produit des résumés d'extrait en cinq langues (et a été liée aux moteurs de traduction pour ces langues dans le système MUST). SUMMARIST est basée sur la « équation » suivante: Summarization = Identification + Sujet + interprétation génération. Le summarizer texte de CGI / CMU utilise une technique appelée Maximal Marginal Pertinence (ROR) qui mesure la pertinence de chaque phrase dans le document à l'utilisateur fourni requête, ainsi que les phrases qui ont été sélectionnés et ajoutés dans le résumé [Goldstain et al.]. La sélection des phrases qui sont très pertinents à la requête de l'utilisateur, mais sont différents les uns des autres crée le résumé du texte. Le système de gestion des connaissances (KM) de -.Résumé SRA International Inc. propose l'utilisation analyse morphologique, le marquage du nom et de la résolution co-référence [SRA]. Ils ont utilisé une technique d'apprentissage de la machine afin de déterminer la combinaison optimale de ces caractéristiques en combinaison avec des informations statistiques du corpus d'identifier les meilleures phrases pour inclure dans un résumé. R. Barzilay et M. Elhadad mis au point une méthode qui crée des résumés de texte en trouvant des chaînes lexicales du document [Barzilay ET Elhadad, 1997]. Le système Cornell / Sabir utilise les capacités de récupération de classement de documents et le passage des moteurs de recherche de texte SMART pour identifier efficacement les passages pertinents dans un document [ET BUCKLEY ET AL., 1999]. B. Chauve victoire et T.S. Morton a développé un summarizer qui sélectionne des phrases du document jusqu'à ce que toutes les phrases dans la requête sont couverts [ET BALDWIN T.S. MORTON, 1998]. Une phrase dans le document est considéré comme couvrir une phrase dans la requête si elles co-réfèrent à la même personne, organisation, événement, etc. Le papier par Yihong Gong et Xin Liu [Gong et Liu, 2001], compare les résumés manuels avec les résumés automatisés à l'aide du retrait (R), de précision (P) le long de F. Ils montrent que les exécute sur la base de la méthode IR mieux sur la moyenne. Dans cet article, nous examinons les performances des quatre méthodes de résumé qui sont discutés en détail dans la section suivante. 3. Summarizers Quatre méthodes de résumé de texte générique sont présentés. Ils créent des résumés de texte par le classement et l'extraction des phrases à partir des documents originaux. Les méthodes sont les suivantes: - SUMMARIZER 1: utilise des méthodes standard IR à la pertinence de la peine de rang; - SUMMARIZER 2: utilise la technique de LSA pour identifier les phrases sémantiquement importantes pour les créations sommaires; - SUMMARIZER 3: utilise une combinaison de la technique d'analyse sémantique latente, la réduction et la pertinence mesure; - SUMMARIZER 4: utilise système de pondération TF * IDF à des peines de rang et de phrases choisit haut pour former un résumé. Texte Résumé Utilisation de l'indexation sémantique et Latent recherche d'information technique RNTI - E - 2 Chaque méthode tente de sélectionner des phrases qui couvrent les principaux thèmes du document, autant que possible et en même temps, maintient la redondance au minimum. Les premières mesures prises pour chacun sont les suivants: 1. Décomposer le document en phrases individuelles; 2. Créez un vecteur de fréquence à long terme pondérée pour chaque phrase. Le vecteur de fréquence de terme pondérée Si = [S1i S2i ... SNI] T de phrase i est définie par: sji = L (SJI). G (SJI) (1), L (SJI) est la pondération locale pour terme j dans la phrase i, G (SJI) est la pondération globale pour terme j dans tout le document. Un document est décrit par une matrice de similarité, où chaque colonne représente le vecteur de fréquence terme- de chaque phrase. La matrice peut être soit normalisée ou non normalisée et peut utiliser l'une des méthodes de pondération suivantes: - Les poids locaux sont: n poids: L (SJI) = tf (SJI), le poids binaire: L (SJI) = 1, si tf (SJI)? 1, L (SJI) = 0, sinon, le poids augmentée: L (SJI) = 0,5 + 0,5 * (tf (SJI) / tf (max)) où, tf (max) = max {tf (1i), tf ( 2i), ..., tf (mi)}, poids Logarithme: L (SJI) = log (1 + tf (ji)) - Les poids globaux sont les suivants: Aucune pondération: G (SJI) = 1, la fréquence du document inverse ( IDF): G (j) = log (N / n (j)) où N est le nombre total de phrases dans le document, et n (j) est le nombre de phrases qui contiennent terme j. - Si par Normalise Normalization sa longueur | Si |. Utilise sa forme originale Si. 3.1. Summarizer 1 Cette summarizer prend en entrée, le document pour résumer, la taille de synthèse souhaitée, le choix des systèmes de pondération locaux et mondiaux, et le choix du calcul de mesure de pertinence (intérieure Produit / Cosinus similarité / coefficient Jaccard, tel que défini plus tard). Il produit, un résumé qui est un extrait sur la base la plupart des phrases pertinentes dans le document. Les principales étapes de SUMMARIZER 1 sont les suivants: 1. Décomposer le document en phrases individuelles et d'utiliser ces phrases pour former la phrase ensemble candidat S. 2. Créer le vecteur terme de fréquence pondérée Ai pour chaque phrase i? S et le vecteur de fréquence à long terme pondérée D pour l'ensemble du document. Texte Résumé En utilisant Latent Semantic Indexing et de recherche d'information technique RNTI - E - 2 3. Pour chaque phrase i? S, calculer la mesure de pertinence entre Ai et D, qui est le produit intérieur ou Cosinus similarité, ou coefficient de Jaccard entre Ai et D. 4. Sélectionnez k phrase qui a le score de pertinence plus élevé et l'ajouter au résumé. 5. Supprimer k de S, et d'éliminer tous les termes contenus dans k du document. Recalcule le vecteur fréquence terme pondérée D pour l'ensemble du document. 6. Si l'engourdissement er des phrases dans le résumé atteint la valeur prédéfinie, mettre fin à l'opération: sinon, passez à l'étape 3. Pour déterminer la pertinence mesure, les fonctions suivantes sont envisagées: - Produit intérieur (IP): IP (Ai, D) =? tk = 1 (AIK * dk) où, AIK est le poids de k terme dans la phrase i et dk est le poids de terme k dans le document. - Cosinus similarité (Cos): Cos (Ai, D) =? tk = 1 (aik * dk) / (? tk = 1? a2ik * tk = 1? d2k). - Coefficient Jaccard (JC): JC (Ai, D) =? tk = 1 (AIK * dk) / (tk = 1 a2ik + tk = 1 d2k -?? tk = 1 (AIK * dk)) Dans l'étape 4, k phrase qui a la mesure de pertinence plus élevé avec le document celui qui représente le mieux le contenu important du document. Sélection de phrases en fonction de leurs mesures de pertinence garantit que le résumé couvre les sujets principaux du document. D'autre part, ce qui élimine tous les termes contenus dans k du document à l'étape 5, assure que la sélection de la phrase suivante choisira les phrases avec un chevauchement minimal avec peine k. 3.2. 2 Cette Summarizer Summarizer sélectionne les phrases les plus élevées de chaque classement sujet / concept de saillant à l'aide de l'analyse sémantique latente (LSA de). LSA implique l'application de la décomposition en valeurs singulières (SVD). Étant donné un m? n, termes par phrase matrice A = [A1 A2 ... An] où, chaque vecteur de colonne Ai représente le vecteur terme de fréquence pondérée de phrase i dans le document, m est le nombre total de termes et n est le nombre total de Phrases. Le SVD de A est défini comme [PRESSE ET ET AL, 1992.]: A = U? VT (2) où U = [uij] est une matrice colonne de orthonormé mxn dont les colonnes sont appelés à gauche vecteurs singuliers; ? = Diag (1?, ...,? N) est une matrice diagonale nxn dont les éléments diagonaux sont des valeurs singulières non négatifs triés par ordre décroissant, et V = [vij] est un nxn orthonormé matrice dont les colonnes sont appelés vecteurs singuliers droit. Si le rang (A) = r, alors? satisfait? 1 ? ? 2? ...? ? r>? r + 1 = ... =? n = 0 (3) L'interprétation de l'application de la SVD aux termes de phrase matrice A peut être faite à partir de deux points de vue différents. Du point de transformation de vue, le SVD tire un résumé de texte à l'aide Latent Semantic Indexing et de recherche d'information technique RNTI - E - 2 correspondance entre l'espace de dimension m engendré par les vecteurs de fréquence à long terme pondéré et l'espace vectoriel singulier r dimensions avec tous les ses axes linéairement indépendants. Cette cartographie projets chaque vecteur de colonne i de la matrice A, qui représente le vecteur de fréquence terme- pondérée de phrase i, à vecteur colonne? i = [VI1 VI2 ... vir] T de la matrice VT, et des cartes de chaque vecteur rang j dans la matrice A, qui indique le nombre d'occurrences du terme j dans chacune des pièces, à vecteur ligne? j = [uj1 uj2 ... UJR] T de la matrice U. Ici, chaque élément de vix de? i, ujy de? j est appelé l'indice de la x? e, y? e vecteurs singuliers, respectivement. Du point de vue sémantique, la SVD dérive la structure sémantique latente du document représenté par la matrice A [Deerwester ET AL., 1990]. Cette opération reflète la répartition du document original en r vecteurs de base linéairement indépendants ou des concepts. Chaque terme et phrase du document est conjointement indexé par ces vecteurs / concepts de base. L'ampleur de la valeur singulière correspondant indique le degré d'importance de ce modèle dans le document. Toutes les phrases contenant ce modèle de combinaison de mots seront projetés le long de ce vecteur singulier, et la phrase qui représente le mieux ce modèle aura la plus grande valeur d'index avec ce vecteur. Les principales étapes de SUMMARIZER 2 sont les suivants: 1. Décomposer le document D en phrases individuelles, et d'utiliser ces phrases pour former l'ensemble phrase candidat S, et ensemble k = 1. 2. Construct les termes de phrases matrice A pour le document D. 3. Effectuer la SVD sur A pour obtenir U, la matrice de valeurs singulières? Et la matrice de vecteur singulier droit VT. Dans l'espace vectoriel singulier, chaque phrase i est représenté par la vecteur colonne ? i = [VI1 VI2 ... vir] T de VT. 4. Sélectionnez le kème droit vecteur singulier de la matrice VT. 5. Sélectionnez la phrase qui a la plus grande valeur d'index avec le k-ième vecteur singulier droit, et l'inclure dans le résumé. 6. Si k atteint le nombre prédéfini, mettre fin à l'opération: sinon, k incrément par un, et passez à l'étape 4. À l'étape 5, trouver la phrase qui a la plus grande valeur d'index avec le k-ième vecteur singulier droit est équivalent à trouver le vecteur de colonne? i dont l'élément k-ième vik est le plus grand. Cette opération équivaut à trouver la meilleure phrase décrivant le concept / sujet représenté par le vecteur singulier kème. Depuis sont classés les vecteurs singuliers dans l'ordre décroissant de leurs valeurs singulières correspondantes, le vecteur singulier kème représente le concept important kème / sujet. Parce que tous les vecteurs singuliers sont indépendants les uns des autres, les phrases sélectionnées par cette méthode contiennent un minimum de chevauchement. 3.3. Summarizer 3 Cette summarizer prend en entrée, le document pour résumer, la taille de synthèse souhaitée, le choix des systèmes de pondération locaux et mondiaux, le choix du calcul de mesure de pertinence (intérieure Produit / Cosinus Similitude / Jaccard Co-efficace tel que défini précédemment). Il produit, un résumé qui est un extrait de la taille désirée. Summarizer 3 Réalise décomposition de valeur singulière, suivie d'une réduction puis par le calcul de la mesure de pertinence pour déterminer les phrases à ajouter au résumé. Après avoir effectué SVD sur A (comme décrit dans résumeur 2), les valeurs singulières obtenues signifient les concepts pondérés maximales possibles dans la collection de phrases. L'équation (3) Résumé du texte à l'aide et indexation sémantique latente recherche d'information technique RNTI - E - 2 ci-dessus montre que le nombre de r »valeurs singulières non nulles signifient le nombre de concepts pondérés. La réduction de la dimension des composants de SVD-à-dire U,? et VT est défini comme suit: la Fig. 1 SVD Dimension Reduction Après la décomposition, la dimension de chaque composant est réduite en fonction de la valeur de « r » à savoir les non nuls des valeurs singulières, comme indiqué ci-dessus. Dans notre cas, nous utilisons r = n. A nouveau, on obtient, après re-multiplier les composantes réduites (matrices). A contient maintenant les informations les plus pondérée dans un espace de dimension caractéristique très élevée. Chaque vecteur de colonne A représente une phrase. Effectuez la pertinence Mesure de calcul entre chaque phrase et le Document D. Rang toutes les phrases et choisir la phrase avec le score le plus élevé et l'ajouter au résumé. Retirez tous les termes dans la phrase du document et recalcule D. Répéter le calcul de mesure de la pertinence et de construire le résumé de la taille désirée. Le flux de fonctionnement est suit comme: 1. Décomposer le document en phrases individuelles, et d'utiliser ces phrases pour former l'ensemble de la phrase candidate S. 2. Construct la matrice des termes par des phrases A pour le document. 3. Effectuer la SVD sur A pour obtenir U, la matrice de valeurs singulières? Et la matrice de vecteur singulier droit VT. Dans l'espace vectoriel singulier, chaque phrase i est représenté par le vecteur de colonne? i = [VI1 VI2 ... vir] T de VT. 4. Effectuez la réduction r = n. 5. Obtenir une nouvelle fois par re-multiplier U,? et VT. Ured? rouge VTred A m x r r r r x x n x m x n x = réduction de la dimension A U? VT = x Résumé du texte à l'aide et indexation sémantique latente recherche d'information technique RNTI - E - 2 6. Maintenant, utilisez la mesure souhaitée Pertinence et ajouter la phrase la plus élevée classé k au résumé. 7. Retirer tous les termes de k à partir de D et A et re-calcul D. 8. Répéter à travers l'étape 6 jusqu'à ce que la synthèse de la taille souhaitée est formée. L'opération SVD équivaut à trouver les concepts saillants / thèmes représentés par les vecteurs singuliers. Étant donné que les vecteurs singuliers sont classés par ordre décroissant de leur correspondant valeurs singulières, r vecteurs singuliers représentent les concepts importants r / thèmes. Après réduction, l'application (r = n) pour chaque matrice, A contient les informations les plus pondéré dans un espace de caractéristique dimensionnelle très élevée. En outre, la sélection des phrases en fonction de leurs scores de pertinence garantit que le résumé couvre les sujets principaux du document. D'autre part, ce qui élimine tous les termes contenus dans k du document à l'étape 7 assure que la sélection de la phrase suivante choisira les phrases avec un chevauchement minimal avec k. 3.4. Summarizer 4 Cette summarizer sélectionner des phrases du TF * du schéma de pondération IDF pour sélectionner des phrases. Il est le plus simple parmi toutes les techniques proposées. Il fonctionne comme suit: 1. Décomposer le document en phrases individuelles et d'utiliser ces phrases pour former la phrase ensemble candidat S. 2. Créer le vecteur terme de fréquence pondérée Ai pour chaque phrase i? S en utilisant TF * IDF. 3. Somme le TF * IDF score pour chaque phrase et le rang eux. 4. Sélectionnez le nombre prédéfini de phrases dans le résumé de A. 4. Évaluation du rendement Dans cette section, nous comparons les sorties automatiques (récapitulation extraits) de chaque Summarizer, avec les résumés manuels (résumés) générés par les évaluateurs humains indépendants. Nous avons utilisé des jeux de données conférences Comprendre le document (DUC) du NIST pour l'évaluation de la performance. L'ensemble de données comprend trois séries de documents de chaque évaluateur / sélecteur humain indépendant. Chaque jeu a entre 3 et 20 documents. Chaque sélecteur construit résumés (résumés) pour chaque document dans le jeu d'une longueur approximative de 100 mots. Un échantillon des données DUC a été choisi pour nos fins de test. Il se compose de 2 ensembles de documents (un ensemble de chacun des sélecteurs 2). L'ensemble de sélection 1 se compose de 5 documents, alors que l'ensemble de sélecteur 2 de 4 documents. Chaque sélecteur crée un résumé (abstract) intitulé « Résumé original », pour chaque document dans sa / son ensemble. En outre un sélecteur, autre que le sélecteur d'origine du document, crée un résumé (abstract) intitulé « Résumé double », pour chaque document. Ainsi, il y a deux résumés manuels (résumés) pour chaque document. Résumés (Original / double) pour le même document peuvent être de tailles différentes (pas de phrases.). Nous créons des résumés automatisés (taille similaire aux résumés manuels) pour tous les neuf documents des deux sélectionneurs. Nous comparons ensuite les résumés automatisés avec le résumé du texte à l'aide Latent Semantic Indexing et recherche documentaire Technique RNTI - E - 2 résumés manuel à l'aide mesure « Cosinus similarité » pour voir ce qui correspond au document résumé de plus près. En raison du manque d'espace dans cet article, nous présentons uniquement les résultats de la comparaison entre les résumés automatisés et des résumés originaux. D'autres résultats peuvent être trouvés dans [Bellaachia et Mahajan, 2003]. La figure 2 et la figure 3 montre le rendement des quatre summarizers à l'aide de produit interne et NTN: N = Non pondération local, T = FID pour la pondération globale et N = Aucune normalisation (voir schémas de pondération) Les chiffres montrent que SUMMARIZER 1 et 3 ont des performances comparables . Notez que SUMMARIZER 4 a la tête le plus bas parmi tous les autres résumeurs. Sélecteur 1, NTN, résumés d'origine 0,7 0,75 0,8 0,65 0,85 0,9 0,95 1 2 3 4 5 Document N ° C en os e Si m ila ri ty Summarizer1 Summarizer2 Summarizer3 Summarizer4 FIG. 2 - Sélecteur 1 Résultats Sélecteur 2, NTN, résumés d'origine 0,75 0,8 0,85 0,7 0,9 0,95 1 1 2 3 4 Document N ° C en os e Si m ila ri ty Summarizer1 Summarizer2 Summarizer3 Summarizer4 FIG. 3 - Sélecteur 2 Résultats Texte Résumé aide Latent Semantic Indexing et de recherche d'information technique RNTI - E - 2 La figure 4 montre la similitude cosinus d'une phrase (dans un résumé automatisé généré par chaque summarizer) avec le document d'entrée. SUMMARIZER 2 a la plus faible mesure et Summarizer 3 a la mesure la plus élevée, tandis que SUMMARIZ ER 1 et 4 ont des performances comparables. Cosinus de similarité de synthèse automatisé (par phrase) avec le document, généré en utilisant chaque résumeur, NTN et intérieure du produit 0,16 0,161 0,162 0,159 0,163 0,164 0,165 0,166 0,167 0,168 1 2 3 4 No. résumeur C os dans e Si m ila ri ty FIG. 4 - Performance Phrase 5. Concllusion Le présent document a présenté quatre méthodes de résumé de texte qui créent des résumés de texte génériques par le classement et l'extraction des phrases à partir des documents originaux. La première méthode utilise des méthodes de recherche d'information standard à la pertinence de la phrase de rang, tandis que la seconde méthode utilise la technique de LSA pour identifier les phrases sémantiquement importantes. La troisième méthode, SUMMARIZER 3, utilise une combinaison de la technique d'analyse sémantique latente, la réduction et la mesure de pertinence. La quatrième méthode utilise simplement le TF * schéma de pondération IDF. Nous avons utilisé des jeux de données conférences Comprendre le document (DUC) du NIST pour l'évaluation de la performance. Deux séries de documents ont été choisis pour notre évaluation de la performance. Summarizer 4, avec son plus bas frais généraux, a des performances comparables à summarizer 1. La technique LSI, tel qu'il est utilisé dans SUMMARIZER 2 et 3 summarizer, n'améliore le texte summarization. La combinaison de plusieurs techniques utilisées dans Summarizer 3 a la meilleure performance moyenne. Remerciement: Nous tenons à remercier M. Avinash K. Kanal pour sa mise en œuvre de la technique LSA. Texte Résumé Utilisation de l'indexation sémantique et Latent recherche d'information technique RNTI - E - 2 Références [Gong et Liu, 2001] Yihong Gong et Xin Liu. Summarization texte générique avec pertinence Mesure et analyse sémantique latente. Dans Actes de la conférence internationale annuelle 24 ACM SIGIR sur la recherche et le développement dans la recherche d'information, pp. 19 - 25, 2001. [Hovy et Lin, 1998] E.Hovy et C. Lin. texte automatisé dans summarist summarization. Dans Actes de l'atelier TIPSTER, Baltimore, MD, 1998. [Goldstain et al.] J. Goldstain, M. Kantrowitz, V. Mittal et J. Carbonell. Résumant documents texte: sélection des phrases et des mesures d'évaluation. Dans Proceedings of ACM SIGIR '99, Berkeley, CA, août 1999. [SRA] http://www.SRA.com. [Barzilay et Elhadad, 1997] R. Barzilay et M. Elhadad. En utilisant des chaînes lexicales pour le texte « , résumé dans les Actes de l'atelier sur le texte évolutif intelligent Summarization, Madrid, Espagne, août 1997. [Buckley et et al., 1999] C. Buckley et al .. et L'empire intelligent / pronostiqueur ir système. In Proceedings of TIPSTER Phase III d'atelier. 1999. [Baldwin et T. S. Morton, 1998] B. Baldwin et T. S. Morton. Dynamique à base coréférences summarization. Dans Actes de la troisième Conférence sur les méthodes empiriques dans le traitement du langage naturel (EMNLP3), Grenade, Espagne, Juin 1998. [Luhn, 1958] Luhn, H.P. La création automatique de résumés Littérature. dans Maybury, M. T. ed. Les progrès réalisés dans automatique. Summarization Texte Le MIT Press, Cambridge, 1958, 15-22. [ET FIRMIN CHRZANOWSKI, 1999] Firmin, T. et Chrzanowski, M.J. Une évaluation du automatique des systèmes texte récapitulation. dans Maybury, M. T. ed. Les progrès réalisés dans automatique Summarization texte, MIT Press, Cambridge, 1999. [McDonald et al.,] D. McDonald et al., « Utilisation de la peine-peine Heuristique pour classer les segments de texte dans TXTRACTOR, » MIS Département, Université de l'Arizona , Tucson, AZ. W. Appuyez sur et al, numérique Recettes en C [Presse et et al., 1992]. L'art de calcul scientifique. Cambridge, Angleterre. Cambridge University Press, 2 ed, 1992. S. Deerwester et al, « l'indexation par analyse sémantique latente, » JASIS, vol [Deerwester et al., 1990].. 41, pp 391-407, 1990. [Bellaachia et Mahajan, 2003] et Anand Abdelghani Bellaachia Mahajan. Comparaison des trois méthodes texte récapitulation. La 12e Conférence internationale sur les systèmes intelligents et adaptatifs et génie logiciel, San Francisco, Californie, Juillet 2003. Texte Résumé L'utilisation Latent Semantic Indexing et de recherche d'information Te chnique RNTI - E - 2 CV Ce papier d'methods Présente extraction quatre saisons de résumé de texte automatique. Elles Sont présentées à their visiter Chacune et Comparées à un CV de manuel Processus (extraction par de phrases pertinentes). Les methods 2 et 3 la décomposition aux utilisent facts Singulières (documents Divisés en phrases) pour selectionner les phrases better Les plus TYPIQUES. Les methods were Quatre personnes évaluées collection de juin Utilisant texte de NIST."
1223,Revue des Nouvelles Technologies de l'Information,EGC,2004,How well go Lattice algorithms on currently used machine leaning TestBeds ?,"Many research papers in classification or association rules increase the interest of Concept lattices structures for data mining (DM) and machine learning (ML). To increase the efficiency of concept lattice-bases algorithms in ML, it is necessary to make us of an efficient algorithms to build concept lattices. In fact, more than ten algorithms for generating concept lattices were published. As real data sets for data mining are very large, concept lattice structure suffers form its complexity issues on such data. The efficiency and performance of concept lattices algorithms are very different from one to another. So we need to compare the existing lattice algorithms with large data. We implemented the four first algorithms in Java environment and compared these algorithms on about 30 datasets of the UCI repository that are well established to be used to compare ML algorithms. Preliminary results give preference to Ganter's algorithm, and then to Bordat's algorithm, which do not fil well with the recommendations of Kuznetsov and Obiedkov. Furthermore, we analyzed the duality of lattice-based algorithms.","Huaiguo Fu, Engelbert Mephu Nguifo",http://editions-rnti.fr/render_pdf.php?p1&p=1001078,http://editions-rnti.fr/render_pdf.php?p=1001078,en,
1232,Revue des Nouvelles Technologies de l'Information,EGC,2004,Mediating the Semantic Web,"Cet article développe une extension d'une architecture de médiation pour intégrer le Web sémantique. Plus précisément, XLive est un médiateur tout XML développé à PRiSM. Il permet d'exécuter des XQuery sur des sources de données hétérogènes. Après une rapide présentation de XLive et du Web sémantique, une architecture à trois niveaux d'ontologies et de schémas est introduite pour connecter des adaptateurs pour le Web sémantique. Cette architecture vise à intégrer des sources de type Web service d'information conformément à une ontologie globale de référence. Elle conduit à étendre XLive avec le support de vues, un outil de conception de vues et de mappings, et des adaptateurs pour les Web services.","Georges Gardarin, Tuyet-Tram Dang-Ngoc",http://editions-rnti.fr/render_pdf.php?p1&p=1000883,http://editions-rnti.fr/render_pdf.php?p=1000883,en,"Microsoft Word - MediatingtheSemanticWebV21.doc Mediating le Web sémantique Georges Gardarin, Tuyet-Tram Dang Ngoc-PRiSM Université Laboratoire de Versailles 45, avenue des Etats-Unis. 78035 Versailles Cedex FRANCE georges@gardarin.org http://www.gardarin.org dntt@prism.uvsq.fr http://www.prism.uvsq.fr/users/dntt/ CV. Cet article developpe Une prolongation d'architecture de juin integrer le verser médiation Web sémantique. De plus précisement, xlive is a tout XML Médiatrice à PRiSM Développé. Il Përmet d'XQuery sur des exécuteur des sources de Hétérogènes Données. Une après de xlive rapide et présentation du Web sémantique, l'architecture juin à trois levels d'ontologies et de schemes is verser introduite des connecteur for the Adaptateurs Web sémantique. Architecture étaux Ë This integrer des sources de service type Web d'information Conformement à juin de référence globale ontologie. Elle conduit à xlive Avec le étendre le soutien de vues, un outil de conception de et de mappings vues, Et des services Pour Les Adaptateurs Web. 1. Introduction Les systèmes d'intégration de l'information typique ont adopté une architecture wrapper-médiateur [Wiederhold, 1992]. Dans cette architecture, les médiateurs offrent une interface utilisateur uniforme à des vues intégrées de requête de sources d'information hétérogènes. Wrappers offrent des vues locales de sources de données dans un modèle de données globales. Les vues locales peuvent être interrogés de façon limitée en fonction des capacités wrapper. Alors que dans les années 90 de la plupart des études étaient fondées sur l'utilisation du modèle d'objet comme modèle d'intégration de données, l'accent est venu à XML comme modèle global au début du nouveau siècle. architectes Mediator sont utilisés pour distinguer l'approche locale vue (LAV) par rapport au monde que des vues (GAV) approche, dans laquelle les vues intégrées sont conçues en fonction des vues locales des sources. Pour répondre à une ontologie donnée dans un domaine, l'approche LAV semble plus appropriée, mais l'approche GAV donnée rend plus facile à prendre en compte les sources de données existantes des schémas. Ainsi, les approches mixtes sont possibles soutenues par des outils de conception de schéma [Haas, 1999]. comprennent des projets de recherche bien connus et des prototypes en matière de médiation ail [Haas, 1999], Tsimmis [Garcia-Molina, 1997], Tukwila [Ives, 1999].], Artemis [Castano, 2000], ENOSYS EXIP [Papakonstantinou, 2003], et xlive [Dang-Ngoc, 2003], un descendant de médiateur e-XML [Gardarin, 2002]. Tim Berners-Lee, l'inventeur du WWW, a imaginé le Web sémantique. L'objectif est de donner un sens bien défini à l'information Web, ce qui permet de mieux les applications Web intelligentes nécessitant la recherche ou le raisonnement, faire les échanger sens plutôt que d'informations. Il y a beaucoup de activités (parfois source de confusion) au sein du W3C le long de cette ligne. Les résultats les plus importants sont RDF, RDF Schema (RDFS) et OWL (Web Ontology Language). RDF fournit un modèle de données sémantique basée sur trois <ressources, la propriété, la valeur> Mediating le Web sémantique RNTI - E - 2 pour décrire les ressources Web [RDF, 2002]. RDFS apporte un modèle de saisie de données pour RDF. En utilisant le schéma RDF, les utilisateurs peuvent spécifier les types de ressources, créer des propriétés et des classes avec des sous-classes, ainsi que la définition des gammes et des domaines pour les propriétés. OWL est un langage pour développer ontologies [OWL, 2003]. Une ontologie définit un vocabulaire commun à partager l'information dans un domaine. Il comprend des définitions formelles de concepts dans le domaine et les relations entre eux. En pratique, OWL comprend des constructions pour définir les classes, hiérarchies sous-classe-superclasse, les propriétés et les valeurs ainsi que des constructions de définir des contraintes et des règles entre les classes et les propriétés. Le problème abordé dans cet article est de savoir comment interroger le Web sémantique en utilisant une architecture de médiation. Dans le projet xlive, nous avons mis au point un médiateur XML complet aux données structurées sources de requêtes, y compris les bases de données relationnelles cartographiés dans des fichiers XML et XML stockés dans Xyleme, un système d'entreposage pour XML [Abiteboul, 1999]. Nous sommes qui étend le médiateur xlive à la requête et l'intégration des services Web. Nous partons du principe que chaque site fournit une API de service Web pour récupérer des pages Web. Tout d'abord, à l'aide d'un extracteur nous avons construit une vue locale du site comme une collection de documents XML qui peuvent être interrogés sur certains éléments. Ensuite, en fonction de plusieurs couches (en général, 2) d'ontologies, nous définissons des vues intégrées de sources dans un domaine donné (par exemple, le tourisme). En conséquence à la classification des [Wache, 2001], nous mettons en œuvre une approche hybride où la sémantique de chaque source est décrite par sa propre ontologie et la sémantique des données intégrées est définie par une ontologie de domaine. Chaque ontologie est associée à une vue XML défini dans XQuery des données extraites. Ce document est organisé comme suit. Dans la section 2, nous rappelons quelques arrière-plans sur XQuery et de médiation XML, puis, nous décrivons le médiateur xlive. Nous nous concentrons sur l'architecture et la gestion des métadonnées xlive. Dans la section 3, nous rappelons quelques arrière-plans sur le Web sémantique, l'introduction OWL et des services Web, qui sont des bases solides pour le projet xlive. Dans la section 4, nous présentons les extensions xlive pour interroger le Web sémantique. Ce sont principalement l'introduction de vues ontologique, le développement des enveloppes de services Web, et une boîte à outils de conception pour la cartographie de vues à ontologies et vice versa. 2. La technologie XML Médiation basé sur XML et XQuery est en cours de développement. Certains produits sont déjà disponibles. Nous passons en revue cette nouvelle technologie et de décrire notre médiateur xlive (voir www.xquark.org pour une version open source industrielle). 2.1 Principes de base et arrière-plans Avec l'avènement de XQuery comme norme pour l'interrogation des collections XML [XQuery, 2003], plusieurs systèmes de médiateurs ont été développés en utilisant XQuery et le schéma XML comme langage pivot et le modèle. Des exemples de médiateurs XML complets sont la plate-forme d'intégration XML ENOSYS (de EXIP), le médiateur XML AG Software EntireX, le médiateur des données liquide de BEA dérivé de EXIP [Papakonstantinou, 2003], le médiateur XML e-XMLMedia, un prédécesseur de notre courant xlive projet [Gardarin, 2002]. XML Médiateurs se concentrent sur le soutien du langage de requête XQuery sur les vues XML de sources de données hétérogènes. Les données sont intégrées de manière dynamique à partir de plusieurs sources d'information. Les requêtes sont utilisées comme Georges vue Gardarin et al. RNTI - E - 2 définitions. Pendant d'exécution, les problèmes d'application des requêtes XML contre les vues. Les requêtes et les vues sont traduites dans une algèbre XML et sont combinés dans les plans de requête unique d'algèbre. Les sous-requêtes sont envoyées aux emballages locaux qui les traitent des résultats XML localement et de retour. Enfin, le processeur global de requête évalue le résultat, en utilisant des algorithmes d'intégration et de reconstruction appropriés. XQuery est un langage puissant, qui englobe SQL et plus beaucoup. Notamment, il est capable d'interroger les types de données riches et extensibles; il est un langage fonctionnel, de sorte que toute expression valide appliquée à une expression valide est une requête valide; il sera bientôt incorporer XQuery texte pour les requêtes en texte intégral. XQuery texte fournit des fonctionnalités comme la recherche simple mot, recherche d'expression, le soutien aux mots d'arrêt, la recherche sur le préfixe, Postfix, infix, recherche de proximité, la normalisation des mots, des signes diacritiques, le classement et la pertinence. Tout ce que les caractéristiques feront XQuery un langage idéal pour effectuer des requêtes de la sémantique Web. médiateurs XQuery sont des éléments intéressants pour effectuer l'intégration sémantique de l'information Web. médiateurs Web visent à transférer de l'information Web à partir de différents sites Web dans le même domaine (par exemple, le tourisme) à un modèle d'information de common spécifiques au domaine de haut niveau, ce qui rend le document Machine sémantique compréhensible. médiateurs Web devraient intégrer trois types de technologies de pointe: description basée sur l'ontologie des sources Web, l'extraction de données Web et la cartographie, et distribué le traitement des requêtes en fonction des médiateurs. 2.2 Vue d'ensemble du médiateur xlive Dans le projet xlive, nous utilisons une architecture de médiation suppo rt intégration de l'information Web et la recherche sémantique comme le montre la figure 1. Il suit l'architecture de médiateur wrapper- classique tel que défini dans [Wiederhold, 1992]. La communication entre wrappers et médiateur fait suite à une interface commune, qui est définie par une interface de service Web Java ou applicatif nommé XML / DBC. Avec XML / DBC, les demandes sont définies dans XQuery et les résultats sont retournés au format XML. Interface Web Java Application Java Application RDB1 Oracle RDB2 MySQL XML Xyleme DB3 Wrapper Wrapper Wrapper Mediator Mediator Médiateur figure. 1 - xlive architecture médiatrices Web sémantique RNTI - E - 2 Architecture Notre est composée de médiateurs qui traitent des sources XML distribués et les emballages qui faire face à l'hétérogénéité des sources (SGBD, pages Web, etc.). Le médiateur xlive est un middleware d'intégration de données de gestion des vues XML de sources de données hétérogènes. L'utilisation d'un médiateur xlive peut intégrer des sources de données hétérogènes sans répliquer leurs données alors que les sources restent autonomes. médiateur xlive est entièrement basé sur la technologie standard du W3C: XML, XQuery, XML Schema, SAX, DOM et SOAP. Tous les échanges d'informations se fondent sur le format XML. XML-Schema est utilisé pour la représentation des métadonnées. Wrappers fournissent des schémas à l'information à l'exportation sur les structures de données locales. XQuery est utilisé pour effectuer des requêtes à la fois le médiateur et les emballages. Connectivité de médiateur et wrappers repose sur l'interface de programmation XML / DBC, une extension de JDBC pour intégrer XQuery. Plus d'informations sur le médiateur xlive se trouve dans [Dang-Ngoc, 2003]. Pour intégrer une nouvelle source dans l'architecture de médiation, une enveloppe doit être construit. Il doit implémenter l'interface de programmation XML / DBC, traiter certaines demandes de XQuery, et les résultats de retour au format XML. SGBD sont des sources orientées de données et les métadonnées sont fournis pour décrire les sources et les mappages. wrappers SGBD se traduisent par des sources de données en XML et traiter un ensemble peut-être réduite de XQuery sur les données source. Dans le cas de la source Web, l'emballage apporte plus de renseignements. Elle vise à intégrer sémantiquement information sur le Web dans un modèle commun accessible aux programmes. 2.3 Les métadonnées du médiateur xlive Le médiateur xlive maintient les métadonnées d'une manière simple. Chaque fois qu'une source se connecte à un médiateur, il fournit l'arbre de structure des documents XML, il est en mesure de récupérer. Plus précisément, un médiateur maintient l'ensemble des chemins XML qui peuvent être interrogés et extraites de chaque emballage, il est connecté. Il est appelé le jeu de chemin et peut être considéré comme un schéma faible ou une DTD résumant la source. Traitement des requêtes utilise le jeu de chemin pour vérifier la validité et la requête pour déterminer les sources de données pertinentes pour une requête. Le jeu de chemin est également utilisé pour développer des chemins incomplets. Squelettes de schémas peuvent également être affichés à l'utilisateur final afin de l'aider à formuler des requêtes. Dans la suite, nous allons discuter de la façon d'étendre ces métadonnées à ontologies rencontrer. 3. Web sémantique Dans cette section, nous présentons quelques rappels sur le Web sémantique. Nous discutons également le lien avec les services Web. 3.1 Principes de base et arrière-plans Commençons par une définition du W3C: «Le Web sémantique est la représentation des données sur le World Wide Web. Il est un effort de collaboration mené par le W3C avec la participation d'un grand nombre de chercheurs et de partenaires industriels. »Il est basé sur le Cadre Resource Description [RDF, 2003], qui intègre une variété d'applications utilisant XML pour la syntaxe et URIs pour nommer. descriptions RDF expriment des métadonnées sémantiques sur le document Georges Gardarin et al. RNTI - E - 2 contenu (pages Web, etc.) en utilisant des triplés <ressources, propriété, valeur>. Ressources situées par URIs sont décrites par des propriétés et des valeurs, une valeur étant une ressource ou un littéral. RDF est utilisé pour les documents annoter avec des descriptions sémantiques. Les descriptions sont exprimées dans les termes définis ontologies. Une ontologie est une définition des concepts et relations qui existent pour un domaine. Il est généralement exprimé en termes de hiérarchies de classes, les propriétés d'une valeur littérale ou classe, avec des règles formelles liant les classes définies et les propriétés (par exemple, l'équivalence de classe, propriété transitivité, etc.) [OWL, 2003]. Les ontologies sont des définitions de métadonnées sur les données (par exemple, les pages Web) qui peuvent être utilisés pour le raisonnement. schéma RDF est une première langue de niveau à la définition d'écriture des métadonnées dans le vocabulaire RDF Description Language proposé par le W3C. Lorsque les métadonnées RDF sont basées sur un schéma RDF, propriétés RDF doivent faire référence au schéma RDF de base à travers un espace de noms. schéma RDF ne fournit pas les capacités de raisonnement. En outre, une ontologie comprend le raisonnement des capacités sur les choses qui peuvent aider à comprendre ce que les ordinateurs sont affaire quand traitement de l'information dans le moteur de recherche, systèmes de requête, les systèmes de médiation, agents intelligents, etc. 3.2 OWL pour définir ontologies OWL (Ontology langue web) une langue pour définir des ontologies sur le Web basé sur schéma RDF. Il étend les constructions de base du schéma RDF pour fournir plus d'interopérabilité (par exemple) les équivalences, plus de facilités de raisonnement (par exemple, la logique de description), et un plus grand soutien de l'évolution (par exemple, l'intégration et la version). Il est inspiré de DAML (DARPA) et d'huile (CEE). OWL incorpore les caractéristiques de base de RDFS RDF (schéma): définitions de classe, de la propriété, le domaine, la plage, la sous-classe et de sous-propriété. Une propriété concerne une classe à une classe ou un littéral. Les types de données de schéma XML sont pris en charge pour la saisie des propriétés littérales. Individuel peut également être défini en tant que membre d'une classe. En outre, OWL fournit des constructions de définir des relations entre les classes telles que l'équivalence et expressions ensemble (par exemple, A est B union C moins D). Les propriétés peuvent aussi être qualifié comme fonctionnel, symétrique, ou transitif; deux propriétés peuvent être déclarées comme inverse. Cardinalités sont étendues et les restrictions peuvent être imposées aux valeurs de propriété. Enfin, des informations de versioning et annotations (par exemple, des commentaires) peuvent être définis pour ontologies. 3.3 Lieu de services Web services Web sont des applications dont la logique et les fonctions sont publiées sur le Web dans les registres UDDI et accessibles via des messages XML standard codés dans SOAP. Une étape supplémentaire dans le développement de composants logiciels (étapes précédentes ont été le développement de composants d'objet comme EJB ou Active X), Services Web représente des fonctions abstraites qui peuvent être réutilisés sans se soucier de la façon dont les services sont mis en œuvre. L'interface d'un service Web est défini strictement en termes de messages service accepte et produit. Les applications utilisant les services Web peuvent être mis en œuvre sur une plate-forme dans un langage de programmation, tant qu'ils peuvent créer et consommer des messages définis pour l'interface de service. Un service Web peut également être une composition d'autres services pour fournir des fonctionnalités de niveau supérieur. Langue pour composer et orchestrer les services Web sont sur leur chemin vers la normalisation (par exemple, BPML, BPEL, BPSS). Web sémantique médiation RNTI - E - 2 services Web et le Web sémantique sont complémentaires. Web Services visent à normaliser et automatiser le protocole aux demandes de transmission et des réponses aux ressources Web actifs, alors que les objectifs du Web sémantique pour normaliser et automatiser les descriptions sémantiques des ressources sur le Web pour l'intégration de l'information et de raisonnement. Les adresses Web sémantique la description sémantique du contenu tandis que les services Web traite de l'échange syntaxique du contenu. L'intégration des services Web et le Web sémantique dans une architecture de médiation est une autre question discutée ci-dessous. 4. les architectures Plusieurs Mediating Web sémantique des médiateurs ont été proposés pour interroger le Web sémantique. Nous vous proposons une architecture hybride avec ontologies définition vocabulaires des schémas décrivant l'information. Cette architecture est actuellement mis en œuvre à la fois comme un outil de conception de schémas et de soutien de vue de l'exécution dans le XLi ve projet. 4.1 Cartographie hybride Présentation de l'architecture Web La plupart des médiateurs mettent en œuvre l'approche LAV pour concevoir des schémas de sources locales que des vues de certains schéma global préexistant souvent appelé ontologie. Il y a souvent des confusions entre un schéma et une ontologie. À notre avis, une ontologie définit les concepts et les relations entre les concepts. Ces concepts sont utilisés à l'information annoter et de définir le plan conceptuel de l'information. Un schéma (XML) définit les structures et les types de collections de documents. L'un des problèmes de médiation sur le Web est à la carte (généralement pauvres) les schémas de documents (généralement riches) l'ontologie et vice versa. La plupart des médiateurs travaillent avec des schémas de collections, et non avec des descriptions ontologiques de l'information. Ces derniers schémas conceptuels Superposer et les contraintes sur l'information à l'aide des annotations basés sur l'ontologie. Pour intégrer le Web sémantique dans notre architecture de médiateur, nous proposons un système de cartographie hybride, où: (i) Les sources locales sont mises en correspondance à un ou plusieurs ontologies locales en utilisant une approche GAV; annotations locales élaborées conformément aux ontologies source peuvent être utilisés pour soutenir la mise en correspondance. (Ii) ontologies de domaine sont mis en correspondance avec les ontologies locales en utilisant une approche de LAV; annotations de sources locales peuvent également être ajoutés pour aider le processus de cartographie. On distingue clairement vue décrivant le schéma de l'information et ontologies donnant ensemble de concepts à l'information annoter et en aidant à la construction de vue. Nous définissons toutes les applications dans XQuery, qui est un langage puissant pour des informations cartographiques et annotations d'une manière intégrée. Cela est possible car tout est XML et peut être consultée sur XQuery à base, éventuellement avec des fonctions spécifiques autorisées dans XQuery. La figure 2 donne une vue d'ensemble du schéma proposé et l'ontologie architecture à la couche de médiateur. Une ontologie globale est définie dans OWL. Une vision globale ontologique correspond à un schéma intégré; il définit une collection logique en termes (à savoir, avec un point haut) défini dans l'ontologie globale. annotations sémantiques peuvent être ajoutés à la vue locale en termes de l'ontologie globale. Vues locales proviennent de vues globales à l'aide XQuery suivant Georges Gardarin et al. RNTI - E - schéma de définition 2 LAV. Cartographie des instances locales de vue ontologique à vue ontologique global est par un XQuery calculé au moment de la conception comme l'inverse des définitions de vue. Mondial Ontological Voir Domaine global Ontologie local Ontological Voir Local 1 Local 2 Voir ontologiques ontologiques Voir 3 ... XQuery Voir Définition XQuery Voir Définition XQuery Voir définition figure. 2 - Voir et Ontologie à couche de médiateur. La figure 3 donne un aperçu du schéma proposé et l'ontologie architecture à la couche d'emballage. ontologies locales sont également définies dans OWL. Une vue locale ontologique est définie par un ensemble de chemin d'accès et une requête; elle correspond à une collection logique exprimée dans les termes définis dans l'ontologie locale. annotations sémantiques peuvent être ajoutés à un site Web en termes de l'ontologie locale. les instances de vue ontologique locales sont dérivées de vues et annotations logiques locales en utilisant XQuery. instances vues logiques locales proviennent des appels Web Services ou requêtes de sources locales (par exemple, les requêtes SQL). Local Vue Logique locale Ontological Voir sémantique Annotation locale Ontologie locale Ontologie RDF Site Web local XQuery Vue Logique locale Ontological Voir sémantique Annotation locale Ontologie locale Ontologie RDF Site Web XQuery figure. 3 - Affichage et Ontologie à couche wrapper. Web sémantique médiation RNTI - E - 2 4.2 Emballage Web Services en tant Vues logiques locales des sites Web deviennent de plus en plus ouvert aux services Web. De bons exemples sont Amazon.com et Google. Ils fournissent tous les deux une API de service Web pour interroger leurs catalogues. Alors que le catalogue Amazon décrit les produits, catalogue Google décrit les sites de World Wide Web. Avec Google, la principale opération de service Web est la requête de recherche. Il soumet une requête st anneau et un ensemble de paramètres au service Web API Google et reçoit en retour un ensemble de résultats de recherche (voir la figure 4). Résultats de la recherche sont des documents XML dérivés de l'index de Google de plus de 2 milliards de pages Web. Ils sont composés de divers éléments, le plus important étant un tableau de <ResultElement> s ', chaque élément donnant un résumé, une URL, un extrait, un titre, une taille, le nom d'hôte et d'autres informations d'autres (voir la figure 5) . FIGUE. 4 - jeu de chemin de Google. FIGUE. 5 - chemin de réponse Google ensemble. Georges Gardarin et al. RNTI - E - 2 Pour envelopper les services Web à un médiateur XQuery, il est nécessaire de définir une représentation du service Web comme une ou plusieurs collections de documents XML. L'ensemble du chemin de la collection (s) doit couvrir à la fois la chaîne de requête et les résultats de la recherche. Pour simplifier, nous proposons de définir une collection par opération de service modélisation des messages d'invocation et de réponse. Une collection provenant d'une opération contient un ensemble de trajet couvrant les DTD de tous les documents qui peuvent être interrogés et récupérées à l'aide de l'opération. Par exemple la recherche Google pourrait être considéré comme une collection d'ensembles de chemin représenté dans la figure 4 et 5. Les requêtes pourraient alors être présentées, ce qui la collection en utilisant XQuery avec des extensions de texte, comme il est prévu dans XQuery texte. Nous appelons ce point de vue d'un service Web la vue logique locale, définie comme suit: Définition: Vue logique locale Représentation d'une fonction de service Web en tant que collection partiellement interrogeable dont l'ensemble chemin est l'ensemble des chemins définis dans les messages d'appel de fonction et retour. Le schéma de collecte est dérivé de la description du service WSDL. Toutes les requêtes ne sont pas réalisables sur la vue logique locale. Capacités devraient être associées à l'enveloppe pour définir quelles requêtes sont possibles. Par exemple, avec les requêtes de recherche Google doit instancier l'élément q avec quelques expressions de recherche de texte soit valide. D'autres éléments peuvent être donnés ou générés par défaut. En résumé, la recherche d'information des services Web peuvent être modélisés sous forme de collections interrogeables correspondant aux messages de fonctionnement. Cette cartographie simple rend possible la traduction de la recherche XQuery dans l'invocation de service Web. La cartographie définit la vue virtuelle d'un service Web comme une collection de documents virtuels XML. La traduction de XQueries impliquant des collections virtuelles dans l'invocation de services Web doit être développé sur une base par cas, en fonction de la sémantique de service Web. 4.2 Service Vues Web Mapping ontologies Comme décrit ci-dessus, tout service Web peut être enveloppé comme une collection XML qui prend en charge certaines requêtes XML plus ou moins généraux. Cela fournit un moyen simple de carte site Web à des vues structurelles définies comme des collections XML avec jeu de chemin associé. En général, pour chaque site, des vues logiques locales peuvent être construites qui ressemble à la structure de la source d'information peut-être encapsulées par des services Web. Il fournit un schéma XML local pour représenter la source définie en termes locaux. Si elle permet d'extraire des données de la source à l'aide XQuery, il ne nous aide pas beaucoup à intégrer les informations de diverses sources avec une sémantique plus ou moins différentes. Ainsi, d'autres applications sont tenus de fournir une vue intégrée de l'information. Plusieurs groupes de travail définissent actuellement ontologies dans divers domaines, comme le vin, Voyage, tourisme, santé, sciences et de la fabrication. Définition: Définition globale Ontologie des classes ayant des propriétés développées par des spécialistes d'un domaine donné, en spécifiant un vocabulaire des termes de leurs relations. Web sémantique médiation RNTI - E - 2 Malheureusement, les sites Web existants ne suit pas ces ontologies mondiales et leurs modèles spécifiques. Toutefois, pour être en mesure d'intégrer des informations provenant de diverses sources de sujets connexes, il est nécessaire d'utiliser une ontologie commune à un moment donné. Pour les ontologies sources comparables les unes aux autres, vocabulaires globales partagées doivent être construits. Un vocabulaire partagé contiennent s termes de base (les primitives) d'un domaine. Chaque site doit mettre en œuvre son point de vue local de l'ontologie commune, que nous appelons une ontologie locale. Définition: Ontologie locale Projection sur un site local d'une ontologie globale, en définissant les classes et les propriétés globales compris à un site local. ontologies locales sont définies par exemple dans OWL et utilisés pour générer des descriptions RDF de la source associée. A partir d'une ontologie locale, un ensemble de chemin XML peut être dérivé décrivant la source comme une collection de documents XML exprimés en termes de l'ontologie. Cette collection est appelée le point de vue ontologique de la source. Définition: Local Ontological Voir la représentation d'une source locale comme un ensemble de collections partiellement interrogeables des documents dont l'ensemble chemin est dérivé de l'ontologie locale. En général, une vue ontologique est construit en utilisant un vocabulaire d'ontologie. Son ensemble de chemin d'accès est composé d'éléments définis dans l'ontologie. Son schéma doit être dérivé de l'ontologie. Cela est également vrai pour la vue intégrée, définie comme suit: Définition: Global Ontological Voir la représentation des sources intégrées comme un ensemble de collections partiellement interrogeables des documents dont l'ensemble chemin est dérivé de l'ontologie globale. Comme ontologies locales sont dérivées de celui global par projection, il est naturel d'utiliser une approche locale de vue de tirer le point de vue ontologique locale de celle mondiale. Ainsi, nous proposons de définir chaque point de vue ontologique locale comme une ou plusieurs requêtes XQuery sur la vue globale ontologique. En général, nous utilisons XQuery pour cartographier les collections XML logiques à une ontologie liée au domaine, localement en utilisant une approche GAV, et globalement en utilisant une approche LAV. Ceci rend l'application plus flexible aussi clairement divisé en deux étapes, logiques pour ontologique (correspondant essentiellement à l'intégration sémantique) et global au local (correspondant essentiellement à l'intégration de localisation). Les vues et les ontologies sont résumés dans la figure 6. 4.3 Architecture traitement des requêtes et l'outil de conception de schéma Le xlive est actuellement en cours d'extension à ontologies rencontrer. Xlive reçoit des demandes XQuery exprimées sur un schéma simple, intégré composé comme l'union des vues locales. Xlive décompose les requêtes en fonction des ensembles de chemin source. sous-requêtes locales sur les vues locales sont ensuite expédiées aux systèmes locaux en ce qui concerne les capacités de source. Pour intégrer pleinement l'architecture proposée et l'ontologie vue, xlive doit d'abord être étendu avec le soutien de la vue sur le médiateur. En second lieu, chaque enveloppe locale devrait être en mesure de traduire XQuery exprimé sur les vues locales ontologiques aux requêtes sur les vues logiques locales, qui sont finalement traduits aux appels de service Web. La figure 7 montre le traitement de la xlive Georges Gardarin étendu et al. RNTI - Architecture 2 - E. Plusieurs fonctions d'optimisation sont actuellement proposées pour soutenir efficacement les applications à plusieurs niveaux. Mondial Ontological V IEW G lobal O O ntology local ntological V IEW logique V iewW SD L de la section locale O local en tant que V IEW X Q uery M apping G lobal en V IEW X Q uery M Apping W eb service Encapsulated D ata source de la Fig. 6 - ontologies et Vues locales Ontological Local View Vue Logique XQuery Voir Définition globale Ontological Voir XQuery Voir Définition globale enrichissement de requêtes XQuery Web Service appels de requête de décomposition de requête de traduction MÉDIATEUR ENVELOPPE Fig. 7 - traitement des requêtes en Étendu xlive. Web sémantique médiation RNTI - E - 2 Un outil de conception devrait contribuer à la conception des points de vue et en spécifiant les correspondances. Le rôle de l'outil de conception est de générer: schémas de vue logique locale. schémas de vue ontologique locale. les schémas globaux de vue ontologique. Mappings de vues globales ontologiques aux vues locales ontologique, puis vue logique locale. Les entrées de l'outil de conception sont l'ontologie globale, la description WSDL des services Web, et la correspondance sémantique donnée par un administrateur. La correspondance peut être définis par de simples correspondances d'identité (par glisser-déposer), mais les correspondances plus complexes sont possibles en utilisant les bibliothèques d'opérations développées dans XQuery pour l'application ou générique. annotations sémantiques des sources locales peuvent être ajoutées à l'aide dans le développement de la cartographie. La figure 8 donne une vue d'ensemble de l'outil de conception prévu. Notez que les composants de créateurs locaux et internationaux devraient collaborer afin de concevoir la vue locale ontologique comme un compromis entre la vision globale ontologique et la vue logique. Nous sommes en train de concevoir plus précisément notre outil de conception. Designer locale Les collectivités locales Ontology Web Services Définitions local Ontological Vue locale Vue Logique OWL Mondial Designer mondial Ontologie mondial Ontological Voir OWL Projection processus de collaboration XQuery Définitions XQuery Définitions ENTRÉE DE SORTIE WSDL OUTIL DE CONCEPTION Fig. 8 - Outil de conception Vue d'ensemble Georges Gardarin et al. RNTI - E - 2 5. Conclusion Dans cet article, nous avons présenté xlive, un projet de médiation développé à PRiSM dans le cadre du projet websi. Le médiateur est actuellement opérationnel pour soutenir les sources relationnelles ou XML couplées par les enveloppes spécifiques. Nous considérons étendre xlive pour interroger le Web sémantique en utilisant XQuery. Nous résumons la contribution du Web sémantique. Ontologies et description du site Web par le biais d'annotations basées sur l'ontologie doivent être pris en considération. Les services Web doivent également être intégrés. L'extension xlive avec l'ontologie et le service Web n'est pas une tâche facile. Il nous conduit à une architecture hybride, mélange haut en bas et schéma en bas et la conception de la cartographie. Plus précisément, nous avons introduit trois niveaux de vue des services Web en tant que collections de documents, à savoir la vue logique locale, le point de vue ontologique locale, et les vues globales ontologique. Nous avons défini clairement ces trois couches et de discuter outil de conception de traitement et de la cartographie requête. Des recherches supplémentaires sont nécessaires pour pouvoir utiliser correctement l'ontologie dans l'intégration de données à base de médiateur. Nous croyons que XQuery est une excellente langue à la fois pour exprimer les correspondances et la formulation de requêtes. Optimisation des techniques doivent être développées pour rendre le processus de médiation Web sémantique efficace tout ce que les correspondances; Heureusement, certaines applications peuvent être résolus au moment de la compilation. Plus de techniques d'optimisation sont discutées dans [Dang-Ngoc, 2003]. Enfin, nous tenons à souligner que plus de deux niveaux de cartographie pourraient être pris en charge pour répondre à une ontologie globale. Ceci est possible grâce à l'architecture récursive de xlive qui permet de voir un médiateur comme source locale. médiateur intermédiaire peut progresser vers le vocabulaire d'une ontologie donnée, laissant médiateur de la couche supérieure mappages supplémentaires pour atteindre l'ontologie souhaitée. Références [Abiteboul, 1999] S. Abiteboul, S. Cluet, Ferran G., Rousset M.C. Le projet Xyleme. Computer Networks 39 (3): 225-238 (2002) S. Abiteboul sur la stratégie et XML. PODS 1999: 1-9 [Adali, 1996] Adali S., K. Selçuk, Candan ,, Papakonstantinou Y., Subrahmanian V. S. Interrogation Mise en cache et optimisation Mediator Distributed Systems. Conférence SIGMOD 1996: 137-148. [Castano, 2000] S. Castano, V. De Antonellis, S. De Capitani di Vimercati, M. Melchiori: un cadre fondé sur XML pour l'intégration de l'information sur le Web, Proc. du 2ème Atelier international sur l'information et l'application basée sur le Web et les services (IIWAS 2000), Yogyakarta, Indonésie, 2000 [Chaudhuri, 1995] Chaudhuri S. Krishnamurthy R., Potamianos S., K. Shim Optimisation des requêtes avec des vues matérialisées. Proc. de l'IEEE Conf. sur l'ingénierie des données, 1995. [Chawathe, 1994] Chawathe S., Garcia-Molina H., J. Hammer, Irlande K., Papakonstantinou Y., J. Ullman et Widom J. Le TSIMMIS projet: intégration de l'information Sources Heterogeneous . Dans Proc. de la Conférence IPSJ, pp.7-18, 1994. [Dang-Ngoc, 2003] Dang Ngoc-T.-T. et Gardarin G. Fédérer sources de données hétérogènes avec XML. Dans Proc. de IASTED Conférence IKS 2003 .. [El-Sayed, 2002] El-S ayed M., Wang L., Ding L., Rundensteiner E.A. Une approche algébrique pour l'entretien supplémentaire des vues matérialisées XQuery. WIDM 2002: 88-91. Web sémantique médiation RNTI - E - 2 [Garcia-Molina, 1997] H. Garcia-Molina, Y. Papakonstantinou, D. Quass, A. Rajamaran, Y. Sagiv, J. Ullman, V. Vassalos et J. Widom, l'approche TSIMMIS à la médiation: Modèles de données et langues, Journal des systèmes d'information intelligents, 8 (1997) 117- 132. [Gardarin, 2002] Gardarin G., A. Mensch, Tomasic A .: introduction à l'e-XML Data Integration Suite. EDBT 2002: 297-306 [Gardarin, 2003] Gardarin G, H Kou, K. Zeitouni, Meng Wang et X. H., SEWISE: Une information Web basée ontologies Search Engine, 8 NLDB, Allemagne, Juin 2003. [ Haas, 1999] Haas, LM, RJ Miller, B. Niswonger, M. Tork Roth, P.M. Schwarz, E.L. Wimmers: Transforming Heterogeneous données avec la base de données Middleware: Au-delà de l'intégration. IEEE Tech. Taureau. Données techniques 22 (1), 1999: 31-36. [Ives, 1999] Z. G. Ives, D. Florescu, M. Friedman, A. Levy, D. S. Weld. Un système d'exécution de la requête Adaptive pour l'intégration de données. SIGMOD Conférence sur la gestion des données, Philadelphie, PA 1-3 Juin, 1999 [Manolescu, 2002] I. Manolescu, Florescu D., Kossmann D .: ""répondre à des requêtes XML sur des sources de données hétérogènes"", Procédant du 27 VLDB, pp. 241-250, Roma, Italie, sept 2001. [OWL, 2003] http://www.w3.org/TR/owl-features/ [Papakonstantinou, 2003] Papakonstantinou Y., et Borkar. al, ""Requêtes et algèbre dans la ENOSYS plateforme d'intégration"", ingénierie des données et des connaissances, Volume 44, numéro 3 (Mars 2003), numéro spécial:.. intégration de données sur le Web, pp 299-322 [RDF, 2002] http: //www.w3.org/RDF/ [Wache, 2001] Wache H., Vogele T., U. Visser, Stuchenschmidt H., Schuster G., Neumann H. et S. Hubner ontologie basée sur l'intégration des approches, IJCAI Atelier: ontologies et partage de l'information, pp.108-11, 2001. [Wiederhold, 1992] Wiederhold G., médiateurs dans l'architecture des futurs systèmes d'information, IEEE Computer, vol.25, N.3, pp.38-49, 1992. [XQuery, 2002] Résumé http://www.w3.org/TR/2002/WD-xquery-20021115/ Cet article présente une extension d'une architecture médiateur-wrapper pour intégrer le Web sémantique. Les extensions sont mises en œuvre xlive, un médiateur XML développé à PRiSM dans le cadre du projet européen websi. Xlive est décomposait XQuery mondiale pour les locaux en fonction des capacités de source. Après une brève description de xlive et quelques rappels sur le Web sémantique, une architecture qui englobe trois niveaux de schémas basés sur l'ontologie est proposée. Il est une architecture hybride intégrant à la fois local comme vue (LAV) et mondial vue (GAC) approche. Il permet le développement d'emballages Web sémantique et l'intégration progressive des sources selon l'ontologie de domaine global. Enfin, un médiateur de XQuery étendu avec l'enrichissement de vue, un outil de conception de la cartographie et les enveloppes de services Web permet soutien des trois architecture en couches."
1239,Revue des Nouvelles Technologies de l'Information,EGC,2004,MUSETTE : a framework for knowledge capture from experience,"Nous présentons dans cet article une nouvelle approche de modélisation de l'expérience d'utilisation d'un système informatique, avec pour objectif de réutiliser cette expérience en contexte pour assister l'utilisateur à effectuer sa tâche. Quatre scénarios illustrent cette approche.","Pierre-Antoine Champin, Yannick Prié, Alain Mille",http://editions-rnti.fr/render_pdf.php?p1&p=1000912,http://editions-rnti.fr/render_pdf.php?p=1000912,en,"Microsoft Word - EGC-Champin-prie- mille.rtf MUSETTE: un cadre pour la connaissance de l'expérience Capturer Pierre Antoine Champin, Yannick Prié, Alain Mille LIRIS - Bât Nautibus - Informatique Université Claude UFR Bernard Lyon 1 / F-69622 Villeurbanne Cedex prenom. nom@liris.cnrs.fr http://liris.cnrs.fr/prenom.nom CV. L'article de nous Présentons this Une nouvelle approche de l'expérience de modélisation d'utilisation d'un Système d'informatique, with répandrai de réutiliser this Objectif expérience en l'assisteur verser Contexte à Utilisateur sa Effectuer Tâche. Quatre scénarios this approach illustrent. 1. Introduction Il est un trivialité de dire que les ordinateurs sont largement utilisés, pour de plus en plus diverses et de nombreuses tâches, qui reposent principalement sur la « gestion de l'information »: l'organisation de l'information, le stockage, la communication, la recherche, le partage ... De plus, les environnements de gestion de l'information se de plus en plus personnalisable, afin de se rapprocher des pratiques des utilisateurs, des usages et, plus généralement, à leurs besoins de traitement de l'information quelle que soit sa forme. Par exemple, on pourrait considérer qu'un environnement composé d'un / le web en tant que fournisseur de ressources (documents, données) et b / un outil pour la visualisation et l'édition des documents HTML, est adapté à une tâche impliquant la collecte d'informations et la publication. Le spectre des tâches assistées par ordinateur devient plus large, et des outils pour l'exécution de ces tâches deviennent plus polyvalent et personnalisable. Depuis, d'autre part, ils ont de plus en plus les utilisateurs (souvent inexpérimentés), il y a un besoin croissant d'aider ces derniers dans leurs tâches tout en utilisant des outils ou des ensembles d'outils. Par conséquent, il est nécessaire de concevoir des agents logiciels comme assistants, qui tirer profit des connaissances décrivant la tâche à accomplir. En effet, il devient nécessaire de prendre dans les tâches de l'utilisateur du compte afin d'être en mesure d'interpréter, dans leur contexte, les traces laissées par l'utilisation de l'environnement informatique. Bien sûr, dans de nombreuses situations d'assistance aux utilisateurs, il peut y avoir une grande variété de questions qui peuvent être formulées, en fonction du contexte d'utilisation. Soulignons le fait que cette notion de « contexte » n'a rien à voir avec ce qui est communément abordée dans le soi-disant « aide contextuelle »: celle-ci envisage exclusivement le contexte de l'environnement informatique (par exemple, l'élément sélectionné, le menu en cours) tandis que nous nous concentrons sur le contexte de l'utilisateur, en particulier la tâche qu'il ou elle est prêt à effectuer. Plus précisément, nous considérons deux types de tâches. Tout d'abord, nous considérons les tâches qui sont bien identifiés, pour lesquels une assistance se fonderait sur les connaissances décrites dans ontologies soigneusement conçu. Mais il est également important d'envisager un deuxième type de tâches, qui sont difficiles à prévoir, et doivent être reconnus de leurs manifestations et définies à la volée, compte tenu de l'expérience réelle d'utilisation du système. Nous voudrions répondre à ces tâches, notre principale question est: comment est-il possible de modéliser et de l'expérience de capture dans l'utilisation d'un système, afin qu'il puisse être réutilisé en tant que connaissances pour l'assistance aux utilisateurs? Par ailleurs, comment pourrait-il que l'aide elle-même pourrait évoluer avec l'expérience concrète? MUSETTE: Un cadre pour la capture des connaissances de l'expérience de notre groupe, qui vient du domaine de la recherche Raisonnement Case-Based (CBR) comme moyen de suivi et de réutilisation d'expérience, a tenté de proposer des solutions pour relever ce défi à travers les travaux de recherche antérieurs [Prié et Mille, 2000; Champin et Prié, 2003]. Cela nous a conduit à élaborer Musette (Modélisation et tâches USAGES pour le traçage d'expérience), un cadre général pour représenter une expérience concrète en relation avec son contexte d'utilisation. La section suivante présente l'approche globale Musette. Les deux sections suivantes présentent les notions de modèle d'utilisation et des traces, puis d'expliquer la signature de la tâche comme un moyen de diviser une trace dans les épisodes, ou les cas réutilisables potentiels. La cinquième section traite des scénarios démontrant différents types d'assistants en action, Musette tandis que la sixième section est consacrée à l'examen des travaux connexes, principalement dans le domaine de la recherche communautaire et la modélisation des tâches. 2. Approche globale La figure 1 présente le cadre général de notre approche, ce qui (dans le sens horaire) à partir du niveau d'observation (coin supérieur droit) au niveau de réutilisation de l'expérience (-bas à gauche), avec deux niveaux de modélisation de l'expérience. Un utilisateur interagit avec un système, ce qui conduit à des changements dans ce système informatique (événements, fichiers ...). Un agent d'observation, en observant les changements selon un modèle d'observation, génère une trace primitive, qui est conforme à un modèle d'usage général. Ensuite, un analyseur de trace générique extrait des épisodes significatifs de la trace primitive, d'après les signatures de tâches expliquées. Ces épisodes peuvent être (re) utilisés par les agents auxiliaires, ce qui peut aider l'utilisateur soit comme agents clairement distincts du système (aide directe) ou par une modification du système (système d'aide à médiation). Dans ce dernier cas, l'interaction d'assistance peut à son tour être observé par l'agent d'observateur, ce qui permet des épisodes d'assistance à également réutilisés. Soulignons le fait que Musette est seulement un cadre général. Il peut être mis en œuvre en utilisant différentes langues ou formalismes de représentation, à condition que ceux permettent la représentation de toutes les composantes de l'approche Musette. 3. Modèle d'utilisation et des traces Puisque l'approche nécessite une représentation MUSETTE, comme une trace primitive, des interactions entre l'utilisateur et le système, la première étape dans l'application de cette approche est de décider ce que la trace sera faite, et comment seront effectivement construits. Ces questions doivent être posées respectivement par le modèle d'utilisation et le modèle d'observation, afin de construire l'agent d'observateur en charge de la production de la trace (cf. fig. 1). En gros, la trace sera composé d'objets d'intérêt (OI). Ceux-ci peuvent appartenir à l'une des trois catégories: les entités, les événements et les relations. Les entités peuvent être caractérisées comme des objets utilisateur agent Observer trace Primitive épisodes Épisodes interaction système d'aide directe assistance à médiation extraction épisode de génération de trace d'observation .... ............ Episode réutilisation système d'observation modèle signature Tâche 1 signature Tâche 2 Modèle d'utilisation des agents adjoints analyseur de trace générique FIG. 1 - Présentation de notre approche et le vocabulaire Champin et al. être présent à l'utilisateur dans leur interaction avec le système, alors que les événements peuvent être caractérisés comme des objets qui se produisent lors de l'interaction. Objets du sens à la fois l'application de l'ordinateur et de l'utilisateur. Les relations sont binaires, et peuvent impliquer soit des entités ou des événements. Le modèle d'utilisation d'un système particulier décrit ce type d'entités, les événements et les relations seront réellement observables pour produire la trace primitive. Des contraintes supplémentaires, y compris celles sur la structure interne de l'OIS, peuvent également faire partie du modèle d'utilisation. Soulignons le fait que les objets d'intérêt utilisés pour décrire un système, comme « intérêt » le mot implique, dépend de l'attention particulière, mais en général, du modèle d'utilisation. Décider comment l'interaction utilisateur-système sera observée est limitée à être biaisé par les objectifs du concepteur du modèle d'utilisation. Décrire les composantes de la trace à être produite ne suffit pas de construire un observateur, cependant. Le modèle d'observation doit encore être décrit comme un ensemble de moyens d'accès de données pertinentes dans le système, ainsi que des règles contraignantes du processus de production du -eg de trace, le sous-ensemble pertinent de toutes les entités observables doit être écrit à la trace à un moment donné? Contrairement au modèle d'utilisation, le modèle d'observation est non spécifiée par l'approche MUSETTE pour le moment, et un observateur ad hoc doit être construit pour chaque système et avec un modèle d'usage particulier à l'esprit, et le modèle d'observation doit être Hard- codé manuellement dans un tel observateur. Une fois que l'agent d'observateur a été spécifié par le modèle d'utilisation et le modèle (éventuellement logic codé) d'observation, on peut produire des traces de l'observation des interactions entre l'utilisateur et le système. La structure de la trace ne se limite pas à un flux continu d'entités et d'événements, éventuellement en relation avec l'autre. En effet, les entités sont utilisées pour représenter l'état du système à un moment donné (ou au cours d'une période considérée comme un instant dans le cadre du modèle d'utilisation). D'autre part, les événements se produisent dans la période transitoire entre deux états. D'où le groupement, dans la trace, des IO en fonction de leur catégorie, transforme la trace en une séquence alternée d'états et de transitions. Il convient de noter que les états et les transitions dans l'approche ont simplement Musette un rôle temporel. Ils ne visent pas à donner un sens causal prédéfini. Bien sûr, un modèle d'utilisation donnée peut ajouter telle sémantique de cause à effet à certains types d'entités, les événements et les relations qu'il définit. 4. Extasis et épisodes en supposant que le modèle d'utilisation permet une description appropriée des interactions de l'utilisateur avec le système, l'expérience de l'utilisateur est potentiellement récupérable de la trace primitive. Plus précisément, nous appelons un épisode d'une partie de la trace correspondant à une expérience spécifique dans l'exécution d'une tâche spécifique, et qui peut être réutilisé dans une situation similaire. Des morceaux de la trace sont reconnus comme des épisodes liés à une tâche particulière grâce à Explained TÂCHE SIGNATURES (Extasis). Nous avons besoin d'un moyen de localiser des épisodes dans la trace primitive. De manière plus générale, nous considérons que les caractéristiques communes peuvent être exprimées par: 1 / un motif du graphe constitué par les objets d'intérêt (par exemple des événements et des entités) et leurs relations; 2 / des contraintes sur les positions relatives des infections opportunistes dans le tracé (par exemple des co-occurrences dans les observations - Etats ou des transitions -, la distance entre les observations de la trace); 3 / contraintes dépendant de la langue sur la structure interne des infections opportunistes (par exemple, Les valeurs d'attributs). Une fois ces caractéristiques communes ont été identifiées pour une tâche particulière, ils peuvent être considérés comme une signature de cette tâche. En effet, leur instanciation dans la trace peut être interprétée comme une preuve de l'utilisateur qui effectue cette tâche dans la période correspondante. Épisodes ne sont pas limités à des parties de la trace instanciation d'une signature de la tâche, cependant: une fois identifié la tâche effectuée par l'utilisateur, on peut améliorer leur interprétation de la trace. Les rôles que le MUSETTE: cadre A pour la capture des connaissances de l'expérience dans le jeu IOs cette trace peut être plus facile à comprendre; relations supplémentaires, non capturés par l'agent d'observateur, et ne sont pas présents dans le modèle d'utilisation, peuvent être déduites; etc. Par conséquent, l'épisode peut être annotés ou expliqué, par un certain nombre d'informations provenant du fait qu'il a été reconnu comme un événement d'une tâche particulière. Les explications peuvent prendre la forme d'annotation de texte libre (qu'un humain pourrait interpréter), ou d'annotation de connaissances formelles (visant à une interprétation de l'agent automatisé). 5. Scénarios Montrons comment les différents types d'assistants peuvent être conçus sur le dessus de l'approche MUSETTE, compte tenu d'un scénario de navigation sur le Web. Un assistant spécifique peut prendre en une tâche donnée de compte en réutilisant l'épisode instanciation de l'Extasi correspondant. Par exemple, nous pourrions construire un assistant qui informe l'utilisateur chaque fois qu'elle navigue une page dans un site intéressant, c'est-, chaque fois qu'il trouverait un épisode correspondant à un Extasi « un site intéressant d'un signet » dont le site est le site actuel. Un tel assistant spécifique peut appliquer des mécanismes de raisonnement à base de cas [Aamodt et Plaza, 1994] de réutiliser des épisodes pertinents dans un contexte donné. Cependant, un avantage de l'approche est que la Musette même base de connaissances (les traces primitives) peut être partagée par de nombreux assistants différents différents épisodes de l'extraction il. De plus, un nouvel assistant peut être ajouté sans avoir besoin de construire un tout nouveau bas de connaissances e: seulement une nouvelle Extasi doit être fourni afin d'extraire de nouveaux épisodes de traces existantes. Le second scénario implique un assistant générique capable de traiter un certain nombre de tâches, via leur Extasi, de façon régulière: explorer la trace actuelle par rapport à une tâche particulière. Par exemple, l'utilisateur peut parcourir tous les sites intéressants qu'elle a déjà visité, ou toutes les pages qu'elle préférait lire en français. Il pourrait également être suggéré de mettre un signet sur une page ou modifier le paramètre de langue, quand il l'a déjà fait avec une page similaire. Encore une fois, l'assistant pourrait même effectuer ces actions automatiquement. L'avantage de Musette est que chaque tâche particulière est réifié et provoquée par la Extasi correspondante, et que les explications fournies dans le Extasi peut être utilisé pour guider l'assistant, ainsi que pour le rendre compréhensible par l'utilisateur. Dans le troisième scénario, l'utilisateur demande explicitement l'assistant de l'aide. Celui-ci doit alors identifier l'être des tâches effectuées par l'utilisateur afin de sélectionner des épisodes appropriés pour la réutilisation. identification des tâches peut varier de sélection explicite d'un Extasi par l'utilisateur, à la détection entièrement automatique dans la trace de courant selon l'Extasis disponible. Une solution intermédiaire est destiné au système de proposer plusieurs interprétations de l'activité de l'utilisateur, sur la base du Extasis partiellement instancié et les explications correspondantes. L'utilisateur peut alors accepter ou de rejeter ces interprétations jusqu'à ce qu'un consensus soit atteint. Le quatrième scénario va plus loin que le précédent. On suppose qu'aucun des Extasis proposée satisfaire l'utilisateur. L'assistant pourrait fournir un moyen pour lui de préciser encore plus précisément quelle tâche il est prêt à effectuer. Par exemple, notre utilisateur ne veut ni demander à quelqu'un, ni d'affiner sa requête en ajoutant d'autres mots-clés, mais préférez envoyer la même requête à un moteur de recherche plus spécialisés (par exemple, spécifique de champ ou spécifique de la langue). Ce que l'utilisateur est en train de faire est la description d'une nouvelle tâche avec sa propre Extasi -sans préciser complètement, bien sûr, puisque l'utilisateur a besoin d'aide pour trouver le moteur de recherche spécialisé approprié. Même si ce Extasi ne savait pas avant, cette tâche aurait déjà été réalisée, et par conséquent des traces peut contenir des épisodes pour cette Extasi nouvellement créé. L'utilisateur peut en effet décrire une tâche jusque-là inconnue à la volée, et obtenir encore une aide à ce sujet. On voit ici comment la séparation dans Musette, entre Champin et al. le modèle d'usage général et les signatures de tâches spécifiques, met à profit l'utilisateur de assistanceMerci les styles Suivants respecter Les Pour Les listes. 6. Autres travaux Bien avant les ordinateurs numériques et l'Internet, [Bush, 1945] a été le rêve de ce qu'il appelait le « MEMEX », un dispositif qui capture tout un scientifique a examiné ou commenté au sujet, la création d'une piste d'information dont il d'autres scientifiques ont pu récupérer plus tard. Dans les années 90, [Hill et al., 1992] a vu des interactions cumulées des utilisateurs avec des objets numériques comme une forme d'usure, comparable aux pages déchirées dans un manuel. Plus récemment, [Wexelblat et Maes, 1997] ont mis au point un outil permettant aux utilisateurs de visualiser les chemins les autres ont pris par un site. Ces travaux préliminaires ont porté sur ce qui pourrait être présenté directement à l'utilisateur que des empreintes à suivre pour elle (informulée) demande. Ils visent à travailler comme assistants génériques. Quelques travaux ont porté sur la tâche de l'utilisateur de mettre en contexte l'assistance [Farell et al., 2000; Francisco-Revilla et Breimer, 2000] tandis que d'autres ont essayé de capturer des épisodes de navigation web général sur les signatures statiques [Corvaisier et al., 1997; Jaczinski et Trousse, 1998] ou, comme [Takano et al., 2000], passé les cas de procédure utilisée pour aider leur utilisation dans des applications spécifiques. Ces derniers travaux reposent sur le paradigme Raisonnement Case-Based, qui est largement utilisé pour aider les systèmes de bureau. Ces cas de besoin d'être décrit par des formes [Herbeaux, 1999] ou du sonner une « conversation » (ce qui suppose que les questions peuvent être structurées pour répondre aux cas d'une bibliothèque) [Aha et al., 2001]. Par conséquent, la structure de cas doit être défini dans les bibliothèques d'avance et cas sont construits selon elle. L'expérience qui y sont stockés devient donc guère exploitable pour des questions imprévues. Si les cas doivent être assouplies comme des épisodes d'utilisation dynamique trouvés dans la trace, il est néanmoins nécessaire de prendre en compte les tâches utilisateur contexte. la modélisation des tâches a été largement étudiée par la communauté de l'ingénierie des connaissances. Cette modélisation peut être réalisée dans les systèmes à base de connaissances conception de contexte [Schreiber et al., 1999] et est de plus en plus utilisé à des fins KM [Holz et al., 2001]. De la même façon, l'ontologie conception est de plus en plus explicitement tâche axée sur [Reynaud, 1997]. L'ingénierie des connaissances propose des modèles, des méthodes et des outils, ce qui implique souvent de grands efforts pour susciter des tâches des comportements réels des utilisateurs. D'autre part, les assistants informatiques qui permettent aux utilisateurs d'effectuer une tâche seulement besoin d'être exprimé en termes de ressources pertinentes pour l'assistance, plutôt que la modélisation des tâches à part entière. Néanmoins, dans une approche de réutilisation de l'expérience, il est nécessaire de repérer des morceaux des traces d'utilisation qui référeraient à la tâche de certains utilisateurs. Par conséquent, nous avons proposé Éxtasis que des vues simplifiées des modèles de travail. De telles signatures peuvent être conçues en fonction de l'ingénierie des connaissances classiques approches afin d'être intégré dans un environnement informatique, avant est jamais utilisé le système; mais ils peuvent aussi être conçus sur demande à temps géré par l'utilisateur lui-même. Cette approche suppose que les utilisateurs finaux sont co-créateurs de leur environnement d'assistance. 7. Conclusion L'application du modèle MUSETTE à une interaction a besoin d'une phase de prototypage: dans de nombreux cas, les agents observateurs devront être codées à partir de zéro, et donc des efforts de développement importants. Pour aller plus loin que la modélisation simple papier stylo, nous avons besoin d'un outil graphique pour le modèle d'utilisation rapide prototypage, des traces primitives et les premières signatures de la tâche. Nous avons développé un prototype d'un tel qu'un plug-in pour PROTÉGÉ (http://protege.stanford.edu/), en utilisant OKBC comme langue de mise en œuvre MUSETTE. Outre les systèmes que nous avons déjà construit, sur dont l'expérience la création, nous avons conçu MUSETTE, et donc qui - MUSETTE: Un cadre pour la capture des connaissances de l'expérience dans une certaine mesure - les parties mises en œuvre de l'approche MUSETTE, nous sommes maintenant plus particulièrement applique cette approche la conception de plusieurs agents adjoints concernés par divers outils. Références [Aamodt et Plaza, 1994] A. Aamodt, E. Plaza. raisonnement à partir de cas; questions fondamentales, les variations méthodologiques et approches du système. AI Communications, Vol.7, n ° 1 [Aha et al., 2001] D.-W. Aha, L. Breslow et H. Munoz-Avila. raisonnement à partir de cas conversationnel. Intelligence appliquée, 14 (1): 9-32, 2001. [Bush, 1945] V. Bush, comme on peut penser, Atlantic Monthly, 176 (1), 101-108, 1945 [Champin et Prié 2003] P-A. Champin et Y. Prié. : Annotation musettes utilisations basée sur le Web sémantique, Dans Annotation pour le Web sémantique, IOS Press, Amsterdam (NL), 2003. [Corvaisier et al., 1997] F. Corvaisier, A. Mille et J.-M. Épingler à. Recherche d'information sur le Web en utilisant un système de prise de décision. En RIAO 1997, pages 284-295, juin 1997. [Farell et al., 2000] R. Farrell, P. Fairweather et E. Brumer. Une tâche architecture pour adjonctions au courant d'application, ACM Conférence internationale sur les interfaces utilisateur Intelligent, La Nouvelle-Orléans, LA USA, pages 82-85, 2000 [Francisco-Revilla et Breimer, 2000] L. Francisco-Revilla et E. Breimer. diffusion de l'information médicale adaptative combinant l'utilisateur, les modèles de travail et la situation, Conférence Internation sur les interfaces intelligentes, Nouvelle-Orléans, LA, États-Unis pages 94-97, 2000 [Hill et al., 1992] W.C. Colline J.D. Hollan, D. et T. Wroblewski McCandless. Modifier et Wear Lire l'usure. Dans Proc. de la Conférence ACM sur les facteurs humains dans les systèmes informatiques, CHI'92, pa gements 3-9, New York ACM Press, 1992 [Holz et al., 2001] H. Holz, A. Könnecker et F. Maurer. la gestion des connaissances tâches spécifiques dans un processus centré voir. Dans Althoff, Feldmann et Müller, rédacteurs en chef, les progrès d'apprentissage organisations logiciels, Proc. de LSO 2001, LNCS 2176, 2001. [Jaczinski et Trousse, 1998] M. Jaczinski, B. Trousse. WWW Assisted navigation en réutilisant passé Navigations d'un groupe d'utilisateurs, dans Advances in Raisonnement à partir de cas, 4 EWCBR, Dublin, Irlande, 1998 [et Mille Prié, 2000] Y. et A. Mille Prié. La réutilisation des conteneurs de connaissances: une sémantique locale approche. Dans M. Minor, rédacteur en chef, atelier sur les stratégies souples pour le maintien de conteneurs connaissances, ECAI 2000, numéro 33, pages 38-45, août 2000. [Reynaud, 1997] C. Reynaud et F Tort. En utilisant des ontologies explicites pour créer des méthodes de résolution de problèmes. Inter-national Journal of-Homme-études, 46: 339-364, 1997. [. Schreiber et al, 1999] G. Schreiber, A. Hakkermans, A. Anjewierden, R. de Hoog, N. Shadbolt, W. Van de Welde et B. Wielinga. L'ingénierie des connaissances et de la gestion: La méthodologie CommonKADS. Le MIT Press, 1999. [Takano et al., 2000] A Takano, Y. Yurugi et A. Kaenaegami. Procédure à base du système d'aide de bureau, ACM IIU 2000, Nouvelle-Orléans, LA USA, pages 264-272, 2000 [Wexelblat et Maes, 1997] A. Wexelblat et P. Maes. Empreintes: Histoire riche en navigation sur le Web, en RIAO'97, pages 75-84, 1997. Résumé Nous présentons une nouvelle approche pour la modélisation de l'expérience de l'utilisateur dans l'utilisation d'un système informatique, avec la perspective de réutiliser cette expérience que les connaissances dans le contexte pour aider l'utilisateur le long de sa tâche. Les avantages de cette approche sont démontrées par quatre scénarios."
