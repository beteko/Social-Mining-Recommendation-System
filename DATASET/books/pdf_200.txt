 Régression logistique pour la classification d’images à grande échelle Thanh Nghi Do François Poulet Université de Can Tho Rue 3 2 Ninh Kieu Can Tho Vietnam dtnghi cit ctu edu vn Université de Rennes I IRISA Campus de Beaulieu 35042 Rennes Cedex France francois poulet irisa fr Résumé Nous présentons un nouvel algorithme parallèle de régression logis tique PAR MC LR pour la classification d’images à grande échelle Nous pro posons plusieurs extensions de l’algorithme original de régression logistique à deux classes pour en développer une version efficace pour les grands ensembles de données d’images avec plusieurs centaines de classes Nous présentons un nouvel algorithme LR BBatch SGD de descente de gradient stochastique de ré gression logistique en batch équilibré avec un apprentissage parallèle approche un contre le reste multi classes sur de multiples cœurs Les résultats expérimen taux sur des ensembles de données d’ImageNet montrent que notre algorithme est efficace comparés aux algorithmes de classification linéaires de l’état de l’art 1 Introduction La classification d’images vise à assigner automatiquement une catégorie prédéfinie à une image Parmi ses nombreuses applications on peut citer la reconnaissance de caractères ma nuscrits la reconnaissance d’empreintes digitales oula reconnaissance de visages Le nombre d’images stockées dans les différentes bases de données ne cesse de croître par exemple Fa cebook contient aujourd’hui plus de 90 milliards d’images il a été estimé que les utilisateurs moyens d’appareils photos numériques prendront environ 100 000 photos au long de leur vie Sur internet 65% du volume des données chargées correspond à des images La catégorisation d’images par le contenu est un véritable challenge d’importance aujour d’hui Les approches les plus performantes de ces dernières années utilisent un modèle de "sac de mots visuels" Bag of Words Bow construits sur des descripteurs locaux des images Le "sac de mots visuels" est une adaptation du sac de mots utilisé en catégorisation de textes où l’on a un ensemble de documents chaque document contenant un ensemble de mots Le modèle de sac de mots correspond aux nombres d’occurrence des mots dans les documents Dans le cas du sac de mots visuels on commence par calculer des descripteurs de bas niveau en des points particuliers de l’image Les plus populaires sont les SIFTs Scale Invariant Fea ture Transform Lowe 2004 les SURFs Speeded Up Robust Features Bay et al 2008 and DSIFTs Dense SIFTs Bosch et al 2007 Ces méthodes d’extraction de descripteurs 309 Régression logistique pour la classification d’images à grande échelle sont basées sur l’apparence des objets en des points particuliers de l’image et sont invariantes aux changements d’échelle aux rotations aux occlusions et aux variations de luminosité En suite on utilise une méthode de quantification sur ces descripteurs par exemple en appliquant un algorithme de clustering comme un k means MacQueen 1967 chaque cluster est alors considéré comme un mot visuel pour finalement obtenir la distribution des SIFTs de chaque image dans l’ensemble des clusters obtenus Le modèle de "sac de mots visuels" conduit à des ensembles de données ayant des très grands nombres de dimensions Les Support Vector Machine ou Séparateurs à Vaste Marge SVM Vapnik 1995 sont les plus utilisés pour la classification de tels ensembles de données Cependant des ensembles d’images tels qu’Ima geNet Deng et al 2010 avec plus de 14 millions d’images et 21841 classes rendent la tâche de classification très complexe Ce challenge nous a conduits à développer un nouvel algo rithme efficace à la fois en temps d’exécution et précision de la classification Nous présentons des extensions de l’algorithme de descente de gradient stochastique SGD Shalev Shwartz et al 2007 Bottou et Bousquet 2008 pour créer un nouvel algorithme parallèle de SGD de régression logistique multi classes PAR MC LG pour la classification de grands ensembles de données d’images avec un nombre important de classes Nos contributions comprennent 1 un algorithme de descente de gradient stochastique de régression logistique équilibré en batch équilibré BBatch LR SGD pour la classification d’ensembles de données avec un très grand nombre de classes 2 un apprentissage en parallèle des classifieurs sur des machines multi cœurs Les expérimentations menées sur des ensembles de données issus d’ImageNet montrent que notre algorithme est efficace comparé aux algorithmes de l’état de l’art Le reste de cet article est organisé de la manière suivante la section deux présente briè vement l’algorithme de descente de gradient stochastique de régression logistique LR SGD pour le problème de la classification à deux classes La section 3 décrit comment étendre l’al gorithme au cas de plusieurs classes et le paralléliser La section 4 présente les résultats des expérimentations avant la conclusion et les travaux futurs 2 Régression logistique pour la classification de deux classes Soit un problème de classification à deux classes avec m points xi i = 1 m en dimen sion n et les étiquettes de classe correspondantes yi = ±1 La régression logistique LR essaye d’apprendre un modèle des données i e un vecteur w de dimension n qui maximise la vraisemblance La probabilité d’appartenance d’un point à la classe positive est p yi = +1 xi = 1 1 + e− w xi 1 et la probabilité d’appartenance d’un point à la classe négative est p yi = −1 xi = 1− p yi = +1 xi = 1− 1 1 + e− w xi = 1 1 + e w xi 2 Les probabilités 1 et 2 peuvent se réécrire sous la forme suivante 310 T N Do F Poulet p yi xi = 1 1 + e−yi w xi 3 Et donc la log vraisemblance est log p yi xi = log 1 1 + e−yi w xi = −log 1 + e−yi w xi 4 La régression logistique vise à maximiser la log vraisemblance et la marge cela conduit au problème min Ψ w [x y] = λ 2 ‖w‖2 + 1 m m∑ i=1 log 1 + e−yi w xi 5 La formule de régression logistique 5 utilise la fonction de perte logistique w [xi yi] = log 1 + e −yi w xi 6 La solution du problème convexe peut aussi être obtenue par une méthode de gradient sto chastique Shalev Shwartz et al 2007 Bottou et Bousquet 2008 La descente de gradient stochastique pour la régression logistique est notée LR SGD L’algorithme LR SGD effectue le calcul de w en T itérations avec un taux d’apprentissage η A chaque itération t il n’utilise qu’un seul point xt yt pour calculer le sous gradient ∇tΨ w [xt yt] et mettre à jour wt+1 comme suit wt+1 = wt − ηt∇tΨ w [xt yt] = wt − ηt λwt +∇tL w [xt yt] 7 ∇tL w [xt yt] = ∇tlog 1 + e−yt w xt = − ytxt 1 + eyt w xt 8 L’algorithme LR SGD utilisant la mise à jour 7 est décrit dans l’algorithme 1 3 Régression logistique pour un grand nombre de classes Nous présentons plusieurs extensions du cas de régression logistique avec deux classes au cas multi classes k classes avec k ≥ 3 Les approches de l’état de l’art peuvent être classées en deux catégories La première considère le cas multi classes comme un problème d’optimisation Ben Akiva et Lerman 1985 Weston et Watkins 1999 Guermeur 2007 et la seconde décompose le problème multi classes en une série de problèmes à deux classes incluant les approches un contre le reste Vapnik 1995 un contre un Kreßel 1999 et graphe de décision acyclique orienté Platt et al 2000 Dans la pratique les approches un contre un et un contre le reste sont les plus utilisées car simples à mettre en œuvre Soit un problème à k classes k > 2 La stratégie un contre le reste 311 Régression logistique pour la classification d’images à grande échelle Algorithme 1 Algorithme LR SGD input training dataset D positive constant λ > 0 number of epochs T output hyperplane w 1 begin 2 init w1 = 0 3 for t← 1 to T do 4 randomly pick a datapoint [xt yt] from training dataset D 5 set ηt = 1λt 6 update wt+1 = wt − ηt λwt − ytxt1+eyt w xt 7 end 8 return wt+1 9 end construit k classifieurs où le ieclassifieur sépare la classe i du reste La stratégie un contre un quant à elle construit k k− 1 2 classifieurs en utilisant toutes les paires possibles de classes Dans les deux cas la classe est ensuite prédite par un vote majoritaire Lorsque l’on traite des ensembles ayant un grand nombre de classes e g plusieurs cen taines la stratégie un contre un peut devenir trop coûteuse à mettre en œuvre car il faut calculer plusieurs milliers de classifieurs La stratégie un contre le reste est alors préférée dans ce cas Donc notre algorithme multi classes LR SGD va aussi utiliser cette stratégie un contre le reste en effectuant k apprentissages Par conséquence l’algorithme LR SGD avec apprentissage un contre le reste conduit aux problèmes suivants 1 l’algorithme doit faire face à des classes très déséquilibrées pour construire ses classi fieurs binaires 2 l’algorithme est très coûteux en temps d’exécution pour effectuer l’apprentissage en mode séquentiel Nous proposons deux améliorations pour la création d’un nouvel algorithme LR SGD capable de traiter un très nombre de classes dans un temps raisonnable La première est la construction des classifieurs équilibrés par une méthode d’échantillonnage de la classe ma joritaire et la seconde est la parallélisation du mécanisme d’apprentissage sur des machines multi cœurs 3 1 Batch équilibré de régression logistique Dans l’approche un contre le reste la tâche d’apprentissage de l’algorithme LR SGD est d’essayer de séparer la ie classe positive des k − 1 autres classes classe négative Lorsque l’on a des grands nombres de classes e g 100 classes cela conduit à un très fort déséquilibre entre la classe positive et la classe négative Le problème de l’algorithme LR SGD vient de la ligne 4 de l’algorithme 1 Pour traiter des centaines de classes la probabilité d’avoir un point positif est très faible par exemple 1% par rapport à la très forte probabilité d’avoir 312 T N Do F Poulet un point de la classe négative par exemple 99% L’algorithme LR SGD se concentre alors essentiellement sur les erreurs de la classe négative et a ainsi des difficultés à séparer la classe positive de la classe négative Différentes solutions aussi bien au niveau des données qu’au niveau de l’algorithme au déséquilibre des classes ont été présentées Japkowicz 2000 Weiss et Provost 2003 Visa et Ralescu 2005 ou encore Lenca et al 2008 Pham et al 2008 Au niveau des données ces algorithmes modifient la distribution des classes soit par sur échantillonnage de la classe minoritaire Chawla et al 2003 ou sous échantillonnage de la classe majoritaire Liu et al 2009 Ricamato et al 2008 Au niveau de l’algorithme une solution est de modifier l’équilibre des classes en jouant sur les coûts de mauvaise classification de l’une ou des deux classes Notre algorithme balancé de batch LR SGD noté LR BBatch SGD appartient à la pre mière catégorie Pour séparer la ie classe positive du reste classe négative les probabili tés sont fortement déséquilibrées par exemple la probabilité d’appartenance à la classe po sitive n’est que de 1% dans les cas des 100 classes d’ImageNet Par ailleurs le coût de sur échantillonnage étant relativement élevé nous proposons l’algorithme LR BBatch SGD utili sant un sous échantillonnage de la classe majoritaire Nous avons modifié l’algorithme 1 pour de batch équilibré au lieu d’un unique point ligne 4 pour effectuer la mise à jour de w à chaque itération t Nous avons aussi modifié la mise à jour de la manière suivante wt+1 = { wt − ηtλwt + ηt 1|D+| ytxt 1+eyt w xt if yt = +1 wt − ηtλwt + ηt 1|D−| ytxt 1+eyt w xt if yt = −1 9 avec |D+| le cardinal de la classe positive et |D−| le cardinal de la classe négative L’algorithme LR BBatch SGD Algorithme 2 sépare la ie classe positive du reste classe négative en utilisant un sous échantillonnage de la classe négative batch équilibré et la mo dification du coût de 9 3 2 Parallélisation de l’apprentissage LR BBatch SGD Bien que l’algorithme LR BBatch SGD puisse traiter de grands ensembles de données dans un temps raisonnable il ne tire pas avantage des possibilités des architectures multi cœurs des machines actuelles De plus l’algorithme LR BBatch SGD effectue k apprentissages indépen dants pour la classification de k classes ce qui est propriété intéressante pour la parallélisation Pour améliorer le temps d’exécution de l’algorithme LR BBatch SGD nous allons effectuer l’apprentissage des k classifieurs binaires en parallèle sur plusieurs machines multi cœurs La programmation parallèle est principalement basée sur deux modèles MPI Message Passing Interface MPI Forum et OpenMP Open Multiprocessing OpenMP Architecture Review Board 2008 MPI est un système standardisé basé sur un mécanisme d’envois de messages pour les systèmes à mémoire distribué C’est actuellement le système le plus utilisé pour la parallélisation Cependant son défaut est de nécessiter tout l’ensemble de données en mémoire pour effectuer l’apprentissage Cela signifie que si l’on utilise MPI pour la classifi cation de k classes en parallèle on aura besoin de k fois la quantité de mémoire pour charger les k ensembles de données ce qui le rend inutilisable dans ce contexte Nous allons utiliser 313 Régression logistique pour la classification d’images à grande échelle Algorithme 2 Algorithme LR BBatch SGD input training data of the positive class D+ training data of the negative class D− positive constant λ > 0 number of epochs T output hyperplane w 1 begin 2 init w1 = 0 3 for t← 1 to T do 4 creating a balanced batch Bbatch by sampling without replacement D′− from dataset D− with |D′−| = sqrt |D−||D+| and a datapoint from dataset D+ 5 set ηt = 1λt 6 for [xi yi] in Bbatch do 7 update wt+1 using rule 9 8 end 9 end 10 return wt+1 11 end OpenMP qui lui n’a pas besoin d’autant de mémoire même si la parallélisation est moins op timisée que celle de MPI L’apprentissage en parallèle de l’algorithme LR BBatch SGD est décrit dans l’algorithme 3 Algorithme 3 Apprentissage parallèle de LR BBatch SGD input D the training dataset with k classes output LR SGD model 1 Learning 2 pragma omp parallel for 3 for ci ← 1 to k do * class ci * 4 training LR BBatch SGD ci − vs− all 5 end 4 Evaluation Pour évaluer les performances de notre nouvel algorithme parallèle de régression logistique multi classes PAR MC LR pour la classification de grands ensembles d’images en un grand nombre de classes nous avons mis en uvre l’algorithme PAR MC LR en C C++ en utilisant la bibliothèque SGD Library Bottou et Bousquet 2008 Nos comparaisons porteront sur la précision et le temps d’apprentissage Nous avons choisi deux algorithmes récents de classifi cation LIBLINEAR A library for large linear classification Fan et al 2008 et OCAS An 314 T N Do F Poulet optimized cutting plane algorithm for SVM Franc et Sonnenburg 2009 car ils sont réputés pour être efficaces pour la classification de SVM linéaires LIBLINEAR et OCAS sont utilisés avec leurs paramètres par défaut C = 1000 l’algo rithme Par MC LR effectue l’apprentissage de batch équilibré de descente de gradient stochas tique de régression logistique en utilisant 50 itérations et le terme de régularisation λ = 0 0001 Toutes les expériences sont effectuées sous Linux Fedora 20 Intel R Core i7 4790 CPU 3 6GHz 4 cœurs et 32 Go de mémoire vive 4 1 Ensembles de données L’algorithme PAR MC LR a été conçu pour la classification de grands ensembles d’images avec un grand nombre de classes nous allons l’évaluer sur les trois ensembles de données suivants ImageNet10 Cet ensemble de données contient les 10 plus grandes classes de l’ensemble ImageNet soit 24 807 images et 2 4Go Pour chaque classe 90% des images servent à l’apprentissage et 10% pour le test Nous commençons par construire le sac de mots visuels en utilisant pour chaque image les descripteurs DSIFTs Dense SIFTs et un vocabulaire de 5000 mots Ensuite on uti lise les featuresmaps de Vedaldi et Zisserman 2012 une représentation en haute dimension de l’image de dimension 15000 Cette méthode a donné de bons résultats pour la classification d’images à l’aide de classifieurs linéaires Nous obtenons finalement un ensemble de données d’apprentissage de 2 6 Go ImageNet100 Cet ensemble de données contient les 100 plus grandes classes de l’ensemble ImageNet soit 183116 images et 23 6Go Dans chaque classe nous prenons 50% des images pour l’ap prentissage et les 50% restants pour le test De la même manière nous construisons un sac de mots visuels en utilisant des DSIFTs et un vocabulaire de 5000 mots visuels Puis nous utili sons les featuresmaps et nous obtenons finalement en entrée de l’algorithme d’apprentissage un ensemble de 8 Go ILSVRC2010 Cet ensemble de données contient les 1000 plus grandes classes de l’ensemble ImageNet soit 1 2 million d’images et 126 Go pour l’apprentissage 50000 images pour la validation 5 3 Go et 150000 16 Go images pour le test Nous utilisons le sac de mots visuels fournit par Deng et al 2010 et la méthode décrite dans Wu 2012 pour encoder chaque image dans un vecteur de dimensions 21000 Nous retenons 900 images par classe pour l’ensemble d’apprentissage donc le nombre total d’images pour l’apprentissage est de 887816 et 12 5 Go Toutes les données de l’ensemble de test sont utilisées pour tester le modèle de SVM obtenu 4 2 Résultats de la classification Tout d’abord on s’intéresse aux performances en ce qui concerne le temps d’exécution Nous avons fait varier le nombre de threads OpenMP 1 4 8 threads pour chaque exécution de notre algorithme PAR MC LR En raison de l’architecture utilisée Intel R Core i7 4790 315 Régression logistique pour la classification d’images à grande échelle TAB 1 – Temps d’apprentissage mn pour ImageNet10 Algorithme 1 thread 4 threads 8 threads OCAS 106 67 LIBLINEAR 3 12 1 50 1 48 PAR MC LR 2 24 0 80 0 76 TAB 2 – Temps d’apprentissage mn pour ImageNet100 Algorithme 1 thread 4 threads 8 threads OCAS 1016 35 LIBLINEAR 63 42 30 49 30 18 Par MC LR 23 40 8 09 6 55 CPU 3 6GHz 4 cœurs lors de nos expérimentations l’algorithme PAR MC LR est le plus rapide avec 8 threads mais pas tellement plus qu’avec 4 threads Pour le petit ensemble d’images multi classes ImageNet10 le temps d’apprentissage des algorithmes dans le tableau 1 montre que notre algorithme PAR MC LR avec 8 threads est 140 fois plus rapide que OCAS et 2 fois plus rapide que LIBLINEAR Les tableau 2 et figure 1 présentent les temps d’apprentissage pour l’ensemble de données ImageNet100 avec un plus grand nombre de classes Ici encore notre algorithme PAR MC LR procure un gain significatif pour le temps d’exécution en utilisant 8 threads Il est 155 fois plus rapide que OCAS et 4 6 fois plus rapide que LIBLINEAR L’ensemble de données ILSVRC 2010 possède un très grand nombre d’images plus d’un million et un grand nombre de classes 1000 OCAS n’a pas pu achever le calcul de l’ap prentissage après plusieurs semaines CPU LIBLINEAR a eu besoin de 3106 minutes soit 2 jours et 4 heures pour l’apprentissage du modèle de classification pour cet ensemble de don nées 16h45 avec 8 threads Notre algorithme PAR MC LR avec 8 threads n’a quant à lui eu besoin que de 38 minutes pour effectuer cette tâche d’apprentissage Ceci montre que notre algorithme est 26 fois plus rapide que LIBLINEAR En ce qui concerne la précision de la classification les résultats sont présentés dans les tableau 4 et figure 3 Sur le petit ensemble de données ImageNet10 et le moyen ImageNet100 l’algorithme PAR MC LR obtient un meilleur taux de bonne précision que OCAS En ce qui concerne la comparaison avec LIBLINEAR les performances sont quasiment identiques Notre algorithme a une meilleure précision sur les ensembles ImageNet10 et ILSVRC2010 et TAB 3 – Temps d’apprentissage mn pour ILSVRC 2010 Algorithme 1 thread 4 threads 8 threads OCAS N A LIBLINEAR 3106 48 1037 1004 Par MC LR 153 58 40 59 37 91 316 T N Do F Poulet FIG 1 – Temps d’apprentissage mn pour ImageNet100 FIG 2 – Temps d’apprentissage mn pour ILSVRC 2010 TAB 4 – Précision de la classification % Algorithme ImageNet 10 ImageNet 100 ILSVRC 1000 OCAS 72 07 52 75 N A LIBLINEAR 75 09 54 07 21 11 Par MC LR 75 21 52 91 21 90 317 Régression logistique pour la classification d’images à grande échelle FIG 3 – Précision de la classification % une précision moindre sur ImageNet100 ILSVRC2010 est un grand ensemble de données avec plus d’un million d’images et 1000 classes C’est donc un challenge d’obtenir des bons taux de classification pour les algorithmes de l’état de l’art En particulier avec l’ensemble de données fournit lors de la compétition ILSVRC2010 les meilleurs algorithmes Deng et al 2010 Berg et al 2010 ont obtenus des taux de précision de l’ordre de 19% Notre algorithme PAR MC LR obtient une meilleure précision 21 9% contre 19% ce qui représente une amélioration relative de plus de 15% De plus comparé à LIBLINEAR on obtient une amélioration relative de plus de 3 7% On peut remarquer que l’algorithme PAR MC LR est beaucoup plus rapide que LIBLINEAR tout en ayant une précision équivalente Enfin il est aussi gourmand en mémoire puiqu’il ne nécéssite que 9 4 Go de mémoire contre 16 9 pour LIBLINEAR et 52 9 pour OCAS Ces résultats montrent que notre algorithme a de bonnes qualités pour traiter l’intégralité de l’ensemble de données ImageNet Plus la taille de l’ensemble de données augmente et plus le rapport entre les temps d’apprentissage entre notre algorithme et LIBLINEAR nous sont favorables on passe d’un facteur de 2 fois plus rapide sur ImageNet10 à 26 fois plus rapide sur ILSVRC2010 5 Conclusion et travaux futurs Nous avons présenté un nouvel algorithme parallèle de régression logistique multi classes qui permet la classification efficace de grands ensembles de données d’images avec un grand nombre de classes Pour cela nous avons utilisé un algorithme de batch équilibré de descente de gradient stochastique de régression logistique pour les apprentissages de deux classes dans le cadre de la classification multi classes Ce nouvel algorithme permet notamment la clas sification de grands ensembles de données d’images sur des architectures multi cœurs Les performances de l’algorithme ont été évaluées sur les 10 et 100 plus grandes classes de l’en semble de données ImageNet et sur l’ensemble de données ILSVRC2010 Notre algorithme présente des gains significatifs en ce qui concerne les temps d’exécution d’un facteur 2 sur 318 T N Do F Poulet l’ensemble de données le plus petit à 26 sur le plus grand tout en ayant une précision similaire Ces résultats montrent que plus l’ensemble de données à traiter est grand et plus le gain par rapport aux autres algorithmes est important et ce gain serait sans doute encore plus important si nous avions utilisé une machine disposant de 8 cœurs au lieu de 4 on note en effet qu’il n’y a pratiquement pas de différences entre 4 et 8 threads Les extensions de ces travaux prévoient une version hybride MPI OpenMP de l’algorithme parallèle de régression logistique pour classifier efficacement de grands ensembles de données multi classes et permettre la classification de la totalité de l’ensemble de données ImageNetsur un ensemble de machines multi cœurs Références Bay H A Ess T Tuytelaars et L J V Gool 2008 Speeded up robust features SURF Computer Vision and Image Understanding 110 3 346–359 Ben Akiva M et S Lerman 1985 Discrete Choice Analysis Theory and Application to Travel Demand The MIT Press Berg A J Deng et F F Li 2010 Large scale visual recognition challenge 2010 Technical report Bosch A A Zisserman et X Muñoz 2007 Image classification using random forests and ferns In International Conference on Computer Vision pp 1–8 Bottou L et O Bousquet 2008 The tradeoffs of large scale learning In J Platt D Kol ler Y Singer et S Roweis Eds Advances in Neural Information Processing Systems Volume 20 pp 161–168 NIPS Foundation books nips cc Chawla N V A Lazarevic L O Hall et K W Bowyer 2003 Smoteboost improving pre diction of the minority class in boosting In In Proceedings of the Principles of Knowledge Discovery in Databases PKDD 2003 pp 107–119 Deng J A C Berg K Li et F F Li 2010 What does classifying more than 10 000 image categories tell us In European Conference on Computer Vision pp 71–84 Fan R K Chang C Hsieh X Wang et C Lin 2008 LIBLINEAR a library for large linear classification Journal of Machine Learning Research 9 4 1871–1874 Franc V et S Sonnenburg 2009 Optimized cutting plane algorithm for large scale risk minimization Journal of Machine Learning Research 10 2157–2192 Guermeur Y 2007 Svm multiclasses théorie et applications Japkowicz N Ed 2000 AAAI’Workshop on Learning from Imbalanced Data Sets Number WS 00 05 in AAAI Tech Report Kreßel U 1999 Pairwise classification and support vector machines Advances in Kernel Methods Support Vector Learning 255–268 Lenca P anf Lallich S T N Do et N K Pham 2008 A comparison of different off centered entropies to deal with class imbalance for decision trees In The Pacific Asia Confe rence on Knowledge Discovery and Data Mining LNAI 5012 pp 634–643 Springer Verlag Liu X Y J Wu et Z H Zhou 2009 Exploratory undersampling for class imbalance lear ning IEEE Transactions on Systems Man and Cybernetics Part B 39 2 539–550 319 Régression logistique pour la classification d’images à grande échelle Lowe D G 2004 Distinctive image features from scale invariant keypoints International Journal of Computer Vision 60 2 91–110 MacQueen J 1967 Some methods for classification and analysis of multivariate observa tions Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability Berkeley University of California Press 1 281–297 MPI Forum MPI A message passing interface standard OpenMP Architecture Review Board 2008 OpenMP application program interface version 3 0 Pham N K T N Do P Lenca et S Lallich 2008 Using local node information in decision trees coupling a local decision rule with an off centered In International Conference on Data Mining Las Vegas Nevada USA pp 117–123 CSREA Press Platt J N Cristianini et J Shawe Taylor 2000 Large margin dags for multiclass classifi cation Advances in Neural Information Processing Systems 12 547–553 Ricamato M T C Marrocco et F Tortorella 2008 Mcs based balancing techniques for skewed classes An empirical comparison In ICPR pp 1–4 Shalev Shwartz S Y Singer et N Srebro 2007 Pegasos Primal estimated sub gradient solver for svm In Proceedings of the Twenty Fourth International Conference Machine Learning pp 807–814 ACM Vapnik V 1995 The Nature of Statistical Learning Theory New York Springer Verlag Vedaldi A et A Zisserman 2012 Efficient additive kernels via explicit feature maps IEEE Transactions on Pattern Analysis and Machine Intelligence 34 3 480–492 Visa S et A Ralescu 2005 Issues in mining imbalanced data sets A review paper In Midwest Artificial Intelligence and Cognitive Science Conf Dayton USA pp 67–73 Weiss G M et F Provost 2003 Learning when training data are costly The effect of class distribution on tree induction Journal of Artificial Intelligence Research 19 315–354 Weston J et C Watkins 1999 Support vector machines for multi class pattern recognition In Proceedings of the Seventh European Symposium on Artificial Neural Networks pp 219– 224 Wu J 2012 Power mean svm for large scale visual classification In IEEE Computer Society Conference on Computer Vision and Pattern Recognition pp 2344–2351 Summary We present a new parallel multiclass logistic regression algorithm PAR MCLR aiming at classifying a very large number of images with very high dimensional signatures into many classes We extend the two class logistic regression algorithm LR in several ways to develop the new multiclass LR for efficiently classifying large image datasets into hundreds of classes We propose the balanced batch stochastic gradient descend of logistic regression BBatch LR SGD for training two class classifiers used in the one versus all strategy of the multiclass problems and the parallel training process of classifiers with several multi core computers The numerical test results on ImageNet datasets show that our algorithm is efficient compared to the state of the art linear classifiers 320 