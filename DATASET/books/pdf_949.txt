mesure entropie asymétrique consistante djamel zighed simon marcellin gilbert ritschard université lumière laboratoire abdelkader zighed simon marcellin lyon2 lyon2 université genève département économétrie suisse gilbert ritschard unige résumé mesures entropie connue celle shannon proposées contexte codage transmission informa néanmoins milieu années soixante elles utilisées autres domaines comme apprentissage particulièrement construire graphes induction arbres décision usage mesures cependant toujours approprié engendrer modèles diction explication pertinents cette faiblesse résulte propriétés tropies particulier maximum nécessairement atteint distribution uniforme insensibilité taille échantillon commençons rappeler propriétés classiques définissons ensuite nouvelle matique mieux adaptée besoins proposons mesure empirique tropie flexible vérifiant axiomes introduction méthodes génèrent règles décision condition alors conclu comme arbres décision breiman quinlan graphes induc zighed rakotomalala mesures entropie fréquemment utilisées celles reposent nombreuses hypothèses implicites toujours justifiées mesures entropie définies mathématiquement ensemble axiomes dehors contexte apprentissage machine trouver travaux détaillés rényi aczél daróczy transfert apprentissage manière hâtive mérite détail présent travail examine discute propriétés entropies cadre arbres induction section suivante fixons quelques notations rappelons contexte utili sation mesures entropie section présentons mesures entropie discutons leurs propriétés leurs conséquences processus induction proposons axiomatique conduisant nouvelle mesure entropie mesure entropie asymétrique consistante notations définitions concepts plaçons cadre arbres décision explicitement appel entropies mesurer qualité partition induite apprentissage population concernée problème apprentissage profil individu décrit variables dites variables exogènes variables explicatives variables peuvent qualitatives quantitatives considérons également variable prédire parfois appelée variable endogène variable classe encore variable réponse ensemble valeurs prises cette variable population ensemble discret nombre valeurs différentes prises nombre modalités ainsi ambiguïté notera classe simplement objectif algorithme induction arbre générer modèle prédiction représente arbre décision chaque branche arbre repré sente règle ensemble règles forme modèle prédiction permet calculer nouvel individu connaît variables exogènes variable endogène développement arbre effectue selon schéma simple ensemble prentissage segmenté itérativement chaque selon variables exogènes sorte engendrer partition faible entropie distribution sommets obtenus chaque itération définissent partition arbre grandit partition devient sommet racine arbre représente partition grossière chaque sommet partition caractérisé distribution probabilités modalités variable endogène arbres induction entropie partition minimiser généralement entropie moyenne calculée comme exemple entropie shannon expression donnée proportion sommet mesures entropie point historique concept entropie introduit hartley réellement promu loppé shannon shannon weaver années proposé mesure information entropie générale distribution finie probabilités suivant théorème caractérise entropie shannon plusieurs auteurs comme khinchin forte aczél fondé axiomatique entropie shannon expérience événements possibles probabilités respec tives suppose entropie zighed shannon distribution probabilité donnée formule continuité autres mesures entropie existent zighed rakoto malala propriétés théoriques mesures entropie considère ensemble distribu tions probabilités considère simplexe ordre mesure entropie définie comme propriétés suivantes négativité symétrie entropie insensible toute permutation vecteur permutation quelconque minimalité existe alors maximalité stricte concavité fonction strictement concave ainsi évaluation entropie partition nécessite connaissance mesure entropie apprentissage inductif propriétés mesures entropie vient lister paraissent apprentissage inductif effet incertitude maximale correspond nécessairement distribution uniforme ainsi cadre détection transactions frauduleuses fréquentes exemple opportun conclure fraude mesure entropie asymétrique consistante probabilité celle dépasse seuil disons voire moins autre pratique calcul entropie repose probabilités estimées devrait tenir compte précision taille échantillon pourquoi proposons nouvelle axiomatique justifions brièvement objectif aboutir entropie pourrait qualifier empirique tient mieux compte considé rations pratiques propriétés requises nouvelle fonction entropie voulons bâtir voulons empirique fonction fréquences sensible taille échantillon lequel elles calculées également paramétrée distribution maximale notera distribution fixée souhaitons possède propriétés suivantes négativité fonction valeur négative maximalité distribution fixée utilisateur comme étant moins souhaitée entropie maximale ainsi toute distribution taille asymétrie nouvelle propriété maximalité remet cause axiome symétrie requis entropies classiques conséquent certaines permutations pourraient affecter valeur entropie facilement identifier conditions lesquelles symétrie serait conservée comme exemple certains seraient identiques fortiori distribution forme minimalité contexte classique entropie nulle distribution concentrée seule classe autres étant vides lorsqu existe cette propriété effet demeurer valide théorique seulement apprentissage probabilités inconnues serait quand gênant entropie nulle distribution concentrée classe prendre considération taille échantillon estimer exige simplement entropie distribution empirique laquelle existe tende quand devient grand consistance donné distribution fixée entropie devrait faible effectif grand zighed probabilité entropie apprentissage inductif classes formules entropie apprentissage inductif estimateur laplace vecteur entropie maximale taille échantillon donnons démonstration théorème suivant théorème mesure entropie processus apprentissage inductif vérifie priétés graphique visualise forme cette entropie classes vecteur maximalité différentes valeurs conclusion travail avons défini nouvelle fonction entropie possède théorique algorithmique bonnes propriétés cette fonction repose choix mètre simple fixer prendre distribution priori observée échantillon apprentissage effet utilisateur œuvre techniques apprentis éloigner possible distribution priori naturel associée forte entropie faute place avons reporter toutes critiques pouvons formuler utilisation entropies classiques apprentissage intérêt notre approche permet répondre façon assez simple mesure entropie asymétrique consistante bidouillage nombreux problèmes comme représentation classes asymétrie coûts classes sensibilité taille échantillon évite apprentissage élagage comme exemple références aczél shannon inequality optimal coding characterization shannon rényi entropies information theory volume xviii academic press aczél daróczy measures information their characterizations academic press breiman friedman olshen stone classification regression trees chapman forte shannon entropy information theory volume xviii academic press hartley transmission information system technological journal khinchin mathematical foundation information theory chapter entropy concept probability theory dover quinlan programs machine learning mateo morgan kaufmann rényi measures entropy information proceedings berkeley symposium mathematics statistics probability volume berkeley university california press shannon mathematical theory communication system technological journal shannon weaver mathematical theory communication urbana university illinois press zighed rakotomalala graphes induction apprentissage mining paris hermes science publications summary initially entropy measures shannon entropy introduced coding information transmission since middle sixteens however other fields instance machine learning nowadays widely decision trees induction graphs nevertheless entropy measures always suited building reliable explanatory prediction models their limitations mainly their properties especially maximum value necessarily reached uniform distribution their insensitiveness sample start recalling these classi properties define suited axioms propose flexible empirical entropy measure these axioms