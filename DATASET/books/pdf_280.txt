Ultramétricité des espaces dissimilitude et son importance pour l'exploration de données Dan Simovici *, Rosanne Vetro **, *** Kaixun Hua University of Massachusetts Boston * dsim@cs.umb.edu ** *** rvetro@cs.umb.edu @ kingsley cs.umb.edu Résumé. Nous introduisons une mesure de ultramétricité pour les espaces de dissemblance et d'examiner les transformations de dissemblances qui ont un impact de cette mesure. Ensuite, nous étudions l'influence de ultramétricité sur le comportement des deux classes d'algorithmes de données (ing classification kNN et regroupement PAM) appliqués sur les espaces de dissemblance. Nous montrons qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour le regroupement, l'augmentation ultramétricité avec assorti- terings générer une meilleure séparation. L'abaissement produisent ultramétricité grappes plus compacts. 1 Introduction ultramétriques se produisent dans l'étude des algorithmes de classification ascendante hiérarchique, les arbres génétiques phylo-, nombres p-adiques, certains systèmes physiques, etc. but Notre est d'évaluer le degré de ultramétricité des espaces de dissemblance et d'étudier l'impact du degré de ultramétricité sur la performance de la classification et le regroupement algorithmes de. La mesure ultramétricité des espaces métriques a préoccupé un certain nombre de chercheurs (par exemple, dans (Rammal et al, 1985).); Toutefois, les mesures proposées sont utilisables dans le cas de mesures spé- ciales et sont liés à la ultramétrique subdominant attaché à une mesure qui nécessite le calcul d'un regroupement unique lien ou un arbre Spanning minimal. Nous vous proposons une mesure native alter- appelée faible ultramétricité qui peut être appliquée au cas plus général des espaces de dissemblance. Un espace de dissimilarité est une paire (S, d), où S est un ensemble et d: S × S - → R est une fonction telle que D (x, y)> 0, D (x, x) = 0, et d (x, y) = d (y, x) pour x, y ∈ S. Nous supposons que tous les espaces de dissemblance considérés sont finis. Un triangle en (S, d) est un triplet (x, y, z) ∈ S3. Pour simplifier la notation, on note t = (x, y, z) par xyz. La cartographie d est un quasi-métrique si elle est une dissimilarité et il satisfait à la triangulaire inégali- lité d (x, y) 6 d (x, z) + d (z, y) pour x, y, z ∈ S. en outre, si d (x, y) = 0 implique x = y, alors d est une métrique. - 89 - ultramétricité des dissimilarités Un quasi-ultramétrique est une dissemblance d: S × S - → R> 0 qui satisfait l'inégalité d (x, y) 6 max {d (x, z), d (z, y)} pour x, y, z ∈ S. Si, en outre, d (x, y) = 0 implique x = y, alors d est un ultramétrique. Dans la section 2, nous présentons une mesure de ultramétricité pour les espaces de dissemblance et une variante plus faible de cette mesure qui est mieux d'un point de vue informatique. Ensuite, nous examinons les transformations de dissemblances qui affectent ultramétricité. L'influence de ultramétricité de dissemblances sur la performance des classificateurs est examinée à la section 2 en utilisant les k plus proches voisins des classificateurs. L'article 4 est consacré à l'étude de l'impact de ultramétricité sur la compacité et la séparation cluster. 2 Evaluation de ultramétricité dans les espaces dissimilarité Soit r un nombre non négatif et soit Dr (S) l'ensemble des dissemblances définies sur S qui satisfont à l'inégalité d (x, y) r 6 d (x, z) r + d ( z, y) r pour x, y, z ∈ S. On notera que chaque dissimilarité appartient à l'ensemble D0; une dissemblance dans D1 est un semimetric. Laissez D∞ = ⋂ r> 0DR. Si d ∈ D∞, alors d est un ultramétrique. En effet, soit d ∈ D∞ et supposons que D (x, y)> d (x, z)> d (z, y). Ensuite, d (x, y) 6 d (x, z) (d (x, z)) r 1 + (d (y, z)) 1r pour chaque r> 0. Puisque limr → ∞ d (x, z ) (1 + (d (y, z) d (x, z)) r) 1r = d (x, z), il en résulte que d (x, y) 6 d (x, z) = max {d ( x, z), d (z, y) pour x, y, z ∈ S, ce qui nous permet de conclure que d est un ultramétrique. Il est facile de vérifier que r 6 s implique (d (x, z) r + d (z, y) r) 1r> (d (x, z) s + d (z, y) s) 1 s (voir ( Simovici et Djeraba, 2014), Lemme 6.15). Ainsi, si r 6 s, nous avons l'inégalité Ds ⊂ Dr Soit (S, d) un espace de dissemblance et laisser t = xyz un triangle. À la suite de la notation de Lerman (Lerman, 1981), nous écrivons Sd ( t) = D (x, y), Md (t) = d (x, z), et Ld (t) = d (y, z), si d (x, y)> d (x, z)> d (y, z). Définition 2.1. Soit (S, d) un espace de dissemblance et laisser t = xyz ∈ S3 un triangle. Le ultramétricité de t est le nombre ud (t) défini par ud (t) = max {r> 0 | Sd (t) r 6 Md (t) r + Ld (t) r}. Si d ∈ Dp, nous avons p 6 ud (t) pour chaque t ∈ S3. La notion de ultramétricité faible que nous sommes sur le point de présenter a des avantages de calcul sur la notion de ultramétricité, en particulier du point de vue de la manipulation des transformations de mesures. Le faible ultramétricité du triangle t, wd (t) est donnée par wd (t) =    1 log2 Sd (t) Md (t) si Sd (t)> Md (t) ∞ si Sd (t) = Md (t). Si wd (t) = ∞, alors t est un triple ultramétrique. Le faible ultramétricité de l'espace de dissimilarité (S, D) est le nombre w (S, d) définie par w (S, d) = médiane {wd (t) | t ∈ S3}. - 90 - Dan Simovici et al. La définition de w (S, d) élimine l'influence des triangles dont ultramétricité est une valeur aberrante, et donne une meilleure idée de la propriété globale de ultramétrique (S, d). Pour un triangle t, nous avons 0 6 Sd (t) -md (t) = (2 1 wd (t) - 1) Md (t) 6 (2 1 w (S, d) - 1) Md (t) Ainsi , si wd (t) est suffisamment grande, le triangle t est presque isocèle. Par exemple, si wd (t) = 5, la différence entre la longueur du côté le plus long Sd (t) et le côté médian Md (t) est inférieur à 15%. Pour chaque triangle t ∈ S3 dans un espace que nous avons dissemblance ud (t) 6 wd (t). En effet, depuis Sd (t) ud (t) 6 Md (t) ud (t) + Ld (t) ud (t) nous avons Sd (t) ud (t) 6 2md (t) ud (t), qui est équivalent à ud (t) 6 wd (t). Ensuite, on discute des transformations de dissemblance qui influent sur les ultramétricité des liens dissimilari-. Théorème 2.2. Soit (S, d) un espace de dissemblance et f: R> 0 - → R> 0 une fonction strictement croissante sur R> 0. Si la fonction g: R> 0 - → R> 0 donné par g (a) = {f (a) si a> 0, 0 si a = 0 est strictement décroissante, alors la fonction e: S × S - → R> 0 définie par e (x, y) = f (d (x, y)) pour x, y ∈ S est une dissimilarité et wd (t) 6 nous (t) pour chaque triangle t ∈ S3. Preuve. Il est immédiat que e (x, y) = e (y, x) et E (x, x) = 0 pour x, y ∈ S. Soit t = xyz ∈ S3 un triangle. Depuis Sd (t)> Md (t) et g est strictement décroissante, g (Sd (t)) 6 g (Md (t)), ce qui implique f (Sd (t)) Sd (t) 6 f (Md ( t)) Md (t). Puisque f est une fonction strictement croissante que nous avons Se (t) = f (Sd (t)) et moi (t) = f (Md (t)). Cela nous permet d'écrire: Se (t) Me (t) = f (Sd (t)) f (Md (t)) 6 Sd (t) Md (t). Par conséquent, wd (t) = 1 log2 Sd (t) Md (t) 6 1 log2 Se (t) Me (t) = nous (t). Exemple 2.3. Soit (S, d) un espace de dissimilarité et soit e la dissemblance définie par e (x, y) = d (x, y) r, avec 0 <r <1. Si f (a) = ar, alors f est strictement croissante et la fonction g: R> 0 - → R> 0 donné par g (a) = {f (a) si a> 0, 0 si a = 0 = {ar-1 si a> 0, 0 si a = 0 est strictement décroissante. Par conséquent, le faible ultramétricité nous (t) est supérieur wd (t), où e (x, y) = (d (x, y)) pour r x, y ∈ S. - 91 - ultramétricité de dissimilarités Exemple 2.4. Soit f: R> 0 - → R> 0 est définie par f (a) = aa + 1. Il est facile de voir que f est strictement croissante sur R> 0 et g (a) = {1 1 + a si a> 0, 0 si a = 0 est strictement décroissante sur le même ensemble. Par conséquent, le faible ultramétricité d'un triangle augmente lorsque d est remplacé par e donnée par e (x, y) = d (x, y) 1 + d (x, y) pour x, y ∈ S. Exemple 2.5. La Schoenberg transformation d'une dissemblance d décrit en (Deza et Laurent, 1997) est la dissemblance e: S2 - → R> 0 définie par e (x, y) = 1- e-kd (x, y) pour x, y ∈ S. soit f: R> 0 - → R> est la fonction f (a) = 1 - e-ka qui est utilisé dans cette transformation. Il est immédiat que f est une fonction strictement croissante. Pour un> 0 nous avons g (a) = 1-e -ka un, ce qui nous permet d'écrire g '(a) = e-ka (ka + 1) - 1 a2 pour une> 0. En tenant compte de l'inégalité évidente ka + 1 <eka pour k> 0, il en résulte que la fonction g est strictement décroissante. Ainsi, les faibles ultramétricité d'un triangle par rapport à la Schoenberg transformation est supérieure à la ultramétricité faible sous la dissemblance d'origine. 3 Clas sification et ultramétricité L'algorithme k-plus proches voisins (kNN) est une méthode de classification qui est basée sur la mémoire et ne nécessite pas un modèle à ajustement. La classification est décidée selon une décision à la majorité simple parmi les plus échantillons de jeu de la même formation. Nous montrons que les performances de kNN appliquée à un espace de dissemblance (S, d) se dégrade avec l'augmentation de la ultramétricité de d. Cela se produit parce que l'augmentation de ultramétricité parmi les éléments de S favorise l'égalisation des distances. Nous commençons par un espace de dissemblance (S, d) et on obtient une nouvelle dissemblance d '= f (d), où f est l'une des transformations examinées dans la section 2. algorithme 1 encapsule le process ci-dessus. Il fonctionne kNNwith t fois la validation croisée et calcule la matrice de confusion générée pour chaque pli ainsi que l'erreur de classification cumulative de l'espace transformé. Nous limitons la précision de la transformée dissemblance d 'en prenant en compte, comme ob- servi dans (2008 Murtagh et al.) Qui ultramétricité peut diminuer avec l'augmentation de la précision. La limitation de la précision d 'à quelques chiffres décimaux favorise l'égalisation de ces distances. Nous avons utilisé dans nos expériences les ensembles de données disponibles Fisheriris et ionosphère de - 92 - Dan Simovici et al. Algorithme 1: Fonctionne kNN avec fonction de la distance transformée entrée: un espace métrique ou de dissemblance S = (M, d), le nombre de plus proches voisins k, le nombre de plis t et une fonction f, telle que f (d) = d ' et u <= u 'où u et u' sont les ultrametricities de S et S '= (M, D'), respectivement. Résultat: L'erreur de classification cumulative de l'espace transformé S 'd' ← f (d), limitée à une certaine partitionM de précision décimal en sous-échantillons de t pour i = 1 tot do formation = partition (i) Test .Formation = partition (i). testSize essai (i) = taille (test) kNN (formation, test, k, d ') err (i) = #misclassified objets retour cerr = somme (err) / somme (testsSize) Diss. Iris ionosphère cancer de l'ovaire k = 3 k = 5 k = 7 k = 3 k = 5 k = 7 k = 3 k = 5 k = 7 d 0,0467 0,0427 0,3860 0,1033 0,3701 0,3852 0,1403 0,1394 0,1431 0,0753 0,0567 D0.1 0,3875 0,1187 0,4097 0,3897 0,1454 0,1431 0,1477 0,2900 0,3000 d0.01 0,5211 0,2700 0,5239 0,5365 0,3574 0,3181 0,3000 TAB. 1: Moyenne de 10 calculs de l'erreur de classification produit par kNN en utilisant une validation croisée t-fiée fois stratigraphique, pour différentes valeurs de k et t = 10. ensembles https://archive.ics.uci.edu/ml/data / jeux de données et cancer de l'ovaire ob- CONTENUES du programme clinique Proteomics FDA NCI Databank (http://home.ccr.cancer.gov/ncifdaproteomics/ppatterns.asp). Nos expériences considérées comme un espace euclidien initial (S, d) où S correspond à l'un des ensembles de données décrits ci-dessus et D à la distance euclidienne. Nous avons d'abord testé notre méthode sur l'espace original et comparé les résultats avec les résultats générés par l'augmentation de ultramétricité de dissemblance d '= f (d), où f (a) = ar pour a> 0. On utilise à la fois t kNN -fois la validation croisée et avec stratifié t fois la validation croisée (où chaque pli a à peu près la même taille et à peu près les mêmes proportions de classe comme dans tout l'ensemble de données). Les distances transformées étaient limitées à 2 chiffres de précision décimale. L'erreur de classification obtenu est toujours plus élevée pour le cas de l'espace transformé (S, d '), dans les deux scénarios de validation. Dans le tableau 2, nous montrons les résultats pour trois valeurs de k (le nombre de voisins) en stratifié validation 10 fois. Des résultats similaires sont obtenus pour 5 plis dans les deux scénarios de validation. - 93 - ultramétricité dissemblances 4 L'impact de ultramétricité sur Cluster et séparation Compacité clustering évalue la validation et évalue la bonté des résultats d'un algorithme de clustering (Maulik et Bandyopadhyay, 2002). Nous avons utilisé des mesures de validation internes qui reposent sur des informations contenues dans les données, à savoir et de la compacité et de la séparation (Tang et al., 2005) (Tang et al., 2005; Zhao et Karypis, 2002). mesures Compacité bien quantifier les objets liés dans un clust er sont. Il fournit des informations sur la cohésion des objets dans un cluster individuel par rapport aux autres objets en dehors du cluster. Un groupe de mesures évaluer la compacité du cluster en fonction de la variance où des valeurs plus faibles indiquent une meilleure compacité. D'autres mesures sont basées sur la distance, par exemple au maximum ou une distance de moyenne par paires, et maximum ou de la distance en fonction du centre de la moyenne. La séparation est une mesure de caractère distinctif entre un groupe et le reste du monde. Les distances entre les centres de paires clusters ou les distances minimales entre les objets par paires dans différents groupes sont souvent utilisés comme des mesures de séparation. La compacité de chaque groupe a été évaluée en utilisant la dissemblance moyenne entre les observations du cluster et le médoïde du cluster. La séparation a été calculée en utilisant la dissemblance minimale entre une observation du groupe et une observation d'un autre groupe. Nous étudions l'impact de ultramétricité sur la compacité et la séparation des grappes en utilisant la partition autour de l'algorithme Medoids (PAM) (Kaufman et Rousseeuw, 1990) à se regrouper objets à l'origine dans l'espace euclidien et plus tard dans un espace de dissemblance transformé avec ultramétricité inférieur ou supérieur. Les expériences montrent que la transformation de la matrice de distance qui diminue l'ultra- metricity de l'espace euclidien d'origine peut effectivement améliorer la compacité, mais aussi de réduire la séparation des agrégats générés par PAM. Cependant, la compacité améliore à un rapport plus rapidement que la diminution de la séparation. Nous avons également observé que l'augmentation de la pro- ultramétricité duces l'effet inverse, la compacité dégradante et la séparation de plus en plus, à différents rapports. Dans ce cas, la compacité diminue dans un rapport plus rapidement que l'augmentation de la séparation. Soit (S, d) un espace de dissimilarité, (S, D ') l'espace de dissimilarité transformée, où d' = f (d) est obtenue en appliquant l'une des transformations décrites dans la section 2 et soit u et u 'soit les faibles ultrametricities de ces deux espaces de dissimilarité, respectivement. L'augmentation de ultramétricité de (S, d) à (S, d ') favorise l'égalisation des valeurs de dissimi- larité. Dans le cas de l'extrême, nous avons un espace ultramétrique où les distances par paires impliquées dans tous les triplets de points forment un triangle équilatéral ou isocèle. Explorer comment l'égalisation (ou le processus inverse) peuvent affecter la qualité de regroupement, une meilleure étude des ef- fets de l'augmentation (ou diminution) ultramétricité sur les résultats générés par un algorithme de clustering largement connu et robuste a été réalisée. Afin d'étudier l'impact de ultramétricité sur la compacité du cluster et la séparation, nous avons mis en place un algorithme qui exécute PAM sur les espaces d'origine et transformées, et calcule les mesure pour chaque groupe de S et S '. Nos expériences considérées comme un espace euclidien initial (S, d) où S correspond à un ensemble d'objets et d à la distance Minkowski avec l'exposant 2. Pour obtenir une comparaison valable de compacité et de la séparation, les grappes obtenues à partir d'un ensemble de données spécifiques doivent S contenir les mêmes éléments dans les espaces d'origine et transformées. - 94 - Dan Simovici et al. Dissemblances dx où x> 1 ont tendance à diminuer la ultramétricité de l'espace d'origine, alors que dissemblances où 0 <x <1 ont tendance à augmenter ultramétricité. mesures de validation des clusters existants et les critères actuels peuvent être affectés par diverses caractéristiques de données (Liu et al., 2010). Par exemple, les données à densité variable est difficile pour les algorithmes de regroupement plu- sieurs. Il est connu que k-moyens souffre d'un effet d'uniformisation qui tend à diviser les objets dans des tailles relativement égales (Xiong et al., 2009). De même, k-means et PAM ne sont pas une bonne performance lorsqu'ils traitent avec des ensembles de données de distribution asymétrique où les grappes ont des tailles inégales. Pour déterminer l'impact de ultramétricité en présence de l'une de ces caractéristiques, les expériences ont été réalisées compte tenu de 3 aspects différents de données: bonne separat ion, la densité et les distributions asymétriques en trois ensembles de données synthétiques nommées WellSeparated, DifferentDensity et SkewDistribution, respectivement. La figure 1 montre les données de synthèse qui a été généré pour chaque aspect. Chaque jeu de données contient 300 objets. Tableaux 2 montre les résultats pour les ensembles de données WellSeparated, DifferentDensity bution et SkewDistri-, respectivement. La mesure (compacité ou de séparation) Le ratio est calculé en divisant la mesure de l'espace transformé par la mesure de l'espace d'origine. Le com- ratio de mesure moyenne imputées au 3 groupes est présentée dans chaque tableau. On notera que le rapport de mesure moyenne est inférieure à un pour les espaces avec ité inférieure ultrametric- (obtenus avec dissemblances d5 et d10). Dans ce cas, le taux de compacité moyenne est également plus faible que le rapport moyen de séparation, qui montre que les transformations générées dissemblances de cluster intra- qui rétréci plus que les inter-grappes, par rapport aux différences d'origine. Dans les espaces avec ultramétricité supérieur (obtenu avec dissemblances D0.1 et d0.01), le rapport de mesure moyenne est supérieure à un. Le rapport de la compacité moyenne est également plus élevé que le rapport de séparation moyenne, montrant que les transformations ont généré des similitudes dis- intra-grappe qui ont élargi plus que les inter-munitions. Cela explique l'effet de l'égalisation obtenue avec l'augmentation ultramétricité. (A) bien séparés (b) densité différente (c) Figure Skewed de distribution. 1: données synthétiques contenant 3 éléments de données différentes: 1a: une bonne séparation, 1b et 1c densité différente: les distributions asymétriques figures 2a, 2b et 2c montrent la relation entre la compacité d'un taux de séparation pour chaque jeu de données. Sur la figure 2 on montre la relation entre les rapports de compacité et de séparation pour les trois ensembles de données synthétiques et pour l'ensemble de données Fisheriris qui présentent des modèles de variation similaires. Comme mentionné précédemment, les données avec des caractéristiques telles que différentes densités et différentes tailles de cluster pourraient imposer un défi pour plusieurs algorithmes de regroupement. - 95 - ultramétricité de dissemblances Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.159852 0.462208 d10 6.782346E-008 0,00000036 0,000004433 006 2.113074E-d5 0,000150339 0,00082451 0,002830298 0,006041492 5,493823933 D0.1 0,835946 0,955770 0,973616 2,073671831 6,433067888 d0.01 0,995943 2.161429967 Résultats pour un ensemble de données avec des grappes bien séparées Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.226611 0.883006 d10 0,000000299 1.225126E-006 0,0085821266 0,0067414224 d5 0,000414 0,001758 0,120677 0,101145 3,829475 1,019217 D0.1 1,247117 0,862157 0,968235 4,302930 1,002328 d0.01 1.234965 Résultats pour un ensemble de données avec des grappes avec des densités variées Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.152911 1.088650 d10 5.001356E-005 0,0001674944 0,0202263733 0,0185757406 d5 0,001707 0,005744 0,240466 0,220866 7,502117 1,042825 D0.1 0,957924 0,815746 0,966675 9,123531 1,004683 d0.01 0.922859 Résultats pour un ensemble de données de distributions asymétriques. Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 2.564313e-01 2.841621e-01-D10 4.495584e 07 1.753134e-06 1.171608e-05 4.123026e-05-d5 7.628527e 04 2.974881e-03 4.583216e-03 1.612888e-02 D0.1-01 8.664974e 3.379062e + 00 8.715969e-01 3.067252e + 00 d0.01 9.630195e-01 3.755467e + 00 9.858841e-01 3.469442e + 00 Résultats pour l'ensemble de données Fisheriris. LANGUETTE. 2: compacité Cluster et la séparation en utilisant PAM sur trois ensembles de données synthétiques et Fish- eriris. dissemblances. Les deux moyennes de rapport sont calculés par rapport à l'ensemble de données com- grappe valeurs de compacité et de séparation donnés par le d de dissimilarité d'origine. - 96 - Dan Simovici et al. (A) WellSeparated Data Set (b) DifferentDensity Data Set (c) SkewDistr Data Set (d) Fisheriris Data Set Fig. 2: Relation entre les taux de compacité et de séparation pour trois ensembles de données de synthèse et pour la Fishe riris ensemble de données Nous montrons un scénario dans lequel PAM, lorsqu'il est appliqué à l'espace euclidien d'origine, ne fonctionne pas bien. Néanmoins, nous sommes en mesure d'améliorer les résultats du PAM en appliquant une trans- formation qui diminue la ultramétricité de l'espace d'origine et en cours d'exécution PAM sur l'espace formé de trans. Considérons l'ensemble de données présenté sur la figure 3a qui a été produite par synthèse dans un espace euclidienne par paire avec distance d par trois distributions normales avec viation de- type similaire, mais des densités différentes. Il a 300 points au total, avec le groupe le plus dense comprenant 200 points et les deux autres contenant 75 et 25 points. Notez que les groupes un peu clairsemés sont également situés très près les uns des autres. Différents symboles sont utilisés pour identifier les trois distributions distinctes. la fonction objectif de PAM tente de minimiser la somme des dissemblances de tous les objets à leur plus proche médoïde. Cependant, il peut ne pas partitionner les données dans les distributions d'origine lorsqu'ils traitent avec les données de densité différentes depuis la scission du groupe les plus denses peuvent se produire. Dans notre exemple, PAM fait exactement cela et combine aussi les deux groupes rares qui ne sont pas bien séparés. Notez que k-means contrairement à (qui effectuent aussi ne pas bien dans ces scénarios, mais peuvent éventuellement trouver la bonne partition - 97 - ultramétricité de dissemblances en raison du caractère aléatoire de la sélection des centres de gravité), PAM sera échoue le plus probable en raison du déterminisme de son BUILD et les étapes de SWAP combinées et le choix de la fonction objective. Pour explorer l'effet positif de l'augmentation de la compacité intra-groupe généré par de nouveaux espaces avec ultramétricité plus bas sur les données contenant ces caractéristiques, nous avons appliqué les mêmes transformations avec des exposants entiers positifs à la matrice ob- CONTENUES de distance euclidienne originale d. Les résultats montrent une amélioration significative du regroupement. La figure 3b montre le résultat de l'application PAM pour regrouper les données synthétiques avec dissemblance d. Notez que le résultat de Tering assorti- ne correspond pas à une partition ressemblant à des distributions qui ont été utilisées pour générer les données. Les figures 3d et 3c montrent que PAM omet également de fournir une bonne partition avec dissemblances; d 0,1 et d 0,01 puisque l'augmentation ultramétricité favorise l'égalisation des dissemblances qui peut dégrader encore plus les résultats. Notez cependant que les partitions obtenues par PAM en utilisant la d5 et la forme d10 dissemblances groupes similaires à ceux géné- erated par les distributions originales. En effet, l'augmentation de la compacité aide PAM à créer des limites qui sont conformes aux distributions normales d'origine. (A) de synthèse des données (b) d (c) d 0,01 (d) d 0,1 (e) d 5 (f) d 10 FIG. 3: La figure 3a montre les données synthétiques générées à partir de distributions de densité différente. 3b à 3f montrent les résultats de l'APM en utilisant la distance euclidienne d et d'autres dissemblances obtenus par des transformations sur d. Le tableau 3 montre les mesures et ratios pour cet ensemble de données. La figure 4 montre la relation entre les rapports de compacité et de séparation. 5 Conclusions et Poursuite des travaux Nous avons examiné l'influence de ultramétricité des espaces de dissemblance en ce qui concerne le classement et le regroupement. - 98 - Dan Simovici et al. Diss. Compacité Compacité Séparation Séparation Avg. Ratio moyen. Moy. Ratio moyen. d 0.138692 0.460486 d10 1.295368e-09 9.339889e-09 0,011426 0,024814 d5 2.868980e-05 2.068598e-04 0,104837 0,227665 D0.1 0,842801 6,076787 0,816082 1,772218 d0.01 0,974571 7,026878 0,978284 2,124458 TAB. 3: Jeu de données comprenant des grappes ayant une densité différente. FIGUE. 4: Relation entre Compacité et ratios de séparation pour les données test Nous avons montré qu'il existe une variation inverse entre ultramétricité et performances indiquées des classificateurs. L'augmentation de cette mesure, obtenue par des transformations appliquées à l'espace d'origine, favorise l'égalisation des distances. Cette égalisation augmente le niveau d'incertitude au cours du processus de classification et dégrade la qualité du résu LTS généré par des classificateurs. Pour le regroupement, l'augmentation de ultramétricité génère clusterings avec une meilleure séparation. Comment- jamais, il diminue également la compacité plus rapide que l'augmentation de la séparation. L'abaissement metricity ultra- produit des grappes plus compactes, mais pas aussi bien séparés que dans l'espace d'origine. Dans ce cas, la compacité croît à un taux plus rapide que la diminution de la séparation. Il existe de nombreuses applications qui peuvent bénéficier de cette étude. Par exemple, en changeant le ultramétricité de l'espace d'origine peut aider à trouver des modèles dans les données qui ne sont pas conformes au comportement attendu, dans un exemple classique de détection des anomalies. L'impact de trametricity sur différents algorithmes ul- de classification hiérarchique semble également un sujet prometteur de l'enquête. Références Deza, M. M. et M. Laurent (1997). La géométrie des coupes et des mesures. Heidelberg: Springer. - 99 - ultramétricité de dissemblances Kaufman, L. et P. J. Rousseeuw (1990). Groupes de données dans Finding - Introduction à l'analyse Cluster. New York: John Wiley & Sons. Lerman, I. C. (1981). Classification et analyse des Ordinale Données. Paris: Dunod. Liu, Y., Z. Li, H. Xiong, X. Gao et J. Wu (2010). La compréhension des mesures de validation des clusters internes. En 2010 IEEE 10e Conférence internationale sur l'exploration de données, p. 911-916. IEEE. Maulik, U. S. et Bandyopadhyay (2002). Évaluation de la performance de certains clusters et indices algorithmes de validité. IEEE Transactions sur le modèle d'analyse et de la machine Intelli gence 24 (12), 1650-1654. Murtagh, F., G. Downs, et P. Contreras (2008). classification hiérarchique des ensembles de données tridimensionnelles élevées massives, en exploitant l'incorporation ultramétrique. SIAM Journal sur le calcul scientifique 30 (2), 707-730. Rammal, R., J. C. A. d'Auriac, et D. Douçot (1985). Sur le degré de ultramétricité. Le Journal de Physique - Letteres 45, 945-952. Simovici, D. A. et C. Djeraba (2014). Outils mathématiques pour l'exploration de données (deuxième éd.). London: Springer. Tang, P., M. Steinbach et V. Kumar (2005). Introduction à l'exploration de données. Lecture, MA: Addison-Wesley. Xiong, H., J. Wu, et J. Chen (2009). K-means par rapport à la validation des mesures: une perspective de distribution de données. IEEE Transactions on Systems, Man et Cybernétique, Partie B: Cybernétique 39 (2), 318-331. Zhao, Y. et G. Karypis (2002). L'évaluation des algorithmes de classification hiérarchique pour les ensembles de données de docu- ment. Dans Actes de la onzième Conférence internationale sur l'information et la gestion des connaissances, pp. 515-524. ACM. Nous introduisons CV Une ultramétricité Mesure d'affluer les dissimilaritées et les transformations des examinons dissimilaritées et sur l'impact their this mesure. Salle de bains, l'influence NOUS étudions de L'ultramétricité-sur-la de Deux cours COMPORTEMENT d'exploration de d'algorithmes Données (le kNN de classification et Algorithme l'algorithme de PAM Regroupement) sur les appli- QUES espaces de dissimilarité. Sur exists Une montre variation Qu'il inverse Entre ultramé- tricité et la performances des classificateurs. Pour les groupes, Une augmentation d'ultramétricité genere with a better regroupements séparation. Une diminution de la ultramétricité pro- duit, plus compacts Groupes. - 100 - B - Classification, Clustering, Similarité ultramétricité des espaces dissimilitude et son importance pour l'exploration de données Dan Simovici, Rosanne Vetro, Kaixun Hua