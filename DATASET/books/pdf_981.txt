 FuzzyClustering_EGCnew4 DVIUne nouvelle approche de la programmation DC et DCA pour la classification floue Le Thi Hoai An Le Hoai Minh Pham Dinh Tao LITA UFR MIM Université Paul Verlaine Metz Ile du Saulcy 57045 Metz Cedex France lethi univ metz fr lita sciences univ metz fr lethi LITA UFR MIM Université Paul Verlaine Metz Ile du Saulcy 57045 Metz Cedex France lehoai univ metz fr LMI INSA de Rouen BP 08 Place Emile Blondel 76131 Mont Saint Aignan Cedex France pham insa rouen fr Résumé Dans cet article nous nous intéressons à Fuzzy C Means FCM une technique très connue pour la classification floue Nous proposons un algorithme efficace basé sur la programmation DC Difference of Convexe functions et DCA DC Algorithm pour résoudre ce problème Les expéri ences numériques comparatives avec l’algorithme standard FCM sur les don nées réelles montrent la robustesse la performance de cet nouvel algorithme DCA et sa supériorité par rapport à FCM 1 Introduction Le problème de classification automatique clustering est considéré comme une des problématiques majeures en extraction des connaissances à partir de données Parmi les techniques de classification la classification floue fuzzy via Fuzzy C Means FCM est très connue FCM a été introduite par Jim Bezdek en 1981 Bezdek 1981 comme une amélioration des méthodes clustering précédentes et a été beaucoup développée dans les années 90 Cette approche a été appliquée avec succès dans plusieurs problèmes diagnostic médical Whitwell 2005 classification de textes Rodrigues and Sacks 2004 et est de plus en plus utilisé dans le domaine du data mining Dans un travail récent Le Thi et al 3 2006 nous avons formulé le modèle de FCM pour la classification floue sous la forme d’un programme DC Difference of Convexe func tions et développé un schéma de DCA DC Algorithm pour sa résolution numérique La programmation DC et DCA ont été introduits par Pham Dinh Tao en 1985 et intensivement développés par Le Thi Hoai An et Pham Dinh Tao depuis 1994 voir Le Thi Hoai An 1997 Le Thi et al 2 2006 Pham Dinh Tao and Le Thi Hoai An 1997 Pham Dinh Tao and Le Thi Hoai An 1998 et leurs références pour devenir maintenant classiques et de plus en plus populaire Ils ont été appliqués avec succès à nombreux problèmes d’optimisa tion non convexe différentiable ou non de grande dimension dans différents domaines des sciences appliquées en particulier aux problèmes du data mining voir par exemple Le Thi et al 1 2006 Le Thi et al 2 2006 Liu et al 2003 Neumann et al 2004 Weber et al 2005 Les résultats numériques présentés dans Le Thi et al 3 2006 montrent que comme pour les autres problèmes déjà traités en data mining DCA est efficace pour FCM Ils prouvent également la superiorité de DCA par rapport à K means Cet algorithme est itératif et consiste en la résolution d’un programme convexe à chaque itération Le temps de calculs de DCA est donc proportionnel à celui de la méthode utilisée pour résoudre les programmes convexes générés Dans Le Thi et al 3 2006 l’algorithme du gradient pro La programmation DC et DCA pour la classification floue jeté a été utilisé du fait que la projection en question est explicite même s’il est connu pour être lent Sans doute qu’avec d’autres décompositions DC on peut améliorer DCA pour la résolution de FCM L’objectif de ce travail est de développer un nouveau schéma de DCA dans lequel la résolution des problèmes convexes générés est moins coûteux Nous proposons une autre décomposition DC qui donne naissance à un DCA très simple dont les calculs sont explicites à chaque itération le sous problème convexe est en fait la projection d’un point sur une boule Les expériences numériques comparatives entre FCM et l’algorithme DCA étudié dans Le Thi et al 3 2006 sur les données réelles montrent la robustesse la performance de cette nouvelle version de DCA et sa supériorité par rapport à FCM Le papier est organisé de la façon suivante Dans la deuxième section nous présentons la formulation du problème FCM La résolution de ce probème par la programmation DC et DCA est étudiée dans la troisième section Finalement les résultats numériques de nos algorithmes DCA et FCM sont rapportés dans la dernière section 2 Une nouvelle formulation du modèle de FCM Soit X = {x1 x2 xn} l’ensemble de n points à classer Chaque point xi est un vecteur dans l’espace IRp Nous avons à classer ces n points dans c 2 ≤ c ≤ n classes différentes Considérons une matrice de pourcentage U de taille c×n dont chaque élément ui k définit le pourcentage d’appartenance d’un point xk à la classe Ci Il est clair que ui k ∈ [0 1] pour i = 1 c k = 1 n c∑ i=1 ui k = 1 pour k = 1 n 1 Si la matrice de pourcentage U est déterminée on en déduit la classification selon la règle suivante le point xk pour k = 1 n est classé dans la classe Ci pour i = 1 c si et seulement si ui k = max{uj k j ∈ {1 c}} Considérons la fonction Jm définie par Jm U V = n∑ k=1 c∑ i=1 umi k||xk − vi||2 2 où ‖ ‖ désigne dans tout le papier la norme Euclidienne de l’espace correspondant V est une c× p matrice dont chaque ligne vi correspond au centre de la classe Ci et m ≥ 1 un paramètre entier qui définit le degré de flou du modèle Chercher une classification revient ainsi chercher la matrice de pourcentage U et les centres vi Le modèle mathématique de FCM s’écrit ainsi    minJm U V = n∑ k=1 c∑ i=1 umi k||xk − vi||2 sous contraintes ui k ∈ [0 1] pour i = 1 c k = 1 n c∑ i=1 ui k = 1 k = 1 n 3 où seule la variable U est a priori bornée En fait on peut aussi restreindre la variable V à un domaine borné En effet la condition nécessaire d’optimalité du premier ordre en U V implique ∇V Jm U V = 0 Le Thi Hoai An et al i e ∂viJm U V = n∑ k=1 umi k2 vi − xk pour i = 1 c k = 1 n ou vi n∑ k=1 umi k = n∑ k=1 umi kxk D’autre part la non vacuité des classes assure que n∑ k=1 umi k > 0 pour tout i = 1 c Par suite ‖vi‖2 ≤ n∑ k=1 umi k maxk=1 n ‖xk‖ 2 n∑ k=1 umi k 2 = max2k=1 n‖xk‖ = r2 Considérons les nouvelles variables ti k telles que ui k = t2i k La contrainte c∑ i=1 ui k = 1 devient c∑ i=1 t2i k = 1 ou ‖tk‖ 2 = 1 avec tk ∈ IRc Soient Sk la sphère de rayon 1 dans IRc et Ri la boule Euclidienne de rayon r dans IRp on peut reformuler le problème FCM comme    minJ2m T V = n∑ k=1 c∑ i=1 t2mi k ||xk − vi||2 sous contraintes T ∈ S = Πnk=1 Sk V ∈ C = Πci=1Ri 4 Ce dernier est un problème d’optimisation non convexe dont la résolution sera décrite dans la suite Remarque Le changement de variables en ti k nous amène à travailler sur S le domaine réalisable des variables ti k qui est le produit des sphères et non le produit des simplexes comme dans le cas des variables initiales ui k On pourrait penser alors que la non convexité de S rend le problème plus difficile qu’avec sa formulation initiale Mais ce n’est pas le cas comme on verra dans la suite car le problème 4 sera reformulé sous une forme équivalente où S est remplacé par le produit des boules de rayon 1 Ce qui est intéressant tant sur le plan algorithmique que numérique la nouvelle formulation DC de 4 donne naissance à un schéma DCA extrêmement simple qui ne nécessite que des calculs explicites et donc non coûteux Puisqu’il s’agit des calculs des projections d’un point sur une boule Euclidienne à chaque itération 3 La programmation DC et DCA pour la résolution de FCM Pour faciliter la compréhension de notre approche nous présentons en premier lieu de cette section une brève description de la programmation DC et DCA 3 1 Introduction à la programmation DC et DCA La programmation DC joue un rôle central en programmation non convexe différen tiable ou non car la quasi totalité des problèmes d’optimisation de la vie courante est de nature DC Elle connaît des développements spectaculaires au cours de cette dernière décen nie DCA est une méthode de descente de type primal dual sans recherche linéaire pour la résolution d’un programme DC de la forme α = inf{f x = g x − h x x ∈ IRp} 5 La programmation DC et DCA pour la classification floue où g h sont les fonctions convexes semi continues inférieurement et propres sur IRp Une telle fonction f est appelée fonction DC et les fonctions convexes g et h les composantes DC de f Il est à noter que la minimisation d’une fonction DC sur un ensemble convexe fermé C de IRp se ramène à un problème de type 5 car la contrainte x ∈ C peut être incorporée dans la fonction objectif à l’aide de la fonction indicatrice χC définie par χC = 0 si x ∈ C +∞ sinon Lorsqu’une de ses composantes DC est polyédrale la fonction f est dite DC polyédrale et le programme DC correspondant DC polyédral La programmation DC polyédrale joue un rôle crucial en programmation non convexe La congugaison d’une fonction convexe g notée g est définie par g y = sup{〈x y〉 − g x x ∈ IRp} La dualité DC est définie via la conjugaison des composantes DC et le programme dual de 5 est donné par ici l’espace dual de IRp est identifié à lui même αD = inf{h y − g y y ∈ IRp} 6 Puisque chaque fonction h ∈ Γ0 IRp est caractérisée comme le supremum d’une famille finie des fonctions affines c à d h x = sup{〈x y〉 − h y y ∈ IRp} on a α = inf{g x − sup{〈x y〉 − h y y ∈ IRp} x ∈ IRp} = inf{α y y ∈ IRp} où α y = inf{g x − [〈x y〉 − h y ] x ∈ IRp} Py Il est clair que Py est un programme convexe et α y = h y − g y si y ∈ dom h et +∞ sinon 7 Par suite α = inf{h y − g y y ∈ dom h } Finallement on obtient avec la convention naturelle +∞− +∞ = +∞ α = αD = inf{h y − g y y ∈ IRp} On observe ainsi la symétrie parfaite entre les programmes DC primal et dual le dual de 6 est exactement 5 Le transport des solutions optimales globales entre l’ensemble des solutions optimales P de 5 et celui de 6 noté D s’exprime de la manière suivante Le Thi Hoai An and Pham Dinh Tao 2005 Pham Dinh Tao and Le Thi Hoai An 1997 ∪{∂h x x ∈ P} ⊂ D et ∪ {∂g y y ∈ D} ⊂ P 8 La relation 8 indique que la résolution d’un programme DC implique celle de son dual D’autre part ce transport reste valable entre les ensembles des solutions locales de 5 et 6 sous certaines hypothèses techniques En analyse convexe Rockafellar 1976 Urruty and Lemarechal 1993 ∂h x0 = {y ∈ IRp h x ≥ h x0 + 〈x− x0 y〉 ∀x ∈ IRp} Le Thi Hoai An et al est appelé le sous différentiel de h au point x0 Tout élément de ∂h x0 est appelé gradient de h en x0 Le sous différentiel ∂h x0 est une partie convexe fermée qui coincide avec le gradient ∇h x0 si et seulement h est différentiable en x0 Pour un � > 0 le �− sous différentiel de h est défini par ∂�h x0 = {y ∈ IRp h x ≥ h x0 + 〈x− x0 y〉 − � ∀x ∈ IRp} L’égalité des valeurs optimales des programmes primal et dual 5 et 6 peut être traduite de manière équivalente par P = {x ∂�h x ⊂ ∂�g x ∀� > 0} Mais sauf des cas très rares cette condition d’optimalité globale est impraticable Nous nous intéressons dès lors aux conditions d’optimalité locale pour les programmes DC voir Le Thi Hoai An 1997 Le Thi Hoai An and Pham Dinh Tao 2005 Pham Dinh Tao and Le Thi Hoai An 1997 Pham Dinh Tao and Le Thi Hoai An 1998 Le Thi Hoai An and Pham Dinh Tao 2003 et références inclues ∂h x ⊂ ∂g x 9 et ∂h x ∩ ∂g x 6= ∅ 10 Un tel point x vérifiant 10 est appelé point critique de g − h La condition nécessaire d’optimalité locale 9 est également suffisante dans plusieurs cas rencontrés en pratique par exemple quand la fonction objectif f = g−h est DC polyé drale avec h polyédrale ou quand f est localement convexe en x Basé sur les conditions d’optimalité locale et la dualité DC DCA consiste en la con struction de deux suites {xk} et {yk} candidats respectifs aux solutions des problèmes primal et dual que l’on améliore à chaque itération les deux suites {g xk − h xk } et {h yk − g yk } sont décroissantes et qui convergent vers des solutions primale et duale x et y vérifiant des conditions d’optimalité locale Le schéma général de DCA prend la forme yk ∈ ∂h xk xk+1 ∈ ∂g yk 11 La première interprétation de DCA est simple à chaque itération on remplace dans le programme DC primal la deuxième composante DC h par sa minorante affine hk x = h xk + 〈x− xk yk〉 au voisinage de xk pour obtenir le programme convexe suivant inf{g x − hk x x ∈ IRp} 12 dont l’ensemble des solutions optimales n’est autre que ∂g yk De manière analogue la deuxième composante DC g du programme DC dual 6 est remplacée par sa minorante affine g k y = g yk + 〈y − yk xk+1〉 au voisinage de yk pour donner naissance au programme convexe inf{h y − g k y y ∈ IRp} 13 dont ∂h xk+1 est l’ensemble des solutions optimales DCA opère ainsi une double linéari sation à l’aide des sous gradients de h et g Il est à noter que DCA travaille avec les com posantes DC g et h et non pas avec la fonction f elle même Chaque décomposition DC de f donne naissance à un DCA Pour un programme DC donné la question de décomposition DC optimale reste ouverte en pratique on cherche des décompositions DC bien adaptées à la structure spécifiques du programme DC étudié pour lesquelles les suites {xk} et {yk} La programmation DC et DCA pour la classification floue sont faciles à calculer si possible explicites pour que les DCA correspondants soient moins coûteux en temps et par conséquent capables de supporter de très grandes dimensions La convergence de DCA Le Thi Hoai An and Pham Dinh Tao 1997 Le Thi Hoai An and Pham Dinh Tao 2005 Pham Dinh Tao and Le Thi Hoai An 1997 Pham Dinh Tao and Le Thi Hoai An 1998 Le Thi Hoai An and Pham Dinh Tao 2003 Soient C resp D l’ensemble convexe qui contient la suite {xk} resp {yk} et ρ g C ou ρ g si C = IRp défini par ρ g C = sup { ρ ≥ 0 g − ρ 2 ‖ · ‖2 soit convexe sur C } DCA est une méthodes de descente sans recherche linéaire qui possède les propriétés suivantes i Les suites {g xk − h xk } et {h yk − g yk } sont décroisantes et • g xk+1 − h xk+1 = g xk − h xk ssi yk ∈ ∂g xk ∩ ∂h xk yk ∈ ∂g xk+1 ∩ ∂h xk+1 et [ρ g C + ρ h C ]‖xk+1 − xk‖ = 0 De plus si g ou h est strictement convexe sur C alors xk = xk+1 Dans ce cas DCA se termine à l’itération k convergence finie de DCA • h yk+1 − g yk+1 = h yk − g yk ssi xk+1 ∈ ∂g yk ∩ ∂h yk xk+1 ∈ ∂g yk+1 ∩ ∂h yk+1 et [ρ g D + ρ h D ]‖yk+1 − yk‖ = 0 De plus si g ou h est strictement convexe sur D alors yk+1 = yk Dans ce cas DCA se termine à l’itération k convergence finie de DCA ii Si ρ g C + ρ h C > 0 resp ρ g D + ρ h D > 0 alors la série {‖xk+1 − xk‖2 resp {‖yk+1 − yk‖2} converge iii Si la valeur optimale α du problème 5 est finie et deux suites {xk} et {yk} sont bornées alors tout valeur d’adhérence x̃ resp ỹ de la suite {xk} resp {yk} est le point critique de g − h resp h − g iv DCA a la convergence linéaire pour les programmes DC généraux v DCA a la convergence finie pour les programmes DC polyédraux Pour une étude complète de la programmation DC et DCA se référer aux Le Thi Hoai An 1997 Le Thi Hoai An and Pham Dinh Tao 2005 Pham Dinh Tao and Le Thi Hoai An 1997 Pham Dinh Tao and Le Thi Hoai An 1998 et références incluses Il est à noter que la recherche d’une décomposition DC adéquate et celle d’un bon point initial sont deux tâches importantes dans la résolution d’un programme non convexe par DCA car elles conditionnent la réussite du résultant DCA 3 2 Nouvelle formulation DC de FCM Dans toute la suite nous utilisons la présentation matricielle qui nous semble plus co mode sachant que l’on peut identifier une matrice et un vecteur par ligne ou par colonne La fonction objectif de 4 peut s’écrire de la manière suivante J2m T V = ρ 2 ‖T‖2 + ρ 2 ‖V ‖2 − [ρ 2 ‖ T V ‖2 − J2m T V ] Pour tout T V ∈ S × C on a J2m T V = ρ 2 n+ ρ 2 ‖V ‖2 −H T V avec H T V = ρ2‖ T V ‖ 2 − J2m T V 14 Dans le lemme suivant nous donnerons les conditions pour que la fonction H soit convexe Le Thi Hoai An et al Lemme soit B = Πnk=1 Bk où Bk est la boule de centre 0 et de rayon 1 dans IR c La fonction H U V est convexe sur B × C pour toute valeur de ρ telle que ρ ≥ m n 2m − 1 α2 + 1 + √[m n 2m − 1 α2 + 1 ]2 + 16 n m2α2 où α = r + max 1≤k≤n ‖xk‖ Preuve on remarque tout d’abord que ρ > 0 car m ≥ 1 Puisque H T V = n∑ k=1 c∑ i=1 [ρ 2 t2i k + ρ 2 ‖vi‖2 − t2mi k ‖xk − vi‖2 ] H est convexe si toutes les fonctions hi k ti k vi = ρ 2 t2i k + ρ 2 ‖vi‖2 − t2mi k ‖xk − vi‖2 i = 1 c k = 1 n sont convexes Considérons la fonction suivante f IR× IR→ IR f x y = ρ2x 2 + ρ2y 2 − x2my2 15 Le Hessien de la fonction f est donné par J x y = ρ − 2m 2m − 1 y2x2m−2−4mx2m−1y −4mx2m−1y ρ n − 2x2m 16 Pour tout x y 0 ≤ x ≤ 1 ‖y‖ ≤ α on a | J x y |= ρ − 2m 2m− 1 y2x2m−2 ρn − 2x 2m − 16m2x4m−2y2 ≥ ρn 2 − [ 2mn 2m− 1 y 2x2m−2 + 2x2m ] ρ − 16m2x4m−2y2 ≥ 1nρ 2 − 2 m n 2m− 1 α 2 + 1 ρ − 16m2α2 Par suite si ρ ≥ m n 2m− 1 α2 + 1 + √[m n 2m− 1 α2 + 1 ]2 + 16 n m2α2 17 alors | J x y |≥ 0 pour tout x y ∈ IR2 tels que 0 ≤ x ≤ 1 | y |≤ α Ainsi avec ρ défini par 17 la fonction f est convexe sur [0 1]× [−α α] Par conséquent les fonctions θi k ti k vi = ρ 2 t2i k + ρ 2 ‖xk − vi‖2 − t2mi k ‖xk − vi‖2 sont convexes sur {0 ≤ ti k ≤ 1 ‖vi‖ ≤ r} avec ρ donné dans 17 et α = r + max1≤k≤n ‖xk‖ Il en est de même pour les fonction hi k car hi k ti k vi = θi k ti k vi + ρ〈xk vi〉 − ρ 2 ‖xk‖2 Ainsi avec les valeurs données ci dessus de ρ et α la fonction H T V est convexe sur B × C Dans toute la suite nous travaillons avec ces valeurs de ρ et α La programmation DC et DCA pour la classification floue Il est clair que pour tout T ∈ B et un V ∈ C fixé la fonction J2m T V est concave en variable T carH T V est convexe par suite son minimum sur B est atteint sur la frontière S de B i e min { ρ 2 ‖V ‖ 2 −H T V T V ∈ B × C } = min { ρ 2 ‖V ‖ 2 −H T V T V ∈ S × C } Le problème 4 peut être alors reformulé comme min {ρ 2 ‖V ‖2 −H T V T V ∈ B × C } ou encore min { χB×C T V + ρ 2 ‖V ‖2 −H T V T V ∈ IRc×n × IRc×p } 18 qui est un programme DC avec la décomposition DC suivante χB×C T V + ρ 2 ‖V ‖2 −H T V = G T V −H T V où G T V = χB×C T V + ρ2 ‖V ‖ 2 19 est bien évidemment une fonction convexe grâce à la convexité de B et C 3 3 Résolution de 18 par DCA Selon la description de DCA dans la section 2 1 la résolution de FCM via la formula tion 18 par DCA consiste en la détermination de deux suites Y l Zl ∈ ∂H T l V l et T l+1 V l+1 ∈ ∂G Y l Zl La fonction H est différentiable et son gradient au point T l V l est calculé de la manière suivante ∇H T l V l = ρ T l V l − 2mt2m−1i k ‖xk − vi‖2 2 n∑ k=1 vi − xk t2mi k 20 Le calcul de T l+1 V l+1 ∈ ∂G Y l Zl se ramène à la résolution du problème suivant voir Section 2 1 min {ρ 2 ‖V ‖2 − 〈 T V Y l Zl 〉 T V ∈ B × C } Il s’en suit que Proj étant l’application de projection T l+1 = Pr ojB Y l V l+1 = Pr ojC 1 ρ Zl Plus précisément V l+1i =    Zl i ρ si ‖ Zl i ‖ ≤ ρr Zl i r ‖ Zl i ‖ sinon i = 1 c 21 et T l+1 k = { Y l k si ‖Y l k‖ ≤ 1 Y l k ‖ Y l k‖ sinon k = 1 n 22 Le Thi Hoai An et al 3 3 1 Schéma DCA Initialisation – Choisir T 0 ∈ IRc×n et V 0 ∈ IRc×p Soit l = 0 – Choisir une tolérance � > 0 Répeter – Calculer Y l Zl ∈ ∇H T l V l à l’aide de 20 – Calculer T l+1 V l+1 ∈ ∂G Y l Zl à l’aide de 21 et 22 – l + 1←− l Jusqu’à ‖ T l+1 V l+1 − U l V l ‖ ≤ � ‖ T l+1 V l+1 ‖ Construction des classes Soient T V la solution calculée par DCA et ui k = t 2i k Le point xk appartient à la classe Ci si uik = max j=1 c uj k 4 Expériences numériques Pour comparer la performance de notre algorithme nous avons réalisé les tests numériques sur deux ensembles des données le premier ensemble de données contient 4 exemples très connus et beaucoup utilisés dans le domaine de classification pour l’évalua tion des algorithmes – PAPILLON un jeu de données connu sous le nom "jeux de papillon" – IRIS IRIS est peut être le plus connu jeu de test dans le domaine de classification Il contient 3 classes chacune a 50 objets – VOTE Congressional Votes dataset Congressional Quarterly Almanac 98th Congress 2nd session 1984 Volume XL Congressional Quarterly Inc Washington D C 1985 – GENE L’ensemble de 384 gènes disponible sur faculty washington edu kayee cluster – ADN L’ensemble de 3186 gènes disponible sur ftp genbank bio net chaque gène est présenté par une séquence de 60 éléments Ces gènes sont classés dans 3 clusters différents donors 767 objets acceptors 765 objets et le rest Le deuxième ensemble de données est composé de deux jeux de données de biopuces ”Yeast” et ”Serum” téléchargeables sur genomics stanford edu ”Yeast” 2945 points gènes dans l’espace de dimension 15 ”Serum” 517 points dans l’espace de dimension 12 voir Dembele et al 2003 pour la description de ces données Les tests ont été réalisés sur un ordinateur de 2 8MHz 512Mb Ram La valeur de � est fixée à 10−7 La valeur de m est égale à 2 pour le premier ensemble de données Pour les données de biopuces nous considérons différentes valeurs de m dans l’intervalle 1 2 il a été prouvé dans Dembele et al 2003 que le choix de m = 2 n’est pas convenable à ces données Dans le Tableau 1 nous comparons notre nouvelle méthode DCA DCA2 avec l’algo rithme DCA développé dans Le Thi et al 3 2006 DCA1 et une implémentation de la méthode FCM téléchargeable sur igbmc u strasbg fr projets fcm Les critères de comparaison sont le temp de calcul en seconds Time le nombre d’iterations Noit et POBC Pourcentage de Objets Bien Classés Nous constatons que dans tous les 4 jeux de données DCA2 donne toujours le meilleur résultat La programmation DC et DCA pour la classification floue Dans les Tableaux 2 et 3 nous présentons les résultats comparatifs de ces trois méthodes sur les données biopuces Les critères de comparaison sont la valeur de Jm U V Jm le coût du cluster CC soit ∑n i=1 mink=1 c ‖xi − vk‖2 le temps de calcul en seconds Time et le nombre d’itération Noit Data DCA1 DCA2 FCM Name n p c Noit Time POBC Noit Time POMC Noit Time POBC PAPILLON 23 4 4 10 0 002 95 7 2 0 001 95 7 18 0 002 95 7 IRIS 150 4 3 23 0 03 92 77 4 0 01 92 77 15 0 03 92 25 VOTE 435 2 2 16 0 05 89 9 4 0 01 92 6 19 0 06 83 7 GENE 384 17 5 16 0 67 88 3 7 0 20 88 9 35 0 73 85 8 ADN 3186 60 3 8 0 78 92 6 0 55 94 25 1 95 89 8 Tableau 1 Résultats comparatifs du premier ensemble de données DCA1 DCA2 FCM m Jm CC N oit Time Jm CC N oit Time Jm CC N oit Time 1 1 23115 64831 33 331 21615 64061 189 33 21712 65868 179 564 1 3 17554 64144 357 64 16789 62681 225 99 16819 62681 543 1886 1 5 10531 43398 54 301 10432 43389 543 167 10652 44367 143 269 1 7 64129 44981 47 197 6126 43792 102 43 6294 43939 44 110 1 9 3676 45012 65 84 3497 43956 101 35 3607 44643 32 77 Tableau 2 Résultats comparatifs de données de biopuces ”Yeast” DCA1 DCA2 FCM m Jm CC Noit Time Jm CC Noit Time Jm CC Noit Time 1 1 1511 11 12237 56 2 3 1511 11 12234 72 1 02 1511 56 13265 198 1 98 1 3 1523 6 9572 78 5 4 1478 6 9572 102 3 4 1554 50 10231 176 5 3 1 5 1645 8642 52 6 7 1586 8034 543 12 1755 9432 69 3 1 1 7 1404 5 6034 41 4 1 1404 5 6021 102 3 2 1415 3 6068 29 1 4 1 9 935 6079 85 2 2 926 6022 101 1 2 935 6079 14 0 7 Tableau 3 Résultats comparatifs de données de biopuces ”Serum” Conclusion Nous avons introduit une nouvelle formulation DC du modèle de FCM pour la classification floue et développé un schéma de DCA pour sa résolution numérique Avec cette décomposition DC notre algorithme itératif est extrêmement simple il consiste en la détermination de la projection d’un point sur une boule Euclidienne qui est explicite et non coûteux Les résultats numériques montrent que comme pour les autres problèmes déjà traités en data mining DCA est efficace pour FCM Ils prouvent la superiorité de DCA non seulement par rapport à l’algorithme FCM standard mais aussi par rapport au schéma de DCA proposé dans Le Thi et al 3 2006 Dans l’étape suivante nous devront exploiter cet algorithme pour la classification des données biologiques en particulier des donnéées de biopuces References Bezdek 1981 Pattern Recognition with Fuzzy Objective Function Algorithm New York NY Plenum Press 1981 Dembélé D Kastner P Fuzzy C means Clustering method for clustering microarray data Bioinformatics Vol 19 No 8 pp 573 580 2003 F Höppner F Klawonn Obtaining Interpretable Fuzzy Models from Fuzzy Clustering and Fuzzy Regression Proc of the 4th Int Conf on Knowledge Based Intelligent Engi neering Systems and Allied Technologies KES Brighton UK pp 162 165 2000 Le Thi Hoai An et al F Höppner F Klawonn Fuzzy Clusteringof Sampled Functions Proc of the 19th Int Conf of the North American Fuzzy Information Processing Society NAFIPS Atlanta USA pp 251 255 2000 F Klawonn F Höppner What is Fuzzy About Fuzzy Clustering – Understanding and Improving the Concept of the Fuzzifier Advances in Intelligent Data Analysis Berlin 2003 254 264 Springer J Neumann C Schnörr G Steidl SVM based Feature Selection by Direct Objective Minimisation Pattern Recognition Proc of 26th DAGM Symposium pp 212 219 LNCS Volume 3175 2004 Le Thi Hoai An Contribution à l’optimisation non convexe et l’optimisation globale Théorie Algorithmes et Applications Habilitation à Diriger des Recherches Université de Rouen 1997 Le Thi Hoai An and Pham Dinh Tao Solving a class of linearly constrained indefinite quadratic problems by DC algorithms Journal of Global Optimization Vol 11 No 3 pp 253 285 1997 Le Thi Hoai An and Pham Dinh Tao Large Scale Molecular Optimization from distances matrices by a DC Optimization approach SIAM J Optimization Vol 14 No1 pp 77 117 2003 Le Thi Hoai An and Pham Dinh Tao The DC difference of convex functions Program ming and DCA revisited with DC models of real world nonconvex optimization problems Annals of Operations Research 2005 Vol 133 pp 23 46 Le Thi Hoai An T Belghiti and Pham Dinh Tao A new efficient algorithm based on DC programming and DCA for Clustering In Press Available July 2006 Journal of Global Optimization Le Thi Hoai An Le Hoai Minh and Pham Dinh Tao Optimization based DC program ming and DCA for Hierarchical Clustering n Press Available online June 2006 Euro pean Journal of Operational Research Le Thi Hoai An Le Hoai Minh Pham Dinh Tao Une approche de la programmation DC pour la Classification floue Actes de XIIIème Rencontres de la Société Francophone de Classification SFC’06 Metz 6 9 Septembre 2006 Pham Dinh Tao and Le Thi Hoai An Convex analysis approach to DC programming Theory Algorithms and Applications Acta Mathematica Vietnamica dedicated to Pro fessor Hoang Tuy on the occasion of his 70th birthday Vol 22 Number 1 1997 pp 289 355 Pham Dinh Tao and Le Thi Hoai An DC optimization algorithms for solving the trust region subproblem SIAM J Optimization Vol 8 pp 476 505 1998 B T Polyak Introduction to optimization Inc Publications Division 1987 Susana Nascimento Boris Mirkin and Fernando Moura Pires Modeling Proportional Membership in Fuzzy Clustering IEEE Transactions on Fuzzy Systems Vol 11 Nř 2 April 2003 Rochafellar R T Convex Analysis Princeton Landmarks in Mathematics and Physics Reprint Princeton University Press M E S Mendes Rodrigues and L Sacks A scalable hierarchical fuzzy clustering algo rithm for text mining In Proc of the 4th International Conference on Recent Advances in Soft Computing RASC’2004 pp 269 274 Nottingham UK Dec 2004 Yufeng LIU Xiaotong SHEN and Hani DOSS Multicategory Multicategory ψ Learning and Support Vector Machine Computational Tools Journal of Computational and Graphical Statistics 14 1 219 236 J B Hiriart Urruty and C Lemaréchal Convex Analysis and Minimization Algorithms Springer Verlag berlin Heidelberg 1993 La programmation DC et DCA pour la classification floue S Weber T Schüle C Schnörr Prior Learning and Convex Concave Regularization of Binary Tomography Electr Notes in Discr Math 20 313 327 2005 Glenn Whitwell Xiao Ying Wang Jonathan M Garibaldi The Application of a Simulated Annealing Fuzzy Clustering Algorithm for Cancer Diagnosis SIP 2005 Japan Summary In this paper a model of Fuzzy C means Clustering FCM one of the most popular and best studied fuzzy clustering measures is discussed A fast and robust algorithm based on DC Difference of Convex functions programming and DCA DC Algorithms is investi gated Preliminary numerical solutions on real world databases show the efficiency and the superiority of the appropriate DCA in both the running time and quality of solutions with respect to the standard FCM algorithm 