Commentaires - Etude et amélioration de la forêt aléatoire de la bibliothèque Mahout dans le contexte des données de marketing d'Orange C. Thao *, **, N. * Voisine, V. Lemaire *, R. Trinquart * * Orange Labs, 2 avenue Pierre Marzin, 22300 Lannion, France ** Predicsis, 5 rue de Broglie, 22300 Lannion, France Résumé. Dans le domaine des systèmes Big Data, Hadoop a émergé comme l'un des systèmes les plus populaires et un écosystème très diversifié a grandi autour d'elle, toutes sortes, satisfait des besoins fonctionnels et techniques. Un créneau qui aurait dû être une place de choix dans cet écosystème sont des données d'analyse: d'abord parce que l'obtention de valeur de grands ensembles de données nécessite des algorithmes d'apprentissage (ML) machine efficace, OND sec- parce que les grandes grappes avec des ressources CPU abondantes semblent comme playfields appropriées pour ML algorithmes qui sont souvent des tâches informatiques très exigeant beaucoup de ressources. Malheureusement, parmi la myriade de projets open source, il y a très peu d'outils d'analyse de données qui ont été portés au cadre Hadoop. Apache Mahout se distingue parmi les rares initiatives: ce projet est principalement connu pour son application de recommandation, mais il offre également un entrepôt d'algorithmes ML, annoncés pour lancer sur la carte / Réduire. Nous avons enquêtons les vingt algorithmes pro- posés dans les Mahout et dans le présent rapport, nous nous concentrons sur les plus prometteurs d'un: la mise en œuvre forêt aléatoire. En se fondant sur des tests approfondis, y compris les données de marketing spécifiques d'Orange, nous fournissons une rétroaction approfondie sur l'utilisation de cet outil, aussi bien du point de vue théorique et pratique, et nous proposons plusieurs améliorations. 1 Introduction La baisse du coût du stockage des données a conduit à l'accumulation de grands ensembles de données complexes, qui sont largement considérées comme de nouvelles opportunités pour les entreprises. Orange - une société de télécommunications multinationale - doit analyser les données de son réseau pour améliorer la rentabilité et de créer de nouveaux services. Pour donner une idée de l'échelle, afin d'accroître la satisfaction des clients sur les services, Orange doit analyser la qualité des services (QoS) et la qualité d'expérience (QoE) des indicateurs pour ses 150 millions de clients mobiles. Ces indicateurs de QoS et QoE résultent de la combinaison de différentes sources de données (sonde de réseau, SI). L'objectif principal consiste à la détection ou la prévision en temps réel de la qualité de service ou QoE. Cela permettrait d'Orange soit d'améliorer la qualité du réseau ou de fournir de nouveaux services basés sur QoE. Par conséquent, l'application des techniques de ing ces grandes quantités de données de données est cruciale. Cela soulève de nombreuses questions telles que l'évolutivité des algorithmes d'exploration de données, l'automatisation du processus d'exploration de données et le contrôle de surajustement. - 413 - Commentaires - Forêt aléatoire des données de marketing de Mahout Le problème d'évolutivité est généralement le premier que les gens ont à l'esprit avec de grandes données. La disponibilité des environnements informatiques efficaces tels que Hadoop (HAD) clusters avec le carte- réduire cadre est souvent considéré comme la solution à la question de l'évolutivité. Un projet open source, nommé Mahout (Mah), prétend en fait de fournir la mise en œuvre de plusieurs algorithme d'apprentissage qui obtiennent non seulement des données de dépôt Big Data, mais en réalité exécuté sur un cluster Hadoop, tirant ainsi profit de la parallélisation. Ce projet a attiré l'attention de plus en plus après certaines entreprises ont déclaré avoir utilisé avec de bons résultats. Pour être plus précis, Mahout fronces bibliothèques pour les deux encadrés et apprentissage non supervisé. Les histoires de succès sur Mahout concernent toutes les bibliothèques d'apprentissage non supervisées. Au contraire, il y a très peu dit au sujet des bibliothèques surveillées. Pourtant, ces environnements informatiques ont été conçus à l'origine pour les tâches de moteur de recherche, basé sur des milliards d'indexation des documents; ils sont efficaces pour certaines familles de tâches, mais ne peuvent pas être considérés comme des modèles universels pour le calcul parallèle. Parmi les tâches d'exploration de données, la phase de déploiement est intensif de données et est susceptible de tenir bien dans le cadre de carte-reduce. Au contraire, la modélisation phase est d'UC, et l'exploitation efficace des ressources d'un cluster Hadoop est un problème ouvert. Dans une étude précédente (Dream, 2013), nous avons observé que aléatoire Forrest (RF) est (un) la meilleure méthode de Mahout, tant en termes de qualité des modèles et en termes d'évolutivité puisque cet algorithme est nativement un processus parallèle . Le but de notre travail est d'étudier et d'améliorer si nécessaire l'algorithme RF de Mahout pour une utilisation sur le cluster Hadoop. Dans la première partie de ce rapport, nous présentons une étude préliminaire de la forêt aléatoire de la bibliothèque Mahout. Dans la deuxième partie, nous vous proposons plusieurs améliorations de la bibliothèque initiale. Dans une troisième partie, nous proposons un nouvel algorithme d'arbre de décision pour améliorer les performances et réduire surajustement. Nous fournissons des résultats sur les deux ensembles de données académiques et les données d'Orange avant d'arriver à une conclusion. 2 Etude préliminaire Dans la première partie de notre étude, nous avons étudié le comportement des forêts aléatoires (__gVirt_NP_NN_NNPS<__ RF) de la bibliothèque Mahout comme il est emballé dans sa dernière version disponible (0,9). Cette section commence par un rappel de l'algorithme RF. Ensuite, la deuxième sous-section fournit les conditions expérimentales que nous avons utilisées pour toutes les expériences de ces rapports. Dans le troisième paragraphe, nous présentons les résultats obtenus, ce qui conduit à une discussion sur les problèmes potentiels et les solutions. 2.1 Forêt aléatoire Les « forêts aléatoires » est un classificateur supervisé présenté par Leo Breiman (Breiman, 2001). Elle est liée à l'approche de l'arbre de décision, mais le modèle prédictif ne consiste plus en un seul arbre: au lieu, il rassemble une multitude d'arbres. forêts aléatoires sont une combinaison de facteurs prédictifs de l'arbre de telle sorte que chaque arbre dépend des valeurs d'un ensemble aléatoire de vecteur échantillonné de façon indépendante et avec la même distribution de tous les arbres dans la forêt. L'erreur de généralisation des forêts converge vers une limite le nombre d'arbres dans la forêt devient grande. L'erreur de généralisation d'une forêt de classificateurs d'arbres dépend de la force des arbres individuels dans la forêt et la corrélation entre les deux. En utilisant une sélection aléatoire de caractéristiques de diviser le rendement de chaque noeud taux d'erreur qui se comparent favorablement à Adaboost (Freund et Schapire, 1996), mais sont plus robustes par rapport au bruit. erreur de contrôle interne des estimations, la force, - 414 - Thao C., N. Voisine, Lemaire V., Trinquart R. et corrélation et ceux-ci sont utilisés pour montrer la réponse à l'augmentation du nombre de fonctionnalités utilisées dans la division. Les estimations internes sont également utilisés pour mesurer l'importance variable. Dans les forêts au hasard, il est nécessaire d'utiliser un ensemble de données de validation. En effet, au cours de la mise en sac, de nombreux cas ne sont pas utilisés pour la construction d'un arbre. Chaque classificateur apprend qu'une partie des données. les données non utilisées sont appelées Out-Of-Bag. Ils fournissent une bonne façon d'estimer les performances de généralisation du classificateur. 2.2 Conditions expérimentales 2.2.1 Contexte industriel Dans ce rapport, nous nous concentrons sur le comportement des Mahout lorsqu'ils traitent des données qui ont les caractéristiques de « Orange Data », à savoir: - contraintes de données: (i) Hétérogène, (ii) des valeurs manquantes, (iii) Deux ou Plusieurs classes, (iv) des distributions très asymétriques, avec - de nombreuses échelles: (i) des dizaines de millions de cas, (ii) des dizaines à des dizaines de milliers de ables Vari; - de nombreux types de données: (i ) numérique, (ii) catégorielles, (iii) texte, (iv) l'image. Nous limitons l'étude à la tâche d'apprentissage supervisé, id problème de classification is tels que la détection de désabonnement, la prévision de appetency ... Et parmi les ensembles de données potentiels pour l'évaluation, nous avons un vif intérêt pour les plus proches des problèmes de commercialisation 2.2.2 les paramètres qui influencent le comportement de Mahout Lors de l'installation de la bibliothèque Mahout sur une plate-forme Hadoop il y a plusieurs paramètres qui vont influencer les performances obtenues à l'aide de RF du Mahout:. le nombre de machines (hochement es) du cluster et de leurs caractéristiques, la configuration des systèmes de fichiers HDFS de distribués, en particulier en ce qui concerne les blocs de manière sont créés (taille et réplication fait r), le critère de répartition utilisé dans l'algorithme RF, le nombre d'arbres dans les forêts, la façon de combiner les arbres. Les paragraphes qui suivent décrivent chacun de ces points. Nous indiquons également à la fin de cette section de la ligne de base nous permet de vérifier la validité des résultats obtenus avec le RF. Cluster: les tests Tous sont effectués sur une petite exploration Hadoop 1 cluster (HAD) de Orange Labs qui a les propriétés suivantes: (i) 6 noeuds avec: 2 Intel (R) Xeon (R) CPU E5-2407 0 @ 2,20 GHz, total des cœurs de processeur par noeud: 8, 32 Go de mémoire vive, la version Hadoop: Cloudera CDH4 (Hadoop 0,20), Mahout 0,7 (tel qu'il est emballé à l'intérieur de Cloudera CDH4.2). Données nœuds et taille de bloc: Les principaux composants d'une plate-forme Hadoop sont HDFS et MapReduce. HDFS est un système distribué conçu pour stocker des fichiers de très gros volumes de données sur un grand nombre de machines, alors que MapReduce est un cadre pour la distribution de Process- tion sur des fichiers volumineux. Lorsque ces deux éléments sont combinés sur le même ensemble de machines (nœuds de la grappe), la plate-forme agit comme un système unique résultant, fournissant une haute disponibilité, de la charge 1. Hadoop (haute disponibilité plateforme orientée objet distribué) est un système distribué qui adresses les questions de « grandes données ». Hadoop utilise un système de stockage distribué, qui est appelé HDFS (Hadoop Distributed File System) et incorpore des systèmes d'analyse comme MapReduce, Mahout ou Spark. Hadoop permet aux données de division et de l'exécution de l'analyse sur le traitement en parallèle. - 415 - Commentaires - Forêt aléatoire des cornac données de marketing d'équilibrage, et le traitement parallèle. Avec HDFS, tout grand fichier de données est décomposé en blocs et ceux-ci sont répartis entre les nœuds du cluster. Sur la base des blocs HDFS, la carte et réduire les fonctions peuvent être réparties sur le cluster et effectuer sur des sous-ensembles de grands ensembles de données, ce qui permet une meilleure évolutivité. Contrairement à un système de stockage classique, où les blocs sont une question de kilo-octets, la taille de bloc est réglé par défaut à 64 MB. Cette valeur par défaut peut être réglée par l'administrateur du cluster et a même changé à la volée par un utilisateur pour une mouche spécifique et / ou ING Process-. tailles de blocs typiques sont 128Mo, 256 Mo, 512 Mo ou 1 Go. Notre groupe a été configuré avec une taille par défaut de 128 Mo. Note: Le comportement des forêts aléatoires dans Mahout est quelque peu différente de la version originale de Breiman et donc sa théorie. Tout d'abord, les échantillons bootstrap ne sont pas effectués sur l'ensemble des données. Étant donné que les données sont réparties dans les noeuds de données (et donc les cartographes), chaque cartographe réalise son propre bootstrap. Les cartographes réalisent donc une formation de plusieurs arbres dans la forêt. L'algorithme des forêts aléatoires de Breiman est donc en partie « remplie ». Chaque mappeur effectue une partie de la forêt, mais sur des données qui ne sont pas un « sac » dans le sens d'une mise en sac. Ensuite, la forêt de chaque cartographe sont combinés à réaliser la forêt « globale ». critère de Split: Le RF Mahout utilise par défaut des arbres non binaires. Le gain d'information (Quinlan, 1986) est le critère de partage par défaut utilisé. Lorsqu'une variable est choisie pour effectuer la séparation: (i) pour les attributs numériques deux feuilles sont élaborés après un noeud, (ii) pour les feuilles catégorique Vari ables Q sont élaborés après un noeud où Q est le nombre de modalités de la variable choisie . Note: Il est important de noter que Mahout ne gère pas les valeurs manquantes. Ainsi, dans une première partie de notre expérience, nous avons remplacé toutes les valeurs manquantes par -9999. Pour une variable, en remplaçant-9999 valeurs manquantes implique que l'on considère la valeur manquante comme information. Pour une variable numérique, l'objectif est de mettre une valeur inférieure à toute autre valeur. Nombre d'arbres: Le nombre d'influences arbres, la valeur de l'ASC: augmenter le nombre d'arbres augmente les chances d'avoir des arbres de discrimination. Cette amélioration est particulièrement observable dans le début d'une courbe lors du tracé de l'AUC par rapport au nombre d'arbres dans la forêt. Pour cette base de données, KDD petite upselling, la bonne valeur asymptotique est proche de 4000 mais le gain en Performa nces après 1000 est faible. Le nombre d'arbres a aussi une grande influence sur le temps qui sera nécessaire pour déployer le modèle. Dans les expériences présentées ci-dessous dans ce document, nous fixons le nombre d'arbres à 1000 (donc 1000 / M pour chaque mappeur où M est le nombre de cartographes). En combinant les arbres: Après un grand nombre d'arbres est généré, ils votent pour la classe la plus populaire. Nos résultats de base Orange a développé un logiciel puissant nommé Khiops www.khiops. com) qui est capable de travaux sur ensemble de données très grand (et Guyon al., 2010) ou jeu de données multi-tables (Boullé, 2014)). Nous avons utilisé ce logiciel comme base de référence dans sa « version standard». Appliquée sur une base de données sin- gle (et non sur une base de données distribuée) afin d'évaluer les résultats obtenus avec le RF de Mahout Le classificateur est un calcul de la moyenne de sélective Naive Bayes décrit dans (Boullé, 2007) - 416 - Thao C., N. Voisine, Lemaire V., Trinquart R. 2.3 datasets Nous avons utilisé 3 types de données (8 jeux de données) qui sont: - Adulte: les données appartenant au référentiel d'apprentissage automatique UCI ( Bache et Lichman, 2013), - OCR du défi d'apprentissage à grande échelle Pascal (Sonnenburg et al, 2008); -. KDD (grandes et petites) la commercialisation des données fournies par orange pour le défi KDD 2009 (orange, 2009, Guyon et al ., 2010) Leurs caractéristiques sont données dans le tableau 1. les critères utilisés pour évaluer les résultats obtenus est l'aire sous la Comme son nom l'indique la courbe ROC (AUC (Fawcett, 2006))., il est aléatoire dans l'algorithme de Random les forêts. Nous avons donc effectué une formation de dix. Dans le reste de cet article Tra l'ASC et de test ASC présentés dans les tableaux seront l'AUC moyenne au cours des dix expériences. Dataset Ni Nn Nc CP PCTrain PCTest #Mappers Adulte 48 842 6 8 2 76,17% 70% 30% 8 OCR 3 500 000 1156-2 50% 60% 40% 71 KDD Grand Upselling 50 000 14740 260 2 7.4% 50% 50% 27 KDD Grand Churn 50 000 14740 260 2 7,3% 50% 50% 27 KDD Grande appétence 50 000 14740 260 2 1,8% 50% 50% 27 KDD Petit Upselling 50 000 190 40 2 7,4% 50% 50% 20 KDD Petit Churn 50 000 190 40 2 7,3% 50% 50% 20 KDD Petit 50 000 190 appétence 40 2 1,8% 50% 50% 20 TAB. 1 - Ensemble de données utilisé: Ni = nombre de cas, Nn = nombre de variables numériques, Nc = nombre de Categorical variabkes, C = nombre de classes P: Pourcentage de la classe cible, PCTrain: Pourcentage utilisé pour la formation, PCTest: Pourcentage utilisés pour le test. 2.4 Résultats avec la bibliothèque « standard » Nous présentons les résultats que nous avons obtenus avec la forêt aléatoire de Mahout lors de leur utilisation avec l'installation par défaut de la bibliothèque: (i) le critère de répartition est le gain d'information, (ii) la valeur manquante sont traités selon la description donnée dans la section précédente, (iii) le nombre d'arbres est de 1000, (iv) le nombre de mappeurs est indiqué dans la dernière colonne du tableau 1, (v) la classe prédite est la plus populaire prédit classe dans la forêt. Le tableau 2 montre (colonnes 1 et 2) Les performances obtenues lors du test AUC et le rapport de train AUC / Test AUC (colonne 3) pour donner un élément de robustesse (une valeur supérieure à 1 indique un surajustement) .Les points clés de ceux-ci les résultats sont (i) RF présentent des résultats médiocres sur le petit ensemble de données, mais ont de bonnes performances sur le plus grand ensemble de données OCR (le résultat semble être connu par la communauté d'apprentissage de la machine (voir (Debreuve) diapositive 17), (ii) les résultats ne sont pas mauvais, mais inférieur résultats de la littérature sur ces ensembles de données, (iii) les résultats sont plus faibles (sauf OCR) que ceux obtenus par Khiops, (iv) un « grand » overfitting existe sur un grand nombre de l'ensemble de données. Nous analysons en profondeur ces résultats pour comprendre où viennent les problèmes l'un d'eux vient du critère de répartition -. le gain d'information -. et l'autre est la façon de traiter les valeurs manquantes pour le premier est biaisé le gain d'information quand il y a des variables catégoriques qui ont beaucoup de modalités (Harris, 2002). Dans thi s cas, les arbres individuels sont moins profondes et plus larges. Pour la seconde la voie initiale pour remplacer la valeur manquante est pas - 417 - Fe edback - Forêt aléatoire des données appropriées de marketing de Mahout et doit être changé pour obtenu de meilleurs résultats. Cependant, le framework Hadoop permet de diminuer beaucoup le temps de formation comme l'a montré dans le tableau 3 Dataset Khiops Mahout Robustesse Mahout Adult 0,925 0,910 1,02 KDD Petit Upselling 0,872 0,648 1,45 KDD Petit Churn 0,731 0,612 1,49 KDD Petit appétence 0,814 0,682 1,44 OCR 0,830 0,953 1,00 KDD Grand Upselling 0,897 0,877 1,07 KDD Grande Churn 0,743 0,681 1,26 KDD Grande appétence 0,852 0,735 1,30 TAB. 2 - Premier résultat avec la « RF standard » dans Mahout OCR KDDSmall Upselling KDDLarge Upselling Khiops 4h55m 28s 45min 39s de les Mahout de TAB. 3 - Le temps de formation pour Khiops et Mahout 2.5 Premières idées pour contourner les problèmes observés Face aux résultats obtenus dans la section précédente, nous essayons d'appliquer plusieurs patchs à la RF de Mahout afin d'améliorer la performance de la bibliothèque: - Mahout « 2 »: où l'on change le gain information du ratio de gain; - Mahout « 3 »: où sont remplacés par la valeur médiane corres- pondant (Breiman) les valeurs manquantes des variables numériques. Dataset Khiops Mahout Par défaut Mahout 2 Mahout 3 adultes 0,925 0,910 0,919 0,917 KDD Petit Upselling 0,872 0,648 0,690 0,778 KDD Petit Churn 0,731 0,612 0,596 0,621 KDD Petit appétence 0,814 0,682 0,667 0,694 OCR 0,830 0,953 0,946 0,953 KDD Grand Upselling 0,897 0,877 0,705 0,873 KDD Grand Churn 0,743 0,681 0,503 0,681 KDD Grande appétence 0,852 0,735 0,597 0,763 TAB. 4 - Résultat avec le « RF modifié » dans Mahout. Mahout 2: Mahout avec un rapport de gain, cornac 3: cornac des informations de gain et les valeurs manquantes remplacées par la valeur médiane. Les résultats obtenus pour l'AUC Test avec ces deux version modifiée sont présentés dans le tableau 4. Seule la version 3 Mahout a de meilleurs résultats que la version standard. Mais cette amélioration pourrait être coûteuse puisque la valeur médiane de toutes les variables numériques est nécessaire et parce que la valeur médiane doit être calculée sur l'ensemble de données complet (qui est divisé en différents noeuds). Nous avons également tester la modification des autres (non présentés ici) pour la base de données déséquilibrée que l'utilisation de la racine carrée dans le calcul du gain d'information ((Flach, 2012), la section « Sensibilité aux distributions de classe faussés » page 143)) ... mais sans amélioration de - 418 - Thao C., N. Voisine, Lemaire V., R. Trinquart les résultats. Face à ces résultats et qui souhaitent encore plus robustes performances, nous nous tournons vers l'approche MODL qui est connu pour être robuste (Boullé, 2006, 2005). 3 régularisation Ajout dans la forêt aléatoire Dans cette section, nous montrons deux modifications de l'arbre de décision utilisés par Mahout Forêt aléatoire pour augmenter les performances de classification et de réduire surajustement. La première repose sur l'approche MODL développée pour arbre de décision avec succès (2010 et al Voisine.). Elle consiste à changer la méthode de fractionnement de la variable numérique et introduit une méthode de regroupement pour les variables catégoriques. La deuxième amélioration consiste à changer le mode de scrutin de Ma- Hout RF. Par défaut Mahout utilise la majorité méthode de vote pour estimer les probabilités de classe. Nous vous proposons d'estimer les probabilités de classe en faisant la moyenne des estimations de probabilité dans chaque feuille de tous les arbres de décision. 3.1 Modifications Ajout de régularisation: Nous décrivons une nouvelle division et les méthodes de regroupement pour deux classes tâche de classification d'apprentissage. La méthode utilisée fractionnement de critères MODL discrétisation basés sur des approches LDM. La méthode de discrétisation MODL pour la classification supervisée offre la plus discrétisation probable compte tenu des données. expériences comparatives rapport détaillé haute performance (Boullé, 2006). Le cas de la valeur de regroupement des variables est traitée dans (Boullé, 2005) en utilisant une approche similaire. Une approche bayésienne est appliquée pour choisir le meilleur modèle de discrétisation, qui se trouve en maximisant la probabilité p (Modèle | données) du modèle compte tenu des données. Utilisation de la règle de Bayes et puisque la probabilité p (Data) est constante sous différents du modèle, ce qui est équivalent t o maximiser p (Model) p (données | Model). Nous avons décidé dans cette étude pour élaborer RF avec un arbre de décision binaire pour le problème de classification des deux classes qui sont plus présents dans le problème Orange. Par conséquent, dans chaque nœud le critère de répartition sera consacrée à trouver le meilleur divisé en deux intervalles pour les variables numériques ou le meilleur groupe en deux groupes pour les variables qualitatives. Ce paramètre permet d'avoir algorithme rapide (pas détaillé dans ce rapport en raison de lieu compte). Soit N le nombre de cas d'une feuille. Ni représente le nombre d'instances dans l'intervalle i et Nij le nombre d'occurrences d'une valeur de sortie j à l'intervalle i. Dans notre contexte, le nombre de cas N et le nombre de classes J sont censées être connues. Le nombre d'intervalle est fixé par deux. Pour les variables numériques (Boullé, 2006), le critère d'évaluation lorsque le nombre d'tervals entrée et le nombre de classes sont tous deux égaux à 2 et le nombre de classes est: log 2 + log (N + 1) + log (N1 + 1 ) + log (N2 + 1) + Σ2 i = 1 log Ni! Ni1! Ni2! . Une fois que le critère d'évaluation est établie, le problème est de concevoir un algorithme de recherche afin de trouver un modèle de discrétisation qui minimise le critère. En Boullé (2006), une heuristique ascendante avide standard est utilisé pour trouver un bon discrétisation. Pour les variables catégoriques Boullé (2005), le critère d'évaluation lorsque les groupes NumberOf est égal à deux groupes est la suivante: V log 2 + log (1- 1 2V -1) + log (N + 1) + log (N1 + 1) + log (N2 + 1) + Σ2 i = 1 log Ni! Ni1! Ni2! , Où V est la valeur numérique d'une variable catagorical. - 419 - Commentaires - Forêt aléatoire des données de Mahout marketing vote: En général, la classe prédite est basée sur les votes des arbres pour la classe la plus populaire. Nous changeons cette règle pour effectuer des estimations de probabilité de la moyenne dans chaque feuille de tous les arbres de déci- sion, tels que: P (CJ | X) = argmaxJ 1T ΣT t = 1 P (C t J), où P (C t J) est le congé de fin de l'arbre t. Dans ce cas, la classe prédite sera celui avec la plus haute probabilité et la probabilité de confiance sera la probabilité de la moyenne correspondante. La confiance estimée permettra une meilleure estimation pour le calcul de la CUA. 3.2 Résultats Dataset Khiops Mahout Mahout 2: Mahout 3: Mahout Mahout Gain par défaut Ratio IG + Média MODL MODL + nouveau vote adulte 0,925 0,910 0,919 0,917 0,911 0,916 KDD Petit Upselling 0,872 0,648 0,690 0,778 0,800 0,820 KDD Petit Churn 0,731 0,612 0,596 0,621 0,619 0,675 KDD petit appétence 0,814 0,682 0,667 0,694 0,555 0,754 OCR 0,953 0,946 0,953 0,830 0,947 0,948 KDD Grand Upselling 0,877 0,705 0,873 0,897 0,866 0,875 KDD Grand Churn 0,681 0,503 0,681 0,743 0,628 0,667 KDD Grande appétence 0,735 0,597 0,763 0,852 0,686 0,725 TAB. 5 - Résultat de l'AUC de test avec le « RF modifié » dans Mahout. Mahout 2: Informations Gain remplacé par le ratio de gain, cornac 3: cornac des informations de gain et les valeurs manquantes remplacées par la valeur médiane, cornac MODL avec prédiction basée sur la classe la plus populaire et cornac MODL avec prédiction basée sur des estimations de probabilité de la moyenne. Dataset Nombre de nœuds Robustesse Mahout Par défaut Mahout MODL Mahout MODL Mahout MODL adultes 1763 60 1,02 1,00 KDD Petit Upselling 9895 5 1,45 1,06 KDD Petit Churn 9954 3 1,49 1,12 KDD Petit appétence 4013 2 1,44 1,15 OCR 3396 969 1,00 1,00 KDD Grande Upselling 1658 13 1,07 1,02 KDD grande baratte 1864 8 1,26 1,10 KDD grande appétence 688 5 1,30 0,94 TAB. 6 - Taille Robutness et modèle des différents RF (* nombre moyen par arbre) Nous comparons la nouvelle RF qui a utilisé l'approche MODL comme critère de répartition des résultats obtenus dans la section précédente. Les résultats du « Mahout MODL Random Forest » sont intéressants (i) les résultats sont meilleurs, voir le tableau 5 (ii) le Robustesse est mieux, voir le tableau 6 et les modèles sont « plus petits » voir le tableau 6. Ces résultats sont intéressants même . si elles sont pires que ceux obtenus par Khiops - 420 - Thao C., N. Voisine, Lemaire V., travaille Trinquart R. 4 Future: regarder de plus près l'échantillonnage bootstrap et les données dist ribution dans HDFS Dans notre étude, nous nous sommes concentrés jusqu'à présent sur la compréhension du réglage RF de Mahout a été l'impact du processus de construction des arbres et surtout nous avons fait l'accent sur le processus de fractionnement. Une chose que nous avons fait mention, mais ne pas creuser dans est la façon dont les ensembles de données sont répartis entre les nœuds et utilisés comme base pour l'échantillonnage bootstrap. C'est ce qui est examinée dans cette section. 4.1 Taille de bloc et les données d'un fichier aléatoire Lorsque l'ensemble de données est tombé dans HDFS, il est divisé en blocs de taille fixe et les blocs sont répartis sur les nœuds du cluster. Sur le cluster Hadoop que nous avons utilisé pour des expériences, la taille de bloc par défaut est de 128 Mo et nous n'a pas changé cela. Le processus d'apprentissage pour Mahout RF consiste à un emploi Plan-Reduce: cartographes construire des arbres qui sont collectés par un seul réducteur. Chaque Mapper procède un seul bloc HDFS: de sorte que les arbres construits par un mappeur sont toutes basées sur un sous-échantillonnage limité par le bloc cartographe travaille. Par conséquent dès le début, le principe de l'échantillonnage d'amorçage est différente de la version originale de Breiman: ce n'est pas un échantillonnage aléatoire avec le remplacement sur l'ensemble des données. Cette différence a deux conséquences majeures: - D'abord la distribution initiale de la cible dans le fichier de jeu de données peut avoir un impact profond sur les arbres générés par différents cartographes. Dans le pire paramètre possible, si les lignes de l'ensemble de données sont triées par valeur cible, on pourrait mappeur procède lignes qui sont toutes associées à la modalité de la même cible. - En second lieu, en fonction du nombre de colonnes, un bloc peut contenir des lignes relativement peu et les arbres peuvent être construits sur un petit échantillon. Cela dépend vraiment de la taille du bloc qui a été efficace lorsque l'ensemble de données a été mise en HDFS. Il peut être un problème pour les datamarts de marketing très grandes, qui facilement englobent plus de 5000 colonnes. Le problème de la taille des blocs peut être abordée en étudiant l'impact de la taille des blocs sur la performance de prédiction pour différents jeux de données. Espérons que nous pouvons trouver où placer le curseur afin que nous puissions configurer notre système HDFS correctement pour un datamart donné. Mais ce n'est pas une approche très agile dans un environnement où nous devons considérer différents datamarts et où il est plus facile d'ajouter des variables à un datamart. Et cela ne résoudrait pas le problème de la répartition cible. Une approche plus robuste pour aborder simultanément les deux problèmes mentionnés ci-dessus serait de concevoir un travail qui prend soin de diviser l'ensemble de données initial tout en prenant soin des contraintes suivantes: - le travail doit produire des sous-ensembles N, où N est le degré de parallélisme nous souhaitons, - le travail doit prendre soin de stocker chaque sous-ensemble de sortie avec une taille de bloc spécifique de sorte que chaque sous-ensemble se entièrement traitée exactement un mappeur, - le travail doit remplir chaque sous-ensemble par un processus d'échantillonnage aléatoire avec rem- placement sur l'ensemble jeu de données initial - le travail doit offrir une option pour contrôler la façon dont les modalités de cibles sont réparties dans les sous-ensembles (d'échantillonnage stratifié) - 421 - commentaires - Forêt aléatoire des cornac données marketing algorithme 1 Préparer l'entrée dataset algorithme: l'apprentissage échantillon Z stocké dans HDFS N: degré de parallélisme colTarget: indice de la colonne cible dans l'ensemble de données de sortie: Echantillon préparé pour l'apprentissage Mahout RF stockés dans HDFS Ceci est une séquence de deux-Carte réduire les emplois: TargetDistribution → DatasetSplit Le travail TargetDistribution prend pour l'ensemble de données d'entrée Z et la distribution des sorties des modalités cibles. Ce premier emploi n'a qu'un seul réducteur. Le travail DatsetSplit prend pour l'ensemble de données d'entrée Z et délivre en sortie l'apprentissage de l'échantillon Splitted. Les cartographes de DatasetSplit ont également accès à la distribution des modalités cibles. Ce second emploi a N Réducteurs. Mapper TargetDistribution 1: la valeur d'extrait de targetModality à la colonne colTarget de la ligne de courant 2: écrire à réducteur <targetModality, 1> Réducteur TargetDistribution 1: entrée est de la forme <targetModality, Liste des compteurs> 2: écriture <targetModality, somme des compteurs> Mapper d atasetSplit 1: Avant itération sur les lignes du bloc d'entrée, charger la distribution cible 2: Pour rangée à partir du bloc d'entrée 3: Pour i = 1 à n 4: mod_target la modalité de la cible d'extrait ← de ligne courante 5: dessiner si vous garder la ligne dans cette division basée sur la distribution des mod_target 6: si oui, écrire <i, filterRows (currentRow> to réducteur réducteur DatasetSplit 1:. d'entrée est de la forme <reducerindex, Liste des rangées> 2: pour aRow ∈ lignes 3: écriture aRow à la sortie Split fichier. un tel travail pourrait facilement être conçue comme une séquence de deux carte-reduce emplois. le premier estimerait les modalités cibles de distribution et le second serait en fait effectuer l'échantillonnage aléatoire stratifié. on notera que la subtilité principale dans ce une réside approche dans le bon déroulement de la taille des fentes par rapport à la taille moyenne d'une ligne et le nombre d'individus dans les ensembles de données d'origine. Si les lignes sont très grandes, il aura un impact sur le nombre de lignes que nous pouvons tenir dans un seul Split, et par conséquent e e taille de l'échantillon pour la construction d'un arbre. Ainsi, les grandes lignes appellent à la taille du plus grand bloc. Une autre stratégie pourrait être de décider que toutes les colonnes doivent être effectuées dans chaque fraction des ensembles de données: la première étape de l'échantillonnage consiste à choisir une sous-famille de toutes les colonnes pour une scission définitive de l'ensemble de données. Cela permettrait aux petites lignes et le nombre ainsi plus de lignes dans chaque fente. Le croquis de la carte et de réduire les fonctions d'un tel travail est présenté dans l'algorithme 2. 4.2 arbres Shifting entre les nœuds L'approche qui a été esquissée dans le paragraphe précédent tournait autour d'une idée principale: préparer les données d'une manière plus appropriée afin que nous puissions prendre le meilleur de Mahout RF sans changer la bibliothèque. Mais il a un coût en termes d'utilisation: on doit dupliquer le - 422 - Thao C., N. Voisine, Lemaire V., les données Trinquart R. de l'ensemble de données inital aux divisions préparées. Ce processus peut prendre le temps additionnel et le coût espace de stockage supplémentaire. Une autre approche pourrait être utilisée pour la même question: faire l'arbre que d'un est construit à partir de la plus large échantillon de l'ensemble de données initial. Notre idée est que chaque cartographe est lié au travail sur un seul bloc HDFS. Ainsi, au lieu d'avoir chaque cartographe responsable de la croissance des arbres complets, nous vous proposons de transformer le processus Mahout RF en une itération de carte-réduction des emplois, avec des arbres que de plus en plus d'une fraction à chaque course. Pensez que vous voulez construire une forêt d'arbres un T (disons un millier) et l'ensemble de données a été divisé en blocs B (par exemple 10). Le processus serait lancé en générant des fichiers B, contenant chacun de ces T / B (qui est une centaine ici) Arbres vides. Ensuite, nous affecter au hasard chacun de ces fichiers à chaque cartographe et exécuter une étape du processus d'apprentissage. Cette étape se développerait les arbres par deux degrés de profondeur et de stocker les arbres mis à jour dans les fichiers B à nouveau. L'étape suivante consiste à associer au hasard les fichiers mis à jour à nouveau cartographes et itérer jusqu'à ce que tous les arbres sont complètement développés. Une telle approche nécessiterait substantielle de codage sur la bibliothèque Mahout existante, plus particulièrement au niveau de stocker et ensemble Parse d'arbres incomplètes. De plus, il y a certainement quelques paramètres à régler (combien devrions-nous développer des arbres à chaque étape?). Mais l'avantage est qu'il est alors possible de construire des arbres à partir d'un ensemble plus large échantillon sans déplacer l'ensemble de données lui-même. 5 Conclusion Cette étude présente le comportement de la forêt aléatoire mis en œuvre dans le travail du cornac sur plusieurs ensembles de données d'intérêt pour Orange. La première déclaration est l'observation, pour la version standard, des performances bien au-dessous celle que nous pouvons observer dans l'état de l'art. La mise en œuvre pratique dans le cadre de Hadoop et Mahout ne respecte pas totalement le cadre théorique. Cela se traduit par des performances ci-dessous des attentes. Nous vous proposons quelques évolutions, le principal étant l'introduction de régularisation explicite lors de la formation des arbres. Nos tests sur divers ensembles de données montrent une amélioration par rapport à la bibliothèque standard. Il y a néanmoins une importation le travail de fourmi de réaliser, selon nous, de sorte que la forêt aléatoire mis en œuvre dans le cadre de Mahout atteint les performances d'un algorithme de traitement par lots. Il y a un compromis entre la précision connue et la vitesse / volumétrie, mais le prix à payer semble assez important pour le moment. Références Le projet ApachTM Hadoop® développe des logiciels open source pour fiable, évolutive, Sacrifiés informatique, http://hadoop.apache.org/ dis-. L'objectif du projet est de ApachTM Mahout® construire une bibliothèque d'apprentissage machine évolutive, http: //mahout.apache.org/. Bache, K. et M. Lichman (2013). référentiel apprentissage automatique UCI http: //archive.ics.uci. edu / ml. Boullé, M. (2005). Une approche optimale Bayes pour partitionner les valeurs des hommages at- catégorique. Journal of Research Machine Learning 6, 1431-1452. - 423 - Commentaires - Forêt aléatoire de données marketing de Mahout Boullé, M. (2006). MODL: une méthode de discrétisation optimale de Bayes pour les attributs continus. Machine Learning 65 (1), 131-165. Boullé, M. (2014). Vers la construction de fonction automatique pour classication supervisé. Dans ECML / PKDD 2014. accepté pour publication. Boullé, M. (2007). la moyenne des classificateurs bayésiens naïfs sélectifs à base de compression. Journal of Research Machine Learning 8, 1659-1685. Breiman, le remplacement L. Valeur manquante pour l'ensemble de la formation https://www.stat.berkeley.edu/ ~ Breiman / RandomForests / cc_home.htm # missing1. Breiman, L. (2001). forêts aléatoires. Mach. Apprendre. 45 (1), 5-32. Debreuve, E. Une introduction aux forêts aléatoires fichiers http://perso.math.univ-toulouse.fr/motimo/ / 2013/07 / forest.pdf-de hasard. Dream, A. (2013). EVALUER l'apport des Hadoop Pour la plateformes classification. Mémoire de maîtrise, Polytech Lille. Fawcett, T. (2006). Une introduction à l'analyse roc. Motif reconna. Lett. 27 (8), 861-874. Flach, P. (2012). Machine Learning: L'art et la science des algorithmes qui ont du sens des données. La presse de l'Universite de Cambridge. Freund, Y. et R. E. Schapire (1996). Des expériences avec un nouvel algorithme de rappel. Dans la Conférence nationale sur Inter Mchine Learning (ICML), pp. 148-156. Morgan Kaufmann. Guyon, I., V. Lemaire, M. Boullé, G. Dror et D. Vogel (2010). Conception et analyse de la coupe de KDD 2009: notation rapide sur une grande base de données clients orange. SIGKDD Explor. Newsl. 11 (2), 68-76. Harris, E. (2002). gain d'information en fonction du rapport de gain: Une étude des biais de la méthode de répartition. En AMAI. Orange (2009). Coupe kdd 2009: prévision de la relation client - données kddlarge et kddsmall http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction. Quinlan, J. R. (1986). Induction d'arbres de décision. Mach. Apprendre. 1 (1), 81-106. Sonnenburg, S., V. Franc, E. Yom-Tov, et M. Sebag (2008). défi d'apprentissage Pascal à grande échelle -.. ocr http://largescale.ml.tu-berlin.de/about/ » Voisine, N., M. Boullé, et C. Hue (2010) Un Bayes critère d'évaluation pour les arbres de décision. Les progrès dans la découverte et la gestion des connaissances (AKDM-1) 292, 21-38. Apprenticeship CV a fait son automatique apparition dans l'ecosystem Hadoop creant, de la promesse par la de puissance, sans d'uNE Opportunité pour ce domaine précédent. dans this éco - système, Apache Mahout is juin à la question Réponse du temps de calcul et / Ou de la vo- lumétrie: il Consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous de s'exécuter Portés sur AFIN Map / Reduce Ce. rapport se CONCENTRE sur le l'utilisation et portage de l'algorithme des Forêts aléatoires Dans Mahout Il montre à notre retour d'Travers les difficultés Qui expérience PEUVENT Être rencontrées Tant et Pratiques Que suggested Une théoriques piste d'improvement -.. 424 - G - session Feedbac Industrielle k - Etude et amélioration de la forêt aléatoire de la bibliothèque Mahout dans le contexte des données de marketing d'Orange Cedric Thao, Nicolas Voisine, Vincent Lemaire, Romain Trinquart