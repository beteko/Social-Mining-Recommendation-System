Un algorithme de co-clusters à deux niveaux pour très grandes quantités de données Bartcus Marius, Marc Boullé, Clérot Fabrice Orange Labs prenom.nom@orange.com Résumé. Co-classification est une technique d'exploration de données qui vise à identifier la structure sous-jacente entre les lignes et les colonnes d'une matrice de données sous la forme de blocs homogènes. Il a de nombreuses applications du monde réel, mais de nombreux algorithmes co-regroupement en cours ne sont pas adaptés à de grands ensembles de données. Une des approches de grands ensembles de données de co-dispersion est utilisé avec succès la méthode de classification co- MODL qui optimise un critère basé sur une probabilité régularisée. Cependant, on rencontre des difficultés avec d'énormes ensembles de données. Dans cet article, nous présentons un nouvel algorithme de co-regroupement à deux niveaux, étant donné le critère MODL al mugissement de traiter efficacement avec de très grands ensembles de données qui ne correspondent pas à la mémoire. Nos expériences, sur des données mondiales réelles et simulées, montrent que l'approche proposée réduit considérablement le temps de calcul sans froissement de manière significative la qualité de dé- la solution de co-regroupement. 1 Introduction Co-regroupement (Hartigan, 1972), le regroupement de blocs a également nommé (Govaert et Nadif, 2008) ou le regroupement bimode (Mechelen et al., 2004) est une technique d'exploration de données. Elle vise à identify- ing la structure sous-jacente entre les lignes et les colonnes d'une matrice de données sous la forme de blocs homogènes. Considérant que, le principe de regroupement norme est de regrouper des individus similaires (observations) par rapport à un ensemble de fonctionnalités, la tâche de co-regroupement est de simultané- groupe tanément individus similaires par rapport à des variables et des variables similaires par rapport aux observations, ainsi l'extraction de la structure de correspondance entre les objets et caractéristiques. Un autre avantage de la co-agrégation par rapport aux techniques classiques de clustering est sa capacité de réduction de la matrice, où une grande table de données peut être réduite dans une plus petite de manière significative tout en ayant la même structure que la matrice d'origine. En effet, cette technique trouve son utilisation dans de nombreuses applications comme dans les télécommunications, l'extraction de texte (Guigourès et al, 2015). (Dhillon et al., 2003; Li et Abe, 1998), l'exploitation minière de graphique (. Guigourès et al, 2015), etc. Plusieurs approches co-regroupement ont été proposées dans la littérature (Bock, 1979; Dhillon et al., 2003; Govaert et Nadif, 2008). Ces procédés diffèrent principalement en fonction du type de données analysées (catégoriques ou numériques), l'hypothèse sous-jacente, le procédé d'extraction et les résultats escomptés. Plusieurs familles d'approches ont alors été proposées pour effectuer des co-regroupement. Govaert et Nadif (2013, 2008) ont étudié les modèles probabilistes avec l'utilisation de variables latentes dans les modèles de mélange. Des difficultés surgissent lors de l'initialisation, un grand nombre de para- mètres d'estimer et l'efficacité de calcul, donc de grandes données sont difficiles à gérer. En effet, - 95 - Un algorithme de co-clusters à deux niveaux pour des ensembles de données très volumineux quelques méthodes capables de grandes quantités de données co-munitions ont été proposées dans la littérature. Par exemple, Pa- padimitriou et Sun (2008) a développé un outil, nommé DisCo mettre en œuvre un pré-traitement et co-regroupement des données distribuées en utilisant Hadoop et une mise en œuvre cartographique réduire. DisCo peut bien évoluer et d'analyser efficacement des ensembles de données extrêmement importantes, cependant, il a besoin d'une grande bué dis- infrastructure. Une autre méthode de co-regroupement, qui exploite des modèles probabilistes pour deux ou plusieurs variables de tout type (numérique ou catégorique) est basée sur l'approche MODL (Boullé, 2011). Les principaux avantages de la co-regroupement MODL est qu'il est facile à utiliser paramètre- gratuit et bénéficie d'algorithmes avec la complexité du temps sous-quadratique w.r.t. le nombre d'instances, ce qui permet de traiter de grands ensembles de données. Selon les avantages précédemment men- tionné nous nous concentrons sur l'approche co-regroupement MODL. En effet, la co-clustering MODL peut traiter de grands ensembles de données atteignant jusqu'à des millions de cas et des dizaines de milliers de valeurs par variable, avec une com- plexité temps sous-quadratique. Cependant, je t peut difficilement être utilisé avec des données très importantes, jusqu'à des milliards de cas et des variables ayant des millions de valeurs. Par exemple, cette limite est atteinte dans le cas de la yse Anal- de Enregistrement d'appel Retard à l'échelle du pays, lorsque la granularité étudiée va de l'antenne niveau (application dimensionnement du réseau) aux clients individuels (application de marketing avec l'identification des grains fins les communautés et la personnalisation de l'expérience client). Dans cet article, nous nous concentrons sur l'extension des algorithmes d'optimisation co-regroupement à ces grandes quantités de données, compte tenu du critère de co-clustering MODL. Malgré le fait que MODL peut faire face à de nombreuses variables numériques ou catégoriques et même mixtes, dans cet article, nous examinons le cas de deux variables. Ce document est organisé comme suit. Tout d'abord, pour des raisons d'auto-retenue, la section 2 rappelle les principes de la méthode de co-classification en utilisant le critère MODL (Boullé, 2011) qui accouple esti- la distribution conjointe entre deux variables. Ensuite, la section 3 présente le algorithme proposé à deux niveaux pour les grandes données co-regroupement. L'article 4, donne des résultats expérimentaux et évalue l'approche proposée sur des données réelles et simulées. Enfin, la section 5 est consacrée aux discussions et aux remarques finales. 2 Le co-regroupement MODL pour deux variables Soient X et Y deux variables avec des valeurs ensembles VX = {} VXI, avec V X = | VX | et VY = {} vYj, avec V Y = | VY |. Soit D = {(xn, yn), xn ∈ VX, VY yn ∈, 1 ≤ n ≤ N} un ensemble withn instances de données. Un exemple de cette représentation des données est donnée à la Fig. 1, dans lequel VX = {a, b} avec V = X 2, VY = {A, B, C} avec V Y = 3 et N = 4. FIG. 1 - Exemple de représentation des données. - 96 - I. Bartcus et al. L'ensemble de données D, représentée par cette table de contingence (voir la figure 1), peut être résumée en utilisant une partition de la valeur de chaque variable en grappes / groupes. Le produit croisé des deux partitions de taille I × J forme une (I × J) co-agrégation avec une cellule par paire de pièces de valeur. Notez que cette méthode diffère de la co-classification traditionnelle (Govaert et Nadif, 2013) qui considère la partition d'observations et de variables. Afin de choisir le « meilleur » modèle de co-regroupement M (compte tenu des données) de l'espace modèle M, nous utilisons une approche Maximum A Posteriori bayésien (MAP). Nous explorons l'espace de modèle tout en minimisant un critère bayésien, appelé coût. Le critère de coût des outils d'un compromis entre la sous-montage et sur-raccord et est définie comme suit: c (M) = - log P (M | D) α - p (M) log - log p (D | M) (1) où p (M) est l'avant et p (D | M) est la probabilité que les données indiquées le modèle de co-agrégation. Les détails sur le critère des coûts et l'algorithme d'optimisation (appelé KHC) sont disponibles dans Boullé (2011). La clé dispose de garder à l'esprit sont: (i) KHC est paramètre- libre, à savoir, il n'y a pas besoin de régler le nombre de groupes / groupes par dimension; (Ii) KHC fournit une solution localement optimale efficace pour la construction du modèle de co-agrégation, dans la complexité du temps quadratique sous-O (N √ N logN) plus précisément O (N * √ N * log *), où N * = Σ VX i = 1 ΣVY j = 1 1 {nij> 0} nij est le nombre de paires de valeurs réelles rencontrées au moins une fois. Comment- jamais, certains ensembles de données sont potentiellement jusqu'à des milliards de cas et des variables ayant des millions de valeurs. Ces ensembles de données ne peuvent pas être analysées à l'aide KHC, à moins d'utiliser des machines équipées de centaines de Go de RAM et d'attente encore des jours de calcul. 3 Scaled co-Clustering MODL Notre objectif est d'étendre les algorithmes d'optimisation co-regroupement à ces données à grande échelle, étant donné le critère co-regroupement MODL, tout en tenant compte des contraintes de mémoire suivantes pour la co-regroupement Scaled MODL. - l'algorithme peut stocker toutes les VX, VY valeurs dans la mémoire, - le N * paires réelles de valeurs ne peuvent pas être stockées dans la mémoire; et ils ne peuvent être stockés sur le disque, - nous pouvons exécuter notre algorithme de co-clusters sur des matrices de taille au plus I2maxi N *. Enfin, l'optimisation co cluster ing modèle doit tenir en mémoire avec la complexité de mémoire O (V X) + O (V Y) + O (I2max). Supposons, les données observées D peut difficilement être co-regroupés en raison d'une ou deux raisons sui- vants. En premier lieu, le nombre d'instances N peut être très grand et, deuxièmement, le nombre de valeurs sur chaque dimension V X V ou Y peuvent être trop grand pour être manipulé par des algorithmes de co-classification actuelle. Pour gérer cela, nous considérons les contraintes de mémoire et de proposer un algorithme des deux co-niveau qui permet KHC de produire des modèles co-regroupement plus rapide avec la plus petite diminution possible de leur qualité. L'algorithme est organisé en deux phases. La première phase consiste dans la phase de Split donnée par les deux étapes suivantes. 1. étape Cloisonnement: vise à obtenir des données sous-ensembles de l'ensemble des données, telles que les futurs co-regroupement sur chacun d'entre eux répondent aux contraintes de mémoire. 2. étape de co-regroupement fine: construit une co-classification de chaque ensembles de données sous en utilisant l'outil KHC. - 97 - Un algorithme de co-agrégation deux niveaux pour les ensembles de données très volumineux La deuxième phase consiste dans la phase d'agrégation avec les deux étapes suivantes. 3. étape Amalgamate: consiste à construire un co-classification globale sur l'ensemble de données initial (large), en fusionnant les co-clusterings obtenues à partir des ensembles de données secondaires. 4. étape de post-optimisation: améliore le modèle par les pistes suivantes. clusters de fusion première et seconde valeurs de déplacement entre les clusters. En conséquence, notre algorithme à deux niveaux proposé est un processus en quatre étapes, qui sont décrites plus précisément. 3.1 Phase de Split Dans notre méthode, nous adoptons une approche diviser pour mieux régner, qui commence par la phase de séparation. 3.1.1 étape de partitionnement FIG. 2 - Exemple de co-classification grossière du jeu de données D, avec I (c) x J (c) secondaires co-clusters. Un grand ensemble de données, avec au moins une violation de contrainte de mémoire, nous conduit à la première étape de notre algorithme proposé, l'étape Cloisonnement. Au sein de la terminologie co-regroupement nous nommons co-regroupement grossier. Cette première étape consiste à co-classification grossière D pour obtenir I (c) des grappes de gros à base de VX et J (c) des agrégats grossiers à base de VY. On obtient G (c) = I (c) x J (c) secondaires co-clusters. Soit, VXα et VYβ les ensembles de valeurs pour les clusters de grossières respectivement α et ß, tels que VX = ⋃i (c) α = 1 VXα et VY = ⋃J (c) β = 1 VYβ. Nous vous proposons un algorithme de partitionnement aléatoire, qui fonctionne comme suit. Tout d'abord, nous mélanger les valeurs de variablesX et Y. Deuxièmement, nous partitionner les valeurs de variables respectivement mélangées dans I (c) et des parties de taille égale J (c). Étant donné que nos valeurs variables sont mélangées, cette solution initiale est susceptible d'être aveugle aux modèles d'information. Ainsi, en utilisant une telle solution et continuer à les prochaines étapes peuvent produire un résultat de co-regroupement non informative. Pour contourner ce problème, une étape de pré-optimisation, similaire à Boullé (2011), est utilisé. Cette étape de pré-optimisation consiste à améliorer le rapport coût MODL (1) en déplaçant les valeurs entre les grappes, améliorant ainsi la solution co-classification initiale en déplaçant les frontières. Theg (c) = I (c) x J (c) grossier co-grappes sont en fait liées à des ensembles de données sous qui sont en outre plus facile à analyser en conséquence un plus petit volume de données. Note Dαβ = {(x, y) ∈ D, x ∈ VXα, y ∈ - 98 - I. Bartcus et al. VYβ} l'intégralité des ensembles de données sous de D, où 1 ≤ α ≤ I (c) et 1 ≤ β ≤ J (c). En outre, D = ⋃ αβ Dαβ. Chacun de ces ensembles de données de sous est adaptée aux contraintes de la mémoire. Fig.2 montre un exemple de co-classification grossière sur les données D. La complexité de cette étape de l'ISO (I (c) J (c) (V X + VY) / 2), ainsi le temps de calcul croît linéairement avec I ( c) et J (c). 3.1.2 étape de co-regroupement fine figure. 3 - Exemple d'amende co-classification pour chaque donnée de sous ensemble Dαβ en I (f) αβ x J (f) co-grappes aß. Cette étape consiste à faire passer KHC sur chacun des ensembles de données de sous précédemment obtenus. Nous nommons l'étape de co-regroupement fine. Sur la base de l'amende co-regroupement pour toutes les données sous-ensembles Dαβ, ∀α = 1,. . . , I (c), ∀β = 1,. . . , J (c), on obtient I (f) le nombre de grappes de αβ fines à base de VXα et J (f) nombre αβ de grappes fines à base de VYβ. Pour résumer, nous HAVEG (f) = αβ i (f) αβ x J (f) aß fines co-groupes pour chaque sous-données définies Dαβ. Fig. 3, montre un exemple de belle co-regroupement pour l'ensemble des données définies D. Notez que cette étape produit de différentes tailles fines co-clusterings, avec différentes clusterings fines, pour chaque ensemble de données sous, donc nous devons combiner tous les résultats de la co-classification pour l'ensemble de jeu de données obtenues D. La complexité de cette étape est O (N √ N / I (c) J (c) logN / I (c) J (c)). Observez que, contrar- ily à l'étape de partitionnement, un grand nombre de partitions diminue le temps de calcul de l'étape de co-regroupement fine, la section 3.3 est donc dédié à montrer comment nous choisissons un nombre optimal de pièces. 3.2 Dans la phase d'agrégation cette phase, nous agrègent les résultats de la phase de séparation. 3.2.1 étape fusion de l'étape fusionnent, qui commence la phase de l'agrégation de l'algorithme à deux niveaux, constitué en grappes de calcul pour l'ensemble de données entier grand D en combinant tous les amas de fines obtenues des ensembles de données secondaires. Dans cette étape, nous faisons référence aux grappes obtenues comme micro clusters. - 99 - Un algorithme de co-agrégation des données à deux niveaux très grands ensembles FIG. 4 - Exemple d'amalgame de l'ensemble SetD de données, avec I (m) × J (m) micro co-clusters. Succédant à l'étape amalgame, on obtient I (m) sur la base des grappes micro VX et J (m) des groupements à base de micro-VY. Fig. 4 illustre un exemple de l'étape amalgame sur l'ensemble du jeu de données. 3.2.2 étape de post-optimisation figure. 5 - Exemple de post-optimisation de l'ensemble des données établies D. En ce qui concerne l'étape amalgamé, nous avons besoin de rappeler la contrainte de mémoire qui dit que notre algorithme co-cluster peut fonctionner sur des matrices de taille au plus I2maxi. Cependant, l'étape consistant à amalgame peut éventuellement produire un trop grand nombre de groupements de micro avec I (m)> Imax ou J (m)> Imax. Avant de procéder à l'étape de post-optimisation, certains résultats fusionner pourrait être nécessaire de réduire I (m) et J (m) de telle sorte que I (m) ≤ Imax et J (m) ≤ Imax. Nous vous proposons une méthode d'échantillonnage qui consiste à regrouper de façon aléatoire les grappes micro dans le nombre maximum de groupes possibles Imax. Cela fonctionne comme suit pour chaque dimension. Tout d'abord, nous mélanger les grappes micro et les regrouper en grappes égales Imax. En second lieu, pour améliorer le modèle que nous déplaçons les grappes micro entre les groupes. Il en résulte un modèle de co-regroupement aléatoire qui est encore être amélioré par l'étape de post-optimisation. - 100 - I. Bartcus et al. Boullé (2011) a proposé deux types de post-optimisation: la fusion exhaustive et le post-optimisation gourmande. Nous utilisons des approches post-optimisation similaire à la fusion des clusters et puis déplacer les valeurs entre les clusters. grappes de fusion: la fusion consiste en grappes jusqu'à ce que le modèle nul est observée. Le meilleur modèle de co-regroupement est alors retenu. mouvement Valeur: déplace les valeurs entre les grappes alternativement pour chaque variable. Fig. 5 illustre un exemple d'optimisation de post sur l'ensemble du jeu de données. 3.3 Le choix du nombre optimal de pièces Un principal problème rencontré dans notre algorithme proposé co-regroupement à deux niveaux est de choisir la taille optimale des partitions I (c) et J (c) dans l'étape de partitionnement. Nous mettons en évidence le fait que pour les petites données, outil KHC est plus efficace que l'algorithme de co-clusters à deux niveaux. En effet, le comportement de temps sur l'ensemble des processus de notre algorithme est plus grand que le comportement temporel d'un processus KHC directement sur l'ensemble des données. D'après ce que nous supposons que chaque sous ensemble de données, nous devons avoir au moins 200 valeurs par variable et 104 cas. Soit T = TS + TA soit le temps d'exécution globale de notre approche de co-regroupement à deux niveaux, où TS est le temps de calcul de la phase Split et TA est le temps de calcul de la phase d'agrégation. Nos expériences montrent que TA est pas impacté par le nombre de partitions, donc nous nous concentrons sur TS pour minimiser T. Rappelons que TS est composé du temps de calcul de l'étape de partitionnement, qui augmente avec la taille de partition, et le Compu temps tational de l'étape de co-classification fine, ce qui diminue avec la taille de la partition. Par conséquent, pour en déduire une proposition théorique de choisir le nombre de partitions, nous utilisons une approche heuristique qui égalise la complexité temporelle de l'étape de partitionnement O (I (c) J (c) (VX + VY) / 2) avec la complexité du temps de l'amende co-classification stepO (N √ N / I (c) J (c) logN / I (c) J (c)) et suppose que la taille des partitions sur X et Y (I (c) et J ( c)) sont proportionnelles à leur nombre respectif de modalités (Vx et Vy). Nous obtenons: J (c) * =   c * √ VY / VX (2N √ N logN + VX VY) 1 3   (2) I (c) * = ⌈√ VX / VYJ (c ) * ⌉ (3) où c * = 1/4 est un facteur constant ajusté de nos expériences. 4 expériences Nous effectuons des expériences à la fois sur des données réelles et simulées afin d'évaluer notre pro- posé algorithme co-regroupement à deux niveaux. Dans ces expériences, nous courons l'approche co-regroupement MODL sur les ensembles de données mondiales ont été et réelles et les comparer avec notre algorithme co-regroupement à deux niveaux, donné par 2L-KHC. En effet, les pistes co-regroupement MODL à la mode à tout moment, jusqu'à ce qu'aucun changement significatif sont observées et sorties des solutions intermédiaires. There- avant, nous montrons les résultats du poing (KHC (1)) et les dernières solutions (KHC) de la co-regroupement MODL. Le but de ces expériences est d'obtenir une bonne et simple résumé des données - 101 - Un algorithme de co-clusters à deux niveaux pour les ensembles de données très grand ensemble. Nous évaluons la qualité du modèle de co-regroupement à partir du coût normalisé, calculé par 1 c (M) c (M0), où c (M) est le coût du modèle estimé et c (M0) est le coût du modèle nul. Ce coût normalisé peut être interprété comme un taux de compression. En outre, afin de montrer l'efficacité de l'algorithme proposé, nous fournissons le temps de calcul pour chaque approche. 4.1 Les expériences sur les données simulées Dans cette expérience, nous avons d'abord générons nos ensembles de données D avec deux variables X et Y. Pour générer les données, on utilise la distribution de probabilité suivante: p (i = xn, yn = j) α 1 - ||| | (i-j) | V ||| b, où (i, j) sont la possible les valeurs nominales pour les variables X et Y, respectivement; V = V X = V Y est le nombre de valeurs de la variable X ou Y, qui pour la simplicité sont considérés comme étant égaux; et b est un paramètre qui commande la concentration des données simulées sur la diagonale de la matrice de données. Nous varions mélanges de données et sparsity en générant trois familles de type de données. Ceux-ci sont sous forme uniforme, les familles biaisées et rares. Fig. 6 montre un exemple de ces trois familles de types de données. Tout d'abord, nous générons des familles de données uniformes et respectivement asymétriques. Les valeurs de X sont donnés a = 1,5, b = 1 a = 1, b = 1 a = 1, b = 0,01 FIG. 6 - Exemple de parcelles asymétrique (à gauche), uniforme (centre) et des ensembles de données éparses (à droite). par i = dU * V e et les valeurs de Y sont données par j = dw * V e, où (u, w) sont des variables aléatoires tirés indépendamment dans l'axe de loi de puissance (a-1), avec aa paramètre de forme con- pêche à la traîne de l'équilibre dans nos données. Pour a = 1 la famille de données uniforme est générée, alors que pour une plus grande famille de données en biais sont générés. Pour nos expériences, nous fixons un = 1,5 pour générer des données biaisées. Pour une meilleure compréhension de la différence entre uniforme et des ensembles de données asymétriques, les données de travers génère moins de données pour les première générés (i, j) des paires de valeurs. En outre, la fixation d'un petit b = 0,01 concentrés les données dans la diagonale, en obtenant ainsi la famille des données éparses. Pour montrer l'efficacité de notre algorithme de co-clusters à deux niveaux que nous générons six types d'ensembles de données en faisant varier le nombre d'instances et valeurs par variable. Le tableau (1) résume les ensembles de données générés. Jeu de données J1 J2 J3 J4 J5 J6 N 106 106 106 107 107 107 V 200 2000 20000 200 2000 20000 TAB. 1 - Generated ensembles de données. - 102 - I. Bartcus et al. Tout d'abord, nous courons nos expériences sur des ensembles de données uniformes. Le tableau 2, montre le coût normalisé obtenu et le temps de calcul, pour KHC et notre approche de co-regroupement à deux niveaux. Rappelons que KHC exécute d'une manière quelconque de temps fournissant de façon intermédiaire lutions. Dans nos résultats, nous présentons la première et les dernières solutions récupérées notées par respectivement KHC (1) et KHC. Notre approche de co-regroupement à deux niveaux est donnée par 2L-KHC. Observer que, lorsque le nombre de valeurs est faible, V = 200 (D1, D4), l'approche 2L-KHC obtient une meilleure solution que celle de KHC (1) donnée en même temps d'optimisation. La solution finale de KHC est améliorée de 0,5%, alors que notre approche est plus rapide dix fois. Pour D2, D3, D5 et D6 lorsque le nombre de valeurs par variable sont V = 2000, 20 000, nous pouvons voir que l'approche 2L-KHC obtient une meilleure solution que celle de KHC (1) à environ 15-50 moins de temps. En outre, il est d'environ 70-150 fois plus rapide que KHC, tout en obtenant des modèles de qualité comparable. coût normalisé Temps (s) de données 2L-KHC KHC (1) KHC 2L-KHC KHC (1) KHC D1 0,005354 0,005311 0,005381 8 10 84 D2 0,003270 0,003127 0,003282 277 3.885 20.324 D3 0 0 0 361 18113 18113 D4 0,005534 0,005525 0,005537 11 11 116 D5 0,003792 0,003718 0,003793 1036 32015 204137 D6 0,002533 0,002447 0,002538 4056 196974 602534 TAB. 2 - Les résultats co-classification obtenue sur les ensembles de données uniformes. En second lieu, nous courons nos expériences sur des ensembles de données rares. Le tableau 3, montre les résultats obtenus sur nos rares données. Notez que, pour V = 200 (D1, D4), l'approche 2L-KHC obtient la même qualité de co-regroupement comme celui de la KHC (1) et KHC, tandis que les temps de calcul pour toutes ces approches sont assez petites. Cependant, quand on a V = 2000 valeurs par variable (D2, D5), on constate que notre approche de co-regroupement à deux niveaux est 2- 10 fois plus rapide que KHC (1) et 15-60 plus vite que KHC, tout en ayant une normalisée coût 5% pire que celle de KHC. Enfin, avec V = 20000 (D3, D6), notre approche de co-regroupement donne une solution légèrement mieux être 40 fois plus rapide que KHC (1) et 80 fois plus rapide que KHC. coût normalisé Temps (s) de données 2L-KHC KHC (1) KHC 2L-KHC KHC (1) KHC D1 0,08474 0,08474 0,08474 48 35 342 D2 0,01535 0,01484 0,01587 2282 3939 34326 D3 0,0041 0 0 427 21586 21586 D4 0,08951 0,08951 0,08951 43 23 262 D5 0,01750 0,01749 0,01750 2409 29449 142887 D6 0,01076 0,01045 0,01046 4966 193273 405939 TAB. 3 - Les résultats co-classification obtenue sur les ensembles de données éparses. En raison du manque d'espace, les ensembles de données asymétriques résultats ne sont pas présentés dans ce document. Cependant, les résultats obtenus sont similaires à celles des données uniformes. - 103 - Un algorithme de deux co-regroupement niveau des ensembles de données très volumineux Pour conclure, l'approche proposée co-regroupement à deux niveaux surclasse KHC dans le temps de mise en Compu sans dégrader considérablement la qualité des solutions de co-regroupement. 4.2 Les expériences sur les données réelles Nous effectuons des expériences sur des données réelles qui nous permet d'évaluer notre approche de clustering coopération à deux niveaux sur des données avec une distribution plus complexe par rapport aux données générées. 4.2.1 Les données Nous menons des expériences sur plusieurs séries de données réelles: 20 Newsgroups (Mitchell, 1997), Web- Spam et Netflix (Bennett et Lanning, 2007) (avec 1% et 10% ran- (Castillo et al., 2008) les utilisateurs domly choisis), dont les caractéristiques sont résumées dans le tableau 4. Les données des variables définies Co-regroupement NVXVY 20 Newsgroups 2.047.830 19.464 11.315 texte × mots webspam 13.068.666 390,130 400,000 site de la source × site cible Netflix (1%) 960.327 16,235 4,649 utilisateurs × films Netflix (10%) 17,764 48,068 10.049.248 utilisateurs de films TAB. 4 - ensembles de données du monde réel. L'ensemble de données 20 Newsgroups est devenu populaire pour les expériences dans les applications texte de l'apprentissage des techniques de la machine, telles que la classification de texte et le regroupement de texte. Il se compose d'une collection d'environ 20.000 documents de newsgroup. Ces données comprend 2.047.830 observations, 19.464 textes et 11.315 mots. L'ensemble de données Webspam vient d'un défi de détection du site de type spam. Les données se compose d'un extrait du graphique web avec 13.068.666 liens de 390.130 sites sources et sites cibles 400.000. L'ensemble de données Netflix se compose de 100 millions d'observations, correspondant aux évaluations des utilisateurs 480.000 liés aux films 18,000. Pour t o avoir des résultats plus rapides, nous avons choisi d'enquêter sur environ 1% et 10% des utilisateurs choisis au hasard. On obtient ainsi deux ensembles de données. La première contient 1% d'utilisateurs choisis au hasard, comprenant des observations avec 960.327 4.649 16.235 utilisateurs et les films. La seconde contient 10% d'utilisateurs choisis au hasard et se compose de 10.049.248 observations avec 48.068 17.764 utilisateurs et les films. 4.2.2 Résultats Le tableau 5 montre le résultat obtenu sur les ensembles de données réelles, où nous évaluons le coût normalisé et le temps de calcul. Tout d'abord, observer les résultats sur les 20 groupes de discussion. On peut voir que notre algorithme à deux niveaux est deux fois plus rapide, avec un coût normalisé est d'environ 10% pire que celle de la première solution KHC, KHC (1). Ensuite, nos résultats sur l'ensemble de données Webspam, montre que notre algorithme de co-clusters à deux niveaux est deux fois plus rapide que KHC (1) ou 15 fois plus rapide que KHC, avec un coût normalisé à 10% de celle du KHC. - 104 - I. Bartcus et al. coût Normalisée Temps (s) de données 2L-KHC KHC (1) KHC 2L-KHC KHC (1) KHC 20 Newsgroups 0,0153 0,0163 0,0170 6510 12840 597600 Webspam 0,2160 0,2331 0,2427 43130 84859 716552 Netflix (1%) 0,0184 0,0190 0,0191 1529 5534 78399 Netflix (10%) 0,0199 0,0202 0,0202 29523 354888 3548888 TAB. 5 - Les résultats co-classification obtenus sur les données du monde réel. Enfin nos résultats sur les deux ensembles de données Netflix montrent une bonne performance pour notre 2L-KHC approche pro- posée. Nous pouvons voir que pour le Netflix avec 1% des utilisateurs, le calcul de l'algorithme de co-clusters à deux niveaux est trois fois plus rapide que la KHC (1) et 50 fois plus rapide que KHC, obtenant 4% pire coût normalisé. En outre, pour le Netflix avec 10% des utilisateurs, nous voyons que nos deux niveaux algorithme co-clusters est d'environ 12 fois plus rapide que KHC (1) et environ 120 fois plus rapide que KHC, perdant seulement 2% de la qualité de co-regroupement. Pour conclure, des expériences sur des données réelles montrent que, nos solutions duces plus rapide pro- approche de co-regroupement à deux niveaux sans diminuer considérablement la qualité des résultats co-regroupement. Ceci est particulièrement remarqué sur des données qui a besoin d'heures de calcul avec notre approche au lieu de jours de calcul avec KHC. En outre, il est à noter que nos deux niveaux co-classification des utilisations des ap- proche beaucoup moins de mémoire que KHC. Par exemple, le Netflix avec 10% des utilisateurs choisis au hasard, nécessite une machine avec au moins 10 Go de RAM pour exécuter KHC, alors que notre approche proposée co-regroupement à deux niveaux peut fonctionner sur la machine avec environ 1 Go de RAM. 5 Conclusions et perspectives Dans cet article, nous avons présenté un algorithme de co-clusters à deux niveaux en utilisant le critère MODL, qui permet de traiter de grands ensembles de données qui ne correspondent pas à la mémoire. Le premier niveau, qui est la phase Split, consiste en la séparation et les étapes consistant à co-classification de fines, tandis que le second niveau, qui est la phase d'agrégation consiste en la fusionner et les étapes postérieures à optimiser. Nous avons étudié chaque étape de notre algorithme de co-clusters à deux niveaux. Pour mettre en évidence les performances de notre algorithme de co-clusters à deux niveaux, nous avons effectué des expériences sur des données réelles et simulées. Nous notons que pour les petits ensembles de données, l'outil KHC est favorable, mais pour des données plus importantes, l'approche proposée est plus approprié si l'on veut obtenir une solution plus rapide, sans diminuer considérablement la qualité des solutions co-regroupement. Enfin, dans notre travail futur, nous allons nous concentrer sur notre co-classification algorithme ameliora- tion à deux niveaux, par exemple par parallélisation il. En outre, des expériences sur des ensembles de données plus importantes seront étudiées. Références Bennett, J. et S. Lanning (2007). Le prix de netflix. Dans Actes de l'atelier Coupe KDD 2007, New York, pp 3-6.. ACM. Bock, H. (1979). regroupement simultané des objets et des variables. Dans E. Diday (ed) Analyse des Données et Informatique, pp. 187-203. INRIA. - 105 - Un algorithme de co-clusters à deux niveaux pour ensembles de données très grand Boullé, M. (2011). modèles de grille de données pour la préparation et la modélisation dans l'apprentissage supervisé. En I. Guyon, G. Cawley, G. Dror, et A. Saffari (Eds.), Travaux Pratiques Motif Reconnaissance: Les défis en matière d'apprentissage machine, volume 1, pp 99-130.. Microtome Publishing. Castillo, C., K. Chellapilla et L. Denoyer (2008). le spam Web challenge 2008. 4ème Atelier interna- tional sur la recherche d'information accusatoire sur le Web (Airweb), Beijing, Chine. Dhillon, I. S., S. Mallela et D. S. Modha (2003). co-regroupement des informations-théorétique. Dans Proc. de l'ACM Neuvième SIGKDD Conférence internationale sur la découverte de connaissances et d'exploration de données, KDD '03, New York, NY, USA, pp. 89-98. ACM. Govaert, G. et M. Nadif (2008). regroupement bloc avec des modèles de mélange Bernoulli: fils de différentes approches comparai-. Informatique Statistiques et analyse des données 52 (6), 3233-3245. Govaert, G. et M. Nadif (2013). Co-Clustering (1re éd.). Wiley-IEEE Press. Guigourès, R., D. Gay, M. Boullé, F. Clérot et F. Rossi (2015). analyse exploratoire à l'échelle du pays des enregistrements détaillés des appels à travers la lentille de modèles de grille de données. Dans Actes du ECML / PKDD, pp. 37-52. Springer International Publishing. Hartigan, J. A. (1972). Direct Clustering d'une matrice de données. Journal de l'American Statistical Association 67 (337), 123-129. Li, H. et N. Abe (1998). cluster Word et homonymie à partir des données de cooccurrences. Dans Proc. de la 17e Conférence internationale sur la linguistique informatique - Volume 2, COLING '98, Stroudsburg, PA, USA, pp 749-755.. Cul. Comp. Linguistique. Malines, I. V., H. H. Bock et P. D. Boeck (2004). méthodes de classification à deux modes: un aperçu struc- Tured. Méthodes statistiques dans la recherche médicale 13 (5), 363-394. Mitchell, T. M. (1997). Machine Learning (1 ed.). New York, NY, USA: McGraw-Hill Companies, Inc. Papadimitriou, S. et J. Sun (2008). Disco: co-cluster réparti sur la carte-reduce: Une étude de cas vers pétaoctet échelle de bout en bout l'exploitation minière. En ICDM, pp. 512-521. IEEE Computer Society. La classification CV Croisée (co-classification) technique is juin Qui d'Përmet la sous la structure Extraire-Entre les existante jacente et les lignes d'colonnes Une Table de sous forme de Données blocs. applications several technique this utilisent, de cependant de co Nombreux algorithmes Clustering Actuels ne Passent pas à l'échelle. Une des approaches utilisées with is the succès MODL méthode, Qui optimize un critere de vraisemblance régularisée. Cependent, verser des tailles en plus importantes, sa reached this méthode limite. Dans this article, un Présentons NOUS de co nouvel algorithme-regroupement à deux levels, Qui du compte Tenu MODL Përmet de critère EFFICACEMENT de Données Traiter de très grande taille, ne pas Pouvant tenir en mémoire. Nos expériences montrent Que l'approach proposed en temps de gagne tout en calcul des solutions de produisant qualité. - 106 -