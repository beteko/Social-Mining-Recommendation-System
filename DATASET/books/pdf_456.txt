Développement d'un système recommender distribué en utilisant le framework Hadoop Raja Chiky, Renata Ghisloti, Zakia Kazi Aoul LISITE-ISEP 28 rue Notre Dame des Champs 75006 Paris firstname.lastname@isep.fr Résumé. La production de recommandations de haute qualité est devenue un défi ces dernières années. En effet, la croissance de la quantité de données impliquées dans le processus de recomman- dation pose des problèmes d'évolutivité et d'efficacité. Ces questions ont encouragé la recherche de nouvelles technologies. Au lieu de dévelop- ing un nouveau système de recommender nous améliorons une méthode déjà existante. Un cadre bué dis- était considéré en fonction de la qualité connue et la simplicité du projet MapReduce. Le projet Open Source Hadoop a joué un rôle fondamental dans cette recherche. Il sans aucun doute encouragé et facilité la construction de notre application, fournissant tous les outils nécessaires. Notre principal objectif dans cette recherche était de prouver que la construction d'un système de recommender distribué était non seulement possible, mais simple et productive. 1 Introduction La quantité d'informations dans le Web a considérablement augmenté au cours de la dernière décennie. Ce phé- nomène a favorisé l'avancée des systèmes de recommender domaine de recherche. Le but de systèmes de recommandations fournit des recommandations personnalisées. Ils aident les utilisateurs par sug- gesting objets utiles à eux, traitant généralement avec des quantités énormes de données. Amazon, par exemple, qui a intégré les systèmes recommender de personnaliser la boutique en ligne pour chaque utilisateur, a enregistré en 2003 plus de 29 millions d'utilisateurs et plusieurs millions d'articles de catalogue Linden et al. (2003). De nombreux systèmes de recommender approches ont été développées au cours des dix dernières années, mais une quantité considérable d'entre eux ont été construits et évalués avec de petits ensembles de données. En outre, le volume d'informations Web a considérablement augmenté au cours des dernières années, et pour cela, plusieurs systèmes de recommender souffrent de problèmes de performance et d'évolutivité lorsqu'ils traitent avec des ensembles de données plus importants. Notre principal objectif dans cet article est décrit une méthode pour surmonter éventuellement ces problèmes. Nous vous proposons un système recommender distribué, et nous avons l'intention de démontrer qu'il pourrait être facilement développé et présenter de bons résultats. Nous avons choisi la pente Un Lemire et Maclachlan (2005) comme algorithme recommender et nous étudions le projet Dean MapReduce et Ghemawat pour construire ce système distribué. - 275 - Développement d'un système recommender distribué en utilisant le framework Hadoop MapReduce est un cadre mis en place par Google qui supports de calcul distribué avec une grande quantité de données sur un cluster d'ordinateurs. Une implémentation open source de ce travail est disponible dans DE CADRE Apache Hadoop avait projet. Dans cet article, nous décrivons le processus d'adaptation de l'algorithme de recommender choisi de plate-forme Hadoop, puis, nous vérifions ses performances en comparant la version distribuée avec la méthode autonome. Le reste de cet article est organisé comme suit. La section 2 décrit l'état de l'art. Dans la section 3, nous présentons l'approche globale de la construction d'un système recommender distribué. La section 4 présente l'étude expérimentale et les résultats. Enfin, la section 5 conclut cet article et donne une perspective sur nos recherches actuelles et futures dans ce domaine. 2 Contexte: la pente Une pente OneLemire et Maclachlan (2005) est un moyen simple et efficace de l'algorithme de type recommender. Introduit par Daniel Lemire et Anna Maclachlan en 2005, elle implique une idée plus simple que la plupart des autres implémentations de filtrage collaboratif. Bien que ces calculent généralement la similitude entre les vecteurs des éléments en utilisant le cosinus ou les méthodes Pearson pois (1994), l'approche Slope One recommande articles aux utilisateurs en fonction de la différence moyenne dans les préférences des articles. L'idée principale de l'algorithme est de créer une relation linéaire entre les préférences des éléments tels que la relation F (x) = x + b Le nom « Slope One » vient du fait qu'ici le « x » est multiplié par « 1 ». Il calcule essentiellement la différence entre les évaluations des éléments pour chaque utilisateur (pour chaque élément que l'utilisateur a évalué). , Il crée alors une différence moyenne (diff) pour chaque paire d'éléments. Pour faire une prédiction du k article pour un utilisateur par exemple, il obtiendrait les évaluations que l'utilisateur A a donné à d'autres éléments et ajouter la différence (diff) entre chaque élément. Avec cela, nous pourrions obtenir une moyenne. Être Raí la note que l'utilisateur A a donné au point i, diff (i, j) = r la différence entre les cotes du point i et le point j, et en supposant que nous avons n éléments: La prédiction de la note que l'utilisateur A pourrait donner à k article est donnée par (rA1 + diff (k, 1)) + (+ rA2 diff (k, 2) + ... + (+ R1N diff (k, n)) n Ci-dessous, nous présentons la version pseudo-code . de l'algorithme Il peut être divisé en deux parties:. le pré-traitement et la phase de prédiction dans la phase pré-traitement, on calcule la différence entre toutes les valeurs de préférence élément-élément 1. pour chaque élément i {2. pour tous les autres éléments j { 3. pour chaque utilisateur u notation i et j {4. ajouter la différence à une diff moyenne (i, j)}}} - 276 - R. Ghisloti et al la phase de prédiction: 1. pour chaque élément i non classé. par un utilisateur u {2. pour chaque élément j {u par note 3. trouver diff (i, j) 4. ajouter cette diff note de u pour j}} 6. retour des éléments de haut prédit en termes d'exécution, le plus la phase coûteuse est celle prétraiter, qui peut être précalculée. la algorithme est très intéressante, car sa partie en ligne, la phase de prédiction, est rapide. Son temps de fonctionnement ne dépend pas du nombre d'utilisateurs, cela dépend principalement de la différence de note moyenne entre chaque paire d'éléments. Supposons que nous ayons les utilisateurs de m et n éléments, le calcul des différences moyennes pourraient prendre jusqu'à pas de temps MN2. Le stockage de la matrice de diff peut aussi être coûteux. Il peut prendre jusqu'à n (n-1) / 2 unités de stockage. 3 Approche MapReduce est un cadre mis en place par Google pour traiter des quantités de données de larges. Le cadre utilise une fonction simple idée dérivée de la carte communément appelé () et réduire () utilisés dans la programmation fonctionnelle (par exemple LISP). Il divise le principal problème en petits sous-problèmes et distribuer ces à un groupe d'ordinateurs. Il combine ensuite les réponses à ces sous-problèmes pour obtenir une réponse définitive. En premier lieu, la carte () reçoit un ensemble de paires de clé / valeur et produit une paire de clés intermédiaire / valeur en sortie. Ensuite, ces valeurs sont triées par le cadre et regroupés de manière que toutes les valeurs appartenant à la même clé sont ensemble (fonction de combinaison ()). Ainsi, en entrée pour réduire (), nous avons la même clé fournie par la carte () avec une liste des valeurs correspondantes. Ces données sont ensuite traitées par réduction () et on produit un signal de sortie final. Les données d'entrée primaire est coupé par le cadre, qui est aussi responsable de Man- vieillissement de la transition de données intermédiaire. Pour l'utilisateur, il ne reste que la tâche de définir les interfaces d'entrée / de sortie et de fournir la carte et réduire les fonctions. Sur la base de cette idée, nous avons commencé par identifier un format d'entrée pour créer notre propre MapReduce la pente d'une méthode. En tant que format, nous décidons d'utiliser un type typique d'un format de texte, avec un code d'utilisateur (identification de l'utilisateur), un ItemID (identification de l'article) et une note de cet utilisateur à cet élément. La pente de phase One peut être pré-traitement divisé en deux parties. La première est lorsque sont calculées les articles diffs pour chaque utilisateur. Dans la deuxième partie, tous les diffs pour chaque paire d'éléments sont ajoutés ensemble pour créer un différentiel moyen entre deux éléments. Notre approche est la suivante: Tout d'abord, les données d'entrée est divisé de manière indépendante afin de regrouper les évaluations des éléments à l'aide des utilisateurs en tant que clés. Ensuite, les diffs pour un utilisateur est calculé. Après cela, tous deux diffs intermédiaires serait uni pour calculer la diff moyenne pour chaque paire d'éléments. En se fondant sur cette méthode deux processus séparés MapReduce sont créés: une qui divise l'entrée en blocs indépendants et calcule le point diffs pour un utilisateur; et une autre que d'avoir les listes d'éléments diff, et calcule la overa ll diff entre deux éléments. - 277 - Mise au point d'un système de recommender distribué en utilisant le framework Hadoop Le premier présente MapReduce une carte () qui reçoit l'entrée d'origine, produit un ensemble de valeurs intermédiaires contenant chaque ligne du fichier d'entrée - avec un utilisateur, un élément et une note relatives à ces deux. Ces valeurs sont triées par le cadre et réduire () reçoit un utilisateur comme clé et une liste avec tous ses éléments et notes. Réduire () calcule les diffs entre chaque paire d'éléments pour ce certains utilisateurs et les retourner en sortie. La deuxième carte () serait une fonction d'identité, ce qui signifie que la même valeur donnée en entrée sera donnée en sortie. D'autre part, la réduire secondaire () recevrait comme la clé de la paire de l'élément <itemi, itemj> et en tant que valeurs d'une liste de tous les diffs liés à cette paire d'éléments. Il serait donc une simple question de calcul de la diff moyenne à chaque paire d'éléments, et nous aurait comme résultat une liste finale pour chaque paire d'éléments, contenant les moyennes diffs. 4 L'évaluation expérimentale Pour nos essais expérimentaux l'ensemble de données utilisé est celui fourni par MovieLens mov (2003). MovieLens est un système de recommender Web où les films de taux d'utilisateurs et reçoivent des recommandations. , Il fournit actuellement trois paquets de jeux de données: Le poing avec 100.000 notes pour 1682 films par 943 utilisateurs, le second avec 1 million d'évaluations pour 3.900 films par 6040 utilisateurs et le troisième avec le paquet de 10 millions d'évaluations pour 10681 films par 71567 utilisateurs . Ces cotes représentent les notes donnant par les utilisateurs de MovieLens aux films, variant de 1 (ne pas aimer un film tout) à 5 (vraiment aimer le film). Les fichiers contiennent des informations dans le UserId, ItemId, format Note. Dans cet article, nous utilisons le 1 million et 10 millions de paquets de notation. Pour assurer l'exactitude des demandes a été utilisé des données d'essai forgé; un petit groupe d'utilisateurs et note pour permettre une comparaison claire et simple des résultats. L'ensemble de données contient 5 utilisateurs, 10 articles et 28 notes. Les deux machines maître et esclave où les expériences ont eu lieu eu les mêmes spécifications du matériel et ont les caractéristiques suivantes: processeur Intel 2,66 GHZ, de 1.7GB ORY mem- et 100 Go de disque. Il fonctionne sur Ubuntu Linux 5.0. Nous utilisons un maître et deux esclaves dans le cas d'expérimentation entièrement distribuée. La première étape de cette procédure expérimentale était l'exécution de chaque approche SlopeOne avec chaque ensemble de données. Ensuite, tous les résultats de sortie ont été comparés à vérifier dans les résultats correspondants. Enfin, le temps d'exécution des approches ont été comparées et analysées. Les tests ont été réalisés en deux phases, une pour chaque ensemble de données. Dans chaque phase, on compare le temps d'exécution en millisecondes entre chaque pente Une approche (autonome, pseudo- distribuée et entièrement distribué) et, bien sûr, l'uniformité du résultat final. La première expérience a été avec les petites données forgées. La sortie de ces trois approches ont été comparées à notre résultat à la main et correcte. Tous les trois ont présenté la sortie correcte, ce qui prouve l'exactitude des algorithmes. Pour générer un temps d'exécution finale, chaque approche a été exécutée trois fois de suite, et le résultat présenté sur la figure 1 (a) sont la moyenne de ces trois exécutions. La deuxième phase expérimentale a testé l'ensemble de données 1 million MovieLens pour encore une fois les trois approches. L'exécution, comme ci-dessus, est une moyenne de trois exécutions simples et les resuls est donnée dans la figure 1 (b). Enfin, la dernière phase a testé l'ensemble de données de 10 millions de MovieLens. L'opération autonome n'a pas été en mesure de traiter cette quantité de données, présentant une exception java qui est java.lang.OutOfMemoryError. Les deux autres approches ont réussi à terminer la tâche et - 278 - R. Ghisloti et al. ! Temps (ms)! Temps (ms)! Temps (ms) (a) (c) (b) FIG. 1 - temps de Exectution pour les trois ensembles de données présente le temps d'exécution moyen représenté sur la figure 1 (c). De l'analyse des résultats de la figure 1, il est possible à noter que seulement à partir d'un certain point, il devient raisonnable d'utiliser Hadoop. Comme chaque cadre qui fournit la couche d'abstraction, l'exécution du cadre a un coût initial. En testant le premier et le deuxième ensemble de données, il est visible que le « poids » de l'exécution Hadoop était plus grand que les avantages qu'elle pourrait apporter. Par conséquent, la durée de fonctionnement de la méthode autonome était plus petite que les deux autres approches même pensé ces avaient plus de threads Java ou nœuds informatiques de travail. Dans le troisième scénario, nous pouvons voir l'approche entièrement distribuée surmonte celui distribué pseudo-. Une raison possible pour le pseudo faire mieux dans les autres scénarios est le coût impliqué dans l'établissement d'un cluster distribué. Avec l'augmentation de la quantité de données, les avantages d'un cluster distribué fait la différence, et ont fourni une performance plus rapide. Notre principal objectif dans cette recherche était de vérifier si l'approche distribuée vraiment apporté des avan- tages au recommender. Vérification des résultats finaux, il est clair que pour un petit ensemble de données telles que égal ou inférieur à 1 million d'évaluations méthode autonome est une meilleure approche. Cependant, il a été prouvé que lorsqu'il s'agit de plus grandes quantités de données, le framework Hadoop peut être une solution réalisable. Lorsque le volume de données croît, entièrement distribué a une meilleure performances indiquées que les deux autres méthodes. La méthode autonome n'a pas été en mesure d'effectuer du tout dans le troisième scénario et ne pouvait donc pas donner un résultat. 5 Conclusion Les systèmes sont confrontés à un défi Recommender important lorsqu'ils traitent avec une énorme quantité de données. Dans cet article, notre objectif principal est l'aide pour résoudre ce problème en décrivant un développe- ment facile d'un système de recommender distribué. Cela a été possible en utilisant des outils puissants tels que l'open source Hadoop, MapReduce et la mise en œuvre slopeone qui est un type efficace de l'algorithme de collaboration recommender. La simplicité de la pente a été ajoutée à la structure forte offerte par Hadoop a permis de compléter notre tâche. Nos résultats ont montré qu'en effet, un cadre distribué peut donner de bons résultats, et, espérons-le, ont encouragé l'intérêt dans ce domaine de la recherche. Notre travail futur sera d'analyser d'autres types d'algorithmes de recommandation d'étudier la possibilité de les rendre efficaces sur de grands ensembles de données en utilisant Hadoop ou tout autre avalaibe DE CADRE - 279 - Développement d'un système recommender distribué en utilisant le framework Hadoop fonctionne Nicolae et al. (2010). Nous envisageons également d'envisager une distribution générique des algorithmes de recommandation qui ne nécessite aucun effort dans la réécriture du code. Références Apache Hadoop, http://hadoop.apache.org/. (1994). GroupLens: Une architecture ouverte pour le filtrage collaboratif de Netnews. ACM. (En 2003). MovieLens ensemble de données, http://www.grouplens.org/data/. Dean, J. et S. Ghemawat. MapReduce: simplifié de traitement des données sur les grands clusters. pp. 137-150. Lemire, D. et A. Maclachlan (2005). Une pente en ligne pour Prédicteurs Note basée sur laboration oration Filtrage. Dans Proceedings of SIAM (Data Mining SDM'05). Linden, G., B. Smith et J. York (2003). recommandations Amazon.com: Point-to-point col- filtrage laborative. Volume 7, pp. 76-80. Nicolae, B., D. Moise, G. Antoniu, L. Bougé, et M. Dorier (2010). BlobSeer: Apporter à haut débit en lourd Concurrency à Hadoop Carte-reduce Applications. Dans 24 parallèle IEEE International et distribué Symposium traitement (IPDPS 2010), Atlanta, États-Unis. IEEE et ACM. La CV de l'information d'un Dañs LeWeb bureaux AUGMENTE Dix Dernières Années. Ce phénomène a la progression de favorisé la recherche Dans le domaine des Systèmes de recommendation. l'intention de Ontario CÉS D'Aider les en fournissant des Utilisateurs suggestions. Utiles Nous proposons papier d'Dans CE un algorithme de UTILISER recommendation et de favoriser existant sa montée en charge.Nous le ACDE verser utilisons cadre de développement Hadoop Qui propose du Paradigme juin MapReduce Implémentation Pour la répartition des Traitements. Notre principale ob jectif Dans papier is this de la construction Que prouver d'un Système de recommendation is non Distribué possible only Mais Qu'elle est simple, et also Bénéfique. - 280 -