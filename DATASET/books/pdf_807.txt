blockGTM_egc2008 dviBinary Block GTM Carte auto organisatrice probabiliste pour les grands tableaux binaires Rodolphe Priam Mohamed Nadif Gérard Govaert LMA Poitiers UMR CNRS 6086 Université de Poitiers BP 30179 86962 Futuroscope Chasseneuil Cedex France rpriam gmail com CRIP5 Université Paris Descartes 45 rue des Saints Pères 75270 Paris France mohamed nadif univ paris5 fr Heudiasyc UMR CNRS 6599 Université de Technologie de Compiègne BP 20529 60205 Compiègne Cedex France gerard govaert utc fr Résumé Ce papier présente un modèle génératif et son estimation permettant la visualisation de données binaires Notre approche est basée sur un modèle de mélange de lois de Bernoulli par blocs et les cartes de Kohonen probabilistes La méthode obtenue se montre à la fois parcimonieuse et pertinente en pratique 1 Introduction Bien que les méthodes d’analyse factorielle soient très puissantes et contribuent efficace ment à la visualisation des données les grands échantillons nécessitent de nouvelles méthodes mieux adaptées En effet les algorithmes de décomposition matricielle rencontrent leurs li mites sur les grands tableaux numériques en outre la construction de nombreux plans de projection du fait des grandes dimensions rend la tâche d’interprétation difficile pour recou per les informations disséminées sur ces plans Finalement une grande quantité de données implique une grande quantité d’informations à synthétiser et des relations complexes entre individus et ou variables étudiés Il est alors possible dans ce contexte d’utiliser les cartes de Kohonen ou cartes auto organisatrices SOM Kohonen 1997 qui sont des méthodes de classification automatique utilisant une contrainte de voisinage sur les classes pour conférer un sens topologique aux partitions obtenues La carte auto organisatrice originelle peut être vue comme une variante de l’algorithme des k means MacQueen 1967 intégrant une contrainte d’ordre topologique sur les centres Lorsque la matrice des données x est définie sur un ensemble I d’objets lignes observa tions et un ensemble J de variables colonnes attributs différentes approches de classifica tion automatique sont utilisées et la plupart des algorithmes proposés concerne généralement un des deux ensembles Ces algorithmes peuvent être modélisés par différentes approches Celle qui a suscité le plus d’intérêt ces dernières années est incontestablement l’approche mo dèle de mélange McLachlan et Peel 2000 Dans ce cadre il a été proposé diverses versions probabilistes de SOM telles que dans Lebbah et al 2007 Verbeek et al 2005 Luttrell 1994 Binary Block GTM Le papier Govaert et Nadif 2003 présente une extension du modèle de mélange pour ré pondre à l’objectif de la classification croisée dite aussi classification par blocs qui permet de tenir compte de I et J simultanément Différents modèles ont été proposés pour tenir compte de chaque type de données Ces méthodes Dhillon 2001 ont un grand intérêt en data mining car elles sont particuliè rement appropriées pour les grands ensembles de données en grande dimension Elles ne sont pourtant pas encore employées en visualisation alors qu’elles ont le potentiel pour fournir un outil très efficace et parcimonieux En effet elles utilisent beaucoup moins de paramètres que les modèles connus usuels tels que les modèles classiques de mélange Pour analyser le contenu d’un ensemble de données la visualisation est une étape crucial pour laquelle les modèles génératifs sont devenus très utiles En effet la taille croissante des ensembles de données rencontrés permet une estimation pertinente de variables cachées syn thétisant de manière interprétable l’information contenue dans les données Pour toutes ces raisons nous proposons dans ce papier de traiter la question de la visualisation par une ap proche basée sur le modèle de mélange croisé parcimonieux et l’algorithme GTM Bishop et al 1998 méthode de auto organisatrice probabiliste basée sur un modèle gaussien Ce papier est organisé comme suit Le deuxième paragraphe présente une brève intro duction du modèle de mélange croisé et une description rapide de l’algorithme Block EM Le troisième paragraphe est consacré au développement dans le cas binaire de l’algorithme Block Generative Topographic Model ou Block GTM Cet algorithme peut être vu comme une extension efficace du GTM à un modèle de mélanges de Bernoulli par blocs Un algorithme d’estimation y est présenté Le quatrième paragraphe présente des expériences numériques à partir de deux matrices binaires textuelles Enfin le dernier paragraphe résume les principaux résultats du papier et les perspectives originales de cette approche Dans la suite la matrice de données est notée x = { xij i ∈ I et ∈ J} où I est un ensemble de n objets lignes observations et J est un ensemble de d variables colonnes attributs Une partition z en g classes de l’échantillon I sera représentée par la matrice de classification zik i = 1 n k = 1 g où zik = 1 si i appartient à la classe k et 0 sinon Une notation similaire sera utilisée pour la partition w en m classes de l’ensemble J Par souci de simplification des formules les intervalles de variation des indices ne seront pas spécifiés par exemple nous noterons ∑ i j k ℓ au lieu ∑n i=1 ∑d j=1 ∑g k=1 ∑m ℓ=1 2 L’algorithme Block EM L’objectif de la classification par blocs est d’essayer de résumer cette matrice par des blocs homogènes Le problème peut être étudié sous l’approche d’une partition simultanée des deux ensembles I et J en g et m classes respectivement Dans Govaert 1983 1995 plusieurs al gorithmes ont été proposés pour obtenir une classification par blocs sur des tableaux de contin gence ou plus généralement sur des tables qui ont les même propriétés des données binaires continues ou catégorielles Dans Govaert et Nadif 2003 ces méthodes ont été modélisées par une approche basée sur des mélanges de lois Dans le contexte du problème de la classification par blocs la formulation du modèle de mélange classique peut être étendue pour proposer un modèle en blocs latents R Priam M Nadif G Govaert défini par une distribution en sommant sur l’ensemble des affectations de I × J f x θ = ∑ z w ∈Z×W p z θ p w θ f x|z w θ où Z et W dénote les ensembles de toutes les affectations possibles z de I et w de J Comme pour l’analyse en classes latentes les n × d variables aléatoires Xij générant les cellules xij observées sont supposées être indépendantes lorsque z et w sont fixés nous avons alors f x|z w θ = ∏ i j k ℓ ϕ xij αkℓ zikwjℓ où ϕ αkℓ est une distribution définie sur l’ensemble des réels R Par exemple lorsque les données sont binaires en notant θ = p q α11 αgm où p = p1 pg et q = q1 qm sont les vecteurs de probabilités pk et qℓ qu’une ligne et une colonne appartienne au kecomposant et au ℓecomposant respectivement nous obtenons le modèle par blocs latents de Bernoulli défini par la distribution suivante ϕ xij αkℓ = αkℓ xij 1 − αkℓ 1−xij Utiliser ce modèle est nettement plus parcimonieux qu’utiliser un modèle classique de mélange sur chaque ensemble I et J Par exemple avec n = 1000 objets et d = 500 variables et des probabilités de classes égales pk = 1 g et qℓ = 1 m si on a besoin de faire la classification automatique d’une matrice binaire en g = 4 classes en lignes et m = 3 classes en colonnes le modèle par blocs latents de Bernoulli impliquera l’estimation de 12 paramètres αkℓ k = 1 4 ℓ = 1 3 au lieu de 4 × 500 + 3 × 1000 pour les deux modèles de mélange de Bernoulli appliqués à I et J séparément Maintenant nous nous intéressons à l’estimation d’une valeur optimale de θ par l’approche du maximum de vraisemblance associé à ce modèle de mélange par blocs Pour ce modèle les données complétées sont le vecteur x z w où les vecteurs non observés z et w sont les labels La vraisemblance classifiante L θ x z w = log f x z w θ est notée LC z w θ L’algorithme EM Dempster et al 1977 maximise la vraisemblance LM θ par rapport à θ itérativement en maximisant l’espérance conditionnelle de la vraisemblance des données complétées LC z w θ par rapport à θ étant donné une estimation précédente courante θ t et les données observées x Q θ θ t = ∑ i k c t ik log pk + ∑ j ℓ d t jℓ log qℓ + ∑ i j k ℓ e t ikjℓ log ϕ xij αkℓ où c t ik d t jℓ et e t ikjℓ sont respectivement les probabilités a postériori sur les lignes les co lonnes et les cellules à l’itération t Malheureusement la structure de dépendance des variables Xij du modèle entraine des difficultés pour la détermination de e t ikjℓ Pour résoudre ce problème une approximation varia tionnelle remplaçant e t ikjℓ par le produit c t ik d t jℓ permet de fournir une bonne solution Govaert et Nadif 2005 Dans la section suivante nous développons un algorithme d’apprentissage intégrant une contrainte d’ordre topologique sur les paramètres αkℓ Binary Block GTM 3 Modèle et estimation Nous présentons le Binary Bock GTM une carte auto organisatrice générative par blocs pour une matrice binaire x dont chaque cellule xij est un réel 0 ou 1 Pour induire une auto organisation topologique des densités gaussiennes une approche par le GTM considère des coordonnées 2d pour les noeuds d’une grille rectangulaire imaginaire qui représente l’espace de projection Ce graphe planaire peut être vu comme une discrétisation d’une partie du plan sur lequel les données les n lignes de la matrice vont être projetées Comme chaque noeud doit correspondre à une classe chaque point 2d sur le plan est alors sujet à une transformation non linéaire afin d’être amené dans un espace de dimension h supérieure Une projection linéaire permet alors d’obtenir des centres de même dimension que les individus vectoriels Plus formellement afin d’obtenir une auto organisation des probabilités αkℓ ces dernières sont paramétrées par les g coordonnées sk dessinant une grille rectangulaire régulière sur le plan Ces coordonnées sont projetées dans un espace de plus grande dimension h soit en prenant pour les applications φ des bases fonctionnelles de type noyau ξk = Φ sk = φ1 sk φ2 sk · · · φh sk avec par exemple φ sk = exp − ||sk−mφ|| 2 σ2 φ où mφ est un centre posé ad’hoc et σφ une variance bien choisie Finalement la paramétrisation nécessite l’estimation de m vecteurs de dimension h inconnus nommés wℓ On écrit les probabilités du BEM binaire original à l’aide de fonctions sigmoïdes αkℓ = σ wTℓ ξk où σ y = e y 1+ ey comme montré en figure 1 Le vecteur de paramètres devient θ = p q w1 w2 · · · wm FIG 1 – A gauche la grille rectangulaire des sk sur la droite l’espace des distributions ϕ Le graphique représente la paramétrisation non linéaire des sigmoïdes Chaque coordonnée sk pour k = 1 · · · g de la grille est transformée de façon non linéaire pour se retrouver dans l’espace des distributions multivariées de Bernoulli par la transformation σ wTℓ ξk pour l = 1 · · · m La matrice g×m de probabilités est remplacée par la matrice h×m et le modèle demeure parcimonieux puisque h est petit en pratique quelques dizaines Donc en reprenant à nouveau l’exemple d’une matrice binaire de 1000 lignes et 500 colonnes de la section précédente nous aboutissons à environ 100 paramètres compte tenu du choix à effectuer sur la valeur de h qui est toujours peu comparativement à une approche de mélange classique Ensuite les para R Priam M Nadif G Govaert mètres inconnus sont estimés en trouvant un maximum local de la log vraisemblance par un algorithme EM Dempster et al 1977 Grâce à l’approximation variationnelle de e t ikjℓ il peut être montré Govaert et Nadif 2005 que la maximisation de la log vraisemblance du Block EM est réalisée en maximisant alternativement deux critères conditionnels Q θ θ t |c et Q θ θ t |d avec c = cik i = 1 n k = 1 g et d = djℓ j = 1 d ℓ = 1 m A la convergence en une position stable de θ le paramètre optimal est nommé θ̂ Ici considérant des paramètres wℓ ces deux critères prennent la forme suivante Q θ θ t |d = ∑ i k c t ik { ∑ ℓ u t iℓ w T ℓ ξk − d t ℓ log 1 + e 1+wTℓ ξk } avec u t iℓ = ∑ j d t jℓ xij d t ℓ = ∑ j d t jℓ et un critère similaire Q θ θ t |c pour les colonnes avec vjk t = ∑ i c t ik xij et c t k = ∑ i c t ik La maximisation de ces deux espérances effectuée à l’aide de la méthode du gradient conduit aux relations suivantes w t+ 1 2 = argmaxw Q θ θ t |d et ensuite w t+1 = argmaxw Q θ θ t+ 12 |c En dérivant les deux critères on obtient les vecteurs de gradient Q t u Q t v et les matrices hessiennes H t u H t v Comme les hessiennes sont diagonales par blocs la log vraisemblance est augmentée à chaque pas EM par deux pas de montée de type Newton Raphson pour ℓ de 1 à m ce qui correspond à un algorithme EM généralisé En posant Φ = ξT1 ξ T 2 · · · ξ T g T la g × h matrice des bases fonctionnelles nous obtenons w t+ 12 ℓ = w t ℓ + 1 d ℓ ΦT GFℓΦ −1 ΦT Cuℓ − d ℓ Φ T Gαℓ w t+1 ℓ = w t+ 12 ℓ + 1 d ℓ ΦT GFℓΦ −1 ΦT V dℓ − d ℓ Φ T Gαℓ où C est la matrice g × n des probabilités a postériori avec c t ik pour cellules V la matrice g × d avec v t jk pour cellules G la matrice diagonale g × g avec c t k sur sa diagonale Fℓ la matrice diagonale g × g avec α t kℓ 1 − α t kℓ à ℓ fixé sur sa diagonale αℓ le vecteur g × 1 avec les α t kℓ à ℓ fixé pour valeurs uℓ le vecteur n × 1 avec les u t iℓ à ℓ fixé pour composantes dℓ le vecteur d×1 composé des d t jℓ à ℓ fixé et d ℓ = d t ℓ Enfin pour 1 ≤ ℓ ≤ m on a w t ℓ ∈ R h En itérant t et le calcul de w t+1 ℓ les valeurs consécutives courantes convergent vers un maxi mum d’une approximation de LM θ Un biais bayésien Bishop et al 1998 peut éventuel lement être ajouté pour améliorer la stabilité numérique des estimations La forme matricielle obtenue par une approche de gradient du second ordre est analogue à une étape d’IRLS Mc Cullagh et Nelder 1983 Une alternative serait un gradient au premier ordre sous optimal en pratique On remarque enfin que la symétrie des formules du BEM originale est ici absente du fait que seules les lignes sont projetées par la méthode proposée 4 Expériences numériques Nous évaluons notre nouvelle méthode de projection à partir de deux matrice binaires de données textuelles Les paramètres utilisés dans nos expériences sont m = 10 g = 81 et Binary Block GTM h = 28 pour les deux bases de textes La projection sur le plan d’un échantillon de données binaires par le modèle Block GTM peut s’effectuer de diverses manières dont essentiellement La représentation matricielle qui place en sk l’ensemble des individus affectés à la classe associée au noeud tels que ẑi = k Cette affectation obéit à la règle du maximum a postériori MAP donc ẑi = argmaxk ĉik Dans le cas du SOM l’affectation utilise la distance eucli dienne entre le vecteur centre et le vecteur donnée La deuxième représentation que nous avons utilisée dans la suite car celle ci est plus fidèle à la classification floue obtenue consiste en une projection par position moyenne sur le plan p̂i = ∑ k ĉiksk On remarque que la projection MAP correspond à la projection moyenne dans laquelle on remplace la matrice de classification floue de cellules ĉik par la matrice de classification dure de cellules ẑik FIG 2 – Projection par Binary Block GTM de la matrice textuelle 449 × 167 des données Classic 3 La première matrice est projetée pour tester le modèle avec trois classes Ces données cor respondent à un échantillon de la matrice Classic 3 Dhillon et al 2003 qui est constituée de trois bases d’articles scientifiques Medline Cisi Cranfield Par tirage au hasard 450 documents ont été sélectionnés avec 150 documents dans chaque classe Seuls les mots les plus fréquents au dessus du seuil 30 ont été retenus La matrice finale est de 449 lignes et 167 colonnes La projection sur la figure 2 sépare les classes sans erreur quasiment Les outliers peuvent s’expliquer par le fait que le tableau original est de contingence et également que les classes ne sont pas exactement disjointes comme le révèle les benchmarks relatifs La seconde matrice textuelle présente quatre classes et compte 400 documents décrits par 100 termes Girolami 2001 Le vocabulaire a été choisi par tri selon l’information mutuelle R Priam M Nadif G Govaert évaluée grâce aux labels des classes La projection de ces textes à la figure 3 révèle 4 clus ters facilement reconnaissables et correspondant aux quatre groupes de discussion "sci crypt" "sci space "sci med" et "soc religion christian" dans chacun 100 news ont été tirés au ha sard Les classes sont bien séparées avec des frontières précises et les classes de mots obtenues peuvent être interprétées Notons que la carte obtenue avec notre approche est assez similaire à la carte auto organisatrice probabiliste basée sur un modèle multinomial asymétrique Kabán et Girolami 2001 qui est moins parcimonieux FIG 3 – Projection par Binary Block GTM de la matrice textuelle 400 × 100 des données newsgroups 5 Conclusion et perspectives Nous avons proposé une carte auto organisatrice probabiliste Celle ci est obtenue par l’uti lisation d’un modèle de mélange de Bernoulli croisé et de l’algorithme GTM Notre méthode appelée Binary Block GTM est efficace et parcimonieuse En effet le nombre de paramètres inconnus est égal à h × m ce qui est très peu comparativement à un modèle contraint de mélange de lois de Bernoulli Girolami 2001 ou une approche dyadique comme un pLSA bi naire contraint Priam et Nadif 2006 Quelle que soit g la taille de la carte de projection quel que soit n le nombre de lignes le nombre de paramètres du modèle ne croît qu’avec le nombre de classes en colonnes En conclusion le modèle présenté apparait clairement comme un ex cellent candidat pour s’attaquer aux problèmes du data mining Il serait intéressant d’étendre cette approche au tableau de contingence en proposant un Block GTM adapté Références Bishop C M M Svensén et C K I Williams 1998 Developpements of generative topo graphic mapping Neurocomputing 21 203–224 Binary Block GTM Dempster A N Laird et D Rubin 1977 Maximum likelihood from incomplete data via the em algorithm J Royal Statist Soc Ser B 39 Dhillon I 2001 Co clustering documents and words using bipartite spectral graph partitio ning In Seventh ACM SIGKDD Conference San Francisco California USA pp 269–274 Dhillon I S S Mallela et D S Modha 2003 Information theoretic co clustering In Proceedings of The Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 2003 pp 89–98 Girolami M 2001 The topographic organization and visualization of binary data using multivariate bernoulli latent variable models IEEE Transactions on Neural Networks 20 6 1367–1374 Govaert G 1983 Classification croisée Thèse d’état Université Paris 6 France Govaert G 1995 Simultaneous clustering of rows and columns Control and Cyberne tics 24 4 437–458 Govaert G et M Nadif 2003 Clustering with block mixture models Pattern Recognition 36 463–473 Govaert G et M Nadif 2005 An EM Algorithm for the Block Mixture Model IEEE Trans Pattern Anal Mach Intell 27 4 643–647 Kabán A et M Girolami 2001 A combined latent class and trait model for analysis and visualisation of discrete data IEEE Transactions on Pattern Analysis and Machine Intelli gence 859–872 Kohonen T 1997 Self organizing maps Springer Lebbah M N Rogovschi et Y Bennani 2007 Besom Bernoulli on self organizing map In International Joint Conferences on Neural Networks IJCNN’2007 Luttrell S P 1994 A Bayesian analysis of self organising maps Neural Computation 6 767–794 MacQueen J B 1967 Some methods for classification and analysis of multivariate obser vations In L M L Cam et J Neyman Eds Proc of the fifth Berkeley Symposium on Mathematical Statistics and Probability Volume 1 pp 281–297 University of California Press McCullagh P et J Nelder 1983 Generalized linear models London Chapman and Hall McLachlan G J et D Peel 2000 Finite Mixture Models New York John Wiley and Sons Priam R et M Nadif 2006 Carte auto organisatrice probabiliste sur données binaires in french RNTI EGC’2006 proceedings 445–456 Verbeek J J N A Vlassis et B J A Kröse 2005 Self organizing mixture models Neu rocomputing 63 99–123 Summary This article presents a generative model and its estimation allowing to visualize binary data Our approach is based on the Bernoulli block mixture model and the probabilistic self organizing maps The obtained method is parcimonious and relevant on real data 