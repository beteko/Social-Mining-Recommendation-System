carine redaction egc2007 egcsoumission dviune approche paramétrique bayesienne estimation densité conditionnelle rangs carine boullé france télécom avenue pierre marzin 22307 lannion cedex carine orange ftgroup boulle orange ftgroup résumé intéressons estimation distribution rangs variable cible numérique conditionnellement ensemble prédicteurs mériques proposons nouvelle approche paramétrique bayesienne effectuer partition rectangulaire optimale chaque couple cible prédicteur uniquement partir rangs individus montrons ensuite comment effectifs grilles permettent construire estimateur univarié densité conditionnelle rangs estimateur multivarié utilisant hypothèse bayesienne naïve estimateurs compa meilleures méthodes évaluées récent challenge estimation densité prédictive estimateur bayésien utilisant ensemble prédicteurs révèle performant estimateur univarié estimateur binant prédicteurs donne résultats malgré simplicité introduction cette introduction décrivons abord situation particulière prentissage supervisé intéresse prédire cible plutôt valeur exposons ensuite approches permettent passer prédiction ponctuelle régression description prédictive présentons ensuite notre contribution fournir estimation densité conditionnelle complète cible approche bayesienne paramétrique régression valeur régression apprentissage supervisé distingue généralement grands problèmes classifi cation supervisée lorsque variable prédire symbolique régression lorsqu prend valeurs numériques certains domaines recherche informations intérêt réside cependant individu rapport variable plutôt valeur cette variable exemple problématique initiale moteurs recherche classer pages associées requête valeur intrinsèque score outil produire classement indépendamment nature problème traiter utiliser rangs plutôt valeurs pratique classique rendre modèles robustes valeurs atypiques hétéroscédasticité régression linéaire exemple estima utilisant rangs centrés équation moindres carrés minimiser proposé approche paramétrique bayesienne estimation densité prédictive rangs hettmansperger mckean apprentissage supervisé dédié variables nales connu terme régression ordinale ghahramani communauté statistique approches utilisent généralement modèle linéaire généralisé notamment modèle cumulatif mccullagh hypothèse relation ordre stochastique espace prédicteurs apprentissage automatique plusieurs techniques employées classification supervisée régression métrique appliquées régression ordinale principe minimisation structurelle risque herbrich algorithme utilisant perceptron appelé pranking crammer singer utilisation machines vecteurs support shashua levin keerthi problèmes considérés auteurs comprennent cepen échelle rangs fixée préalable relativement restreinte ordre autrement problème ramène prédire partile trouve cible ayant défini partiles avant processus apprentissage rapproche alors problème classification algorithmes évalués selon bonne classification erreur prédiction entre partile partile prédit description complète prédictive agisse classification régression prédicteur recherché généralement ponctuel retient alors uniquement classe majoritaire classification espérance conditionnelle régression métrique indicateurs peuvent révèler insuffisants notam prédire intervalles confiance également prédiction valeurs extrêmes contexte régression quantile estimation densité permettent décrire finement prédictive régression quantile estimer plusieurs quantiles conditionnelle quantile conditionnel défini comme petit fonction répartition conditionnelle supérieure reformulé comme minimisation fonction adéquate estimation quantiles exemple obtenue utilisation splines koenker fonctions noyaux takeuchi travaux proposés chaudhuri chaudhuri combinent partitionnement espace prédicteurs selon arbre approche polynômiale locale technique récente forêts aléatoires étendue estimation quantiles conditionnels meinshausen régression quantile quantiles souhaite estimer fixés avance performances évaluées chaque quantile techniques estimation densité visent fournir estimateur densité condi tionnelle approche paramétrique présuppose appartenance conditionnelle famille densités fixée avance ramène estimation estimation ramètres densité choisie approches paramétriques affranchissent cette hypothèse utilisent généralement principes estimateur densité chaque point utilisant données contenues voisinage autour point autre hypothèse émise forme recherchée localement estimateur répandues méthodes dites noyau définissent voisinage chaque point convo luant empirique données densité noyau centrée point forme noyau largeur fenêtre paramètres régler notion voisinage définie techniques diffèrent selon famille estimateurs visée approche polynômiale locale regroupe estimateurs constants linéaires ordre supérieur également chercher approximer densité fonctions splines cette marche estimation complète adoptée régression ordinale keerthi utilisant processus gaussiens cadre bayésien notre contribution proposons approche bayésienne paramétrique estimation conditionnelle cible numérique notre approche utilise statistique ordre amont processus apprentissage manipulation exclusive rangs détriment valeurs notre estimateur invariant toute transformation monotone données sensible valeurs atypiques problème étudié intéresse variables ordinales variables numériques entendu ramener calculant rangs exemples partir leurs valeurs contrairement problèmes habituellement traités régression ordinale considère amont apprentissage échelle globale rangs nombre exemples notre méthode effectue partitionnement optimal utilise information prédicteur mettre évidence plages rangs cible nombre avance disposant échantillon données taille finie souhaitant émettre hypothèse supplémentaire forme densité prédictive restreignons densités conditionnelles rangs constantes chaque cellule notre estimateur ramène vecteur estimateurs quantiles conditionnelle rangs différence régression quantile choix quantiles décidé préalable guidé partitions obtenues suite positionnement seconde partie consacrée description approche partitionnement classification supervisée partitionnement régression détaillons troisième partie comment obtenir estimateur univarié estimateur multivarié bayesien densité prédictive partir effectifs partitionnements quatrième partie estimateurs obtenus testés quatre données proposés challenge récent comparés autres méthodes compétition dernière partie consacrée conclusion méthode partitionnement régres présentons approche partitionnement classification super visée boullé extension partitionnement régression approche classification supervisée classification supervisée objectif méthode partitionnement discréti domaine prédicteur valeurs numériques manière mettre valeur informations variable cible chaque intervalle compromis trouvé entre qualité information prédictive capacité discrétisation discriminer classes cibles qualité statistique robustesse discrétisation généralisation exemple nombre instances chaque classe données newman approche paramétrique bayesienne estimation densité prédictive rangs tracé fonction largeur sépales gauche discré tisation consiste trouver partition donne maximum informations répartition trois classes connaissant intervalle discrétisation sepal width versicolor virginica setosa versicolor virginica setosa total discrétisation variable largeur sépale classification données trois classes approche boullé considère discrétisation comme problème lection modèle ainsi discrétisation considérée comme modèle paramétré nombre intervalles leurs bornes effectifs classes cible chaque intervalle mille modèles considérée ensemble discrétisations possibles cette famille distribution priori hiérarchique uniforme chaque niveau selon laquelle nombre intervalles discrétisation distribuée manière équiprobable entre nombre exemples étant donné nombre intervalles distributions effectifs chaque intervalle équiprobables étant donné intervalle distributions effectifs classe cible équiprobables distributions effectifs classe cible chaque intervalle indépendantes autres adoptant approche bayesienne recherche alors modèle vraisemblable connais données utilisant formule bayes probabilité constante modèle modèle celui maximise produit modèle données modèle formellement nombre exemples nombre classes cible nombre intervalles discrétisation nombre exemples intervalle nombre exemples intervalle appartenant classe cible classification supervisée nombres exemples classe cible connus caractérise modèle discrétisation paramètres remarque mesure discrétisation caractérisée effectifs intervalles invariant toute transformation monotone données considérant ensemble discrétisations possibles lequel adopte distribution priori hiérarchique uniforme décrite précédemment obtient négatif produit modèle données modèle écrire forme critère évaluation suivant trois premiers termes évaluent négatif probabilité priori premier terme correspond choix nombre intervalles second choix leurs bornes troisième terme décrit répartition classes cibles chaque intervalle dernier terme négatif vraisemblance données conditionellement modèle trouvera détails nécessaires obtention critère boullé pouvoir traiter importants volumes données heuristique gloutonne ascendante proposée boullé améliorer solution obtenue optimisations menées voisinage cette solution tentant enchaînements coupures fusion intervalles additivité critère permet réduire complexité algorithme jnlog optimisations comprises exemple donné gauche partition optimale obtenue décrite table contingence figurant droite bornes intervalles obtenues moyennant valeurs dernier individu intervalle premier individu intervalle suivant déduire règles telles largeur sépale comprise entre probabilité occurence classe versicolor approche régression petal length diagramme dispersion variable longueur sépale fonction longueur pétale données petal length petal length grilles discrétisation cellules mettant valeur corrélation entre variables longueur pétale longueur sépale données illustrer problème partitionnement lorsque prédicteur cible variables numériques présentons diagramme dispersion variables gueur pétale longueur sépale données figure montre longueur pétale inférieure toujours sépales longueur inférieure approche paramétrique bayesienne estimation densité prédictive rangs sépare valeurs variable cible longueur sépale intervalles inférieur supérieur décrire répartition cette variable condition nellement prédicteur longueur pétale discrétisation prédicteur comme classification supervisée objectif méthode partitionnement décrire distribution rangs variable cible numérique étant donné prédicteur discrétisation prédicteur cible comme illustrée permettent telle description rapport classification supervisée partition grille modélisée paramètre plémentaire nombre intervalles variable cible compromis trouvé entre qualité information corrélation détectée entre variables capacité généralisation grille formellement caractérise modèle discrétisation paramètres nombre exemples chaque intervalle cible déduit sommation nombres exemples cellules intervalle adopte priori suivant paramètres nombres intervalles indépendents distribués uniformément entre nombre intervalles donné toutes partitions intervalles rangs dicteur équiprobables intervalle source donné toutes distributions exemples intervalles cibles équiprobables distributions exemples intervalles cible chaque intervalle prédicteur indépendantes autres intervalle cible donné toutes distributions rangs équiprobables utilisant modèle discrétisation priori définis précedemment écrire logarithme négatif produit données forme critère modèle discrétisation rapport critère obtenu classification supervisée terme plémentaire prise compte nombre intervalles cible selon priori terme additif évalue vraisemblance distribution rangs exemples chaque intervalle cible présentons succintement algorithme adopté minimiser critère débutons partition initiale aléatoire cible critère décroît optimisons alternativement partition prédicteur partition fixée cible partition cible partition fixée prédicteur répétons processus plusieurs partitions initiale aléatoire cible retournons partition minimise critère pratique converge effectue rapidement trois itérations discrétisations effectuées selon algorithme optimisation utilisé classification supervisée valeur critère modèle discrétisation donné relié probabilité modèle explique cible indicateur évaluer prédicteurs problème régression prédicteurs peuvent classés probabilité décroissante total diagramme dispersion partitionnement effectifs grille données synthetic capacité expliquer cible boullé fournir indicateur norma considère fonction suivante critère modèle unique intervalle prédicteur cible compression prend valeurs entre mesure modèle toujours évalué notre gorithme optimisation modèle maximal meilleure description rangs cible conditionnellement prédicteur discrétisation estimation densité condition nelle rangs univarié passage partitionnement estimation densité conditionnelle univariée illustrée données synthétique proposé récent challenge predictive certainty environmental modelling cawley comporte exemples prédicteur diagramme dispersion ainsi partition obtenue représentés intervalles rangs notés dicteur cible comme partitionnement bornes intervalles notées obtenues moyennage valeurs individu intervalle premier individu intervalle suivant valeur prédicteur nouvel individu plage rangs laquelle appartient effectifs grille permettent calculer directement probabilité cible nouvel individu intervalle donné pmodl supposant densité conditionnelle rangs constante chaque plage rangs cible délimite grille obtient expression probabilités élémentaires pmodl 1fermés gauche ouverts droite approche paramétrique bayesienne estimation densité prédictive rangs estimations fonction répartition conditionnelle univariée rangs intervalles discrétisation prédicteur intervalle obtient timateur fonction répartition conditionnelle rangs cumulant probabilités élémentaires pmodl estimateurs fonction répartition conditionnelle prédicteur tracés chacun intervalles prédicteur multivarié prédicteurs première approche construire estima hypothèse bayesienne naïve prédicteurs soient indépendants conditionnel lement cible soient coordonnées nouvel individu espace prédicteurs intervalle discrétisation auquel appartient chaque composante hypothèse bayesienne naïve probabilité élémentaire multivarié écrit alors cette dernière expression estimée grâce effectifs partitionnements effet directement estimer premier facteur probabilité empirique concerne produit premier facteur numérateur obtient selon principe probabilités univariées considérant plage rangs laquelle appartient après fusion partitions cible induites chaque prédicteur nombre intervalles cible partition cible résultant cette fusion effectif chaque plage rangs construction partitionnement cible associé chaque prédicteur inclus partitionnement multivarié effectif cellule associée intervalle prédicteur intervalle cible partitionnement multivarié chaque fraction produit précédent alors estimer evaluation estimateur densité conditionnelle rangs apprentissage supervisé fonctions score couramment utilisés prédicteur score logarithmique score quadratique prennent différentes formes selon tâche visée classification régression métrique ordinale approche déterministe probabiliste régression ordinale approches déterministes citées introduction évaluées erreur quadratique moyenne entre prédit considérant rangs comme entiers consécutifs prédiction probabiliste score logarithmique également utilisé keerthi données utilisons cette fonction score notre estimateur densité conditionnelle rangs nouvel individu insertion échantillon apprentissage estime probabilité élémentaire décrite evaluation expérimentale comme exposé introduction notre approche distingue problèmes habituellement traités régression ordinale estime distribution échelle globale rangs plage rangs restreinte rangs distincts autre méthodes fournissent estimateur complète positionner méthode avons choisi comparer premier autres estimateurs densité condition nelle valeurs avons choisi quatre données proposés récent challenge predictive uncertainty environmental modelling organisé crits tableau notre approche étant nature régularisée ayant aucun paramètre réglage prenons parti utiliser données apprentissage validation theoval competition approche paramétrique bayesienne estimation densité prédictive rangs synthetic precip prédicteurs apprentissage 22956 10546 10675 données challenge predictive uncertainty environmental modelling nombre prédicteurs nombre individus utilisés apprentissage calculer partitionnements optimaux outre effectifs chaque grille titionnements fournissent également indice compression défini permet évaluer manière univariée chaque prédicteur classer importance prédic données réels avons calculé estimateur multivarié bayesien utilisant ensemble prédicteurs également estimateur univarié utilisant uniquement prédicteur importance prédictive était élevée ainsi estimateur bayesien utilisant couple variables informatif connaissant valeurs associées rangs chaque estimateur rangs fournit estimateur fonction répartition conditionnelle valeurs cibles échantillon valeur cible individu effet pmodl pmodl calculer densité prédictive point partir quantiles conditionnels adopté hypothèses utilisées challenge savoir hypothèse densité condition nelle uniforme entre valeurs successives cible queues distribution exponentielles différents estimateurs obtenus comparées critère données avons abord constaté mauvaises performances estimateur bayesien utilisant ensemble prédicteurs trois données réels supérieur méthode référence calcule partir données apprentissage estimateur empirique marginale lorsque hypothèse pendance forte connu dégrade fortement estimation probabilités posteriori frank mauvaises performances certainement corrélations importantes entre prédicteurs tableau indique données estimateurs univarié bivarié ainsi meilleure méthode challenge méthode rence observe abord données estimateurs meilleurs méthode référence toutes méthodes soumises observe ensuite bonnes performances estimateurs notamment données precip estimateurs univarié bivarié placent bonnes performances prédicteur univarié montrent qualité partitionnement obtenu malgré manipulation exclusive rangs valeurs durant cette étape autre estimateur bivarié toujours meilleur estimateur univarié indique présence effet affecté masse probabilité chaque queue distribution 4exceptée soumission organisateur synthetic precip meilleure méthode soumise bivarié univarié référence valeur chacun données estimateur varié utilisant meilleur prédicteur univarié estimateur bayesien utilisant meilleur couple bivarié meilleure méthode soumise challenge estimateur marginal référence classement chaque méthode figure entre parenthèses après chaque valeur informations supplémentaires encourage améliorer estimateur bayésien biais sélection moyennage conclusion avons proposée approche bayésienne paramétrique estimation conditionnelle cible numérique notre méthode partitionne optimal chaque couple cible prédicteur effectifs chaque partitionnement permettent obtenir estimateurs univariés estimateur multivarié thèse bayesienne naïve indépendance prédicteurs œuvre estimateurs données proposés récent challenge démontrent qualité partitionne ments bonnes performances prédicteur univarié bayesien utilisant meilleur couple prédicteur encourage travailler amélioration bayesien utilisant ensemble prédicteurs références boullé bayes optimal discretization method continuous attributes machine learning boullé optimal bayesian discretization variable ranking gression ninth international conference discovery science cawley haylock dorling predictive uncertainty environmental model international joint conference neural networks 11096 11103 chaudhuri huang piecewise polynomial regression trees statistica sinica chaudhuri nonparametric estimation conditional quantiles using quantile regression trees bernouilli ghahramani gaussian processes ordinal regression journal machine learning research approche paramétrique bayesienne estimation densité prédictive rangs keerthi approaches support vector ordinal regression proceedings international conference machine learning crammer singer pranking ranking proceedings fourteenth annual conference neural information processing systems newman hettich repository machine learning bases estimation conditional densities sensitivity measures nonlinear dynamical systems biometrika frank trigg holmes witten naive bayes regression working paper hamilton waikato university department computer science herbrich graepel obermayer large margin boundaries ordinal regression chapter hettmansperger mckean robust nonparametric statistical methods arnold london koenker quantile regression econometric society monograph series cambridge university press mccullagh regression model ordinal discussion journal royal statistical society meinshausen quantile regression forests journal machine learning research shashua levin ranking large margin principles approaches proceedings fiveteenth annual conference neural information processing takeuchi sears nonparametric quantile estimation journal machine learning research summary regression problems interested predicting output variable value paper propose parametric bayesian approach estimate complete distribution output conditionally input variables first compute optimal partition which exhibit predictive information input variable toward output variable build univariate estimate naïve bayesian estimate conditional distribution output ranks partition frequencies these estimates evaluated datasets proposed during recent predictive uncertainty environmental modelling competition naïve bayesian estimate using input variables performs poorly univariate naïve bayesian estimate using couple input variables performs despite their simplicity