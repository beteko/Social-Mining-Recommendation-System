comparaison bornes théoriques accélération clustering incrémental passe nicolas labroche marcin detyniecki thomas baerecke paris place jussieu 75252 paris cedex france prenom résumé clustering incrémental passe repose affectation chaque nouveau point clusters existants général clusters peuvent représentés moyenne détermination exhaus cluster proche possède complexité quadratique nombre données proposons papier nouvelle méthode affectation stochastique chaque cluster minimise nombre comparaisons effec entre donnée chaque cluster garantir étant donné erreur acceptable affectation cluster proche plusieurs bornes théoriques bernstein hoeffding student comparées papier résultats données artificielles réelles montrent borne bernstein donne globalement meilleurs résultats notamment lorsqu réduite permet accélération forte processus clustering conservant nombre faible erreurs introduction clustering permet exploration ensembles données résumant forme groupes homogènes facilement caractérisables interprétables récemment veaux algorithmes proposés répondre problèmes traitement grands volumes données aggarwal philipp kranen seidl méthodes reposent généralement algorithmes réalisent seule passe données initiales malheureusement applicables données vectorielles lesquelles structures incrémentales description clusters existent zhang travail intéresssons général données nécessai rement vectorielles possible utiliser telles structures solution consiste résumer chaque cluster ensemble données compose possible point appelé médoide similaire autres données cluster problème détermination médoides affectation nouveau point clusters existants contexte incrémental possède complexité quadratique nombre données étant envisageable usage réels algorithmes implémentent généralement mécanismes échantillonnage réduire calculs comparaison bornes théoriques clustering incrémental appartenance cluster cependant échantillonner implique introduction incer titude représentation cluster algorithme passe traduire erreur affectation point cluster aggravé cette donnée ensuite utilisée représenter cluster auquel appartient papier proposons méthode stochastique affectation point cluster inspire principes inégalités concentration mettant œuvre bornes théoriques estimer distance réelle point chaque cluster ainsi gérer incertitude échantillonnage comparons trois bornes théoriques bernstein hoeffding student proposons également réduire artificiellement bornes pourcentage accélérer convergence erreurs nombreuses résultats expérimentaux donnés artificiels réels issus répertoire machine learning repository visent évaluer accélération processus cluste passe comptabilisant erreurs affectation rapport algorithme exhaustif intéressent évaluer qualité partition obtenue telle indices habituels erreur confusion résultats montrent meilleurs performances obtenues borne bernstein offre meilleur ratio nombre comparaisons entre données nombre erreurs observées expéri mentations montrent ailleurs réduction bornes théoriques permet améliorer performances pratique article organisé comme section présente succinct principales méthodes échantillonnage utilisées accélération clustering données vectorielles leurs limites section décrit principes généraux notre méthode sélection cluster décrit différentes bornes théoriques ainsi leurs variantes duites ensuite section présente résultats comparatifs expérimentaux entre méthode exhaustive considérée comme vérité terrain approches basées bornes théoriques réduites ensemble bases tests finalement section présente conclusions perspectives travail méthodes échantillonnage clustering données vectorielles approches principales existent traitement données vectorielles hasenfuss approches basées médoides limitent coordon centres clusters exemples données approches basées données relationnelles travaillent directement partir matrices distances similarité méthodes échantillonnage proposées réduire complexité quadratique résolution ainsi algorithme clara kaufman rousseeuw réduit complexité échan tillonnant alétoirement ensemble données autres méthodes comme clarans ester limitent recherche candidats doides voisins médoides actuels comme méthode floue linearized fuzzy medoids krishnapuram utilise degrés appartenance points clusters autres méthodes comme algorithme leader labroche remplace labroche détermination exhaustive médoide nombre réduit comparaisons aléatoires chaque cluster auteurs présentent mécanismes accélération données relationnelles approximation nyström visant réduire dimension matrice distance patch processing considérant échantillon carré matrice distances traiter grands volumes données cependant toutes méthodes échantillonnage paramètre seulement difficile déterminer priori manque également processus clustering indépendamment exemple taille clusters complexité forme éviter domingos proposent variante means limitée données vectorielles utilise borne hoeff maron moore minimiser nombre données nécessaires détermi nation centres chaque cluster garantissant erreur commise reste bornée erreur similairement papier proposer cadre général accélérer rithmes clustering passe données nécessairement numériques mécanisme compétition clusters reposant bornes théoriques permettre gérer incertitude distances issue échantillonnage accélération algorithmes clustering passe sélection clusters algorithmes passe particulier algorithmes clustering seule passe données considérées séquentiellement affectation cluster dépend uniquement estimation distance cette donnée clusters existants ordre données influence directe partition produite méthodes généralement celles paramétrées seuil distance utilisé décider donnée suffisamment proche clusters existants intégrer initier propre cluster lorsque plusieurs éventuellement clusters éligibles problème revient déterminer cluster optimise mieux fonction objectif algorithme clustering problème encore compliqué lorsque distance estimée partir échantillon données chaque cluster comme travail proposons mécanisme sélection clusters inspiré mécanisme pétition racing introduit heidrich meisner appliquer problème clustering adapter automatiquement taille échantillon nécessaire affectation point cluster erreur maximale tolérée affectation faisant pouvons simultanément accélérer algorithmes clustering passe limitant nombre calculs distances entre nouvelle donnée données classées également garantir borne supérieure erreur affectation rapport algorithme exhaustif réaliserait toutes comparaisons possibles hypothèse toutes comparaisons indépendantes pratique comme montre expérimenta tions hypothèse indépendance nécessairement vérifiée conduit réduire drastiquement nombre comparaisons nécessaires limitant erreurs niveaux faibles comparaison bornes théoriques clustering incrémental méthode statistique compétition racing entre clusters technique racing outil permet prendre décision incertitude résultant moins variables aléatoires ayant recoupement partiel intervalle confiance étant donné erreur racing compa raison entre variables aléatoires affinée lorsque réalisations variables aléatoires observées réalisations variance large plupart distributions attachées variables aléatoires recoupent lorsque obser vations réalisées variance décroît existe moment distributions séparent exception faite distributions exactement identiques assez données échantillonner chaque variable aléatoire après certain nombre observations vient possible prendre décision relation variables aléatoires grand petit fonction objectif problème certain niveau confiance respond borne supérieure erreur tolérée relations estimées petit grand mauvais clusters candidats éliminés variance demeure assez grande conduit concentration efforts calcul meilleurs candidats horvitz zilberstein beyer sendhoff 2007a formellement notre algorithme clustering seule passe représen distance estimée entre point actuel chaque cluster variable aléatoire comme indiqué faisons suite hypothèse naïve lesxi indépendantes distance entre donnée cluster supposée comprise entre bornes partir desquelles possible calculer étendue range anglais supposons également connaissons seuil confiance probabilité erreur proposons après trois méthodes principales estimer bornes intervalle confiance associé chaque variable aléatoire proposons modifier expression théorique bornes introduisant facteur réduction rendre strictes besoin intéressant noter retrouve expression exacte bornes théoriques première borne borne hoeffding maron moore représente moyenne empirique distances clusterxi après comparaisons définie comme désigne distance réelle cluster erreur probabilité indique chances distance réelle bornes seconde borne borne récente bernstein heidrich meisner repose écart empirique comme 3rlog labroche borne bernstein connue stricte borne hoeff audibert conduire accélérer davantage processus compétition entre clusters proposons comparer comportement cependant comme montre équations précédentes limitations potentielles bornes hoeffding bernstein étendue paramètre nécessaire calcul valeurs bornes espaces description données souvent bornés permet déduire valeur paramètre nombreux possible connaître cette valeur priori complexe calculer exhaustivement ferait perdre notre approche ailleurs toutes raisons également accélérer calculs resserrant borne contraignant proposons également borne student indépendante étendue données hypothèse distances clusters suivent normale variance inconnue désigne estimateur biaisé variance distance cluster implémentation méthode compétition algorithme algorithme clustering passe compétition entrée données matrice distances seuil distance construc nouveau cluster sortie partition sortie données initialiser ensemble clusters faire déterminer meilleur cluster utilisant mécanisme compétition algorithme affecter retourner partition calculée partir algorithme détaille schéma global notre méthode clustering chaque itéra nouvel objet considéré clusters existants compétition biais algorithme compétition permet filtrer graduellement ensemble clusters didats jusqu reste candidat données affiner prise décision effet chaque borne inférieure distance empirique cluster grande borne supérieure meilleur cluster actuel supprimé compétition opposé borne supérieure cluster petite borne inférieure vainqueur actuel celui remplace devient nouveau meilleur cluster enfin lorsqu différences singitificatives entre clusters restants vainqueur final compétition celui minimise distance empirique moyenne comparaison bornes théoriques clustering incrémental donnée issue compétition objet affecté cluster vainqueur distance inférieure seuil passé argument contraire donnée construit nouveau cluster algorithme algorithme compétition entre clusters entrée objet données ensemble clusters existants matrice distances sortie indice cluster remporte compétition initialiser cluster vainqueur compétition finie faire clusters faire selectionner aléatoirement nouvelle donnée cluster mettre distance moyenne empirique cluster distance ainsi bornes cluster supcw alors mettre cluster vainqueur supprimer clusters encore compétition supcw alors retourner compétition termine différence significative entre clusters alors retourner cluster minimise distance moyenne empirique résultats expérimentaux présentons résultats comparatifs entre méthode détermination exhaustive cluster proche notre méthode compétition basée bornes théoriques discutons ensuite influence paramètres méthode efficacité protocole expérimental résultats présentés différentes valeurs facteur réduction comprises entre quand trouve borne théorique originale probabilité erreur fixée ensemble tests valeur range exacte valeur seuil détermine nouveau cluster construit estimée similairement proposé algorithme leader labroche comme moyenne distances calculée échantillon aléatoire taille égale effectif total données enfin calcul partition manière exhaustive seulement tests réalisés chaque données chaque valeur facteur réduction borne labroche données statlogshuttle letterrecognition caractéristiques principales données artificielles réelles issues répartoire machine learning repository asuncion newman évaluation qualité résultats objectif notre évaluation détermi qualité partition manière classique indice notre référence donnée qualité partition méthode exhaustive expérimenta tions visent montrer impact notre méthode selon dimensions principalement accélération réduction nombre comparaisons nombre erreurs rapport méthode exhaustive évaluons travail temps calcul diffé rentes méthodes mesurer accélération souhaitons affranchir optimisa tions implémentation liées langage choisi contexte exécution bibliothèques liées façon pouvoir comparer identiquement cours temps méthodes basées bornes affranchir éventuelles erreurs précédentes également déterminer moment surviennent erreurs affectation rapport méthode exhaustive rimentations basées mesure erreur correction affectation chaque nouvelle donnée traitée ainsi chaque point données notre protocole déter cluster idéal méthode exhaustive prédit cluster serait choisi affectation chacune bornes différence entre cluster prédit cluster idéal erreur comptabilisée borne concernée point affecté cluster idéal données utilisation méthode exhaustive complexité quadratique comme référence notre mesure erreur tests effectués données taille intermédiaire permettant finir calcul temps raison nable donnant comportement méthodes grands données notamment accélération façon varier difficultés tests conduits données artificielles générées distributions données normales recouvrement moins important entre groupes autre réelles issues machine learning repository asuncion newman détail données retenus présenté tableau indique chacun nombre objets dimensionalité nombre attributs nombre clusters attendus résultats comparatifs application notre algorithme trois bornes bernstein hoeffding student différents données produit mêmes résultats différentes amplitudes comparaison bornes théoriques clustering incrémental nombre cumulé tests comparaisons données letter gauche erreurs affectation ensemble données droite différentes valeurs facteur réduction labroche comme montre figure borne bernstein permet obtenir meilleurs résultats borne hoeffding terme nombre comparaisons nombre erreurs probablement utilisation variance empirique permet resserer borne bernstein rapport celle hoeffding force cette amélioration dépend données exemple figure représen tatif nombre comparaisons inférieur environ bernstein rapport hoeffding borne student contrainte précédentes hypothèse distribution normale données réalise ainsi beaucoup moins comparaisons nombre erreurs beaucoup élevé effet données tests bornes bernstein hoeffding facteur réduction génèrent quasiment aucune erreur rapport méthode exhaustive alors borne student réduite commet erreurs colonne droite figure enfin accélération augmente nombre itérations nombre données traitées augmente meilleures estimations clusters candidats rapidement éliminés compétition accélère processus enfin autres tests rapportés article suggèrent mécanisme accélération également possible bénéfique petits données comme exemple discussion autour paramétrage accélération basée bornes théoriques présentées section admet comme paramètre probabilité erreur point problème clustering cette erreur indique probabilité nulle mauvais cluster retenu erreur affec tation supposée inférieure cette probabilité pratique comme montre figure données observons augmentation probabilité erreur réduit nombre comparaisons revanche seule borne student erreur affectation augmenter bornes berstein hoeffding générant erreurs borne originale résultat encore étudié pensons moment bornes bernstein hoeffding lâches retrouvent conséquent souvent décision ambigue entre plusieurs candidats affectation faisant alors généralement cluster correct garantie après avoir éliminé plusieurs candidats bornes bernstein hoeffding nécessitent également connaître avance range distances entre données figure illustre comportement observé données lorsque étendue multiplié puissances observe borne bernstein résiste mieux estimation étendue borne hoeffding toujours possible accélérer calculs étendue supérieure était prévue initialement comparaison bornes théoriques clustering incrémental analyse influence probabilité erreur gauche exemple représentatif borne bernstein données montre influence probabilité erreur nombre comparaisons droite influence proba bilité erreur nombre erreurs borne student influence estimation étendue distances range fonction bornes bernstein gauche hoeffding droite données conclusion perspectives papier présente nouvelle méthode clustering passe données vectorielles repose principe inégalités concentration définir nisme compétition racing estime distance nouveau point clusters minimisant nombre comparaisons nécessaires trois bornes bernstein hoeffding student comparées ainsi version réduite chacune entre elles résultats montrent notre algorithme permet réduire drastiquement nombre comparaisons cessaires rapport méthode clustering passe exhaustive pratique garanties théoriques bornes puissent assurées possible dépendance observations facteur réduction permet améliorer encore résultats observés notamment bornes bernstein hoeffding construction lâches borne student observons également cette accélération augmente nombre données classées laisse penser notre méthode particu lièrement adaptée traitement grands données conclusion point labroche général borne bernstein offre meilleur compromis entre accélération meilleur hoeffding nombre erreurs meilleur hoeffding student futur nouveaux tests doivent conduits correction erreurs partir vérité terrain approche exhaustive façon évaluer qualité partitions ainsi impact erreurs affectation affectations suivantes tests pourront conduits données grandes comme données usage internet produites continuellement lesquelles possible utiliser algorithmes incrémentaux classiques limités données numériques autres modèles statistiques statistics autres bornes borne serfling pourront également envisagés obtenir garanties théoriques terme pourra intéresser utilisation information ambiguité possible différencier clusters bornes estimées contexte incrémental déterminer notamment quand cluster quand plusieurs clusters peuvent fusionnés généralement identifier points moins importants cluster importance conséquence réduite références aggarwal watson framework clustering evolving streams asuncion newman machine learning repository versity california irvine school information computer sciences mlearn mlrepository audibert munos szepesvari tuning bandit algorithms stochastic environments algorithmic learning theory sendai japon beyer sendhoff 2007a evolutionary algorithms presence noise sample sample foundations computational intelligence beyer sendhoff 2007b robust optimization comprehensive survey computer methods applied mechanics engineering ester density based clustering evolving stream noise conference mining domingos hulten general method scaling machine learning algorithms application clustering machine learning morgan kaufmann ester kriegel knowledge discovery large spatial databases focusing techniques efficient class identification symposium advances spatial databases volume portland springer rastogi efficient clustering algorithm large databases sigmod management hammer hasenfuss relational neural annual german advances artificial intelligence springer verlag comparaison bornes théoriques clustering incrémental heidrich meisner hoeffding bernstein races selecting policies evolutionary direct policy search machine learning horvitz zilberstein computational tradeoffs under bounded resources artificial intelligence special issue computational tradeoffs under bounded sources kaufman rousseeuw finding groups introduction cluster analysis wiley krishnapuram joshi fuzzy relative medoids algorithm application document snippet clustering fuzzieee korea labroche inspired clustering algorithm usage mining conference paris france maron moore hoeffding races accelerating model selection search classification function approximation advances neural systems szepesvári audibert empirical bernstein stopping proceedings machine learning philipp kranen assent seidl clustree indexing micro clusters anytime stream mining knowledge information systems zhang ramakrishnan livny birch efficient clustering large databases sigmod international conference management montreal canada gisbrecht schleif hammer approximation techniques clustering dissimilarity neurocomput summary single incremental clustering relies efficient assignment point existing clusters general where necessarily possible resent clusters exhaustive assignment point cluster quadratic complexity number objects paper proposes novel stochastic signment method minimizes number comparisons between cluster guaranty given acceptable error point assigned nearest cluster several theoretical bounds considered bernstein hoeffding student compared paper results observed artificial berntein bound overall results especially reduced provides acceleration clustering while maintaining number errors