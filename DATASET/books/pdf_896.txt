microsoft docvers exploitation grandes masses données raphaël féraud boullé fabrice clérot françoise fessant france télécom avenue pierre marzin 22307 lannion contact raphael feraud orange ftgroup résumé tendance lourde depuis siècle dernier augmentation exponentielle volume données stockées cette augmentation traduit nécessairement information riche puisque capacité traiter données progresse aussi rapidement technologies actuelles difficile compromis trouvé entre œuvre qualité information produite proposons approche industrielle permettant augmenter considérablement notre capacité transformer données information grâce automatisation traitements focalisation seules données pertinentes fouille données grande volumétrie sélection variables sélection instances introduction selon fayyad mining processus trivial identification structures inconnues valides potentiellement exploitables bases données plusieurs intervenants industriels proposé formalisation processus forme guide méthodologique nommé crisp cross industry standard process mining chapman modèle crisp propose découper processus mining phases phase recueil besoins objectifs industriels critères succès évalue ressources contraintes hypothèses nécessaires réalisation objectifs traduit objectifs critères industriels objectifs critères techniques décrit résolution atteindre objectifs techniques phase compréhension données réalise collecte initiale données produit description étudie éventuellement quelques hypothèses visualisations vérifie niveau qualité données phase préparation données consiste construction table données modélisation chapman intéressons particulièrement suite phase modélisation procède sélection techniques modélisation place protocole qualité modèles obtenus construit modèles évalue selon protocole phase évaluation estime objectifs industriels atteints assure processus suivi déroulement escompté détermine phase suivante exploitation grandes masses données phase déploiement industrialise utilisation modèle situation opérationnelle définit contrôle maintenance produit rapport final effectue revue projet processus mining crisp modèle crisp essentiellement guide méthodologique conduite projet mining plupart praticiens mining accordent phases préparation données déploiement consomment elles seules ressources projets explication simple utilisation méthodes statistiques nécessite représenter données forme tableau croisé ligne instances colonnes variables caractérisant instances optimiser stockage données stockées bases données relationnelles phénomène étudié gènes transactions cartes bancaires sessions informations clients phase préparation données première tâche analyste extraire tableau croisé système information cette étape anodine nombre représentations potentielles données relationnelles tableau croisé gigantesque pratique analyste faire choix priori ensemble variables explicatives lesquelles étude conséquence perte information données relationnelles importante phase déploiement modèle construit préalablement appliqué toute population concernée produire score chaque instance toutes variables explicatives toutes instances doivent construites cette étape potentiellement couteuse lorsque nombre instances variables explicatives important principaux produits commerciaux mining comme proposent plateformes permettant construire déployer modèles prédictifs néanmoins offrent solution satisfaisante exploiter potentiel information contenue données source applications industrielles construites plateformes logicielles nombre variables explicatives partir desquelles construits modèles reste limité quelques centaines potentiel simplement autre ordre exemple illustratif présenté données contient tables étude nombre usages service semaine pourrait conduire seule construire 10000 variables explicatives féraud données relationnelles impose choisir priori ensemble agrégats conduit perte information technologies difficile compromis trouvé entre œuvre qualité information produite parmi produits commerciaux propose module permettant construire automatiquement agrégats partir données temporelles avantage pouvoir explorer grand nombre variables explicatives avons largement généralisé systématisé cette approche automatiser entièrement processus préparation données construction variables explicatives entièrement pilotée algorithme sélection représentation performant boullé section suivante décrirons principaux éléments cette architecture traitement données avant détailler algorithme sélection représentation faciliter déploiement modèles proposons méthode permettant extraire complète table réduite parangons cette table parangons constituée seules variables explicatives pertinentes score construit instances représentatives variables sélectionnées table parangons reliée complète index construit automatiquement toute information produite table parangons déployée simple jointure ensemble instances algorithmes permettant extraire parangons indexer efficacement décrits section avant conclure présentons section résultats expérimentaux encourageants obtenus données réelles groupe france télécom système pilotage extraction tableaux croisés dépôt numéro 07965 procédé dispositif construction utilisation table profils réduits parangons dépôt numéro 05412 exploitation grandes masses données automatisation préparation données généralités objectif sélection représentation triple améliorer performance prédictive modèles temps apprentissage déploiement modèles permettre interprétation guyon elisseeff sélection variables sujet couvert recherche fouille données aujourd méthodes sélection variables suffisamment robustes permettre construction modèle grande dimension guyon compris lorsque nombre variables grand devant nombre instances porter résultats applications industrielles proposons architecture traitement permettant algorithme sélection représentation piloter extractions tables données grande taille présenterons ensuite méthode boulle robuste rapide utilisée sélectionner meilleure représentation table principale table client clients possèdent services utilisent services paient factures architecture traitements contrairement architecture actuelle fouille données variables explicatives calculées avance datamart notre architecture traitements données permettant construire variables explicatives stockées données relationnelle simple folder variables explicatives construites sélectionnées automatiquement fonction étude menée modèle données folder permet assurer normalisation différentes sources données seront toujours présentées forme schéma étoile table principale correspond domaine étudié analyse données clients cette table comprendra informations directement liées client comme adresse tables faits liens multiples table principale chaque instance table principale correspond nombre variable faits féraud données télécommunications retrouvera exemple table décrivant services détenus table traçant usages services table récapitulant factures modélisation suffisamment expressif adapter types données structure étoile permet optimiser calcul variables lorsque agrégation table principale calcul variable nécessite seule jointure entre table principale table faits alors entrepôt données calcul variable impliquer grand nombre jointures enfin modèle données chaque table défini permet construire langages interrogation formatés automatisables pilotage extractions extractions entre folder tableaux croisés paramétrées trois types dictionnaires dictionnaire sélection filtrer instances dictionnaire requêtes spécifier mises données folder dictionnaire préparation spécifier recodage variables dictionnaires extraction permettent définir requêtes suffisamment simples pilotées automatiquement processus sélection représentation suffisamment expressives produire grande variété variables explicatives quelque objet requête portera toujours tables table principale tables faits agrégation trouvera toujours table principale opérateurs agrégation sélection porteront champs tables faits champs table principale exemple produire nombre utilisations chaque service nommé clients suffira spécifier requête porte table usages appel opérateur comptage consiste croiser champs journommé libellé idservice seule ligne ainsi possible spécifier plusieurs milliers variables explicatives sélection représentation architecture traitements permet algorithme piloter efficacement extractions tableaux croisés pouvant compter dizaines milliers variables sélectionner meilleure représentation possible avons besoin méthode sélection variable particulièrement robuste rapide approches principales filtre enveloppe kohavi proposées littérature sélectionner variables méthodes enveloppes coûteuses temps calcul féraud clérot lemaire féraud pourquoi avons retenu approche filtre déterminer construire variables explicatives pertinentes approche filtre fréquemment utilisée repose œuvre tests statistiques saporta comme exemple variables explicatives catégorielles tests student fisher snedecor variables explicatives numériques tests indépendance simples mettre œuvre présentent nombreux inconvénients exploitation grandes masses données limitent discrimination entre variables dépendantes indépendantes permettre ordonnancement précis variables explicatives contraints hypothèses applicabilité fortes effectifs minimaux hypothèse distribution gaussienne numérique nombreux autres critères évaluation dépendance entre variables étudiés contexte arbres décision zighed rakotomalala critères basés partition variable explicative intervalles numérique groupe valeurs catégoriel recherchant façon paramétrique modèle dépendance entre variables explicatives cible permettent évaluation importance prédictive variables explicatives modèles partitionnement variable explicative envisagés compromis trouvé entre finesse partition fiabilité statistique compromis réalisé approche minimum optimized description length boullé formulant problème comme problème sélection modèles adoptant approche bayesienne résultats obtenus approche particulièrement convaincants dernier performance prediction challenge discrétisation variable largeur sépale trois intervalles classification trois classes cette approche valable aussi variables numériques variables catégorielles variables partionnées intervalles numérique groupes valeurs catégoriel numérique modèle discrétisation correspond liste intervalles auxquels associées fréquences variable cible paramètres discrétisation particulière nombre intervalles bornes intervalles effectifs classes cibles intervalle approche bayesienne appliquée construire critère permettant sélectionner meilleur modèle discrétisation probable connaissant données discrétisation critère optimiser obtenu modelselect index féraud nombre individus nombre classes cibles nombre intervalle nombre individus intervalle nombre individus classe intervalle trois premiers termes représentent priori modèle choix nombre intervalles bornes intervalles distribution valeurs cibles chaque intervalle dernier terme représente vraisemblance observer valeurs variable cible connaissant modèle discrétisation titre illustratif donnons modèle discrétisation variable sepalwidth obtenu optimisation critère séparer différents blake critère construit groupement valeurs critère possède structure similaire celle critère discrétisation remplaçant premiers termes probabilité priori partition intervalles celle partition groupes valeurs nombre répartitions valeurs explicatives groupes valeur edible poisonous effectif brown yellow white cinnamon green purple yellow brown green purple white cinnamon groupement valeurs variable couleur chapeau classification mushroom classes illustre résultat groupement valeurs variable couleur chapeau grâce optimisation critère classification champignons comestibles vénéneux blake discrétisation groupement valeurs optimaux recherchés optimisant critères évaluation moyen heuristique gloutonne ascendante décrite boullé complexité algorithmique cette heuristique associée excellente fiabilité méthode permet traiter grand nombre variables ordre modèle bayesien sélectif boulle sélection régularisée variables moyenne modèles ensuite construit supprimer variables redondantes produire scores exploitation grandes masses données déploiement efficace principe faciliter déploiement modèle proposons extraire folder table parangons table parangons contient individus représentatifs variables explicatives utilisées modèle parangons reliés index toute population scores produits application modèle table parangons déployés toute population simple jointure entre table parangons index chaque instance entrepôt données ainsi attribuer score parangon cette méthode déploiement particulièrement efficace lorsque modèle déployer récurent exemple campagnes marketing mensuelles seule table réduite parangons construite chaque produire score toute population cette approche permet augmenter considérablement nombre scores récurrents pouvant produits architecture technique sélection parangons table parangons déterminante performance finale système table parangons représentative pourrait conduire construction scores inefficace ensemble population contrario parangons grande taille diminuerait sensiblement intérêt utilisation parangons devrons gérer mieux compromis entre réduction volumétrie représentativité sélectionner efficacement instances notre approche utiliser algorithme échantillonnage seule passe optimisant critère représentativité échantillon algorithme reservoir sampling permet construire échantillon grâce réservoir maintenu ligne ajoutant supprimant aléatoirement instances réservoir vitter algorithme stochastique adapté données taille infinie nécessairement stationnaires néanmoins problème cherchons résoudre nature différente connaissons table données voulons échantillonner algorithme déterministe adapté permet optimiser critère représentativité échantillon nombre connu avance itérations versions déterministes reservoir sampling existent algorithme deterministic reservoir sampling akcan minimise réservoir critère local inspiré algorithme distance entre échantillon ensemble total minimisée ajoutant supprimant ligne instances réservoir réduire temps traitement optimiser critère local contentons remplir mesure réservoir jusqu atteigne taille désirée supprimer instances risque théorique tomber minimum local pratique taille notre échantillon suffisamment importante termes proportion nombre instances risque faible ordre taille 10000 algorithme utilisé suivant réservoir initialisé premières instances rencontrées allant instance choisie fenêtre recherche taille manière minimiser critère qualité échantillon féraud fenêtre ensuite décalée instances manière obtenir échantillon taille lorsque table complète taille parcourue taille fenêtre recherche permet régler compromis entre traitement précision algorithme grand algorithme rapide précis paramètre permet initialiser algorithme optimisation petite valeur risque rendre totalement inefficace premières itérations optimisation utilisons critère mesurer proximité entre échantillon table complète algorithme discrétisation groupement valeurs décrit section précédente permet extraire représentation binaire chaque variable binaire correspond fréquence modalité variable discrétisée critère minimiser écrit alors fréquence théorique variable binaire donnée algorithme discrétisation groupement valeurs fréquence observée échantillon taille indexation problème cherche résoudre simple énoncer étant donné individu trouver proche voisin parmi parangons souhaite faire individus entrepôt recherche proche voisin opération coûteuse implémentation naïve implique recherche exhaustive parmi parangons complexité étant nombre individus cherche voisins nombre variables espace représentation nombre parangons accélérer recherche proche voisin amené préférer compromis entre vitesse performance plutôt viser performance maximale trouver proche voisin précisément permet algorithme locality sensitive hashing gionis repose technique hachage sélectionner candidats parmi parangons proche voisin instance considérée applique ensuite recherche exhaustive parmi candidats notre implémentation cette technique permet ramener complexité recherche proche voisin facteur parangons laisse utilisateur contrôle compromis vitesse performance expérimentations mesurer fiabilité scores produits notre approche avons comparé scores construits campagnes marketing france télécom notre technologie avons alimenté plateforme différents données provenant applications décisionnelles groupe france télécom avons consolidé informations clients groupe passé récent entre janvier exploitation grandes masses données quatre premiers utilisés construire profils clients derniers calculer variable cible clients réservés évaluation modèles évaluer qualité modèle utilisons courbe courbe permet sélectionner modèle rapport efficacité économique abscisses correspond proportion population visée courriers campagne ordonnées identifie pourcentage population cible touchée campagne marketing aléatoire déploiement direct parangons parangons aléatoires parangons 15000 parangons modèle actuel courbe différents modèles résiliation courbes différents modèles prévision résiliation abonnement tracées diagonale représente performance modèle aléatoire contactons population moyen toucherons clients rendre abonnement prochains modèle utilisé actuellement services marketing france télécom variables explicatives lorsque population contactée clients fragiles contactés rapport ciblage aléatoire automatisation recherche représentation conduit sélectionner modèle basant variables explicatives choisies ensemble variables modèle ensuite déployé toutes instances utilisant nombre variable parangons déploiement direct population déploiement direct toutes instances lorsque population contactée basant ciblage clients rendre abonnement prochains touchés rapport technique actuelle nombre courriers supplémentaire population cible touchée cette amélioration ciblage vraie toute courbe lorsque technique déploiement scores parangons utilisée perte potentielle fiabilité dépend nombre parangons utilisés nombre important ciblage proche meilleur possible couteux utiliser lorsque parangons utilisés représenter clients population clients fragiles touchés reste rapport féraud aléatoire rapport technique actuelle parangons performances obtenues quasiment similaires celles obtenues déploiement direct évaluer qualité algorithme sélection parangons avons comparé performances obtenues lorsque parangons sélectionnés manière aléatoire performances obtenues lorsque parangons sélectionnés optimisant entre distribution théorique variables celle obtenue échantillon parangons obtenus population cible atteinte sélection aléatoire contre optimisation locale processus complet extraction table parangons partir million clients espace recherche variables correspond heures calcul serveur comprenant quatre processeurs chacun mémoire tiers temps traitement correspond sélection représentation tiers recherche indexation parangons parangons obtenus production scores partir table parangons faite moins minute alors heures traitements nécessaires méthode habituelle générer table million instances caractérisées variables explicatives appliquer modèle cette table utilisation parangons efficace déployer score récurrent utilisant ordre parangons représenter population totale perte précision modèle négligeable déploiement divisé conclusion avons décrit plateforme fouille données permettant construire modèles prévision basés nombre variables explicatives ordres grandeurs dessus actuellement conséquence nette augmentation qualité modèles cette plateforme repose architecture novatrice permettant automatiser traitements couplée méthodes performantes construction sélection indexation variables instances temps traitement données reste principale limite exploration espace recherche grand aller exploration grandes masses information devrons élaborer méthode parcours espace variables permettant diriger rapidement zones contenant variables pertinentes références akcan astashyn brönnimann bukhman sampling multi dimensional technical report department polytechnic university blake repository machine learning databases mlearn mlrepository boullé bayes optimal approach partitioning values categorical attributes journal machine learning research boullé bayes optimal discretization method continuous attributes machine learning exploitation grandes masses données boullé compression based averaging selective naïve bayes classifiers journal machine learning research chapman clinton kerber khabaza reinartz shearer wirth crisp datamining guide scheuermann phase sampling based algorithm discovering association rules sigkdd fayyad piatetsky shapiro smyth mining knowledge discovery overview advances knowledge discovery mining press féraud clérot methodology explain neural network classification neural networks gionis indyk motwani similarity search dimensions hashing conference guyon elisseeff introduction variable feature selection journal machine learning research guyon nikravesh zadeh feature extraction applications springer kohavi wrappers feature selection artificial intelligence lemaire féraud driven forward features selection comparative study neural networks iconip reduction adaptive sampling communication information systems preparation mining morgan kaufmann saporta probabilités analyse données statistique editions technip vitter random sampling reservoir trans software zighed rakotomalala graphes induction hermes summary increase stored volumes strong trend recent years processing capabilities cannot exponential increase current technologies necessary strike difficult balance between processing quality resulting information client analysis platform which allows increasing dramatically capacity value added information automating exploration potential explicative variables space discovery relevant allowing treatments focused relevant