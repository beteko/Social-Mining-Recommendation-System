 dfinal dviA Metric Approach to Supervised Discretization Dan Simovici Richard Butterworth University of Massachusetts Boston Department of Computer Science Boston MA 02125 USA dsim cs umb edu University of Massachusetts Boston Department of Computer Science Boston MA 02125 USA rickb cs umb edu Résumé Nous présentons une nouvelle approche à la discrétisation su pervisée des attributs continues qui se sert de l’espace métrique des parti tions d’un ensemble fini Nous discutons deux nouvelles idées fondamen tales une généralisation des techniques de discrétisation de Fayyad Irani basée sur une distance sur des partitions dérivée de l’entropie généralisée de Daroczy et un nouveau critère géométrique pour arrêter l’algorithme de discrétisation Les arbres de décision résultants sont plus petits ont moins de feuilles et montrent des niveaux plus élevés d’exactitude etablis par la validation croisée stratifiée 1 Introduction Many machine learning and data mining algorithms can deal only with nominal attributes however many data sets of interest have numerical domains and this makes discretization the conversion from numerical to nominal domains an important task for data preparation The literature that deals with discretization is vast and it in cludes ideas ranging from fixed k interval discretization [Dougherty et al 1995] fuzzy discretization see [Kononenko 1993] Shannon entropy discretization due to Fayyad and Irani presented in [Fayyad 1991 Fayyad et Irani 1993] proportional k interval discretization see [Yang et Webb 2003] or techniques that are capable of dealing with highly dependent attributes cf [Robnik et Kononenko 1995] The goal of this paper is to introduce a new approach to supervised discretization using the metric space of partitions over finite sets We present two new basic ideas a generalization of Fayyad Irani discretization techniques that relies on a metric on partitions defined by Daróczy’s generalized entropy and a new geometric criterion for halting the discreti zation process that extends a similar approach proposed by Cerquides and López de Màntaras in [Cerquides et de Màntaras 1997] using a metric generated by Shannon’s entropy A partition of a non empty set S is a non empty collection of non empty subsets of S π = {Pi | i ∈ I} such that ⋃ {Pi | i ∈ I} = S and i j ∈ I i 6= j implies Pi ∩ Pj = ∅ The set of partitions of S is denoted by PART S For a subset L of M the trace of the partition π on the set L is the partition πL = {Pi ∩ L | 1 ≤ i ≤ k and Pi ∩L 6= ∅} Daróczy’s β entropy for a partition π = {P1 Pk} ∈ PART S is Supervised Discretization Hβ π = 1 21−β−1 ∑k i=1 |Pi| |S| β − 1 where β is a positive number It is easy to see that limβ→1 Hβ π is the Shannon’s entropy The entropy of a partition πL serves to measure the impurity of the set L relative to the partition π the larger the entropy the more L is scattered among the blocks of π If π σ are two partitions in PART S the average impurity of the blocks of σ relative to π is the conditional entropy of π relative to σ H π|σ = ∑m j=1 |Qj | |S| H πQj where σ = {Q1 Qm} López de Màntaras proved that the function d PART S ×PART S −→ R defined by d π σ = H π|σ +H σ|π where H is the Shannon entropy is a metric on PART S see [de Màntaras 1991] For σ π ∈ PART S where π = {P1 Pk} and σ = {Q1 Qm} the Daróczy’s conditional β entropy Hβ π|σ is given by Hβ π|σ = ∑m j=1 |Qj | |S| β and thus Hβ π|σ = 1 21−β − 1 |S|β   k ∑ i=1 m ∑ j=1 |Pi ∩ Qj | β − m ∑ j=1 |Qj | β   A related result obtained in [Simovici et Jaroszewicz 2003] shows that the function dβ PART S × PART S −→ R given by dβ π σ = Hβ π|σ + Hβ σ|π 1 = 1 21−β − 1 |S|β  2 · k ∑ i=1 m ∑ j=1 |Pi ∩ Qj | β − n ∑ i=1 |Pi| β − m ∑ j=1 |Qj | β   is a distance we used it in Simovici 2003 to obtain small and accurate decision trees For π σ ∈ PART S we write π ≤ σ if each block of π is included in a block of σ If π1 π2 ∈ PART S then we denote by π1 ∧ π2 the partition whose blocks are all non empty intersections of the form K ∩ H where K ∈ π1 and H ∈ π2 The generalized conditional entropy is dually monotonic in its first argument and monotonic in its second that is π ≤ π′ implies Hβ π|σ ≥ Hβ π′|σ and σ ≤ σ′ implies Hβ π|σ ≤ Hβ π|σ′ as we have shown in [Simovici et Jaroszewicz 2003] If T is a table and A is an attribute of T we refer to the set of members of the domain of B that occur under B in T as the active domain of B this set is denoted by adomT B or if there is no risk of confusion simply by adom B The partition of the set of tuples of T that corresponds to a partition π of adomT B is denoted by π A block of π consists of all tuples whose B projections belong to the same block of π Discretization of a numeric attribute B involves selecting a set of cutpoints S = {t1 t`} in the active domain of the attribute adom B where t1 < t2 < · · · < t` This set of cutpoints creates a partition πS = {Q0 Q`} of adom B where Qi = {b ∈ adom B | ti−1 ≤ b < ti} for 0 ≤ i ≤ `+1 where t0 = −∞ and t`+1 = +∞ If the set S consists of a single cutpoint t we shall denote πS simply by πt The discretization process consists of replacing each value that falls in the block Qi of π S by i for 0 ≤ i ≤ ` Let πA be a partition of the set of tuples of a table determined by the values of an attribute A If the list of tuples sorted on the values of an attribute B is t1 tn define the partition πB A of adom B as consisting of the longest runs of consecutive B components of the tuples in this list that belong to the same block K of the partition RNTI E 2 Dan Simovici and Richard Butterworth πA The boundary points of the partition πB A are the least and the largest elements of each of the blocks of the partition πB A It is clear that πB A ≤ πA for any attribute B Fayyad proved that to obtain the least value of the Shannon’s conditional entropy H πA|πt the cutpoint t must be chosen among the boundary points of the the par tition πB A which limits drastically the number of possible cut points and improves the tractability of the discretization [Fayyad 1991] Our main results show that the same choice of cutpoints must be made for a broader class of impurity measures namely the impurity measures related to generalized conditional entropy Moreover when the purity of the partition πt is replaced as a discretization criterion by the minimality of the entropic distance between the partitions πA and π t introduced in [Simovici et Jaroszewicz 2003] the same method for selecting the cutpoint can be applied 2 A Generalization of Fayyad’s Result We are concerned with supervised discretization that is with discretization of attributes that takes into account the classes where the tuples belong Suppose that the class of tuples is determined by the attribute A and we need to discretize an attribute B The discretization of B aims to construct a set S of cutpoints of adom B such that the blocks of πS be as pure as possible relative to the partition πA that is the conditional entropy Hβ πA|πS is minimal The following theorem generalizes and amplifies Fayyad’s result Theorem 5 4 1 of [Fayyad 1991] Theorem 2 1 Let T be a table where the class of the tuples is determined by the attribute A and let β ∈ 1 2] If S is a set of cutpoints such that the conditional entropy Hβ πA|πS is minimal among the set of cutpoints with the same number of elements or if dβ πA π S is minimal among the set of cutpoints with the same number of elements then S consists of boundary points of the partition πB A of adom B To discretize adom B we shall seek a set S of cutpoints such that dβ πA π S is minimal Before introducing cutpoints we have S = ∅ πS = ω and therefore Hβ πA|ω = Hβ πA When the set S grows the entropy Hβ πA|πS decreases The use of conditional entropy Hβ πA|πS tends to favor large cutpoint sets for which the partition πS is small in the partial ordered set PART S ≤ In the extreme case every point would be a cutpoint a situation that is clearly unacceptable Fayyad Irani technique halts the discretization process using the principle of minimum description We adopt another technique that has the advantage of being geometrically intuitive and produces very good experimental results Using the distance dβ πA π S = Hβ πA|π S +Hβ π S |πA the decrease in the value of Hβ πA|πS when the set of cutpoints grows is balanced by the increase in Hβ π S |πA Note that initially we have Hβ ω|πA = 0 The discretization process can thus be halted when the distance dβ πA π S stops decreasing Thus we retain as a set of cutpoints for discretization the set S that determines the closed partition to the class partition πA RNTI E 2 Supervised Discretization Input A table T a class attribute A and a real valued attribute B Output A discretized attribute B Method sort table T on attribute B compute the set BP of boundary points of partition πB A S = ∅ d = ∞ while BP 6= ∅ do let t = arg min t∈BPdβ πA π S∪{t} if d ≥ dβ πA π S∪{t} then begin S = S ∪ {t} BP = BP − {t} d = dβ πA π S end else exit while loop end while for πS = {Q0 Q`} replace every q ∈ Qi by i for 0 ≤ i ≤ ` Fig 1 – Discretization Algorithm As a result we obtain good discretizations as evaluated through the results of various classifiers that use the discretize data with relatively small cutpoint sets 3 Discretization Algorithm and Experimental Re sults The algorithm is shown in Figure 1 It makes successive passes over the table and at each pass it adds a new cutpoint chosen among the boundary points of πB A The while loop is running for as long as there exist candidate boundary points and it is possible to find a new cutpoint t such that the distance dβ πA π S∪{t} is less than the previous distance d = dβ πA π S An experiment performed on a syntetic database shows that a substantial amount of time about 78% of the total time is spent on decreasing the distance by the last 1% Therefore in practice we run a search for a new cutpoint only if |d − dβ πA π S∪{t} | > 0 01d Our discretization algorithm was tested on several machine learning data sets from UCI [Blake et Merz 1998] that have numerical attributes After discretizations perfor med with several values of β typically β ∈ {1 5 1 8 1 9 2} we built the decision trees on the discretized data sets using the WEKA J48 variant of C4 5 [Witten et Frank 2000] The size number of leaves and accuracy of the trees are described in below where trees built using the Fayyad Irani discretization method of J48 are designated as “standar d” The discretization technique had a significant impact of the size and accuracy of the decision trees The experimental results show that an appropriate choice of β can reduce significantly the size and number of leaves of the decision trees roughly main taining the accuracy measured by stratified 5 fold cross validation or even increasing the accuracy as shown by the experiments on the glass data set RNTI E 2 Dan Simovici and Richard Butterworth Database Experimental Results Discretization method Size Number of leaves Accuracy stratified cross validation heart c standard 51 30 79 20 β = 1 5 20 14 77 36 β = 1 8 28 18 77 36 β = 1 9 35 22 76 01 β = 2 0 54 32 76 01 glass standard 57 30 57 28 β = 1 5 32 24 71 02 β = 1 8 56 50 77 10 β = 1 9 64 58 67 57 β = 2 0 92 82 66 35 ionosphere standard 35 18 90 88 β = 1 5 15 8 95 44 β = 1 8 19 12 88 31 β = 1 9 15 10 90 02 β = 2 0 15 10 90 02 iris standard 9 5 95 33 β = 1 5 7 5 96 β = 1 8 7 5 96 β = 1 9 7 5 96 β = 2 0 7 5 96 diabetes standard 43 22 74 08 β = 1 8 5 3 75 78 β = 1 9 7 4 75 39 β = 2 0 14 10 76 30 Tab 1 – Experimental Results 4 Conclusions and Open Problems With an appropriate choice of the parameter β that defines the metric used in dis cretization standard classifiers such as C4 5 or J48 generate smaller decision trees with comparable or better levels of accuracy when applied to data discretized with our tech nique We explored only the use of decision trees Other classification techniques that work with nominal attributes such as naive Bayes classifiers should also be explored Also we intend to examine metric discretization for data with missing values Summary We introduce a new approach to supervised discretization of continuous valued at tributes that makes use of the metric space of partitions We present two new basic RNTI E 2 Supervised Discretization ideas a generalization of Fayyad Irani discretization techniques that relies on a me tric on partitions derived from Daróczy’s generalized entropy and a new geometric criterion for halting the discretization process The resulting decision trees are smal ler have fewer leaves and display higher levels of accuracy as verified by stratified cross validation Références [Blake et Merz 1998] C L Blake et C J Merz UCI Repository of machine lear ning databases University of California Irvine Dept of Information and Computer Sciences ics uci edu ∼mlearn MLRepository html 1998 [Cerquides et de Màntaras 1997] J Cerquides et R López de Màntaras Proposal and empirical comparison of a parallelizable distance based discretization method In Proc of the 3rd International Conference on Knowledge Discovery and Data Mining KDD’97 pages 139–142 Newport Beach CA 1997 [de Màntaras 1991] R López de Màntaras A distance based attribute selection mea sure for decision tree induction Machine Learning 6 81–92 1991 [Dougherty et al 1995] J Dougherty R Kohavi et M Sahami Supervised and un supervised discretization of continuous features In Proc of the 12th International Conference on Machine Learning pages 194–202 1995 [Fayyad et Irani 1993] U M Fayyad et K Irani Multi interval discretization of continuous valued attributes for classification learning In Proc of the 12th Interna tional Joint Conference on Artificial Intelligence pages 1022–1027 1993 [Fayyad 1991] U M Fayyad On the Induction of Decision Trees for Multiple Concept Learning PhD thesis University of Michigan 1991 [Kononenko 1993] I Kononenko Inductive and Bayesian learning in medical diagno sis Applied Artificial Intelligence 7 317–337 1993 [Robnik et Kononenko 1995] M Robnik et I Kononenko Discretization of continuous attributes using relieff In Proc of ERK 95 pages 149–152 1995 [Simovici et Jaroszewicz 2003] D Simovici et S Jaroszewicz Generalized conditional entropy and decision trees In Extraction et Gestion des connaissances EGC 2003 pages 363–380 Paris 2003 Lavoisier [Witten et Frank 2000] I H Witten et E Frank Data Mining – Practical Machine Learning Tools and Techniques with Java Implementations Morgan Kaufmann San Francisco 2000 [Yang et Webb 2003] Y Yang et G I Webb Weighted proportional k interval dis cretization for naive Bayes classifiers In Proc of the PAKDD 2003 RNTI E 2