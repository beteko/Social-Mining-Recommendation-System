machines vecteurs support actionnables approche fondée classement ansaf salleb aouissi huang david waltz center computational learning systems columbia university 10115 ansaf waltz columbia columbia résumé principales critiques puisse faire séparateurs vaste marge manque intelligibilité résultats effet technique boite noire fournit explications indices quant raisons classification résultats doivent quels confiance système produits pourtant selon notre expérience pratique experts domaine préfèrent largement méthode apprentis explications recommandation actions plutôt boite noire aussi performante prédictive cette thématique proposons nouvelle approche consiste rendre actionnables atteint couplant dèles classement résultats méthodes apprentissage concepts présentons application notre méthode diverses données médicales concernant patients athérosclérose résultats empiriques semblent prometteurs montrent utilité notre approche quant intelligibilité actionnabilité résultats produits séparateurs vaste marge classement apprentissage règles actionnabilité introduction durant dernière décade machines vecteurs support séparateurs vaste marge connu immense succès principalement comme puissants classifieurs cependant principales limitations manque intelligibilité résul effet produisent explications indices quant raisons classification résultats produits doivent quels faisant confiance proposons rendre actionnables classant ordonnant exemples seulement classifiant effet moyens action plupart temps limités permet petite partie exemples population classement utile tamiser exemples apprentissage garder exemples réellement importants représentatifs classes jacente notre machines vecteurs support actionnables méthodologie consiste contraster résultats ordonnés découvrir cipales propriétés caractéristiques discriminantes entre partie haute désignée suite partie basse désignée bottom classement produit sommes concernés problème intelligibilité notre expérience pratique experts domaine clairement beaucoup confiants quand exemples hautement classés raisons classement fournies liste donnée exemples aussi important comprendre partie basse classement grand intérêt expert domaine diriger actions comprendre système schéma général notre approche comme ordonner exemples utilisant créer ensembles données ordonnées exemples exemples bottom ordonnancement typiquement total exemples extraire ensemble propriétés importantes analysant motifs attributs ensembles bottom notons ignorer partie milieu ordonnancement concentrer seulement extrémités permet faciliter extraction motifs intéressants voulons identifier caractéristiques exemples liste contrastant exemples effet possible analyser fréquence relative différentes propriétés attribut valeur calculer importance telles propriétés utilisant indices statistiques leverage présentons application notre méthode diverse données données médicales concernant patients athérosclérose résultats empiriques semblent prometteurs montrent utilité notre approche quant intelligibilité actionnabilité résultats produits certain nombre travaux récents barakat diederich barakat bradley costa chaves nunez zhang martens intéressés problème extraction explications partir nunez nunez suggéré méthode géométrique permettant transformer classifieur règles classification faire auteurs utilisent moyennes déterminer ensemble vecteurs prototypes lorsqu combinés vecteurs support trouvant marge vecteurs aident construire frontières ellipsoïdes hyper rectangles derniers traduits équations alors règles intervalles respectivement approche similaire proposée zhang zhang suggère algorithme extraction règles hyper rectangles principale différence approche précédente regroupement vecteurs support utilisé trouver vecteurs prototypes chaque classe moyennes évite choisir nombre groupes priori barakat bradley barakat diederich combinent arbres décision produire explications réalisé comme abord construire sifieur ensuite sélectionner vecteurs support générés modèle écarter salleb aouissi leurs étiquettes classes modèle ensuite utilisé prédire classe teurs support conduit nouvel ensemble données enfin construire arbre décision utilisant nouvelles données produire règles symboliques règles décision ainsi produites alors évaluées ensemble vérifier nouveaux exemples classés correctement arbre décision mesures utilisées principalement exactitude accuracy fidélité étendues barakat bradley courbe proposent approche convertir linéaires ensemble règles chevauchant forme équivalentes classifieur linéaire réalisé résolvant simple problème programmation linéaire variables étant nombre attributs chaque règle représente hyper espace dimension surfaces parallèles ensemble règles optimales calculé utilisant critère maximisation volume hyper critère couverture maximise nombre points espace règles exprimées forme disjonctions conjonctions approche extraction règles floues proposée costa costa chaves extraction règles accomplie étapes abord vecteurs support projetés coordonnées ensuite chaque coordonnée ensemble règles floues construit constituent antécédents règles lement chaque vecteur support règle floue extraite mesures définies évaluer qualité règles générées exactitude couverture règle floue cette proche génère autant règles vecteurs support conduire grand nombre règles attaquent problème extraction règles alors partir linéaire comme étant donné vecteur support classe hyper rectangle construit utilisant points intersection vecteurs support bordure intervalles hyper rectangle conduisent règle initiale suite cette règle réglée exclure exemples classes réduire erreur enfin règles fusionnées obtenir ensemble règles concis cette approche avantage mettre évidence attributs importants règles extraites utilisation critère fidélité évalue point système fondé règles proche classifieur principal désavantage cette approche particulièrement couteuse lorsque nombre vecteurs support élevé récemment martens conduit étude comparative différentes proches montré était possible atteindre degré élevé exactitude presque aussi celui obtenu produisant modèle interprétable approche proposons diffère fondamentalement approches décrites dessus utilisons vecteurs support extraire explications place focalisons plutôt exemples trouvant bottom exemples ordonnés moins couteux utiliser ensemble vecteurs support potentiellement grand notre principal argument utilisation vecteurs support nécessairement choix effet autour marge bruitée vecteurs support séparent classes nullement représentatifs classes machines vecteurs support actionnables notre approche notre approche décompose étapes première consiste classer ordonner ensemble exemples utilisant cette étape décrite section conde étape concerne extraction explications proprement dites partir ensemble ordonné exemples décrivons cette étape donnons algorithme classement intéressons problème classement données terme classement signe processus consiste considérer ensemble données classer ordre significatif utile classement supervisé permet atteindre objectif utilisant attributs exemples ainsi leurs étiquettes classe formellement drions classer exemples vecteurs attributs décrivant objets chaque objet étiqueté classe produire classement données problème similaires problème classification pourquoi utilisons méthode classification convertissons résultats fournis classement précisément classons objets triant objets valeurs décision linéaires vapnik variables dites ressort pénalisent erreur commise paramètre détermine compromis entre régularisation pénalisation erreurs classification paramètre ajuste pénalité classe positive puisque voudrions pénaliser reurs étiquetage exemple proportion popultation classe pouvons fixer paramètre nombre vrais négatifs nombre vrais positifs typiquement produit classifieur étiquette exemples mettons seuil résultats telle manière pouvoir ordonner exemples selon fermeté laquelle classifieur prédit classe chaque exemple trement gardons plutôt score donné exemple seulement signe utilisons courbes receiver operating characteristic bradley évaluer qualité notre classement procurent bonne façon mesurer qualité classement lorsque seule vérité disposons exemple appartient classement étiqueté plutôt étiqueté essentiellement norma cardinalité classe similaire normalisation fonction perte avons apprendre qualité courbe facilement mesurée courbe under curve trouve intervalle atteinte classement aléatoire données alors atteinte ordonnant parfaitement exemples positifs négatifs salleb aouissi exemple considérons ensemble composants électriques chaque composant décrit numéro série taille fabriquant étiqueté statut panne label panne autrement classement permet ordonner composants selon susceptibilité pannes classement aurait composants sensibles pannes alors composants classement moindre tendance pannes ainsi expert domaine focaliser composants classement exemple planifiant inspections remplacements extraction explications phase extraction explications consiste sélectionner comparer abord exemples classement ainsi groupons exemples trois ensembles exemples positifs négatifs trouvent classement respectivement exemples milieu classement autour marge plutôt exemples bruités question poser pourquoi consi dérer classement comparer exemples classe positive classe négative raison laquelle comparons simplement classes certains exemples considérés comme étant négatifs alors classe vérité inconnue arriver fréquemment applications réelles exemple précédent sommes exemples négatifs réelle composants panne peuvent avenir proche devraient classer exemples profondément espace classe négative plutôt autour marge séparant classes approche suggé écarterait exemples focalisant seulement classement étiquettes exemples fiables focalisé extrémités classement recherchons ensemble règles intéressantes forme propriete concept propriete paire attribut valeur concept concept évaluons importance propriétés utilisant indice statistique leverage piatetsky shapiro raison laquelle avons choisi cette mesure combine pouvoir discriminant capture propriétés associées fréquentes support élevé mesure leverage utlisé autres tâches apprentissage telles caractérisation exemple turmeaux trouvera aussi littérature autres nouveauté leverage règle dessus donné leverage propriete concept propriete concept mesure leverage évalue proportion exemples additionnels couverts partie gauche droite règle dessus attendus cotés règle indépendants autre clairement avons leverage propriété intéressante concept donné valeur leverage fortement positive indique forte association entre propriété concept alors forte valeur négative indique forte association entre propriété négation concept machines vecteurs support actionnables notre approche leverage règle estimé empiriquement concept concept concept ensemble ensemble bottom propriété notation fonction booléenne telle exemple avons propriété satisfaite algorithme input liste classée exemples seuil leverage minlev pourcentages bottom output ensembles propriétés ensembe histogrammesh attribut exemples exemples aubottom chaque attribut faire4 chaque valeur faire5 attribut valeur leverage minlev alors7 sinon leverage bottom minlev alors9 histogramme attribut retourner algorithme donné algorithme explore espace recherche proprié possibles découvrir celles importantes parties gauches règles conduit classer exemples avant autres meilleure visualisation algorithme extrait aussi histogramme chaque attribut donnant fréquence relative valeurs bottom avons étendu traiter conjonctions propriétés avons également étendu essayer différentes valeurs bottom sélectionner tailles conduisent grand nombre propriétés intéressantes exemple considérons liste ordonnée composants électriques illustrée table extraire principales propriétés comme montre tableau identifier facteurs pannes exemple important trouver motifs attributs exemples ordonnés comme savoir composants particuliers certain fabriquant disproportionnellement responsables failles ultime aider expert choix quant achat composants fiables planification inspections salleb aouissi série taille fabriquant 15b25 13b28 58c25 88a25 18b22 63a11 12a25 15a54 55a95 41b77 exemple liste ordonnée propriete freq_top leverage_top freq_bottom leverage_bottom taille taille fabriquant fabriquant taille fabriquant taille taille ensemble propriétés extraites exemple fabriquant plutôt particulier composants grande taille alors fabriquantb plutôt petits mauvais composants tests empiriques avons implémenté python avons conduit tests empiriques divers benchmarks avons utilisé svmlight1 obtenir classements différentes bases exemples données synthéthiques avons abord vérifié capturait attributs avons généré exemples aléatoires synthétiques exemples chaque exemple décrit attributs étiquettes classes assignées comme autres termes étiquette classe combinaison linéaire premiers attributs parmi attributs réussi découvrir attributs focalisant seulement bottom classement leverage minimum possible découvrir attributs utilisant exemples jusqu réduit leverage minimum valeur faible conclut pouvoir discriminant bottom toute exemples svmlight joachims machines vecteurs support actionnables données atherosclerose décrivons tests courbe données rosclerose représente classe normalisé avons effectué données dicales contexte projet stulong données concernent étude étalée concernant facteurs risque athérosclérose popula hommes avons utilisé ensemble données préparé fixant identi principaux facteurs risque cette maladie attributs utilisés donnés annexe tableau patients classés groupes groupe normal groupe risque enfin groupe ayant pathologie alors attribut utilisé durant aprentissage classement patients ayant risques classés avant patients normaux cible prentissage attribut death figure montre courbe résultats apprentissage parties bottom mises évidence regroupe patients malades bottom meilleure santé rapport maladie avons utilisé différentes valeurs bottom avons retenu valeurs bottom donnant grand nombre propriétés intéressantes résul reportés figure histogrammes associés annexe figure tests différents bottom montré augmentait moins obtenait propriétés intéressantes conernant temps éxecution compter quelques secondes générer propriétés taille manque place détails fournis données stulong objet nombreuses publications exemple lucas facteurs athérosclérose connus principalement consommation durée consommation tabac surpoids activité physique alors évidence quant impact consommation alcool comme facteur facteurs découverts comme montre figure figure enfin table compare nombre propriétés intéressantes lorsque utilise bottom versus exemples apprentissage vecteurs support avons minleverage taille propriétés avons utilisé autres benchmarks mlearn discussion conclusion article décrit approche simple utile rendre résultats action nables principale originalité notre approche aider utilisateur comprendre euromise stulong salleb aouissi propriété freq_top leverage_top freq_bottom leverage_bottom activ_job activ_job time_job time_job birth_year birth_year alco_cons toba_conso toba_conso toba_conso toba_conso toba_dura education education toba_dura education toba_dura rsk_toba education liste quelques importantes propriétés découvertes données rosclérose telles extraires propriété caractéristique concept bottom leverage fortement positif histogramme leverage propriétés simples athérosclérose barres bleues rouges représentent propriétés importantes bottom classement taille taille bottom5 exemples vecteurs support bottom5 exemples vecteurs support atherosclerosis australian heart synthetic nombre propriétés découvertes lorsque considère bottom versus toute exemples versus vecteurs support machines vecteurs support actionnables raisons classification donnée travers classement résultats jacente consiste contraster résultats principalement bottom classe détecter propriétés différencient classes propriétés utiles domaine diriger actions comprendre système ignorer milieu classement concentrant extrémités simplifie extraction propriétés intéressantes notons notre algorithme utilisé importe quelle liste classée quelle méthode classement utilisée faisant varier taille populations bottom permettons compro entre généralisation exactitude accuracy utilisant petites tailles obtenons règles claires leverage peuvent connaître apprentissage overfit grands tailles obtenons règles moins leverage généralisent peuvent offrir suffisamment information utiles selon applications différentes valeurs peuvent utilisées validation croisée selon tailles bottom incluant moyenne leverage exactitude règles convient discuter notre approche rapport arbres décision quoique interpré tables arbres décision produisent descriptions discriminantes classes raisons lesquelles actionnables selon gamberger lavrac proposons descriptions longues discriminantes caractéristiques mettent mettre lumière modèle reproduire concenant mesures évaluation régles avons choisi utiliser leverage comme indice pertinence raisons évoquées cependant cette mesure monotone permet élaguer espace recherche lorsque combine propriétés comme apriori agrawal proposons utiliser leverage comme mesure filtrage bonnes règles explorer utilisation autres mesures statis tiques parcours efficace espace recherche remerciements travail partiellement financé contrat recherche conso lidated edison remercions sergey sigelman implantation merci également relecteurs leurs commentaires suggestions références agrawal imielinski swami mining association rules between items large databases buneman jajodia proceedings sigmod international conference management washington press barakat bradley extraction support vector machines measuring explanation capability using under curve barakat diederich learning based extraction support vector chines computer theory applications siegelmann vapnik support vector clustering learn bradley under curve evaluation machine learning algorithms pattern recognition costa chaves vellasco tanscheit fuzzy extraction support vector machines proceedings fifth international conference salleb aouissi hybrid intelligent systems washington computer society extracting knowledge embedded support vector machines international joint conference neural networks sandilya extraction linear support vector chines proceeding eleventh sigkdd international conference knowledge discovery mining press gamberger lavrac generating actionable knowledge expert guided group discovery proceedings european conference principles mining knowledge discovery springer verlag lucas sebag atherosclerosis identification visual analysis discovery challenge workshop program martens baesens gestel vanthienen comprehensible credit scoring models using extraction support vector machines european journal operatio research nunez angulo catala support vector machines symbolic interpre tation proceedings brazilian symposium neural networks washington computer society piatetsky shapiro discovery analysis presentation strong rules ledge discovery databases press turmeaux salleb vrain cassard learning characteristic rules lying quantified paths european conference principles mining knowledge discovery springer verlag vapnik nature statistical learning theory springer verlag zhang extraction trained support vector machines pakdd summary support vector machines attracted great attention achieved success mainly powerful classifiers however drawbacks intelligibility results black systems provide insights reasons classification explanations results produced taken faith concerned about problem intelligibility because practical experience domain experts strongly prefer machine learning explanations context developed approach provide explanations results actionable underlying produce explanations applying symbolic machine learning produced ranking results precisely contrasting results bottom rankings detect characteristic properties classes which useful practitioner direct actions understand system applied approach several datasets empirical results promising utility methodology regard intelligibility actionability output words support vector machines ranking extraction actionability machines vecteurs support actionnables appendix attribute description attribute description identification patient hypll medicines hyperlipidemia activ_job physical activity stands walks carries heavy loads stated moc_suc urine sugar activ_aft physical activity after activity great activity stated moc_alb urine albumen transp_job transport public means transport stated bolhr chest time_job hours hours stated chlst cholesterol birth_year birth trigl triglycerides entry_year entry study blood pressure systolic alco_cons alcohol consumption diast blood pressure diastolic toba_cons tobacco consumption height height toba_dura smoking duration weight weight marit_stat marital status married married index education reached education university versity triceps myocardial infarction subsc subscapularis ictus rsk_fami family hypertension rsk_obes obesity medicines rsk_toba smoking diabetes rsk_hype hypertension diabd rsk_chol cholesterol hyperlipidemia group normal pathological death patient attributs table athérosclérose désigne catégorique numérique histogrammes quelques attributs importants selon