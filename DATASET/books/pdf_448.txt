clustering hiérarchique paramétrique données fonctionnelles boullé romain guigourès fabrice rossi orange avenue pierre marzin 22300 lannion prenom orange université paris tolbiac 75013 paris prenom paris1 résumé article question clustering courbes posons méthode paramétrique segmente courbes clusters discrétise intervalles variables continues décrivant points courbe produit cartésien partitions forme grille données inférée utilisant approche bayésienne sélection modèle faisant aucune hypothèse concernant courbes enfin technique traitement réduire nombre clusters améliorer interprétabilité clusters proposée consiste fusionner successivement façon optimale clusters revient réaliser classification hiérarchique cendante mesure dissimilarité correspond variation critère manière intéressante cette mesure somme pondérée vergences kullback leibler entre distributions clusters avant après fusions intérêt approche cadre analyse exploratoire fonctionnelles illustré données artificiel introduction analyse données fonctionnelles ramsay silverman observations fonctions courbes données fonctionnelles présentes nombreux domaines comme exemple enregistrement précipitations station météorologique encore surveillance matériel chaque courbe série temporelle quantité physique enregistrée fréquence spécifiée méthodes analyse exploratoire grandes bases données fonctionnelles nécessaires nombreuses applications pratiques comme exemple surveillance consommation électrique hébrail elles réduisent complexité données combinant techniques clustering méthodes approximation fonction modélisant exemple ensemble données fonctionnelles courbes prototypiques comme exemple ensemble segments linéaires splines approches nombre prototypes nombre segments paramètres utilisateur limite utilisateur risque obtenir clusters complexes également induire apprentissage modèle rapport données approches bayésiennes paramétriques basées processus dirichlet clustering hiérarchique paramétrique données fonctionnelles également appliquées problème clustering courbes elles cherchent déterminer distribution clustering modèle infini mélanges nguyen gelfand modèle clustering obtenu échantillonnant distribution posteriori méthodes inférence bayésienne article propose nouvelle méthode analyse exploratoire paramétrique données fonctionnelles basée modèles grilles boullé méthode hypothèse distribution données fonctionnelles bruit mesures méthode requière paramètre utilisateur permet obtenir manière totalement autonome résumé optimal données fonctionnelles utilisant approche maximum posteriori termes priors dépendants données certains particulièrement données volumineux nombre optimal clusters intervalles important approximer finement données grand nombre clusters prêtant interprétation utilisateur analyse exploratoire pourquoi traitement associé méthode permet réduire nombre clusters selon procédure optimale condition emboîtement nombre intervalles quant ajusté parallèlement nombre clusters traitement consiste fusionner successivement clusters manière moins coûteuse depuis clustering jusqu obtenir unique cluster contenant toutes courbes apparait fusion clusters somme pondérée divergences kullback leibler clusters fusionnés cluster formé interpréter comme mesure dissimilarité entre clusters fusionnés ainsi traitement comme classification hiérarchique ascendante hastie outils décision peuvent utilisés comme exemple dendrogramme encore courbe pareto modèle fonction nombre clusters reste article construit façon suivante section présente problème clustering courbes positionne notre méthode rapport approches alternatives ensuite section méthode clustering basée estimation densité jointe présentée technique traitement détaillée section enfin expérimentations données artificiels réels présentées section avant conclure section analyse exploratoire données fonctionnelles cette section données objectif analyse décrits manière formelle ensemble courbes fonctions définies intervalles chaque courbe composée valeurs formant série observations notées comme toutes problématiques analyse exploratoire notre réduire complexité données découvrir motifs données chamroukhi hébrail motifs fonctionnels simples fonctions telles fonctions indicatrices intervalles polynômes simples fonction approximée combinaison linéaire fonctions simples hébrail générée processus logistique polynômes degrés chamroukhi splines peuvent également utilisées comme abraham notons nombre clusters courbes méthode proposée hébrail détermine partition ensemble courbes clusters modélisés boullé fonction simple fonction constante morceaux composée segments exemple minimisant cluster correspond sorte means contraint choix segments espace fonctionnel approche chamroukhi optimise critère similaire obtenu maximisation vraisemblance paramètres modèle génératif approches bayésiennes comme celle présentée nguyen gelfand consi dèrent ensemble courbes représenté courbes moyennes générées suivant processus gaussien organisées clusters clusters décrits fonction étiquetage réalisation distribution multinomiale prior dirichlet alors modèles paramétriques utilisant nombre paramètres peuvent souffrir apprentissage approches bayésiennes paramétriques proposées éviter problème utilisant modèle complexité bornée apprentissage atténué alors approche bayésienne calcul approximation distribution posteriori paramètres réduit risque apprentissage final distribution paramètres clustering obtenue échantillonnant distri bution posteriori paramètres utilisant méthodes inférence bayésienne comme chaines markov monte carlo inférence variationnelle jordan traitement permettant choisir clustering parmi distribution prior dirichlet nécessite paramètres utilisateur paramètre concentration distribution paramètre concentration données contenant courbes espérance nombre clusters wallach paramètre concentration impact significatif nombre clusters obtenus cette raison selon possible estimer manière fiable paramètre notre méthode appelée détaillée section comparable approches basées processus dirichlet elles estiment probabilité posteriori basée vraisemblance distribution priori paramètres modèle méthodes également paramétriques complexité bornée puisque nombre paramètres croit fonction quantité données disponibles cependant intrinsèquement différent méthodes basées abord approches basées bayésiennes génèrent distribution clusterings clustering final étant obtenu traitement consistant exemple choisir distribution posteriori encore étudiant matrice occurrences contrario approche modèle probable directement sélectionné utilisant algorithmes optimisation ensuite applique valeurs données permet éviter valeurs aberrantes problèmes échelle utilisant statistique ordre modèles obtenus invariants toute transformation monotone données entrée puisque méthode focalise corrélations entre variables valeurs variables ensuite méthodes basées étudient distribution paramètres définis mesure conséquent continue quant corrélations entre variables modélisées échantillon clusters courbes variables clustering hiérarchique paramétrique données fonctionnelles point mesure valeur courbe point identifiant courbe permet travailler espace discret simplifier réalisation problème résume ainsi principalement problème dénombrement final approche clairement dépendante données première étape échantillon données utilisé construire prior espace modélisation regardant variables façon indépendantes seuls taille échantillon valeurs rangs empiriques chaque variables exploitées modèle corrélation inferré seconde phase utilisant approche conséquent prouver consistance cette technique modélisation dépendante données demeure problème ouvert résultats expérimentaux obtenant motifs précis fiables montrent cohérence méthode approche appliquée données fonctionnelles cette section principes modèles grilles données détaillés boullé résumés appliqués données fonctionnelles modèles grille données modèles grilles données basés partitionnement chaque variable intervalles numérique groupement valeurs catégoriel produit cartésien partitions univariées forme partition multivariée espace représentation ensemble cellules cette partition multivariée appelée grille données estimateur paramétrique constant morceaux probabilité jointe conditionnelle meilleure grille données obtenue utilisant approche bayésienne sélection modèle algorithmes combinatoires efficaces application données fonctionnelles ensemble courbes représenté données contenant instances trois variables représentant identifiant courbe coordonnées points courbes modèles grilles appliqués estimer probabilité jointe entre trois variables variable segmentée clusters courbes alors chacune variables continues discrétisée intervalles produit cartésien partitions univariées forme grille données comme méthode aussi interprétée comme estimateur densité jointe entre variables continues coordonnées points constante cluster courbes definition introduit notion modèle clustering données fonctionnelles definition modèle clustering données fonctionnelles défini nombre clusters courbes nombre intervalles variables continues répartition courbes clusters distribution points données cellules grilles données distribution points chaque cluster courbes cluster boullé notation ensemble courbes taille ensemble points définis dimensions formant taille variable identifiant courbe variables coordonnées points nombre clusters courbes nombre intervalles variables kckxky nombre cellules grille données nombre courbes cluster nombre points courbe nombre points cluster nombre points intervalles micjxjy nombre points cellule considérons nombres courbes points connus avance souhaitons modéliser distribution jointe points leurs courbes coordonnées associées sélectionner meilleur modèle employons approche utilisant distribution priori paramètres modèle décrit définition definition priori paramètres modèle clustering données fonction nelles choisi hiérarchiquement uniformément chaque niveau nombres clusters intervalles indépendants autres uniformément distribués entre courbes entre coordonnées nombre clusters donné toutes partitions courbes clusters équiprobables modèle taille toutes distributions points kckxky cellules grille données équiprobables cluster courbes donné toutes distributions points courbes cluster equiprobables intervalle donné toutes distributions rangs valeurs points equiprobables prenant logarithme négatif probabilité posteriori modèle connaissant données critère évaluation obtenu donné théorème adapté données fonctionnelles theorem modèlem clustering données fonctionnelles distribué selon priori hiérarchique uniforme optimal bayes valeur critère suivant minimal logmicjxjy logmic logmi logmjx logmjy clustering hiérarchique paramétrique données fonctionnelles nombre partitions éléments parties éventuellement parties vides lorsque correspond nombre général écritb nombre stirling seconde espèce abramowitz stegun revient nombre façons partitionner ensemble éléments parties vides étant donné logarithme négatif probabilités longueur codage shannon technique sélection modèle similaire approche minimum descrip length introduite rissanen première ligne formule correspond distribution priori nombres clusters intervalles partitionnement courbes clusters seconde ligne représente spécification paramètres distribution multinomiale points cellules grille données ainsi points chaque cluster courbes cluster troisième ligne correspond vraisemblance distribution points cellules quant dernière ligne vraisemblance distribution points chaque cluster courbes cluster suivie vraisemblance distribution rangs valeurs chaque intervalle algorithme optimisation article heuristiques optimisation détaillées boullé utilisées elles possèdent propriétés scalabilité complexité spatiale temporelle heuristique principale heuristique gloutonne ascen dante démarre modèle points intervalle courbes cluster considère toutes fusions entre clusters intervalles cents réalise meilleure fusion permet faire décroitre critère cette heuristique améliorée étape optimisation déplacement bornes intervalles changement clusters courbes englobée métaheuristique hansen mladenovic profit plusieurs lancements algorithme initialisations aléatoires différentes algorithme optimisation résumé dessus évalué nombreux figures boullé vraie distribution jacente connue final méthode résistante bruit capable détecter motifs complexes mesure approximer importe quelle distribution données condition avoir suffisamment instances ensemble apprentissage clustering hierarchique ascendant alors modèle obtenu méthode détaillée section optimal selon critère introduit théorème proposons technique traitement simplifier clustering minimisant perte informations premier temps impact fusion critère étudié propriétés mesure dissimilarité proposée détaillées enfin méthode clustering hiérarchique ascendant décrite notons paramètres modélisation utilisés construction clustering initial fusion clusters méthode agglomérative mêmes boullé fusion clusters soient modèles clustering premier correspondant modèle avant fusion clusters second modèle après fusion contenant nouveau cluster notons fusion défini comme résulte theorème modèle clustering explication moins probable selon données suivant facteur focalisons maintenant comportement asymptotique lorsque nombre points données infini theorem variation critère asymptotiquement égale somme pondérée divergences kullback leibler clusters estimée discrétisation bivariée m1cdkl m2cdkl preuve complète détaillée raisons concision résumer calcul élimine certains termes prior premières lignes formule borne autres ensuite utilisant approximation stirling variation vraisemblance dernières lignes formule peuvent réécrites comme somme pondérée divergences kullback leibler variation critère fusion clusters basée divergences kullback leibler cette mesure symétrique caractérise différence entre distributions cover thomas principe clustering hiérarchique ascendant fusionner successivement clusters construire arbre appelé dendrogramme construisons utilisant variation critère propriétés cette mesure dissimilarité dendrogramme construisons équilibré effet étant donné faisons compromis entre fusion clusters similairement distribués fusion petit cluster classification obtenons clusters tailles comparables chaque niveau hiérarchique dendrogramme notons processus agglomératif meilleure fusion aussi effectuée clusters intervalles variables ainsi granularité représentation courbes dégradera temps nombre clusters diminuera expérimentations cette partie propriétés notre approche premier temps illustrées utilisant données artificiel ensuite méthode appliquée données clusters fusionnés successivement enfin exemples analyses exploratoires présentés clustering hiérarchique paramétrique données fonctionnelles expérimentations données artificiel variable échantillonnée suivant uniforme notons bruit blanc gaussien soient distributions suivantes distributions générées aléatoirement ensemble courbes distribution généré utilisant distributions cédemment définies ensemble points également généré chaque point triplet valeurs identifiant courbe choisi parmi valeur générée suivant distribution correspondant courbe choisie appliquons notre méthode clustering données fonctionnelles introduite section ensembles tailles croissantes expérience renouvelée chaque ensemble points échantillonné chaque petits ensembles dessous points suffisamment données découvrir motifs significatifs notre méthode produit unique cluster contenant toutes courbes intervalle partir points nombre clusters intervalles commence croitre finalement points courbe moyenne points total notre méthode retrouve motifs jacents génère clusters courbes correspondent distributions méthode retrouve véritable nombre clusters dessous points clusters peuvent totalement certaines courbes placées expériences points moyenne courbes placées alors points courbes systématiquement classées cluster notons augmentant taille ensemble points points nombre motifs obtenu constant contrario nombre intervalles croit nombre points montre comportement asymptotique méthode retrouve nombre motifs méthode exploite quantité croissante données mieux approximer forme motifs cette expérience permet mettre évidence propriété intéressante méthode nécessite position valeurs variable toutes courbes clustering courbes méthode applique distributions ainsi boullé possible détecter clusters distributions multimodales comme celles générées analyse données consommation électrique données enregistrement consommation électrique foyer français pendant données disponibles jours hébrail chaque courbe composée mesures donnent consommation électrique journalière enregistrée toutes minutes total 50256 points mesure variables mesure temporelle consommation électrique identifiant journée étude regrouper jours ayant profil consommation électrique grille optimale grille optimale définie clusters intervalles signifie courbes classées clusters chaque discrétisé plages horaires plages consommation électrique résultat permet déterminer profils caractéristiques jours jours travail jours chômés encore jours personne domicile prototypes moyens représentés fonctions continues morceaux permettent apprécier consommation moyenne plage horaire probabilité conditionnelle intervalles consommation sachant plages horaires représentée cellules grisées niveau modélise probabilité conditionnelle associée cellule première représentation choisie simplifier interprétation clusters courbes alors seconde permet détecter multimodalités plages horaires exemples parmi clusters traits représentent prototypes cellules grisées probabilités conditionnelles figure représente cluster caractéristique jours personne domicile consommation quasi constante basse figure représente second cluster montre travail consommation basse pendant heures bureau consommation matin figure prototypes courbes clusters figures distributions multimodales figure présente multimodalité plage horaire prototype situé entre cellules denses signifie plupart mesures consommation électrique prises modalités rarement intervalle lequel passe prototype illustré figure notons figure présente autre illustration distribution multimodale laquelle clustering hiérarchique paramétrique données fonctionnelles points majoritaires modalité inférieure manière générale méthode étend clustering courbes clustering distributions fusion clusters alors clustering produit clustering riche clusters caractéristiques précis étude synthétique interprétable consommation électrique annuelle souhaitable certaines applications pourquoi fusions successives effectuées représentées figure dendrogramme courbe pareto présentant pourcentage information conservée fonction nombre clusters definition modèle cluster courbes intervalle temporel consommation grille données composée unique cellule propriétés détaillées boullé modèle optimal selon critère optimisé défini théorème résultant fusions successives jusqu clusters pourcentage information conservée clusters défini dendrogramme équilibré courbe pareto concave permet diviser trois nombre clusters gardant information initiale dendrogramme courbe pareto quantité information conservée fonction nombre clusters expérimentations détaillées réalisées différents niveaux hiérarchie étudions grille données simplifiée clusters information conservés affichant calendrier différentes couleurs clusters possible mettre évidence certaine saisonnalité comme illustre figure effet manière courbes groupées présente entre météo températures france cette année clusters caractérisent saison estivale septembre consommation électrique basse reste année températures basses consommation électrique importante période avril début était particulièrement chaude cette année explique classée clusters estivaux manière intéressante clusters figure personne était domicile regroupés clusters retrouve février octobre novembre boullé calendrier année chaque ligne représente semaine couleurs cluster jours blancs correspondent données manquantes conclusion article sommes concentrés analyse exploratoire données fonctionnelles particulièrement clustering courbes méthode proposée considère ensemble courbes points décrits trois variables continues position point catégorielle identifiant courbe groupant courbes discrétisant variables continues sélectionnant meilleur modèle selon approche méthode comporte comme estimateur paramétrique densité jointe courbes coordonnées points données volumineuses meilleur modèle précis faire interprétation simple éviter problème traitement proposé cette technique fusionner successivement clusters jusqu obtenir clustering simplifié perdant moins précision possible processus équivalent clustering hiérarchique ascendant mesure dissimilarité serait variation critère correspond somme pondérée divergences kullback leibler clusters fusionnés cluster généré expérimentations menées données consommation électrique annuelle foyer clustering évidence phénomènes intéressants distributions multimodales certaines plages horaires quant traitement dendrogramme équilibré courbe pareto concave soulignent possibilité simplifier modèle perdant minimum information ainsi obtenir clustering interprétable clustering courbes méthode proposée capable générer clustering distributions prochains travaux prévu étendre méthode distributions multidimensionnelles considérant dimensions références abraham cornillon matzner molinari unsupervised curve clustering using splines scandinavian journal statistics abramowitz stegun handbook mathematical functions dover publications clustering hiérarchique paramétrique données fonctionnelles jordan variational inference dirichlet process mixtures bayesian analysis boullé multivariate models supervised unsupervised learning technical report france telecom perso francetelecom boulle publications boullenttsi5mb08 boullé models preparation modeling supervised learning guyon cawley saffari hands pattern recognition microtome press chamroukhi govaert aknin hidden process regression model functional description application curve discrimination neurocomputing cover thomas elements information theory wiley interscience hansen mladenovic variable neighborhood search principles applications european journal operational research hastie tibshirani friedman elements statistical learning springer hébrail hugueney lechevallier rossi exploratory analysis functional clustering optimal segmentation neurocomputing markov chain sampling methods dirichlet process mixture models journal computational graphical statistics nguyen gelfand dirichlet labeling process clustering functional sinica statistica ramsay silverman functional analysis springer series statistics springer rissanen modeling shortest description automatica shannon mathematical theory communication system technical journal dirichlet processes encyclopedia machine learning springer prabhakaran fuchs translation invariant wishart dirichlet process clustering distance fürnkranz joachims proceedings international conference machine learning haifa israel omnipress wallach jensen dicker heller alternative prior process nonparametric bayesian clustering journal machine learning research proceedings track summary paper problem curves clustering propose nonparametric method which partitions curves clusters discretizes dimensions curve points intervals cross product these partitions forms which obtained using bayesian model selection approach while making assumption regarding curves finally processing technique aiming reducing number clusters order improve interpretability clustering proposed consists optimally merging clusters which corresponds agglomerative hierarchical classification whose dissimilarity measure variation criterion interestingly measure other kullback leibler divergences between clusters distributions before after merges practical interest approach functional exploratory analysis presented artificial world dataset