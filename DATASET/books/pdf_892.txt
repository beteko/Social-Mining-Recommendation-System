 revision_egc2008 dviUne nouvelle méthode divisive en classification non supervisée pour des données symboliques intervalles Nathanaël Kasoro André Hardy Université de Kinshasa Département de Mathématique et d’Informatique B P 190 Kinshasa République Démocratique du Congo kasoro mulenda yahoo fr Université de Namur Unité de Statistique Département de Mathématique 8 Rempart de la Vierge B 5000 Namur Belgique andre hardy fundp ac be Résumé Dans cet article nous présentons une nouvelle méthode de classifica tion non supervisée pour des données symboliques intervalles Il s’agit de l’ex tension d’une méthode de classification non supervisée classique à des données intervalles La méthode classique suppose que les points observés sont la réali sation d’un processus de Poisson homogène dans k domaines convexes disjoints de Rp La première partie de la nouvelle méthode est une procédure monothé tique divisive La règle de coupure est basée sur une extension à des données intervalles du critère de classification des Hypervolumes L’étape d’élagage uti lise un test statistique basé sur le processus de Poisson homogène Le résultat est un arbre de décision La seconde partie de la méthode consiste en une étape de recollement qui permet dans certains cas d’améliorer la classification obte nue à la fin de la première partie de l’algorithme La méthode est évaluée sur un ensemble de données réelles 1 Introduction Le but de la classification non supervisée est de décomposer un groupe d’objets sur les quels on mesure un ensemble de variables en un nombre relativement restreint de sous groupes d’objets semblables De nombreuses méthodes de classification ont été publiées dans la litté rature scientifique La plupart d’entre elles utilisent un critère de classification basé sur une mesure de dissimilarité Pour éviter ce choix bien souvent arbitraire d’une dissimilarité nous utilisons un modèle statistique pour la classification basé sur le processus de Poisson homogène Hardy 1983 De ce modèle est issue la méthode de classification des Hypervolumes Hardy 1983 Pirçon 2004 a développé une nouvelle méthode divisive de classification basée sur le critère de classification des Hypervolumes Notre objectif est d’étendre cette méthode à des données intervalles Une variable Y dont le domaine d’observation est Y est appelée à valeurs d’ensemble si ∀xi ∈ E Y E → B xi �−→ Y xi où B = P Y = {U �= ∅ | U ⊆ Y} Une nouvelle méthode de classification pour des données intervalles Une variable à valeurs d’ensemble Y est appelée une variable intervalle si Y = R et ∀x i ∈ E il existe α β ∈ R tels que Y xi = [α β] 2 Un modèle statistique pour la classification basé sur le pro cessus de Poisson homogène 2 1 Définition le processus de Poisson homogène N est un processus de Poisson homogène d’intensité q q ∈ R sur un ensemble D ⊂ R p 0 < m D < ∞ si les deux conditions suivantes sont satisfaites Cox et Isham 1980 – ∀ A1 Ak ⊂ D ∀i �= j ∈ {1 k} où Ai ∩ Aj = ∅ N Ai ⊥⊥ N Aj Les variables aléatoires qui comptent le nombre de points dans des régions disjointes de l’espace sont indépendantes – ∀ A ⊂ D ∀k > 0 P N A = k = q m A k k e−q m A La variable aléatoire N A a une distribution de Poisson de moyenne m A où m est la mesure de Lebesgue multidimensionnelle 2 2 Le critère de classification des hypervolumes La méthode de classification des Hypervolumes Hardy et Rasson 1982 Hardy 1983 suppose que les n observations p dimensionnelles x1 xn représentent un échantillon aléa toire simple d’un processus de Poisson homogène N dans un ensemble D inclus dans l’espace Euclidien Rp avec 0 < m D < ∞ L’ensemble D est l’union de k domaines convexes com pacts disjoints D1 Dk Le problème statistique consiste à estimer les domaines inconnus Di dans lesquels les points ont été générés On désigne par C i ⊂ {x1 xn} l’ensemble des points appartenant à Di 1 ≤ i ≤ k Les estimations du maximum de vraisemblance des k domaines inconnus D1 Dk sont les k enveloppes convexes H Ci des k sous groupes de points Ci telles que la somme des mesures de Lebesgue des enveloppes convexes disjointes H Ci est minimale Le critère de classification des Hypervolumes est donc défini par Wk = ∑k i = 1 m H Ci Le problème de classification des Hypervolumes consiste donc à trouver la partition P telle que P = arg minPk∈Pk ∑k i = 1 m H Ci où Pk représente l’ensemble de toutes les partitions de C en k classes Par exemple dans le plan la mesure de Lebesgue d’un domaine D est l’aire de ce domaine Donc si on mesure sur chacun des n objets la valeur de deux variables quantitatives la méthode de classification des Hypervolumes re cherche les k groupes Ci contenant tous les points tels que la somme des aires des enveloppes convexes des ensembles Ci est minimale 2 3 Un test statistique pour le nombre de classes le Gap test Grâce au modèle statistique pour la classification basé sur le processus de Poisson homo gène on peut définir un test du quotient de vraisemblance pour le nombre de classes Kubu shishi 1996 On teste H0 les n = n1 +n2 points observés sont la réalisation d’un processus Kasoro de Poisson homogène dans D contre l’alternative H1 n1 points sont la réalisation d’un pro cessus de Poisson homogène dans D1 et n2 points dans D2 où D1 ∩ D2 = ∅ Les ensembles D D1 D2 sont inconnus La statistique du test est donnée par Q x = 1 − m � m H C n où � = H C \ H C1 ∪H C2 est l’espace vide Gap space entre les classes et m la mesure de Lebesgue multidimensionnelle La règle de décision est la suivante Kubushishi 1996 on rejette H0 au niveau α si distribution asymptotique nm � m H C − log n − p − 1 log log n ≥ − log − log 1 − α 3 La méthode de classification HOPP HOPP Pirçon 2004 est une méthode de classification non supervisée divisive issue du modèle statistique décrit ci dessus La première étape consiste à couper successivement les noeuds de l’arbre en deux sous noeuds jusqu’à ce qu’un critère d’arrêt soit vérifié le nombre de points dans un noeud A chaque coupure on recherche la bipartition de la classe C en deux sous classes C1 et C2 qui minimise le critère de classification des Hypervolumes W2 = m H C1 + m H C2 La méthode est monothétique on choisit chaque fois le noeud et la variable tels que W2 est minimal A la fin du processus de coupure on obtient un arbre de grande taille La deuxième étape permet d’élaguer l’arbre Afin de vérifier si les coupures effectuées sont valides on utilise le Gap test On teste donc à chaque noeud les hypothèses suivantes H0 les points sont distribués dans un seul domaine D contre l’hypothèse alternative H1 les points sont distribués dans deux domaines D1 et D2 D1∩D2 = ∅ Lorsque l’hypothèse nulle n’est pas rejetée on conclut que la coupure est mauvaise Par contre si l’hypothèse nulle est rejettée on décide que la coupure est bonne A la fin du procédé on utilise la règle suivante élaguer toutes les branches qui ne contiennent que des mauvaises coupures Dans certain cas la structure naturelle des données n’est pas obtenue à la fin de l’étape d’élagage La troisième étape est un outil de recollement Des tests sont effectués uniquement sur les classes qui ne sont pas issues du même noeud au niveau précédent Pour ce faire nous utilisons à nouveau le Gap test Si au moins un regroupement est effectué dans l’étape de recollement HOPP perd son caractère hiérarchique Elle devient alors une méthode de parti tionnement 4 La méthode de classification symbolique SPART Dans ce paragraphe nous présentons l’extension de la méthode HOPP à des données inter valles Bock et Diday 2000 Pour ce faire on représente chaque intervalle par son centre et sa demi longueur donc par un point dans un espace bidimensionnel Comme dans la méthode classique la première étape consiste à trouver la meilleure bi partition d’une classe C en deux sous classes C1 et C2 Comme SPART est une méthode monothétique nous travaillons dans les p m � espaces dans lesquels les intervalles de viennent des points On considère donc toutes les bipartitions d’une classe C en deux classes {C1 C2} en respectant l’ordre des centres des intervalles Il s’agit donc des bipartitions géné rées par des droites verticales figure 1 On définit une extension à des données intervalles de Une nouvelle méthode de classification pour des données intervalles la mesure de l’espace vide Δ entre les classes de la façon suivante mE � = mi+1−mi + max li li+1 −min li li+1 On choisit l’intervalle ]mi mi+1[ tel que mE Δ est maximal Une valeur de coupure c est prise arbitrairement dans l’intervalle ]m i mi+1[ Habituellement on choisit le centre de l’intervalle figure 1 •�� � � � � � � 0 •• mi mmi+1 �i �i+1 � Y xi+1 Y xi c FIG 1 Bipartition d’une classe Si x� ∈ C Y x� = [α� β�] et m� = α�+β�2 Une classe C est divisée en deux grâce à une question binaire de la forme "m� ≤ c " où c est la valeur de coupure On définit une fonction binaire qc C −→ {0 1} par qc x� = 0 si m� ≤ c et 1 sinon On obtient alors la bipartition souhaitée C1 = {x ∈ C qc x = 0} et C2 = {x ∈ C qc x = 1} Les étapes d’élagage et de recollement sont effectuées de la même façon que dans le cas classique On utilise ici aussi une extension symbolique du Gap test à des données intervalles en représentant chaque intervalle par son centre et sa demi longueur 5 Application On applique la méthode SPART à un jeu de données réelles Nous comparerons les ré sultats donnés par SPART avec ceux obtenus par deux autres méthodes de classification non supervisées monothétiques divisives pour des variables intervalles SCLASS Rasson et al 2007 est une méthode de classification hiérarchique monothétique divisive basée sur une extension à des variables intervalles du critère de classification généralisé des Hypervolumes DIV Chavent 1998 est quant à elle une méthode de classification hiérarchique monothétique divisive basée sur une extension du critère de l’inertie intra classe Le jeu de données "cars" est constitué de 33 voitures disponibles en 2001 sur lesquelles ont été mesurées 8 variables intervalles Il est répertorié dans les bases du logiciel SODAS 2 SODAS2 2004 Les objets sont repris sur la figure 2 Les variables sont les suivantes prix empattement cylindrée longueur vitesse maximale largeur accélération maximale hauteur Une partition en 4 classes est obtenue après l’étape d’élagage L’étape de recollement ne mo difie pas cette partition en 4 classes Kasoro � ���� ���� ��� ���� �� ��� ������ � ������ ����� �� � ����� � �� �� ������ ��� � �� � ��� �� �� "� ���� � � ��% � "'� � � ��% � "� � � ��% � " �� *�+ � �� ���� ��� � ������ � �� � � ��� � �� � ���� ��� � � � ��� ������� + ����� � ���� ���� � 0' � ���� ���� ������ � ����� 1���� �� ��% ���2��� ��� ���� 3� � �� � � �� ����2 ���� �� �� � *�+ � �� � *�+ � �� � �� � � ��� 4 � �� � � ��� � " ���� 2� � � " �� ���� ���5 � � � FIG 2 Arbre pour l’ensemble de données "Cars" La première variable de coupure est le prix des voitures cher bon marché Pour les voitures bon marché la deuxième variable de coupure est la longueur de la voiture tandis que pour les voitures chères il s’agit de la hauteur de la voiture La figure 2 montre l’arbre hiérarchique produit par SPART Les 4 classes peuvent être étiquettées de la manière suivante classe 1 voitures citadines classe 2 voitures berline classe 3 modèles sport et classe 4 voitures limousines La méthode DIV donne la même partition en 4 classes SCLASS produit une autre parti tion dont les classes ne semblent pas correspondre à une structure utile de l’ensemble des 33 voitures 6 Conclusion SPART est une nouvelle méthode de classification non supervisée pour des données in tervalles Elle est basée sur une extension aux données intervalles du critère de classification des Hypervolumes et du Gap test L’originalité de l’approche est double D’une part le modèle sous jacent à la méthode n’utilise pas de mesure de dissimilarité le critère de classification est déduit d’un modèle statistique basé sur le processus de Poisson homogène D’autre part la méthode inclut une étape de recollement qui lui permet dans le cas de structures particulières de retrouver les classes naturelles d’un ensemble de données multidimensionnelles SPART et DIV donnent souvent des résultats semblables SPART produit cependant des résultats meilleurs que DIV lorsqu’on est en présence de classes allongées Ceci s’explique principalement par le fait que DIV utilise une extension à des données intervalles du critère de la variance intra classe et que ce critère est biaisé par rapport aux classes de forme ellipsoï dale SCLASS Rasson et al 2007 utilise un modèle statistique pour la classification basé sur le processus de Poisson non homogène Le critère à minimiser est l’intensité intégrée du Une nouvelle méthode de classification pour des données intervalles processus de Poisson sur les enveloppes convexes des classes Cette méthode exige donc l’esti mation de l’intensité du processus de Poisson non homogène Elle est donc plus complexe d’un point de vue temps calcul De plus dans sa version actuelle SCLASS ne comporte pas d’étape d’élagage ni d’étape de recollement Les résultats obtenus par SCLASS sont donc générale ment qualitativement moins bons que ceux produits par SPART Enfin la procédure SPART détermine automatiquement le nombre de classes qui doit être fixé au préalable dans SCLASS et DIV Références Bock H et E Diday 2000 Analysis of Symbolic Data Exploratory Methods for Extracting Statistical Information from Complex Data Berlin Heidelberg Springer Verlag Chavent M 1998 A monothetic clustering method Pattern Recognition Letters 19 989– 996 Cox D et V Isham 1980 Point Processes London Chapman and Hall Hardy A 1983 Statistique et classification automatique un modèle un nouveau critère des algorithmes des applications Thèse de doctorat FUNDP Université de Namur Hardy A et J P Rasson 1982 Une nouvelle approche des problèmes de classification auto matique Statistique et Analyse des Données 7 2 41–56 Kubushishi T 1996 On some Applications of Point Process Theory in Cluster Analysis and Pattern Recognition Thèse de doctorat FUNDP Université de Namur Pirçon J Y 2004 La classification et les processus de Poisson pour de nouvelles méthodes monothétiques de partitionnement Thèse de doctorat FUNDP Université de Namur Rasson J P J Y Pirçon P Lallemand et S Adans 2007 Unsupervised divisive classi fication In E Diday et M Noirhomme Eds Symbolic Data Analysis and the Sodas 2 Software Wiley SODAS2 2004 Logiciel info fundp ac be asso Summary We present a new clustering method for symbolic interval data It is an extension to interval data of a classical clustering method The classical method assumes that the observed data points are a realisation of a homogeneous Poisson point process in k disjoint domains of R p The first part of the new method is a monothetic divisive procedure The cut rule is based on an extension to interval data of the Hypervolumes clustering criterion The pruning step uses a statistical hypothesis test based on the homogeneous Poisson process The output is a decision tree The second part of the method is a merging process that allows in particular cases to improve the classification obtained at the end of the first part of the algorithm The method is applied to a real data set 