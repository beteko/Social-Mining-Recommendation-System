 Apprentissage de structure des réseaux bayésiens et données incomplètes Olivier François et Philippe Leray INSA Rouen Laboratoire PSI FRE CNRS 2645 BP 08 Av de l’Université 76801 St Etienne du Rouvray Cedex {Olivier Francois Philippe Leray} insa rouen fr bnt insa rouen fr Résumé Le formalisme des modèles graphiques connait actuellement un essor dans les domaines du machine learning En particulier les réseaux bayésiens sont capables d’effectuer des raisonnements probabilistes à partir de données incomplètes alors que peu de méthodes sont actuellement capables d’utiliser les bases d’exemples in complètes pour leur apprentissage En s’inpirant du principe de ams em proposé par Friedman 1997 et des travaux de Chow Liu 1968 nous proposons une méthode permettant de faire l’apprentissage de réseaux bayésiens particuliers de structure ar borescente à partir de données incomplètes Une étude expérimentale expose ensuite des résultats préliminaires qu’il est possible d’attendre d’une telle méthode puis montre le gain potentiel apporté lorsque nous utilisons les arbres obtenus comme initialisation d’une méthode de recherche gloutonne comme ams em 1 Introduction La détermination d’un réseau bayésien B = G θ nécessite la définition d’un graphe acy clique dirigé dag G dont les sommets représentent un ensemble de variables aléatoires X = {X1 · · · Xn} la structure et de matrices de probabilités conditionnelles du nœud i connaissant l’état de ses parents Pa Xi dans G θi = [P Xi XPa Xi ] les paramètres De nombreuses méthodes d’apprentissage de structure de réseaux bayésiens ont vu le jour ces dernières années Alors qu’il est possible de faire de l’apprentissage de paramètres de réseaux bayésiens à partir de données incomplètes et que l’inférence dans les réseaux bayésiens est pos sible même lorsque peu d’attributs sont observés Jensen 1996 Pearl 1998 Näım et al 2004 les algorithmes d’apprentissage de structure avec des données incomplètes restent rares Il est possible de différencier trois types de données manquantes selon le mécanisme qui les a générées Le premier type représente les données manquantes au hasard mar missing at ran dom Dans ce cas la probabilité qu’une variable ne soit pas mesurée ne dépend que de l’état de certaines autres variables observées Lorsque cette probabilité ne dépend plus des variables observées les données manquantes sont dites mcar missing completely at random Par contre lorsque la probabilité qu’une variable soit manquante dépend à la fois de l’état de certaines autres variables observées mais également de phénomènes extérieurs les données sont dites nmar Par la suite nous supposerons que nous sommes en présence d’une base de données incomplètes suivant un mécanisme mar ou mcar Ainsi nous possédons toute l’information nécessaire pour estimer la distribution des données manquantes dans la base d’exemples Lorsque les données sont incomplètes il est possible de déterminer les paramètres et la structure du réseau bayésien à partir des entrées complètes de la base Comme les données manquantes sont supposées l’être aléatoirement nous construisons ainsi un estimateur sans biais Néanmoins dans l’exemple d’une base de 2000 cas sur 20 attributs avec une probabilité de 20% qu’une mesure soit manquante nous ne disposerons en moyenne que de 23 cas complets Les autres données à notre disposition ne sont donc pas négligeables et il serait donc préférable de faire l’apprentissage en utilisant toute l’information à laquelle nous avons accès Un avantage des réseaux bayésiens est qu’il suffit que seules les variables Xi et Pa Xi soient observées pour estimer la table de probabilité conditionnelle correspondante Dans ce cas il est alors possible d’utiliser tous les exemples même incomplets où ces variables sont observées dans RNTI E 3127 Réseaux bayésiens apprentissage de structure et données incomplètes l’exemple précédent en supposant que Xi possède trois parents nous aurions 819 exemples en moyenne pour estimer les paramètres correspondants La recherche de structure de réseaux bayésiens peut utiliser des bases de données incomplètes par exemple par le biais d’un échantillonnage de Gibbs Myers et al 1999 ou encore en utili sant une approche comme l’algorithme EM Friedman 1997 Friedman 1998 D’autres travaux utilisent des techniques plus originales comme Sebastiani Ramoni 2001 qui effectue l’appren tissage de sous structures locales ou encore Dash Druzdzel 2003 qui utilise une méthode à base de recherche d’indépendances conditionnelles Nous proposons de nous inspirer de la méthode de Friedman 1997 en associant un score à une structure arborescente à partir de données incomplètes puis en choisissant le meilleur arbre selon ce score Dans la section suivante nous rappelons comment associer un score à un modèle et une manière d’estimer ce score lorsque la base d’exemples est incomplète Puis nous introduisons la méthode mwst em qui est une généralisation de la recherche d’arbre de recouvrement maximal adaptée aux réseaux bayésiens Chow Liu 1968 Heckerman et al 1994 Nous finissons par exposer quelques résultats préliminaires et apports éventuels d’une telle technique notamment avec l’uti lisation de l’arbre rendu par mwst em pour initialiser une recherche gloutonne comme ams em Friedman 1997 2 Score d’un réseau bayésien et données incomplètes Soient X = {X1 · · · Xn} un ensemble des variables aléatoires et Dc une base de m tirages de X indépendants et identiquement distribués Supposons par ailleurs que seule une version incomplète D de la base Dc soit disponible celle ci peut alors se décomposer en D = [[Xli]]1 6 i 6 n 1 6 l 6 m = [O H] où O est l’ensemble des variables Xli mesurées et H l’ensemble des variables X l i manquantes Score bayésien et données incomplètes Le score bayésien est défini par BD G D = P G D = P G P D|G où P D|G = ∫ Θ P D|G Θ P Θ|G dΘ Pour une base d’exemples complète Dc et en supposant que toutes les variables sont discrètes Cooper Hersovits 1992 ont donné le résultat suivant P Dc|G = n ∏ i=1 qi ∏ j=1 ri − 1 ∑ri k=1 Nijk + ri − 1 ri ∏ k=1 Nijk 1 où Nijk est le nombre d’instances où {Xi = xi k et Pa Xi = pai j} dans la base Dc Or en présence de données incomplètes nous avons P D|G = ∑ H ∫ Θ P O H|G Θ P Θ|G dΘ 2 L’équation 2 peut alors être évaluée par application multiple de l’équation 1 pour toutes les complétions possibles des variables manquantes H La complexité d’un tel calcul est alors expo nentielle en fonction du nombre de valeurs manquantes dans la base d’exemples En pratique ceci n’est pas utilisable il faut donc utiliser une méthode d’approximation pour P D|G Approximation d’un score avec des données incomplètes Soit S B|Dc une fonction de score quelconque pour un modèle B = G Θ et pour une base complète Dc Le score S peut être le score bayésien ou tout autre score pour plus d’information RNTI 1 RNTI E 3 128 Olivier François et Philippe Leray sur les fonctions de score voir Näım et al 2004 Il est possible d’estimer le score de ce modèle avec des données incomplètes en calculant QS B|D = EH∼P H [ S B|O H ] 3 Malheureusement nous n’avons pas accès à la loi P H Il va donc falloir approcher cette loi à partir d’un modèle supposé générateur de D Soit B0 un tel modèle alors il est possible d’écrire QS B B0|D = EH∼P H|B0 [ S B|O H ] = ∑ H S B|O H P H|B0 4 La loi P H|B0 peut être obtenue par inférence dans le réseau bayésien B0 Cette méthode permet donc à partir d’une fonction de score S B|Dc quelconque de créer une fonction de score QS B B0|D qui donne un résultat approché sur des bases d’exemples incomplètes De plus ce score à la particularité de conserver la propriété de décomposabilité si S B|Dc = n ∑ i=1 s Xi Pa Xi |Dc alors Q S B B0|D = n ∑ i=1 qs Xi Pa Xi B 0|D Par exemple pour le score bic cela donne q BIC i Xi Pa Xi B 0|D = X Xi X Pa Xi log bθXi|Pa Xi EH∼P H|B0 NXi Pa Xi − 1 2 Dim Xi Pa Xi log N 5 3 Algorithme EM et apprentissage de structure Structural EM Friedman 1997 a été un des premiers à proposer une méthode déterministe efficace pour faire de la recherche de structure à partir de données incomplètes Le principe général de la méthode ams em proposée par Friedman 1997 est rappelé dans l’algorithme 1 Algorithme 1 Algorithme EM pour l’apprentissage de structure 1 k = 0 2 Choisir le graphe Gk et les paramètres Θk 0 aléatoirement ou en utilisant une heuristique 3 Tant que l’on a pas convergence et k 6 kmax Faire 4 l = 0 5 Tant que l’on a pas convergence et l 6 lmax Faire 6 l = l + 1 7 Θk l = argmax Θ Q Gn Θ Gk Θk l−1 8 Fin Tant que 9 k = k + 1 10 Gk = argmax G Q G Gk−1 Θk l 11 Θk 0 = argmax Θ Q Gk Θ Gk−1 Θk−1 l 12 Fin Tant que Nous ne pouvons pas déterminer l’élément qui maximise le score de l’étape 10 parmi tous les dag Robinson 1977 Heureusement la version généralisée de l’algorithme em Dempster et al 1977 montre qu’il suffit de trouver ’une’ meilleure solution plutôt que ’la’ meilleure sans perdre la convergence de la méthode Dans le cadre de la méthode ams em l’argmax est ainsi calculé sur l’ensemble des voisins du graphe Gk−1 c’est à dire les graphes obtenus par suppression ajout ou inversion d’un arc 3 1 MWST EM une nouvelle méthode dans l’espace des arbres En s’inspirant des recommandations de Heckerman et al 1994 François Leray 2004 ont montré que dans le cas de données complètes la méthode mwst maximum weight spanning tree de recherche de l’arbre de recouvrement maximal adaptée aux réseaux bayésiens permet d’obtenir RNTI 1 RNTI E 3129 Réseaux bayésiens apprentissage de structure et données incomplètes 1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 Fig 1 – Structures utilisées pour générer les bases d’exemples a Jouet 1 b Jouet 2 c Jouet 3 d Jouet 4 e Jouet 5 f Asia rapidement une structure simple ayant un bon score et pouvant également servir d’initialisation pour une méthode de recherche gloutonne Nous proposons ici d’adapter cette méthode aux bases de données incomplètes L’algorithme mwst em est très proche de celui présenté précédemment La différence principale réside dans la recherche de l’argmax de l’étape 10 Nous proposons de rechercher le meilleur graphe dans l’ensemble des arbres plutôt que le meilleur dans le voisinage Cette étape nécessite le calcul d’une matrice de similarité pour laquelle nous devons utiliser une fonction de score décomposable localement Le terme générique de cette matrice Mkij 16i j6n est donné par M k ij = EH∼P H|Bk−1 h s Xi Xj |O H − s Xi ∅|O H i si i 6= j 0 sinon 6 L’algorithme de Kruskal permet ensuite d’obtenir grâce à cette matrice l’arbre qui maximise Q G Gk−1 Θk l L’algorithme mwst em tout comme l’algorithme ams em est une méthode itérative Il doit donc être initialisé avec un modèle de départ Friedman 1997 propose de choisir une châıne reliant toutes les variables comme initialisation de l’algorithme ams em Nous allons faire de même pour mwst em MWST EM comme initialisation de AMS EM Nous proposons ensuite d’initialiser ams em avec l’arbre rendu par mwst em pour étudier les apports éventuels d’une telle initialisation Variante pour les problèmes de classification Le réseau bayésien näıf est souvent utilisé pour les tâches de classification La structure de ce réseau correspond à une hypothèse d’indépendance des variables conditionnellement à la variable classe Il est possible d’assouplir cette hypothèse d’indépendance en ajoutant des arcs entre les variables En particulier la méthode tanb tree augmented naive Bayes ajoute à la structure de Bayes näıve le meilleur arbre reliant les variables autres que la classe Nous pouvons donc maintenant utiliser la même approche avec des données incomplètes et notre algorithme mwst em pour obtenir une variante pour les problèmes de classification que nous appellerons tanb em 4 Expérimentations Nous avons testé les algorithmes suivants arbre de recouvrement maximal avec données man quantes mwst em Structural EM ams em et Structural EM initialisé avec l’arbre issu de mwst em ams em+t Les bases de données incomplètes utilisées ont été générées à partir des réseaux bayésiens dont les structures sont décrites sur la figure 1 avec des données manquantes de type mcar pour différentes probabilités 20% 30% Durant l’apprentissage nous avons utilisé l’approximation du score bic décrit dans la section 2 Des bases de test complètes ont également étés générées La table 1 contient la moyenne des scores bic sur les bases de test des dag obtenus à partir de 5 bases d’apprentissage différentes ainsi que les moyennes des temps de calculs en secondes données à titre indicatif qui ont été obtenu avec une machine de 1GHz Interprétations des résultats Pour des structures initialement arborescentes jouet 1 et jouet 2 mwst em trouve un meilleur graphe que ams em et plus rapidement RNTI 1 RNTI E 3 130 Olivier François et Philippe Leray Jouet 1 Jouet 2 Jouet 3 Jouet 4 Jouet 5 Asia napp ntest 300 500 500 1000 1000 1000 1000 1000 2000 1000 2000 1000 dag init 1339 1 2950 8 2676 0 2581 3 3869 5 2281 1 mwst em 1375 6 63 9 3047 9 101 1 2862 8 89 1 2711 9 216 3 4134 2 534 3 2903 6 772 8 20 ams em 1375 3 136 6 3083 6 539 5 2846 0 282 0 2713 7 660 4 4041 2 1898 2831 9 12393 ams em+t 1421 187 4 3097 1 550 2 2729 8 356 0 2698 2 885 2 4003 2 2855 2713 0 9633 mwst em 1374 4 69 5 3034 1 135 0 2852 7 103 4 2708 1 217 3 4121 5 923 5 2885 3 796 30 ams em 1404 5 173 6 3098 7 461 4 2846 9 325 0 2694 6 565 2 3997 6 3123 2638 7 16476 ams em+t 1421 1 191 0 3112 9 465 6 2821 2 594 8 2721 9 736 4 4035 2 4482 2552 5 11266 Tab 1 – Scores bic calculés sur des données de test pour les réseaux initiaux et pour les réseaux appris par les méthodes mwst em ams em et ams em+t calculés avec des bases d’apprentissage vidées aléatoirement de 20% puis 30% Les temps de calculs sont précicés entre parenthèses Les résultats sont une moyenne sur 5 exécutions pour différentes bases d’apprentissage Pour des réseaux plus complexes mwst em trouve une structure moins bonne que ams em mais avec un score souvent proche et toujours beaucoup plus rapidement De plus les résultats de ams em+t sont souvent meilleurs que ceux de ams em L’initialisation que nous proposons semble permettre de converger plus rapidement vers la même solution ou de converger vers une meilleure solution Nous remarquons également que cette méthode est plus stable que la méthode ams em En effet la recherche gloutonne possède l’inconvenient de s’arrêter dans les optimum locaux alors qu’une initialisation par mwst em permet de trouver un meilleur graphe 5 Conclusions et perspectives Nous avons proposé une méthode originale d’apprentissage de structure arborescente de réseaux bayésiens à partir de bases de données incomplètes Puis nous avons comparé empiriquement cette méthode à l’algorithme ams em Friedman 1997 Les premiers résultats nous permettent de dire que mwst em est une méthode est assez efficace et peu complexe Elle permet de retrouver des structures ayant un bon score et ceci plus rapidement que ams em Néanmoins notre méthode a le désavantage de travailler dans l’espace des arbres ce qui est restrictif Ensuite nous avons utilisé notre méthode pour initialiser l’algorithme ams em Cette initiali sation permet souvent d’obtenir de meilleurs résultats qu’en utilisant l’initialisation proposée par Friedman 1997 Nous envisageons maintenant de tester et d’évaluer ces algorithmes sur un éventail de problèmes plus large notamment avec des données mar et pour des problèmes de classification Les méthodes mwst em et ams em sont des adaptations d’algorithmes de recherche de struc ture dans l’espace des graphes acycliques dirigés mais Chickering 2002 a proposé récemment ges un algorithme optimal de recherche dans un autre espace l’espace des représentants des équivalents de Markov La suite logique de nos travaux est donc l’adaptation de ges pour le cas des bases de données incomplètes Remerciements Ce travail a été partiellement financé par le programme de la communauté européenne IST sous le réseau d’excellence PASCAL IST 2002 506778 Cette publication ne reflète que les opinions de ses auteurs Références Chickering 2002 Chickering D M 2002 Learning equivalence classes of bayesian network structures Journal of machine learning research 2 pages 445 498 2002 RNTI 1 RNTI E 3131 Réseaux bayésiens apprentissage de structure et données incomplètes Chow Liu 1968 Chow C Liu C 1968 Approximating discrete probability distributions with dependence trees IEEE Transactions on Information Theory 14 3 pages 462 467 1968 Cooper Hersovits 1992 Cooper G Hersovits E 1992 A bayesian method for the induction of probabilistic networks from data Maching Learning 9 pages 309 347 1992 Dash Druzdzel 2003 Dash D Druzdzel M 2003 Robust independence testing for constraint based learning of causal structure Proceedings of The Nineteenth Conference on Uncertainty in Artificial Intelligence UAI03 2003 Dempster et al 1977 Dempster A Laird N Rubin D 1977 Maximum likelihood from incomplete data via the EM algorithm Journal of the Royal Statistical Society B 39 pages 1 38 1977 François Leray 2004 François O Leray P 2004 Evaluation d’algorithmes d’appren tissage de structure dans les réseaux bayésiens In 14ieme Congrès francophone de Recon naissance des formes et d’Intelligence artificielle pages 1453 1460 2004 Friedman 1997 Friedman N 1997 Learning belief networks in the presence of missing values and hidden variables In in the Proceedings of the 14th International Conference on Machine Learning Morgan Kaufmann pages 125 133 1997 Friedman 1998 Friedman N 1998 The Bayesian structural EM algorithm In G F Co oper S Moral Eds Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence UAI 98 San Francisco Morgan Kaufmann pages 129 138 1998 Heckerman et al 1994 Heckerman D Geiger D Chickering M 1994 Learning Bayesian networks The combination of knowledge and statistical data In R L de Man taras D Poole Eds Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence San Francisco CA USA Morgan Kaufmann Publishers pages 293 301 1994 Jensen 1996 Jensen F V 1996 An introduction to Bayesian Networks Taylor and Francis London United Kingdom 1996 Myers et al 1999 Myers J W Laskey K B Lewitt T S 1999 Learning bayesian network from incomplete data with stochatic search algorithms In In the Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence UAI99 1999 Näım et al 2004 Näım P Wuillemin P H Leray P Pourret O Becker A 2004 Réseaux bayésiens Eyrolles ISBN 2 212 11137 1 1999 Pearl 1998 Pearl J 1998 Graphical models for probabilistic and causal reasoning In D M Gabbay P Smets Eds Handbook of Defeasible Reasoning and Uncertainty Management Systems Volume 1 Quantified Representation of Uncertainty and Imprecision pages 367 389 Dordrecht Kluwer Academic Publishers 1998 Robinson 1977 Robinson R W 1977 Counting unlabeled acyclic digraphs In C H C Little Ed Combinatorial Mathematics V volume 622 of Lecture Notes in Mathematics Springer Berlin pages 28 43 1997 Sebastiani Ramoni 2001 Sebastiani P Ramoni M 2001 Bayesian selection of de composable models with incomplete data Journal of the American Statistical Association Vol 96 No 456 2001 Summary The framework of graphical models is more and more used in the area of Machine Learning Specially Bayesian Networks allow to carry out probabilistic reasonning from incomplete datasets but few methods can use incomplete datasets to learn their structure Using the principle of the Structural EM Friedman 1997 and the work of Chow Liu 1968 we propose a new method to perform a structural search of bayesian network in the space of tree graphs from incomplete datasets An experimental study gives first results of this method and then shows the potential improvement if we use trees that it gives to initialise a greedy method like the Structural EM RNTI 1 RNTI E 3 132