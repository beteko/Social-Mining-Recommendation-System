valuation algorithmes donne continues jollois nadif crip5 universite paris saint 75270 paris cedex france francois xavier jollois paris5 universite saulcy 57045 cedex france nadif populaire efficace estimation parame lange algorithme sente inconve nient converger parfois lentement application tableaux grande taille devient ainsi alisable proble plusieurs thodes propose sentons comporte thode connue variante avons cemment celles permettent convergence algorithme obtenant sultats similaires celui travail concentrons aspect classification illus trons comportement notre variante donne continues simule elles introduction plusieurs thodes classification utilise distance mesure dissimilarite utilisation lange classification devenue approche classique puissante exemple banfield raftery celeux govaert traitant classification cette approche algorithme dempster compose tapes estimation maximisation devenu quasiment incontournable celui populaire estimation parame ainsi nombreux logiciels cette approche comme mclust emclust fraley raftery emmix mclachlan mixmod biernacki autoclass cheeseman stutz malheureusement principal inconve nient lenteur nombre rations parfois cessaire convergence utili sation inapproprie donne grande taille ayant teste plusieurs thodes nadif jollois avons retenu algorithme thiesson utilise partielle estimation comple partir algorithme avons cherche liorer performance avons propose variante efficace donne qualitatives simule elles formances cette nouvelle version encourageantes principal objectif valuation algorithmes donne continues travail tendre rimentale donne continues utilisant lange gaussiens lange algorithme approche lange individus classifier provenir lange densite proportions inconnus donne continues utilisons classiquement distributions gaussiennes ainsi chaque objet alisation densite probabilite crite repre sentent proportions lange repre sente densite gaussienne dimension classe vecteur moyenne matrice covariance repre sente vecteur parame lange estimer classes forme ellipsoidale centre matrice covariance termine leurs caracte ristiques triques travail avons choisi prendre rique repre sente volume classe propre chacune vraisemblance donne suite allons aborder proble classification approche estimation parame abord estime partition duite thode maximum posteriori estimation parame passe maximisation solution rative solution proble algorithme dempster principe algorithme maximiser manie rative rance vraisemblance comple conditionnellement estimation courante donne probabilite conditionnelle posteriori chaque ration tapes estimation calculer contexte lange revient calculer probabilite posteriori jollois nadif maximisation calculer maximise estimation condi tionnelle ration algorithme thiesson cherche duire temps estimation cherche identifier gulie rement individus impor tants utiliser ensuite pendant plusieurs rations individu conside comme important changement probabilite entre rations successives grande notons ensemble individus importants ylazy ensemble restants chaque ration requiert estimation standard estimation suivie ensuite maximisation standard comple calcule individus probabilite posteriori tablit liste individus importants partie probabilite posteriori estimation standard calculer probabilite posteriori identifier ylazy comme ensemble individus ignorer durant tapes cette calcule probabilite posteriori toutes observations appartenant quand autres obser vations appartenant ylazy avons seule rance conditionnelle associe autrement quantite globale cherchera maximiser maximisa maximisation cherche comme algorithme classique parame maximise roulement ration standard suivie rations jusqu convergence algorithme viabilite algorithme partiellement toutes donne importance aussi calcul terminer importance chaque individu stockage garder cette information lange peuvent grandement duits voire simplement supprime stockage crite portance derrie crite suivante individu forte probabilite appartenir classe approprie assigner autre fallait serait soudainement pluto progressivement ainsi supposons observations fortement classe contribuent volution parame individu conside comme important toutes probabilite appartenance rieures certain seuil valuation algorithmes donne continues monstration hinton convergence algorithme oriquement justifie applicable chaque coupage arbitraire dividus moment visite gulie rement version thiesson carter certain nombre vidus conside comme important calculs cette notion importance rapporte volution probabilite posteriori effet individu montre volution importante entre tapes priori stable fortes chances rester moment prend cision carter calculs prendre compte pendant certain nombre rations contraire volution significative ressant garder calculs avons choisi diffe rences entre probabilite posteriori avant estimation standard remet individus comparons moyenne valeurs absolues diffe rences chaque classe seuil cette version efficace meilleure donne qualitatives nadif jollois tudions portement donne continues utilisant lange gaussien riences riques illustrer performances avons applique donne simule suivant lange gaussien sente trois situations classes moyennement figure avons restreint parame plusieurs tests ainsi utilisons seuils nombre rations utilisons tableau sentons coefficient ration moyen calcule tempsem tempslem toutes parame trisations donnent vraisemblance partir sultats sente observons clairement rapide trois situations avons galement applique compare thodes donne elles machine learning repository1 premier tableau mlearn mlrepository jollois nadif distribution donne simule classes classes moyennement yeast concerne localisation prote selon certaines mesures scores calcule partir mesures contient instances attributs donne parties classes second tableau donne german credit concerne banquiers allemagne contient instances crites variables riques certaines scores classes sentes payeur mauvais payeur partitions obtenus performances terme rapidite enregistre reporte table elles montrent riorite donne simule elles yeast german credit coefficient ration moyen conclusion perspectives travail sommes resse proble ration gorithme avons sente variantes algorithme premie thiesson appele seconde tient compte volution probabilite posteriori notre thode performante donne continues confirmant sultats obtenus donne qualitatives actuellement sommes menons riences intensives partir autres lange gaussiens valider performances aussi cherchons proposer strate efficace permettant surmonter difficulte choix parame duisant temps cution valuation algorithmes donne continues rences banfield raftery model based gaussian gaussian clustering biometrics biernacki celeux govaert langrognet vernaz mixmod performance model based cluster discriminant analysis fcomte mixmod index celeux govaert gaussian parcimonious clustering methods cheeseman stutz bayesian classification autoclass theory results advances knowledge discovery mining fayyad piatetsky shapiro uthurusamy press dempster laird rubin mixture densities maximum lihood incomplete algorithm journal royal statitical society fraley raftery mclust software model based cluster discriminant analysis university washington mclachlan guide emmix version university queensland nadif jollois ration donne qualitatives comparative diffe rentes versions extraction gestion connaissances hinton algorithm justifies incremental sparse other variants jordan learning graphical models thiesson heckerman accelerating large databases machine learning summary popular efficient mixture parameters estimation algorithm major inconvenient converge sometimes slowly application large becomes unsuitable accelerating methods proposed present behavior known variant proposed these variants speed convergence yield similar results focus clustering context illustrate behavior method synthetic continuous