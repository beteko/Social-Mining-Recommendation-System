résultat / 5nnSlimBaseline-all-both.eps Unsupervised vidéo Tag correction.Système Hoang-Tung Tran *, Elisa Fromont *, François Jacquenet *, Baptiste Jeudy *, Adrien Martins * * Laboratoire Hubert Curien, UMR CNRS 5516 18 Rue du Professeur Benoît Lauras, 42000 Saint-Etienne {hoang.tung.tran, elisa.fromont, francois.jacquenet}@univ-st-etienne.fr {baptiste.jeudi, adrien.martins}@univ-st-etienne.fr Résumé. Nous présentons un nouveau système de marquage automatique vidéo qui vise à recting cor- et en complétant les balises fournies par les utilisateurs pour les vidéos téléchargées sur Internet. Contrairement à la plupart des systèmes existants, nous n'apprenons pas classificateurs d'étiquette ou d'utiliser les informations textuelles discutables pour comparer nos vidéos. Nous vous proposons de comparer directement le contenu visuel des vidéos décrites par différents ensembles de fonctionnalités telles que le sac-de-visuels mots ou motifs fréquents construits d'eux. Ensuite, nous propageons les balises entre les vidéos visuellement similaires en fonction de la fréquence de ces balises dans un quartier de vidéo donné. Nous vous proposons également une exper- contrôlée imental mis en place pour évaluer un tel système. Les expériences montrent que des caractéristiques appropriées, nous sommes en mesure de corriger une quantité raisonnable de balises dans des vidéos sur le Web. 1 Introduction Les moteurs de recherche classiques à base de texte offrent déjà un bon accès à des contenus multimédias dans le monde en ligne. Cependant, ils ne peuvent pas indexer le nombre important de vidéos en ligne à moins que ces vidéos sont soigneusement annotés avant d'être mis sur le Web. Cependant, les notations an- fournies par l'utilisateur sont souvent erronées, à savoir sans rapport avec la vidéo (par exemple pour augmenter le nombre de vues de la vidéo), et incomplète. Pour remédier à ces inconvénients, nous allons nous concentrer sur la tâche de ting progammation un système automatique pour améliorer les annotations de vidéos sur le Web. Il y a déjà eu beaucoup d'efforts pour les vidéos annoter (par exemple (Morsillo et al., 2010), (Shen et al., 2011)). Cependant, la plupart des systèmes proposés utilisent des concepts limités (tags) et une formation in- supervisée pour apprendre un ou plusieurs classificateurs pour marquer un jeu de données vidéo. Ces approches semblent donc inappropriées pour une vidéo sur un grand site comme Youtube où le nombre de balises possibles est illimité et où les vraies étiquettes sont inaccessibles a priori. Nous aimerions donc proposer une approche non supervisée basée sur la comparaison du contenu visuel des vidéos pour propager les balises des vidéos voisines en fonction de leur fréquence textuelle. Dans cette ap- proche des principaux verrous scientifiques résident i) dans le choix des caractéristiques qui seront utilisées pour faire des comparaisons sans surveillance pertinentes, ii) dans la méthode de comparaison elle-même, iii) dans le processus de propagation et iv) dans l'évaluation de la ensemble du système. Un examen des travaux connexes concernant les problèmes ci-dessus mentionnés est brièvement donnée dans la section 2. Dans la section 3, nous décrivons en détail comment appliquer les données techniques minières ainsi que notre méthode proposée pour comparer des vidéos. Les expériences faites jusqu'à présent sont présentés à la section 4 et nous concluons à l'article 5. Système de correction de la balise vidéo Unsupervised 2 Cadre général et les travaux connexes Trouver les caractéristiques pertinentes (étape 1 et 2). La première étape de notre procédé consiste à décomposer une vidéo en une séquence d'images clés (en utilisant par exemple (Zhuang et al., 1998)). Ensuite, nous DE- Scribe la vidéo sur la base des images. Différentes fonctionnalités sont généralement les mieux adaptés pour différentes tâches. La tendance actuelle en vision par ordinateur est de concaténer différents types de fea- tures bas niveau dans un vecteur de grande dimension qui sera ensuite utilisé pour résoudre les tâches de vision. Par exemple, on peut utiliser des histogrammes de distribution de pointe, des moments de couleur ou de la couleur de la texture ondelettes autocorrélation relograms (Moxley et al., 2010), histogrammes de gradient (HOG) ou des fonctions audio orienté, LAB et histogrammes de couleur HSV global, Haar ou Gabor vaguelettes ( voir (Morsillo et al., 2010)). Une autre technique très populaire est de construire un sac de mots visuels (BoVW) à partir des vecteurs de caractéristiques à faible niveau Inal initiatrices (voir (Yang et al., 2007)). Cependant, lorsque vous utilisez seulement contenu visuel pour comparer les vidéos, les caractéristiques mentionnées ci-dessus pourraient ne pas être assez discriminante. techniques minières fréquentes de modèle sont de plus en plus souvent utilisés dans la vision informatique commu- nauté pour obtenir de meilleures caractéristiques (voir par exemple (Sivic et Zisserman, 2004), (Yuan et al., 2011) et, plus récemment, (Fernando et al. , 2012)). Les approches reposent souvent sur des informations de classe pour pouvoir sélectionner un ensemble compact de caractéristiques pertinentes de la sortie des algorithmes d'exploration. similitudes entre le calcul de vidéos (étape 3). Même si une vidéo est considérée comme une séquence d'images, les variations de la durée des vidéos ou du nombre d'images clés les rendent plus difficiles à comparer. Une première méthode consiste à prendre la moyenne des histogrammes des trames (par exemple (Yang et Toderici, 2011)), pour produire une description unique pour l'ensemble vidéo. L'histogramme peut être seuillée pour enlever un peu de bruit potentiel. Voici les fonctions de distance classique (par exemple L1) peuvent être utilisées pour estimer la similitude entre les vidéos. Même si cette méthode est efficace, on perd beaucoup de l'information disponible en faisant la moyenne de tous les cadres. La seconde approche consiste à comparer des paires d'images clés, par exemple, calcul de la similitude entre les deux trames les plus semblables des vidéos comme dans Moxley et al. (2010). La comparaison des deux vidéos est faite en utilisant une paire unique de cadres et aucune information séquentielle est prise en compte. La dernière utilise des cadres identiques communs (mais différents en termes de mise en forme, points de vue, les paramètres de l'appareil photo, etc.) appelés près de double pour comparer des vidéos (voir par exemple (Zhao et al., 2010)). Ces deux exemplaires près ne peut être trouvé dans toutes les vidéos. procédure de propagation de marque (étape 4). Comme la plupart des systèmes de marquage auto vidéo apprendre classificateurs multiples, l'étape de propagation de l'étiquette n'est pas nécessaire. Cependant, la méthode de la double-près présentée dans Zhao et al. (2010) utilisent cette procédure de propagation sur laquelle repose la nôtre. Pour chaque V vidéo, une liste des balises possibles pertinentes est obtenue à partir des k plupart des vidéos similaires (en utilisant un algorithme voisin K-le plus proche). Après cela, une fonction de score est appliquée pour chaque étiquette d'estimer la pertinence de cette étiquette selon un V vidéo donné. Cette fonction de score dépend de la fréquence d'étiquette, le nombre d'étiquettes associées à une vidéo, et la similitude vidéo. Enfin, seuls les tags avec un plus grand score à un seuil sont considérés comme appropriés pour la V vidéo. 3 Amélioration du système de marquage automatique proposé caractéristiques proposées Comme il est expliqué à la section 2, nous pouvons utiliser de nombreuses fonctionnalités possibles pour décrire une vidéo et ceci est un point crucial de travailler sur une propagation de l'étiquette correspondant à la fin Tran et al. du procédé. Nous vous proposons d'utiliser BOVW construit à partir des descripteurs EIPD (Lowe, 2004) obtenus régulièrement dans chaque image-clé d'une vidéo que nos fonctionnalités de bas niveau. Nous voulons ensuite d'utiliser un algorithme d'exploration de modèle pour extraire mieux que l'on appelle des fonctionnalités de niveau moyen pour comparer nos vidéos. La plupart des algorithmes proposés dans la prise de la littérature en tant que vecteurs binaires d'entrée. Comme l'a expliqué Fernando et al. (2012), doit être fait avec soin le « binarisation » du BOVW d'origine. Nous vous proposons d'utiliser un simple taille égale-bin discrétisation (avec un nombre de cases égal à 4) pour chaque mot visuel pour transformer notre histogramme d'origine dans un vecteur binaire. En outre, les données minières sortie des techniques un grand nombre de modèles (exponentielle du nombre de dimensions des vecteurs binaires). Ces motifs peuvent être filtrés à l'aide des informations surveillées comme, par exemple, représenté sur Fernando et al. (2012). Cependant, dans notre cas, aucune information supervisée est disponible ainsi critères différents doivent être proposés. Nous avons donc décidé d'utiliser l'algorithme SLIM (Smets et Vreeken, 2012). Cet algorithme optimize un critère basé sur la longueur Description minimum pour réduire le nombre de modèles de sortie à ceux qui « bien » Compresser les données. Elle emploie une heuristique simple et précise pour estimer le gain ou le coût de l'ajout d'un candidat au motif de sortie ensemble. Si F est l'ensemble des motifs fréquents obtenus en utilisant SLIM, nous construisons un vecteur binaire V de taille | F | pour chaque image clé. Dans ce vecteur, V (i) est réglé sur 1 si le motif i de F apparaît dans cette image-clé et 0 sinon. Étant donné que le nombre de motifs en F peut encore être grand, nous utilisons également une analyse en composantes principales (ACP de) pour réduire la dimension du vecteur V. Enfin, le vecteur décrivant chaque image clé est soit que l'histogramme BOVW, seul le vecteur V des motifs SLIM (réduite par PCA) ou ces deux vecteurs concaténés. vidéo asymétrique proposée mesure de similarité La première étape de notre méthode consiste à calculer toutes les similitudes entre toutes les paires d'images clés des vidéos. Ensuite, on calcule la moyenne de toutes les similitudes maximum correspondant à une vidéo. En d'autres termes, pour chaque image-clé d'une vidéo A, nous cherchons dans toutes les images clés de vidéo B pour le score correspondant le plus élevé et nous enregistrons par paires cette valeur. Ensuite, on calcule la moyenne de toutes les valeurs enregistrées pour toutes les images clés de la vidéo A pour revenir le score de similitude de la vidéo A vers la vidéo B. Si l'on note A (i) la keyframe ième A et | A | le nombre d'images-clés dans A, puis la carte SIM (A, B) = 1 / | A | Σ i sim max j (A (i), B (j)). La similitude sim (A (i), B (j)) entre les trames est juste l'inverse d'une distance entre les vecteurs représentant les trames. 4 expériences Nous avons d'abord effectué une série d'expériences sur des ensembles de données d'image pour évaluer la ingness interest- des motifs fréquents que les caractéristiques, les différentes distances et la méthode PCA sur le motif de sortie histogramme. En raison du manque d'espace, ces expériences ne sont pas présentés ici, mais ils ont montré que i) les motifs fréquents (FP) peuvent être caractéristiques intéressantes par rapport à de simples sac de mots si elles sont soigneusement choisis; ii) la distance L1 peut être une bonne mesure de distance pour comparer deux vecteurs haut dimensions, qui décrit une vidéo (il est préférable que le noyau de tion habituelle utilisée dans intersections vision par ordinateur pour comparer les histogrammes); iii) un PCA où nous gardons des composants assez pour expliquer 90% de la variance peut contribuer à réduire la dimensionnalité des vecteurs de caractéristiques sans endommager la précision. Système de correction de balise vidéo non surveillée 0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 pe rc en ta ge ofe rr ou vi de os pourcentage de bruit introduit Ajouté BoW bruit tous deux 0 5 10 15 20 25 30 0 5 10 15 20 25 30 pourcentage de bruit introduit pe rc en ta ge ofe rr ou vi de calcul de la moyenne os - 10NN - Seuil = 7 BoW motif fréquent deux Fig. 1 - Résultat de l'algorithme de correction d'étiquette sur un ensemble de données vidéo réel (à gauche) et un synthétique (à droite) en utilisant uniquement sac de caractéristiques de mot ou sac de mots et motifs fréquents. La deuxième série d'expériences pour but de proposer un nouveau protocole expérimental pour évaluer la méthode de propagation de la balise. Nous utilisons d'abord un 51 vidéos ensemble de données réelles tirées d'un ensemble de données de référence des vidéos YouTube (Cao et al., 2009). Chaque vidéo est décomposé en images clés. Il y a environ 27 images clés pour une vidéo. La dimension du vocabulaire SIFT-BOVW est 1000. Une vidéo est donc représentée par une matrice qui contient, pour toutes les images clés de la vidéo de l'histogramme de mots visuel qui décrit la trame. Nous avons gardé cet ensemble de données assez petit pour être en mesure d'évaluer manuellement l'intérêt des étiquettes originales et celles propagées pour chaque vidéo. Les 51 vidéos ont été choisis de telle sorte qu'ils appartiennent à 4 sujets pour faire en sorte que cet ensemble de données contient des paires de vidéos similaires et des paires de vidéos différentes. Nous étiquetterons manuellement les vidéos avec 35 balises. Comme les résultats sur cet ensemble de données ne sont pas concluants, nous avons créé un ensemble de données synthétique de 182 vidéos construites à partir de 7 vidéos très différentes de l'ensemble de données précédente. Dans les deux cas, étaient intéressés à évaluer les caractéristiques à base de motifs fréquents par rapport aux caractéristiques à base BOVW. propagation Tag Un ensemble de données vidéo est un triplet (V, T, étiquette) où V est l'ensemble des vidéos V = {v1, ..., vn}, l'ensemble des balises possibles i s T = {t1, ... tm} et l'étiquette est une relation sur V × T tel que balise (v, t) est vrai si et seulement si la vidéo v a tag t. Notre procédure d'évaluation est alors: - ajouter un peu de bruit sur les balises, par exemple, choisir un bruit proportion 0 <p <1 et réalise une fonction tag bruyante tagnoisy telle que, pour chaque t ∈ T et v ∈ V, avec une probabilité p que nous avons : tagnoisy (v, t) = ¬tag (v, t) (c.-à retourner la valeur d'une variable donnée, avec une probabilité p); - appliquer notre technique de correction d'étiquette, la sortie de l'étape de correction de la balise est tagcorr; - calculer la proportion des balises incorrectes après l'étape de correction comme: err (étiquette, tagcorr) = {‖ (v, t) ∈ V × T | tag (v, t) = 6 tagcorr (v, t)} ‖ / (‖V ‖.‖T‖) Le cas idéal est err (tag, tagcorr) = 0. Notez que err (tag, tagnoisy) ≈ p. Cela signifie que dès que err (étiquette, tagcorr) <p, il y a des balises moins incorrectes sur le jeu bruyant après l'étape de propagation de l'étiquette que précédemment. Dans la figure. 1, nous traçons l'erreur ERR (étiquette, tagcorr) contre la valeur de p. Lorsque la courbe est en dessous de la ligne diagonale, nous pouvons affirmer que notre algorithme a diminué le nombre de balises incorrectes. Les résultats sur l'ensemble de données réel Nous avons appliqué notre procédure d'évaluation sur le réel 51 vidéos ensemble de données présenté au début de la présente section. Nous les résultats sur fait la moyenne 100 pistes pour chaque niveau de bruit. Les résultats sont présentés sur la Fig. 1 (à gauche). Pour presque tous les niveaux de bruit, le nombre d'étiquettes incorrectes est plus élevé après l'algorithme de correction que précédemment. Ces erreurs peuvent être le résultat de l'algorithme de correction ou le fait que la distance calculée entre les vidéos ne Tran et al. reflètent pas la réelle similitude des vidéos. En particulier, le nombre de vidéos que nous utilisons est assez faible. Dans un jeu de données de millions de vidéos, k devrait être beaucoup plus semblables que dans notre petit jeu de données les plus proches voisins d'une vidéo donnée (et donc des balises très similaires). Un autre problème réside dans les balises elles-mêmes: notre algorithme utilisent la similitude visuelle entre les vidéos pour corriger les tags. Ainsi, il peut être efficace que sur les étiquettes qui sont en corrélation avec le contenu visuel. Résultats sur l'ensemble de données de synthèse Le nombre maximum de balises pour cet ensemble de données est de 182 * 7 = 1274. Cela signifie que lors de l'ajout de 5% du bruit dans l'ensemble de données, 63 valeurs de balise sont retournées dans le jeu de données (certaines balises sont ajoutées, certaines ont été retirées ). Ensuite, pour construire la vidéo de synthèse, nous 1) choisir au hasard entre 2 et 4 vidéos de l'ensemble de données vidéo réelle; 2) choisir au hasard des cadres de chacune des vidéos réelles choisies. L'ensemble de trames ainsi obtenues est la vidéo synthétique; 3) Marquer cette vidéo de synthèse avec A si elle contient des images de la vidéo A, avec B si elle contient des images de la vidéo B et ainsi de suite. Chaque vidéo de synthèse a donc entre 2 et 4 balises sur 7 balises possibles. Par cette construction, si deux vidéos synthétiques partagent par exemple l'étiquette A, cela signifie qu'ils contiennent tous deux cadres similaires extraites de la vidéo réelle A. De plus, par construction, chaque étiquette est associée au contenu visuel de la vidéo. Nous évitons donc le dernier problème rencontré avec l'ensemble de données réelles. Pour un niveau de bruit compris entre 0 et 30%, on voit sur la figure. 1 (à droite) que la proportion de balises incorrectes diminue de manière significative. Par exemple, à un niveau de bruit de 20%, la proportion d'erreur après correction de l'étiquette est d'environ 16%. L'algorithme a ainsi éliminé environ un quart des erreurs introduites par le bruit. Notez que pour un niveau de bruit plus élevé, le nombre de balises incorrectes est trop grand pour attendre l'amélioration des résultats par la propagation de l'étiquette. L'analyse des résultats Bien que donnant des résultats très prometteurs sur la propagation de l'étiquette comme le montre la figure. 1 (à droite), les dernières séries d'expériences sur les jeux de données vidéo des questions l'utilité de notre méthode de comparaison vidéo par paires et du niveau élevé proposé fréquent poncifs. En effet, les résultats en utilisant la comparaison par paires introduites dans la section 3 sont similaires à ceux obtenus à l'aide d'une simple moyenne des cadres bien que celui plus tard est plus efficace de calculer. Fig. 1 montre de également que les motifs fréquents construits à l'aide l'algorithme SLIM n'améliore pas la comparaison de l'étiquette par rapport aux simples caractéristiques de BOVW. La combinaison des deux vecteurs de caractéristiques donne également des résultats similaires qui montre que pour les vidéos, au contraire que pour les images, les modèles calculés par l'algorithme SLIM ne semblent pas donner des informations supplémentaires par rapport à la BOVW dont ils sont construits. 5 Conclusion Nous avons présenté un système de marquage automatique sans supervision complète qui corrige et Pletes com- tags originaux sur les vidéos. Le système semble efficace, surtout lorsque le nombre de vidéos dans l'ensemble de données est suffisamment élevé pour avoir un quartier suffisamment pertinent pour chaque vidéo. Cependant, les nouvelles fonctionnalités proposées et la procédure de comparaison vidéo par paire ne semblent pas améliorer nos résultats par rapport aux méthodes de référence. Comme les travaux futurs, nous pose donc pro- de prendre en compte l'information séquentielle dans la vidéo pour créer de meilleures fonctionnalités de haut niveau et de prendre en compte la position spatiale des caractéristiques des cadres. Nous envisageons également de travailler sur l'évolutivité du système proposé pour lutter contre les jeux de données réels plus importants. système de correction tag vidéo Cao Références non surveillée, J., Y. Zhang, Y. Song, Z. Chen, X. Zhang, et J. Li (2009). Mcg-webv: Un ensemble de données de référence pour l'analyse vidéo sur le web. Rapport technique, TIC-09-001 MCG. Fernando, B., E. Fromont et T. Tuytelaars (2012). L'utilisation efficace de l'exploitation minière pour la classification fréquente itemset d'image. Dans Conférence européenne sur l'ordinateur Vision, pp. 214-227. Lowe, D. G. (2004). image distinctive de caractéristiques keypoints échelle invariantes. International Journal of Computer Vision 60 (2), 91-110. Morsillo, N., G. S. Mann et C. Pal (2010). échelle Youtube, grande annotation vidéo vocabulaire. Dans Recherche vidéo et des mines, Volume 287 des études en informatique Intelligence, pp. 357- 386. Springer. Moxley, E., T. Mei, et B. Manjunath (2010). annotation vidéo par le renforcement et la recherche graphique minière. IEEE Transactions on Multimedia 12 (3), 184-193. Shen, J., M. Wang, S. Yan, et X.-S. Hua (2011). marquage multimédia: passé, présent et futur. Dans Actes de la 19e conférence internationale ACM sur le multimédia, pp. 639-640. Sivic, J. et A. Zisserman (2004). Les données vidéo MINING en utilisant des configurations de point de vue invariance régions de fourmis. Vision par ordinateur et reconnaissance de formes (1), pp. 488-495. Smets, K. et J. Vreeken (2012). Slim: l'exploitation directe des modèles descriptifs. En SIAM Conférence nationale sur l'exploration de données inter, p. 236-247. Yang, J., Y. Jiang, A. Hauptmann et C. Ngo (2007). L'évaluation de sac-visuels Mots de sentations de la classification de la scène. Dans l'atelier international sur la recherche d'information multimédia, p. 197-206. ACM. Yang, W. et G. Toderici (2011). l'apprentissage tag discriminante sur des vidéos youtube avec des sous-balises latentes. Dans Vision par ordinateur et reconnaissance, p. 3217-3224. Yuan, J., M. Yang, Y. et Wu (2011). Exploitation minière modèles de co-occurrence discriminants pour la reconnaissance visuelle. En CVPR: Conf. sur Vision par ordinateur et reconnaissance, p. 2777-2784. Zhao, W., X. Wu et C. Ngo (2010). Sur l'annotation de vidéos web par la recherche en double quasi efficace. IEEE Transactions on Multimedia 12 (5), 448-461. Zhuang, Y., Y. Rui, T. Huang et S. Mehrotra (1998). extraction adaptatif de trame en utilisant la clé classification non supervisée. Dans Int. Conf. sur le traitement de l'image (1), pp. 866-870. Nous proposons un résumé nouveau Système de marquage de vidéos Vasant automatique à et Corriger les « Agenda item automatiquement des tags » Fournis Par les LORs de la Utilisateurs mise en ligne d'Une nouvelle vidéo sur Internet. Au Contraire des Systèmes existants, nous ne décidons de pas l'informations UTILISER textuelle Fourni par fausse possiblement les techniques de ni Utilisateurs d'apprentissage supervisez verser les décisions de viles. Nous comparons le contenu visuel Directement des vidéos en nous Basant sur des attributes discriminants APPRI LORs D'une étape de motifs de fouille Fréquents. Ce papier decrit also Une méthode simple, de la propagation des balises ENTR e et vidéos un visuellement proches protocole d'EVALUER expérimental permettant notre approche.