bay14R.dvi Structure Inference de réseaux bayésiens de données: une nouvelle approche basée sur Entropie Généralisée conditionnelle Dan A. Simovici *, Saaid Baraty * * Univ. du Massachusetts Boston, Massachusetts 02125, Etats-Unis {DSim, sbaraty} @ cs.umb.edu Résumé. Nous vous proposons un nouvel algorithme pour extraire la structure d'un réseau bayésien à partir d'un ensemble de données. Notre approche est basée sur entropies conditionnelles généralisées, une famille paramétrique de entropies qui prolonge l'habituel Shannon entropie conditionnelle. Nos résultats indiquent que, avec un choix approprié d'une général- isé conditionnelle entropie on obtient des réseaux bayésiens qui ont des scores supérieurs par rapport à des structures similaires obtenues par des méthodes classiques d'inférence. 1 Introduction Une structure de réseau de croyance bayésien (BBN) est un graphique acyclique orienté, qui représente les dépendances probabilistes entre un ensemble de variables aléatoires. Une structure BBN induire pour l'ensemble des attributs d'un ensemble de données est un problème bien connu et un défi en raison de l'énormité de l'espace de recherche. Le nombre de structures de BBN possibles croît de façon exponentielle super par rapport au nombre des nœuds. Dans Cooper et Herskovits (1993), où l'algorithme heuristique K2 est introduit, une me- sûr de la qualité de la structure est dérivée en fonction de sa probabilité postérieure en présence d'un ensemble de données. Une autre approche pour calculer une structure BBN est basée sur le principe des- cription minimum Longueur (LDM) introduit en Rissanen (1978). Les algorithmes de Lam et Bacchus (1994) et Suzuki (1999) proviennent de ce principe. Nous vous proposons une nouvelle approche pour induire des structures BBN des ensembles de données basés sur la notion d'entropie β-généralisée (β-GE) et son entropie conditionnelle β-généralisée correspondant (β- GCE) introduit dans Havrda et Charvat (1967) et axiomatisée dans Simovici et Jaroszewicz (2002) comme une famille à un paramètre de fonctions définies sur les partitions (ou la probabilité de des distributions). La flexibilité qui en découle nous permet de générer RDBA avec de meilleurs scores que les résultats publiés. Un avantage important de notre approche est que, contrairement à Cooper et Herskovits (1993), il ne repose sur aucune hypothèse distributive pour le développement de la formule. 2 Generalized Entropy et structure Inference L'ensemble des partitions d'un ensemble S est notée par la partie (S). La trace d'une partition π sur un sous-ensemble T de S est la partition πT = {T ∩ Bi | i ∈ I et T ∩ Bi 6 = ∅} de T. L'ordre habituel entre les partitions ensemble est désigné par « ≤ ». Il est bien connu que (PARTIE (S), ≤) est une inférence bayésienne délimitée de réseau des réseaux. La borne inférieure de deux partitions π et π '= {Bj | j ∈ J} sur S, notée avec π ∧ π', est la partition {Bi ∩ Bj | i ∈ I, j ∈ J, Bi ∩ Bj 6 = ∅} sur S. Le moindre élément de ce réseau est la partition aS = {{s} | s ∈ S}; la plus grande est la partition = {S co S}. La notion d'entropie généralisée ou β-entropie a été introduite dans Havrda et Charvat (1967) et axiomatisé pour les partitions en Simovici et Jaroszewicz (2002). Si S est un ensemble fini et π = {B1,. . . , Bm} est une partition de S, la β-entropie de π est le nombre Hp (π) = 1 à 21 janvier-β (1 - Σmi = 1 (| Bi | | S |) β) pour β> 1 . L'entropie de Shannon est obtenu sous forme d'une limβ → Hp (π). Pour β ≥ 1 la fonction Hp: PARTIE (S) - → R≥0 est anti-monotones. Ainsi, Hp (π) ≤ Hp (aS) = 1-nβ-1 (21-β-1) · nβ-1, où n = | S |. Laissez π, σ ∈ PARTIE (S) deux partitions, où π = {B1,. . . , Bm} et σ = {C1,. . . ,} Cn. L'entropie de β-conditionnelle de π et σ est Hp (π | σ) = Σn j = 1 (| Cj | | S |) β Hp (πCj). Il est immédiat que Hp (tc | co S) = Hp (π) et que H (tc | aS) = 0. En outre, dans Simovici et Jaroszewicz (2006), il est démontré que Hp (π | σ) = Hp (π ∧ σ) -Hβ (σ), une propriété qui étend la propriété similaire de l'entropie de Shannon. Lorsque β ≥ 1, le β-GCE est doublement anti-monotone par rapport à son premier argument et est monotone par rapport à son second argument. De plus, nous avons Hp (π | σ) ≤ Hp (π). Soit D un ensemble de données avec ensemble d'attributs attr (D). Le domaine d'attribut Ai ∈ Attr (D) est Dom (Ai). La projection d'un tuple t ∈ D sur X est la restriction t [X] de t à l'ensemble X. L'ensemble des attributs X définit une partition πX sur D, qui regroupe les tuples qui ont les projections égales sur X. Soit A un attribut et laissez-X de parents pour A, où Dom (A) = {v1, v2, ...,} et Dom vl (X) = Π B∈X Dom (B) = {u1, u2, ..., um}. Définir pij = P (t [A] = vi | t [X] = uj). Nous avons 1 nβ-1 ≤ Σni = 1 p β ij ≤ 1 pour β ≥ 1. X est considéré comme un ensemble parent « bien » A si connaître la sa valeur nous permet de prédire la valeur de A avec une forte probabilité , qui est, si aj = Σni = 1 p β ij est proche de 1 pour chaque j où P (t [X] = uj) est suffisamment grande. De toute évidence, X est un parent « parfaite » si Σm j = 1 aj = m. Les captures β-GCE exactement cette mesure de la qualité de la hotte. Parent- En effet, supposons que πA = {Bi | 1 ≤ i ≤ n} et πX = {Cj | 1 ≤ j ≤ m}, où T ∈ Bi nous avons t [A] = vi, et de s Cj nous avons s [X] = uj. Ensuite, pij = P (t [A] = vi | t [X] = UJ) = P (t ∈ Bi | t ∈ Cj) = | Bi∩Cj | | Cj | , Ce qui implique Hp (πA | πX) = 1 à 21 jan-β Σm j = 1 P β (Cj) (1 - aj). Ainsi, ce qui réduit Hp (πA | πX) revient à réduire les valeurs de (1 - aj), autant que possible pour les j est où | Cj | est grand, qui est, P (Cj) = P (t [X] = uj) est non triviale. Nous nous référons à la quantité Hp (πA | πX) comme l'entropie du noeud A, en présence de X ensemble. Cependant, même si X = argminX (Hp (πA | πX)), la valeur du minimum lui-même peut être trop élevé pour assurer une bonne prévisibilité. Une alternative consiste à me- assurer la réduction de l'entropie du noeud A à la suite de la présence du groupe X comme Hp (π A | πX) Hp (πA). Depuis 0 ≤ Hp (πA | πX) ≤ Hp (πA), nous avons 0 ≤ Hp (π A | πX) Hp (πA) ≤ 1. Si X est un ensemble de parent parfait A, puis aj = 1 pour 1 ≤ j ≤ m, de sorte Hp (πA | πX) = 0. soit ∈ [0, 1] un numéro appelé seuil de prédiction. Nous considérons X en tant que parent -convient de A si Hp (π A | πX) Hp (πA) ≤. Pour éviter les cycles dans le réseau, nous partons d'une séquence d'attributs A1, A2, ..., Ap et nous cherchons l'ensemble des parents Ai dans l'ensemble Φ (Ai) = {A1,. . . , Ai-1}, une RNTI hypothèse fréquente - X - D. et S. A. Simovici Baraty figure. 1 - Visualisation de l'algorithme (voir Cooper et Herskovits (1993), Suzuki (1999)). De plus, nous avons établi un r lié au nombre maximum de parents. L'ensemble Φ (Ai) peut contenir plusieurs sous-ensembles qui sont -convient. Une solution possible est de choisir un ensemble parent -convient X ⊆ Φ (Ai) avec β-GCE minimum Hp (π A | πX). Par la propriété de β-monotonicity GCE par rapport au deuxième argument que nous avons Hp (πAi | πΦ (Ai)) ≤ Hp (πAi | π {A1, A2, ... Ai-2}) ≤ · · · ≤ Hp ( πAi | π {A1}) ≤ Hp (πAi). Ensuite, pour une donnée, si X a le minimum Hp (πAi | πX) parmi tous les parents -Convient de Ai, alors X a la taille maximale possible. Pour simplifier la structure, nous échangeons une certaine prévisibilité pour la simplicité en adoptant une approche heuristique qui trouve un ensemble minimal de parents pour un noeud avec la plus grande réduction possible de l'entropie de ce nœud enfant sur sa présence. Définir Θl (Ai) = {X ⊆ Φ (Ai) | X est un parent de -convient Ai et | X | = L} et μ = min {n ∈ N | Θn (Ai) 6 = ∅}. Lorsque μ ≤ r, on a la séquence des collections non vides de jeux d'attributs Θμ (Ai), Θ μ + 1 (Ai), ..., Θ r (Ai) par la propriété de monotonicité de β-GCE . Laissez X` = argminX∈Θ `(Ai) (Hp (π Ai | πX)) le premier jeu de taille` (dans l'ordre lexicographique) qui réduit Hp (πAi | πX). Nous limitons notre recherche parent à la séquence des ensembles s = (Xμ, Xμ + 1,..., Xr), où les ensembles sont énumérés dans l'ordre croissant de taille. Pour la séquence S = (... Xμ, Xμ + 1,, Xr) défini ci-dessus, nous avons Hp (πAi | πμ) ≥ Hp (πAi | πXμ + 1) ≥ · · · ≥ Hp (πAi | πXr). L'ensemble de points {(0, Hp (πAi))} ∪ {(p, Hp (πAi | πXp)) | μ ≤ p ≤ r} dans R 2 peut être placé sur une courbe non croissante avec une hauteur h = Hp (πAi) - Hp (πAi | πXr), comme illustré sur la figure 1. On initialise le parent courant réglé Xu à ∅ et iterate membres plus de S dans l'ordre croissant de leur taille. Les membres Xv ∈ S conduit à une amélioration non négligeable dans la prévisibilité sur Xu si Hp (π Ai | πXu) -Hβ (π Ai | πXv) Hp (πAi) -Hβ (πAi | πXr) ≥ v-u r. Thi s se produit si la diminution de Hp (πAi | πX`) lorsque l'ensemble mère de Ai est changé de Xu à Xv est supérieure ou égale à décroissance linéaire par rapport aux deux points d'extrémité de la courbe non croissante correspondant comme représenté sur la Figure 1. les points d'extrémité de la courbe sont (0, Hp (πAi)) et (r, Hp (πAi | πXr)) et la diminution linéaire par rapport aux deux points d'extrémité de la courbe lorsque l'on passe de u à v de est h · axe x qui correspondent à des ensembles parent Xu et Xv (v-u) r = (Hp (π Ai) -Hβ (π Ai | πXr)) · (v-u) r. Notez que v = u + w 1 ≤ w ≤ r - u. Cela donne à penser que nous ne cessons pas le processus si Xu + 1 ne satisfait pas l'inégalité ci-dessus car il peut y avoir un ensemble parent Xv ∈ S où v> u + 1 avec une amélioration non négligeable en termes de prévisibilité par rapport au jeu de parent actuel Xu. RNTI - X - inférence bayésienne de réseaux Algorithme 1: Entrée BuildBayesNet: Jeu de données D, β réel,, r // ∈ [0, 1] est le seuil de prédiction. // β ≥ 1 est le paramètre β-entropie. // r est le nombre maximum des parents. // Attr (D) est une liste d'attributs de D où si // 1 ≤ i <j ≤ | Attr (D) | le ième élément de la liste peut // être un parent de l'élément jème, mais pas vice versa. Sortie: une structure de réseau D NetworkStructure N pour i ← | Attr (D) | pour faire 1 nœud Ai ← Attr (D) [i]; Nombre entier de 0 ←, m ← min (r, i- 1) Vrai H [m + 1] Set S [m + 1] H [0] ← Hp (π Ai) pour j ← m à 1 ne calculent Θj (Ai) si Θj (Ai) = ∅ alors briser le reste S [j] ← argminx∈Θ j (Ai) (Hp (π Ai | πx)) H [j] ← Hp (π Ai | πS [j ]) ← μ j N.addNode (Ai) si μ 6 = 0, alors u Entier ← 0 pour v ← μ m à faire si H [u] -H [v] v-u ≥ H [0] -H [m ] m puis u ← v forall x ∈ S [u] ne N.addEdge (x → Ai) revenir N; // fin de l'algorithme L'augmentation de la taille de l'ensemble des parents est pénalisé en rendant la stricte condition pour les jeux de parents plus grands. En outre, si aucun des ensembles de parents en S de la taille μ à r - 1 satisfont l'inégalité, puis sera Xr. 3 Résultats expérimentaux Nous avons comparé les résultats obtenus avec des structures bayésienne bien connues dans la littérature en utilisant deux systèmes de notation, LDM utilisé par Lam et Bacchus (1994) et Suzuki (1999) et la méthode de notation de Cooper et Herskovits (1993). Les expériences impliquées l'ensemble de données sur les tumeurs cérébrales (Cooper (1984)), le cancer du sein (Blake et al (1998a).), ALARME (Beinlich et al. (1989)), et IRIS (Blake et al. (1998b)). Les résultats expérimentaux sont présentés dans le tableau 1. La dernière ligne de chaque table contient les deux scores pour les structures publiées (selon Williams et Williamson (2006) et Beinlich et al. (1989)). Nous partons du principe que la distribution sur prieurs des structures pour un ensemble de données est uniforme et Cooper Herskovits (1993). Des expériences ont été réalisées sur une machine avec un processeur Intel Xeon 64 bits. Les scores pour les structures de réseau générées dépend de β et dans de nombreux cas est meilleur que les scores des structures établies (partitions C-H sont des scores plus élevés et MDL sont plus bas). La figure 2 représente quatre structures différentes pour le cerveau jeu de données de la tumeur. Structure A est l'une RNTI - X - D. A. et S. Simovici Baraty TAB. 1 - Résultats expérimentaux générés Structures 10000 lignes de la log r (CH Score) MDL Note Temps (ms) 1,0 1,0 3 -7483 13631,52 57 1,0 0,8 2 -7506 13474,37 51 1,6 0,7 2 -7588 13680,31 45 2,1 0,5 3 -7588 13693,21 55 Structure d'origine -8115 14410,10 - Structures Création 286 lignes log ß r (CH Score) MDL Score Temps (ms) 1,1 0,5 2 1,0 144 -1.172 3210,22 0,6 3 -1197 8640,41 202 1,7 0,3 2 1,8 121 -1.207 3669,88 0,7 3 -1214 3859,67 196 1,0 0,5 3 -1215 3511,35 202 1,2 0,4 2 -1224 4968,50 133 1,0 0,7 3 -1256 13667,40 202 Structure originale -1201 4142,03 - cancer du cerveau Résultats Résultats du cancer du sein Generated Structures 20002 lignes de log de r (CH Score) Mdl Score Temps (s) 1,2 0,5 3 542 1,2 -114931 270298,25 0,5 4 -114981 271590,92 12801 1,2 0,6 4 -116.081 12.802 272665,06 1,1 0,7 3 546 -116914 271469,89 Structure d'origine -159306 378.518,37 - Structures générés 150 lignes de log r (CH de Score) MDL Temps Score (ms) 1,0 0,4 2 109 1,8 -902 127543,87 0,7 3 -905 173 13279,40 Structure originale -932 261481,02 - A Larm Résultats Résultats Iris introduits par G. F. Cooper. Structures B (β = 1,0, α = 1,0, r = 3), C (β = 1,0, α = 0,8, r = 2) et D (β = 2,1, α = 0,5, r = 3) sont ceux générés par notre approche. 4 Conclusions Nous avons développé une approche pour générer une structure de réseau bayésien à partir des données sur la base notion d'entropie généralisée. Les meilleures relations parent-enfant entre les attributs sont obtenus à des valeurs de β qui dépendent fortement de l'ensemble de données, ce qui suggère que l'approche de GCE est préférable d'utiliser l'entropie de Shannon. Références Beinlich, I., H. Suermondt, R. Chavez, et G. F. Cooper (1989). Réglementation générale du système de surveillance d'alarme: Une étude de cas avec deux techniques d'inférence probabiliste pour les réseaux de croyance. Rapport technique KSL-88-84, l'Université de Stanford, laboratoire Knowledge System. Blake, C., D. Newman, S. Hettich, et C. Merz (1998a). dépôt UCI de la machine LEARN bases de données ing. Un ensemble de données de l'Institut de Ljubljana Oncology fourni par l'UCI, disponible à http://www.ics.uci.edu/ mlearn / MLRepository.html. Blake, C., D. Newman, S. Hettich, et C. Merz (1998b). dépôt UCI des bases de données d'apprentissage de la machine. Un ensemble de données créé par R.A. Fisher et offert par Michael Marshall, disponible à http://www.ics.uci.edu/ mlearn / MLRepository.html. Cooper, G. F. (1984). NESTOR: Une aide de diagnostic médical informatisé qui intègre les connaissances décontractée et probabiliste. Ph. D. thèse, Université de Stanford. Cooper, G. F. et E. Herskovits (1993). Procédé bayésien pour l'induction de réseaux de données probabilistes. Rapport technique KSL-91-02, l'Université de Stanford, laboratoire Knowledge System. Havrda, J. H. et F. Charvat (1967). Les méthodes de quantification des processus de classification: con- cepts de l'entropie de l'α structurel. Kybernetica 3, 30-35. RNTI - X - Inférence de Bayesian Networks figure. 2 - Structures de tumeurs cérébrales Lam, W. et F. Bacchus (1994). Apprentissage des réseaux bayésiens: une approche fondée sur le principe de LDM. Computational Intelligence 10, 269-293. Rissanen, J. (1978). Modélisation par la plus courte description des données. Automatica 14, 456-471. Simovici, D. A. et S. Jaroszewicz (2002). Un axiomatization de l'entropie de la partition. transac- tions sur 48 Théorie de l'information, 2138-2142. Simovici, D. A. et S. Jaroszewicz (2006). Un nouveau critère de division métrique pour les arbres de décision. International Journal of Parallel, Emergent et Systèmes Distribués 21, 239-256. Suzuki, J. (1999). Apprentissage des réseaux bayésiens basés sur le principe de: MDL Un algorithme efficace en utilisant la branche et de la technique liée. IEICE Trans. Systèmes d'information et, 356-367. Williams, M. et J. Williamson (2006). La combinaison de l'argumentation et les filets bayésiens pour le pronostic du cancer du sein. Journal de logique, du langage et de l'information 15, 155-178. Nous proposons un résumé nouvel Algorithme la structure de verser d'Extraire un réseau bayésien d'un ensemble de Données. Notre approach is sur les entropies basée conditionnelles généralisées, Une famille d'entropies Qui conditionnelle etend l'entropique de Shannon.Nos RE- conditionnelle sultats indiquent Que, with un choix d'Une entropique approprié conditionnelle généralisée, nous obtenons des Bayésiens Qui réseaux Ontario obte- Nues aux structures de scores de par des des methods d'inférence classiques. RNTI - X -