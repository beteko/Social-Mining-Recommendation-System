ensemble prédicteur fondé cartes organisatrices adapté données volumineuses prudhomme stéphane lallich université lumière laboratoire avenue pierre mendès france 69676 eprudhomme lyon2 stephane lallich lyon2 résumé stockage massif données information pertinente gendre problèmes théoriques volumétrie données disponibles problèmes dégradent capacité prédictive algorithmes extraction connaissances partir données article proposons thodologie adaptée représentation prédiction données volumi neuses cette suite partitionnement attributs groupes attri corrélés créés permettent contourner problèmes espaces grandes dimensions ensemble alors place apprenant chaque groupe carte organisatrice outre prédiction cartes objectif représentation pertinente données enfin prédiction réalisée différentes cartes expérimentation menée confirme fondé cette approche espaces grandes dimensions systèmes information tendent stockage analyse quantité croissante information apprentissage cette évolution conséquence augmentation nombre individus nombre attributs faible stockage collecte facile informations individus intérêt ainsi devenus objets complexes exemple puces décrivent individus travers expression milliers gènes banques images chacune nécessite centaines attributs encore internet documents contient outils classiques apprentissage automatique notamment relatif prédiction perdent partie efficacité traiter données verleysen effet point individus attributs présence nombre important catégories problème première catégorie relative qualité données celle dépend pertinence informations récoltées améliorée phase prétraitement point individus détecter étiquetés comme présentent valeurs atypiques types individus faussant fortement apprentissage nuisant ensemble prédicteur fondé cartes organisatrices généralisation beckman cooks muñoz muruzábal muhlenbach point attributs éliminer apportent informa pertinente redondante prédiction cachant ainsi information apportée autres augmentant inutilement complexité problème rendell volume information important difficile détecter individus atypiques sélectionner attributs pertinents deuxième catégorie relative algorithmes apprentissage oeuvre adord travers complexité algorithmes volumétrie données impact direct temps nécessaire apprentissage ainsi algorithmes complexité linéaire fonction nombres individus attributs adaptés volumineuses place cadre alors soumise utilisation heuris tiques stratégies échantillonnage exemples autre augmentation nombre prédicteurs certains problèmes théoriques affectent algorithmes apprentissage particulier cette augmentation dégrade cision apprentissage malédiction dimension bellmann ailleurs fecte également pertinence mesures distance entre points phenomène concen tration distances demartines apprentissage supervisé données volumineuses tenir compte problèmes assurer qualité données mettre place algorithmes adaptés problématiques particulières espaces article étudie capacité méthode ensembliste fondée cartes organisatrices répondre attentes méthodes ensemblistes considérées valentini masulli inclut bagging breiman boosting freund encore combinaison classifieurs ensembles attributs kuncheva whitaker section suivante présente cartes organisatrises adaptation apprentis supervisé ensembles ensuite introduits utilisation cartes organisatrices avant conclure présentons résultats obtenus appliquant notre méthodologie données grandes dimensions cartes organisatrices supervisé supervisé cartes organisatrices organizing kohonen permettent apprentissage supervisé rapide individus représentation faire elles composent réseau neurones répartis uniformément espace voire dimensions chaque neurone défini vecteur espace individus appelé vecteur poids apprentissage individus présentés successivement réseau chaque individu neurone proche matching voisinage réseau modifiés ensemble rapprochent individu algorithme classique apprentissage individu instant résumer formule suivante modification poids neurone prudhomme lallich apprentissage décroît linéairement temps fonction voisinage définit étendue neurones modifiés autour début apprentissage neurones modifier rapprochent fortement dividus grand nombre neurones modifier autour voisinage large grand suite ampleur modification nombre neurones modifier décroissent grâce algorithme obtient conservation topologie locale entrées individus proximité neurones correspond proximité espace départ complexité cartes organisatrices nombre individus nombre attributs algorithmes apprentissage supervisé rapides propriétés rapidité apprentissage préservation topologie sieurs auteurs adapté cartes organisatrices apprentissage supervisé cadre approche couramment utilisée certainement quantification vectorielle pervisée notée learning vector quantization proposée kohonen neurones cartes organisatrices remplacés vecteurs poids associés classes apprendre apprentissage détermine itérativement vecteur proche chaque individu présenté différence cartes organisatrices règle apprentissage utilise alors classe vecteur individu partagent classe vecteur modifié manière rapprocher individu contraire éloignera cette approche garde simplicité faible complexité algorithme cartes organisatrices volumétrie données cependant effectue aucune projection espace entrées ailleurs étiquette établir position vecteurs entrées conduit grande efficacité phase prédiction aussi problème conservation topologie effet possible repré senter topologie espace entrées puisque position prototypes consiste simple projection espace celui carte obtenir représentation topologique espace entrées indépendante classe manière zighed graphes voisinage autres travaux séparé apprentissage phases première phase réalise construction carte fondant uniquement variables prédictives deuxième phase charge étiqueter neurones obtenus étiquette correspond classe majoritaire individus représentés neurone terme apprentissage fonction prédiction permet attribuer étiquette nouvel individu modèle différentes méthodes proposées elles divergent seulement fonction prédiction méthode kohonen zupan améliorée kohonen hopke kohonen prudhomme lallich 2005b avantage majeur cette approche fournir outre modèle prédiction carte constitue projection espace prédicteurs construction supervisée carte celle indépendante étiquette données cartes organisatrices relativement adaptées données volumineuses abord concerne passage échelle elles possèdent algorithme ensemble prédicteur fondé cartes organisatrices plexité linéaire fonction nombre individus variables ensuite point qualité données cartes organisatrices construisent nouvelle représen tation biais projection linéaire elles intéressent ainsi pertinence différents attributs pondérant implicitement construction carte ailleurs construction cette représentation robuste bruit manquantes carte organisatrice constitue bonne représentation données possible valider statistiquement cette représentation cadre apprentissage supervisé prudhomme lallich 2005b cependant cartes organisatrices restent sensibles problèmes théoriques rencontrés espaces grandes dimensions adapter espaces avons choisi utiliser cadre méthodes ensemblistes méthodes ensemblistes introduction ensembles désignent vaste famille algorithmes apprentissage supervisé comme architecture fondatrice utilisation parallèle plusieurs apprenants liser prédiction précisément information portée individus prédicteurs encore variable classe répartie entre différents apprenants chaque apprenant réalise apprentissage information impartie prédire résultat nouvel exemple résultats chaque apprenant combinés cette architecture utilisée succès nombreuses méthodes exemple bagging boostrap aggre gating construit apprenants partir échantillons bootstrap ensemble appren tissage breiman ainsi composante variance erreur principalement échantillon départ boosting construit quant classifieurs partir échantillons pondérés ensemble apprentissage cours échantillonnages pondération renforce probabilité apparition exemples difficiles apprendre cette manière boosting réduit composante biais erreur classifieur rätsch autres stratégies échantillonnage individus possibles construction ensembles exemple mixtures experts chaque prenant rentre compétition autres apprendre individu jacobs aussi données issues différents capteurs lesquels individu appris apprenants chaque attributs issus capteurs différents liste exhaustive trouvée valentini masulli diversité raisons majeure succès cette architecture résumée notion diversité brown revue cette notion existe définition formelle diversité capacité différents apprenants ensemble commettre mêmes erreurs prédiction effet apprenants trompent régions différentes espace apprentissage probable majorité entre prudhomme lallich trouvent réponse exacte région particulière cette intuition diversité confirmée plusieurs résultats théoriques premier entre obtenu geman ensembles régression montrent erreur quadratique moyenne ensemble apprenants appliqué régression écrit covar biais moyen variance moyenne covar covariance moyenne appre nants ensemble cette covariance quantifie corrélation existe entre erreurs apprenants ensemble constitue expression diversité inverse variance biais covariance prend valeurs négatives constitue terme important erreur quadratique moyenne ensemble corrélation négative entre erreurs classifieurs effet réduire erreur totale valeurs discrètes ordonnées tumer formalisent problème différemment variable prédictive classes ressent probabilités posteriori réelles prédire correctement notées lorsque valeurs suffisent séparer classes existe erreur bayésienne irréductible cause paramètres modèle nombre vidus approximés outre erreur irréductible existe alors erreur ajoutée tumer montré erreur ajoutée ensemble écrit eensadd représente erreur ajoutée apprenant corrélation erreurs approximation commises différents apprenants probabilités posteriori covariance encore mesure diversité effet apprenants indépendants erreur ensemble erreur apprenant inverse apprenants dépendants erreur ensemble alors erreur apprenant résultats montrent diversité coeur performance ensembles assure ensemble performant algorithme apprentissage utilise cependant aucun résultat théorique obtenu concernant cette diversité variables discrètes ordonnées autrement aucune définition unanime dégage différentes mesures existantes diversité semblent corrélées nution erreur prédiction ensemble rapport apprenant kuncheva whitaker cependant plupart auteurs accordent importance effet outre extra polation possible résultats régression variables discrètes ordonnées résultats empiriques nombreux revue menée valentini masulli donne fertilité cette approche apparaît chaque possible améliorer prédiction classifieur différentes bases apprentissage soient construites partir individus attributs variable classe ensemble prédicteur fondé cartes organisatrices ensembles espaces grandes dimensions cadre données grandes dimensions ensembles semblent parti culièrement intéressants puisque problème nombre attributs apprendre classifieur solution adaptée consiste profiter approche ensembliste réduire nombre mettant place plusieurs groupes attributs chacun appris classifieur visons plusieurs objectifs premier contourner problèmes riques espaces grandes dimensions cependant opposition sélection attributs durant phase prétraitement conserver totalité comme souligne verleysen redondance information prend réduction bruit chaque variable deuxième objectif amélioration prédiction résulte diversité classifieurs construits chaque groupe ainsi utilisation ensembles cadre données grandes dimensions permet conserver globalité information contournant problèmes posés grandes dimensions ensembles cartes organisatrices place approche ensembliste nécessite réflexion trois points diversité comment partir données initiales créer données induisant classifieurs divers apprenants quels algorithmes utilisés apprentissage chaque aggrégation comment résultats différents classifieurs agrégés obtenir prédiction nouvel individu concerne algorithme apprentissage cartes organisatrices capacité représentation faible complexité algorithmique suite intéressons précisément création diversité agrégation différents résultats diversité comme souligné diversité point crucial ensembles permet rogner erreur apprenant montre utilisation attributs créer cette diversité bonne cadre espaces grandes dimensions différents données seront instanciés partir férents groupes attributs savoir données initial individus attributs nouveaux données constitués chacun ayant individus seulement attri reste déterminer comment partitions créées considère corrélation entre attributs comme critère regroupement lutions opposées dégagent première consiste former groupes attributs corrélés entre chaque groupe contient petite partie information ensemble cartes organisatrices construit modèle conteste diversité importante puisque chaque représentera information différente inverse seconde solution consiste former groupes attributs corrélés entre prudhomme lallich chaque groupe contiendra partie importante information globale cependant niveau ensemble question savoir diversité suffisamment importante précisément dilemme résume travers grandeurs qualité classi fieur ensemble diversité ensemble premier qualité classifieur faible diversité importante deuxième situation inversée solutions conduisent également qualités différentes matière repré sentation données groupe attributs corrélés fournit biais cartes organisatrices représentation possible données initiales parce partie infor mation enlevée cette représentation optimale cependant unique ensemble étant nature construit partir plusieurs apprenants plusieurs cartes plusieurs représentations représentations peuvent directement utilisées navigation travers exemples recherche information plusieurs représentations donnent plusieurs points inverse groupe attributs corrélés mesure constituer représentation données initiales groupe contient suffisamment information revanche projection attributs corrélés carte constitue synthèse information portée groupe capable limiter bruit présent chaque attribut autres termes données initiales représentées parties dernières représentations peuvent utilisées directement navigation interrogation contraire construire représentation supplémentaire parvenir article sommes intéressés formation groupes attributs corrélés entre groupes serviront instancier données appris cartes organisatrices construction groupes réalisée phases classification attributs cette phase objectif construire grappes attri biais classification supervisée parmi différentes méthodes possibles classification hiérarchique moyennes mcqueen matrice attributs valeurs transposée exemple avons choisi méthode varclus conçue spécifiquement classification attributs présente déterminer automatiquement nombre grappes méthode divisive chaque itération algorithme calcule premiers vecteurs propres matrice tributs valeur construire grappes autour vecteurs affectant chaque attribut vecteur lequel corrélé maximum cependant éviter premier vecteur propre corrélé grande majorité variables orthogonale vecteurs propres effectuée méthode varimax kaiser itère chaque grappe ainsi formée jusqu deuxième valeur propre obtenue inférieure issue procédure obtient partition grappes attributs formation groupes partir grappes initiales nouveaux groupes construits ajout attributs groupes manières différentes groupes complets chaque groupe variable choisie hasard intérieur chaque grappe initiale obtient groupes variables chacune représentant grappe différente comme illustré lorsqu grappe contient moins attributs certains attributs réutilisés inverse lorsqu grappe contient attributs certains ensemble prédicteur fondé cartes organisatrices groupes complets lorsque grand devant limiter sélection grappes représenter groupe limite taille données apprendre chaque carte contexte lorsqu attribut grappe selectionné groupe aucun attribut cette grappe sélectionné autres grappes fourni attributs manière lorsqu grappe nouveau sélec tionnée fournir attribut attribut choisi parmi encore sélectionnés groupe règles permettent répartir mieux informa contenue grappes issue cette procédure obtient groupes variables chacune représentant grappe différente hasard seuls attributs sélectionnés représenter groupe attributs tirés remise parmi attributs départ obtient groupes variables relation grappes formées varclus différentes étapes construction ensemble cartes partir groupes attributs corrélés agrégation carte organisatrice prédiction place chacun groupes nouvel individu prédictions doivent agrégées donner prédiction finale utilisons majorité avantage simple donnant résultats aussi satisfaisant autres méthodes cependant prédiction prudhomme lallich cartes passant représentation agrégation fondée cette représentation envisageable particulier statistique corrélée erreur généralisation carte prudhomme lallich 2005a pourrait pondérer efficacement expérimentations avant situer notre approche rapport autres méthodes ensemblistes avons abord validé fondé faire comparons résultats validation croisée résultats obtenus kohonen comparons différentes méthodes création groupes définies données utilisées présentées tableau proviennent newman chacun paramétrages cartes identiques dimension cycles apprentissage optimisés résultats reportés tableau données attributs classes individus ionosphère multi features profile correlations multi features fourier coefficients données montrent abord avantage méthodes ensemblistes carte organisatrice seule intéressant avantage retrouve groupes création guidée connaissance classification attributs colonne hasard cette connaissance création ensemble cartes améliore capacité prédictive remarquera ailleurs création diversité élément majeur réussite notre méthode effet limite préalable données facteurs laquelle applique cartes organisatrices résultats décevants enfin entre groupes complets complets premier performant moyenne correspond affectation attribut chaque grappe chaque groupe cependant groupes complets améliorent également résultats simple carte limitant temps apprentissage partitionnement données kohonen kohonen complet complet hasard comparaison différentes méthodes partitionnement deuxième temps avons comparé notre approche autres méthodes semblistes méthodes choisies boosting arbre décision forêts aléatoires ensemble prédicteur fondé cartes organisatrices données ensemble cartes boosting forêts aléatoires comparaison méthodes breiman méthodes faisant référence implémentation utilisée celle tanagra rakotomalala résultats reportés tableau résultats notre approche toujours supérieurs situe entre forêts aléatoires boosting troisième revanche résultats différents boosting donne résultats mauvais arbre décision différents paramétrages testés fournir amélioration lorsque boosting apprend données produit généralement présence bruit outre notre méthode donne alors meilleurs résultats forêts aléatoires conclusion article montre abord intérêt méthodes ensemblistes traitement grandes dimensions divisant espace prédicteurs construire plusieurs apprenants ensembles contournent problèmes théoriques espaces grandes dimensions montrons cette stratégie payante ensemble fondé cartes organisatrices améliore apprentissage carte diviser espace plusieurs tégies possibles proposons stratégie dirigée connaissance premier gorithme classifie prédicteurs grappe construction connaissance grappes utilisés former groupes cette stratégie payante améliore résultats donnés groupement aléatoire attributs forêts aléatoires inverse utilisent stratégie dirigée hasard arbres décision construits échantillons aléatoires attributs individus forêts aléatoires tirent parti arbres utilisent attributs réaliser prédiction contrairement cartes organisatrices enfin article montre intérêt cartes organisatrices approche ensembliste elles construisent représentation laquelle fonde prédiction représentation assure certaine qualité données grâce cette représentation cartes alors robustes données bruitées associer cartes organisatrices ensembles apparaît comme piste recherche prometteuse cadre données volumineuses travaux poursuivre abord cherchant autres méthodes partionnement ensuite essayant mesurer diversité créée entre différentes cartes enfin étudiant navigation travers ensemble cartes références beckman cooks outliers technometrics prudhomme lallich bellmann adaptive control processes guided princeton university press breiman bagging predictors machine learning breiman random forests machine learning brown wyatt harris diversity creation methods survey categorisation information fusion demartines analyse données réseaux neurones organisés dissertation institut national polytechnique grenoble grenoble france experiments classifier combining rules kittler multiple classifier systems volume lecture notes computer science cagliari italy freund boosting learning algorithm majority proceedings workshop computational learning theory morgan kaufmann publishers geman bienenstock doursat neural networks variance lemma neural computing jacobs jordan barto decomposition through competition dular connectionist architecture where vision tasks cognitive science kaiser varimax criterion analytic rotation factor analysis psychome trika rendell feature selection problem traditional methods algorithm tenth national conference artificial intelligence kohonen organization topogically correct feature biological cyber netics kohonen learning vector quantization neural network kuncheva whitaker feature subsets classifier combination merative experiment proceedings second international workshop multiple classifier systems london springer verlag kuncheva whitaker measures diversity classifier ensembles their relationship ensemble accuracy machine learning weisberg mooers performance evaluation organizing feature extraction journal geophysical research oceans mcqueen methods classification analysis multivariate observa tions fifth berkeley symposium mathematical statistics probability volume rätsch introduction boosting leveraging advanced lectures machine learning muhlenbach lallich zighed identifying handling mislabelled tances journal information intelligent systems muñoz muruzábal organizing outlier detection neurocompu ensemble prédicteur fondé cartes organisatrices newman hettich blake repository machine learning databases prudhomme lallich 2005a quality measure based kohonen supervised learning large dimensional international symposium applied stochastic models analysis asmda brest france prudhomme lallich 2005b validation statistique cartes kohonen appren tissage supervisé actes volume rakotomalala tanagra logiciel gratuit enseignement recherche actes volume guide version fourth edition volume institute hopke kohonen neural network pattern recognition method based weight interpretation analytica chimica tumer theoretical foundations linear order statistics combi neural pattern classifiers technical report computer vision research center university texas austin valentini masulli ensembles learning machines marinaro gliaferri neural vietri springer verlag verleysen limitations future trends neural computation chapter learning dimensional press verleysen françois simon wertz effects dimensionality analysis neural networks international conference artificial natural neural networks computational methods neural modeling volume springer verlag hierarchical grouping optimize objective function journal rican statistical association zighed lallich muhlenbach statistical approach classes separa bility elomaa revue applied stochastic models business industry springer verlag zupan novic gasteiger classification multicomponent analytical olive using different neural networks analytica chimica summary knowledge discovery process encounters difficulties large amount treat indeed theoretical problems related dimensional spaces appear degrade predictive capacity algorithms article propose methodology adapted representation prediction large datasets purpose groups correlated attributes created order overcome problems related dimen sional spaces ensemble learn group organizing besides prediction these providing relevant representation finally prediction achieved different experimentation performed revelance approach