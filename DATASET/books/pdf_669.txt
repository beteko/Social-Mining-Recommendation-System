articles assemblage pdfModèle de Langue à base de Concepts pour la Recherche d’Information Lynda SAID L’HADJ* Mohand BOUGHANEM** *Ecole Doctorale STIC Ecole nationale Supérieure d’Informatique ESI Algérie l_said_lhadj esi dz **Laboratoire IRIT Université Paul Sabatier 118 route de Narbonne 31062 Toulouse Cedex 09 France bougha irit fr Résumé La majorité des modèles de langue appliqués à la recherche d’information repose sur l’hypothèse d’indépendance des mots Plus précisément ces modèles sont estimés à partir des mots simples apparaissant dans les documents sans considérer les éventuelles relations sémantiques et conceptuelles Pour pallier ce problème deux grandes approches ont été explorées la première intègre des dépendances d’ordre surfacique entre les mots et la seconde repose sur l’utilisation des ressources sémantiques pour capturer les dépendances entre les mots Le modèle de langue que nous présentons dans cet article s’inscrit dans la seconde approche Nous proposons d’intégrer les dépendances entre les mots en représentant les documents et les requêtes par les concepts 1 Introduction Les modèles de langue ont acquis une grande popularité en Recherche d’Information RI étant donné la solidité de leur fondement mathématique Ponte et Croft 1998 Ces modèles ne modélisent pas directement la notion de pertinence Cette dernière est vue comme la probabilité conditionnelle P Q�D que la requête Q soit générée par le modèle de langue du document D P Q�D est estimée sous l’hypothèse d’indépendance des mots qui simplifie le calcul mathématique Cependant elle pose un problème majeur lié à la représentation des documents requêtes comme des sacs de mots dénués de sémantique Pour pallier ce problème une nouvelle génération de modèles de langue qui s’inscrit à l’intersection des modèles de langue et de la recherche sémantique d’information a été développée Cao et al 2005 Srikanth et Srihari 2002 2003 Dans cette intersection deux grandes approches peuvent être distinguées l’approche statistique surfacique qui prend en compte des dépendances surfaciques entre les mots et l’approche sémantique basée sur des ressources sémantiques ontologies thésaurus pour identifier les sens des mots Le modèle de langue présenté dans cet article s’inscrit dans la seconde approche Nous proposons de capturer les dépendances entre les mots par l’identification des concepts auxquels renvoient ces mots Même si l’approche conceptuelle souffre du problème de silence1 nous pensons que la définition d’un modèle de langue mixte combinant les concepts identifiés dans l’ontologie et les concepts non identifiés permet de résoudre ce problème 1 dû à la non disponibilité de ressources conceptuelles complètes et générales RNTI E 19 91 Modèle de langue à base de concepts pour la recherche d’information Le reste du papier est organisé comme suit Nous présentons un bref état de l’art sur les modèles de langue sémantiques dans la section 2 La section 3 est consacrée à la présentation du modèle proposé Puis nous déroulons un exemple dans la section 4 Enfin dans la section 5 nous terminons ce papier avec une synthèse du travail présenté 2 Etat de l’art La pertinence d’un document face à une requête est en rapport avec la probabilité que la requête Q vue comme une suite de mots t1t2…tn puisse être générée par le modèle de langue du document Le score de pertinence est alors donné par Score Q D =P Q|MD 2=P t1t2…tn�MD 1 Pour estimer la probabilité de 1 il faut que les ti soient indépendants ainsi ������� ���� � �� �� 2 L’hypothèse d’indépendance des mots pose deux problèmes majeurs le premier est celui des données éparses c’est à dire si un mot ti est absent dans le document P Q�D est alors nulle même si les autres tj j�i sont présents Ce problème a été résolu par les techniques de lissage3 Zhai et al 2001 Le second problème est la représentation en sac de mots qui ne permet pas la prise en compte de deux phénomènes très importants en RI à savoir la polysémie et la synonymie Pour le résoudre la plupart des travaux proposés jusque là a utilisé les techniques de lissage le lissage sémantique afin d’incorporer les sens des mots ainsi que les liens entre ces mots dans les modèles de langue Ces travaux sont classés en deux catégories d’approches Cao et al 2005 L’approche statistique ou surfacique et l’approche guidée par les ressources sémantiques L’approche surfacique tente d’intégrer les relations entre les mots selon des considérations statistiques par exemple les cooccurrences Le modèle de translation statistique de Berger et Lafferty 1999 fut l’un des premiers travaux dans cette direction Gao et al 2004 de leur coté considèrent les dépendances entre les mots comme une variable cachée L représentée par un graphe acyclique non orienté pour modéliser les dépendances entre les mots Srikanth et Srihari 2002 ont proposé un modèle bi termes où ils ignorent la contrainte d’adjacence et de l’ordre des mots imposée dans les modèles bi grammes Enfin Srikanth et Srihari 2003 présentent un modèle uni gramme de concepts des séquences de mots identifiés avec un parseur syntaxique L’approche guidée par les ressources sémantiques se base sur des liens sémantiques extraits de ressources sémantiques comme les ontologies Cao et al 2005 ont proposé une approche qui combine le modèle d’indépendance uni gramme avec le modèle de dépendance des mots en exploitant les techniques de lissage Ces dépendances sont de deux types statistique cooccurrence et sémantique relations entre mots simples de WordNet Dans la même direction Bao et al 2006 ont proposé un modèle de langue uni gramme lissé avec un modèle uni gramme de sens de ces mots identifiés par un système de désambiguïsation basé sur WordNet Les résultats des deux approches sont meilleurs que le modèle de langue uni gramme Cependant l’approche surfacique engendre beaucoup de bruit et donc elle nécessite un filtrage linguistique voire sémantique Nous pensons également que l’intégration de relations 2 On écrit P Q|D pour représenter la probabilité P Q|MD où MD est le modèle de document 3 Attribuer une probabilité non nulle aux mots de la requête absents dans le document RNTI E 19 92 L SaidL’hadj et M Boughanem surfaciques ou sémantiques entre mots simples ou la simple désambigüisation de ces mots ne suffit pas pour capturer le contenu sémantique implicite des documents et des requêtes Nous pensons de ce fait qu’un concept correspondant par exemple à une entrée d’une ontologie est plus précis qu’un mot isolé ou un sens isolé 3 Modèle proposé Le modèle que nous proposons se base sur les concepts Plus précisément tout document respectivement requête est projeté sur une ontologie par exemple WordNet Nous utilisons à cet effet l’algorithme de Baziz 2005 4 pour la détection des concepts Ainsi les termes ayant une entrée dans l’ontologie sont pris comme éléments du document qui permettent la construction de l’arborescence du document de la requête Mais contrairement à Baziz 2005 nous proposons de garder les termes non reconnus dans l’ontologie dans le descripteur car il peut arriver que ces termes renvoient à des concepts importants cas des noms propres ou des néologismes Nous considérons la requête Document comme des sacs de concepts Ainsi Q= c1c2…cn Le score de pertinence est donné par P Q�D estimée comme suit ������ ���� �� ����� 4 Nous distinguons deux cas Si ci ne correspond à aucune entrée de WordNet l’appariement est strict Sinon on exploite la hiérarchie de la requête et du document pour chercher non seulement ci mais aussi les concepts qui lui sont proches Le lissage par interpolation linéaire permet de tenir compte de ce fait ainsi ���� �� � ������������� �� � � �� ��������� �� � 5 ������������ �� � est la probabilité que ci non candidat à l’expansion soit généré par D ������� �� � est la probabilité que le concept ci identifié dans WordNet soit généré par D Estimation de " �% �'� Quand le concept ci correspond à une entrée de l’ontologie on peut non seulement retrouver les documents où il figure effectivement appariement direct mais aussi les documents où figurent les concepts qui lui sont liés par des relations sémantiques de l’ontologie subsomption appariement indirect Ces deux cas sont combinés par un lissage par interpolation linéaire ������� �� � ����� �� � � �� ���*��� �� � 6 ����� �� � est la probabilité que le concept ci soit généré directement par D et ��*��� �� � est la probabilité que le concept ci soit généré indirectement par D Estimation des probabilités �% '� � et " ������% '� � Elles sont estimées en utilisant la formule de pondération de concepts CF Concept Frequency proposée dans Baziz 2005 ����� �� � �+��� �� � �� � ��0 � ��1 ��21�3 7 �4+��� � 5678�+��� � � 0 9�� <�=��>�� < +� � = � AB�C���� �� �� 5678� �D�� Où Count cpi D retourne la fréquence d’apparition de cpi Length cpi représente le nombre de mots dans le concept cpi et sub_concept cpi le nombre de tous les sous concepts 4 Cet algorithme comprend détection des groupes de mots désambigüisation et pondération RNTI E 19 93 Modèle de langue à base de concepts pour la recherche d’information des concepts de l’ontologie dérivés de cpi Quand ci ne correspond à aucune entrée de WordNet nous annulons le deuxième terme de �4+��� � �4+����E������ � 5678������E������ �� Quand ci identifié ou non dans WordNet est absent dans le document alors P Q�D est nulle C’est pourquoi nous lissons �������� et ��������������� en utilisant la méthode “Absolute Discount” appliquée aux concepts Zhai et al 2001 �FB +���� GHI �� ��� ��JK L���� � K������ �M>N����5� Où��� 0 �4��1�� �O �� et �M>N����5� � ��� �0 � ��� �2��P C est le modèle de la collection Estimation de Q�% �'� Pour intégrer les relations entre les concepts nous utilisons le modèle de Berger et Lafferty 1999 ��*������ 0 �������*�� Q�N ����*���R 8 Où E est l’ensemble de tous les concepts ��* liés à ceux de la requête Les approches conceptuelles proposées jusque là mélangent les concepts spécifiques avec les concepts génériques Or il a été constaté que les concepts génériques améliorent le rappel et que les concepts spécifiques améliorent la précision Baziz 2005 Zakos J 2005 Le modèle 8 est séparé pour tenir compte de ce fait par le lissage par interpolation linéaire �� Q ������ S T0 �����U�N�UV� �� � ��� ��� W � �� S� X0 �����Y�N�YV� �� � ��� ��� Z 9 Où ������ � respectivement ������ � est la probabilité conditionnelle que ci soit généré par un concept plus générique cg respectivement cs Nous avons posé les contraintes cgV Q et cs V Q pour ne tenir qu’une seule fois de cg et cs quand ils existent déjà dans la requête Estimation de �% �%[� et de �% �%\� Ces probabilités interprètent la distance sémantique entre ci et cg ou cs Elles sont estimées en utilisant une mesure de similarité basée sur la distance sémantique entre ces concepts Nous avons choisi la mesure de Wu et Palmer 1994 car elle est simple à mettre en œuvre de plus elle est basée sur le principe de la distance sémantique suivant Soient X et Y deux éléments d’une ontologie La similarité entre eux est basée sur les distances N1 et N2 qui les séparent du nœud racine et la distance N qui sépare le concept subsumant CS X et Y du nœud racine D]^ _A�` a� bcc�dcb alors �+�� � � =�e fg��U ��� 0 =�e fg��� �h�2h�i 10 De la même manière on estime la probabilité ������ � Après remplacement des différentes probabilités dans [5] le modèle proposé est donné par �+��� jkk kl ������������� �� � ��� �� m ����� �� � � �� �TS 0 �����U�N�UV� �� � ��� ��� � �� S�0 �����Y�N�YV� �� � ��� ���Wnop ppq���� 11 5 Il s’agit du modèle de l’expansion des concepts à proprement parler RNTI E 19 94 L SaidL’hadj et M Boughanem 4 Exemple Soient D1={Natural 4 Science 6 Geology 10 Geography 5 Geophysics 8 Globe 3 aeroelastic 10 } D2={Earth 7 Natural 3 Science 6 Anatomy 4 Regional 5 } Q= {earth science geography aeroelastic} 4 1 Application du modèle uni gramme mixte Le modèle de langue mixte est donné par ������ r�������� � �� �������5�st� � u Sachant que les ti k correspondent aux mots de la requête et � est fixé à 0 5 l’application numérique du modèle retourne les scores de pertinences suivants P Q�D2 = 0 00225 et P Q�D1 = 0 00125 On remarque ainsi que D2 est plus pertinent que D1 par rapport à Q 4 2 Application du modèle à base de concepts proposé La projection de D1 et de D2 sur la hiérarchie de WordNet retourne les représentations conceptuelles D1 = {Natural Science 9 Geology 10 Geography 5 Geophysics 8 Globe 3 aeroelastic 10 D2= {Earth 7 Natural Science 7 5 Regional anatomy 8 5 } Q {earth science geography aeroelastic} FIG 1 Représentations hiérarchiques des concepts de D1 et D2 La figure 1 permet de distinguer les niveaux de concepts que voici ��������� ={aeroelastic} Cp= {earth science geography} Cg earth science ={natural science science } Cs earth science ={geology geography oceanography geology geophysics} Cg geography = { } Après application du modèle proposé6 les scores de pertinence sont P Q�D1 = 0 000003726 et P Q�D2 = 0 000001481040 Nous remarquons alors que D1 est plus pertinent que D2 par rapport à Q Ce résultat diffère du premier Il est justifié par la fréquence élevée de Earth de D2 dont le sens est loin de celui de la requête qui a influencé le résultat retourné par le modèle uni gramme 6 Les paramètres � � � = 0 3 0 5 0 5 Nous avons donné un poids plus important 0 5 au modèle d’expansion car il capture effectivement la sémantique de la requête et du document RNTI E 19 95 Modèle de langue à base de concepts pour la recherche d’information 5 Conclusion Dans ce papier nous avons proposé un modèle de langue basé sur les concepts Le choix d’intégrer les concepts dans les modèles de langue se justifie aussi bien par les résultats prometteurs de la recherche conceptuelle d’information que par la performance et la flexibilité des modèles de langue à intégrer plusieurs sources de connaissances En effet cette flexibilité nous a permis de tenir compte non seulement des concepts de l’ontologie ainsi que de leurs liens mais aussi des concepts qui n’apparaissent pas dans l’ontologie Ce modèle est en cours d’expérimentation Les résultats nous permettront d’approfondir et de consolider nos hypothèses sur la combinaison mots simples concepts ainsi que de la séparation des deux niveaux de concepts génériques et concepts spécifiques 6 Références bibliographiques Shenghua Bao Lei Zhang Erdong Chen Min Long Rui Li and Yong Yu 2006 LSM Language Sense Model for Information Retrieval WAIM pp 97–108 M Baziz 2005 Indexation Conceptuelle Guidée par Ontologie pour la Recherche d’Information thèse de doctorat de l’université de Paul Sabatier Berger A and Lafferty J 1999 Information retrieval as statistical translation In Proc of the 1999 ACM SIGIR pp 222 229 Guihong Cao Jian Yun Nie Jing Bai 2005 Integrating Word Relationships into Language Models SIGIR’05 Salvador Brazil August 15–19 Jianfeng Gao Jian Yun Nie Guangyuan Wu Guihong Cao 2004 Dependance Language model for information retrieval SIGIR’04 Jay M Ponte and W Bruce Croft 1998 A Language Modeling Approach to Information Retrieval Proc of ACM SIGIR pp 275 281 Munirathnam Srikanth et Rohini Srihari 2002 Biterm Language Models for Document Retrieval ACM SIGIR’02 Tampere Finland Munirathnam Srikanth Rohini Srihari 2003 Incorporating Query Term Dependencies in Language Models for Document Retrieval In SIGIR’03 Canada John Zakos 2005 A Novel Concept and Context based Approach for Web Information Retrieval Doctorate thesis Griffith University 2005 Wu Z Palmer M 1994 Verb Semantics and Lexical Selection In Proc of the 32nd Annual Meeting of the Associations for Computational Linguistics pp 133 138 1994 Zhai C and Lafferty J 2001 A Study of Smoothing Methods for Language Models Applied to Information Retrieval In Proc of the 2001 ACM SIGIR pp 334 342 RNTI E 19 96 