apprendre contraintes topologiques cartes organisatrices guénaël cabanes younès bennani avenue clément 93430 villetaneuse france cabanes paris13 résumé carte organisatrice organizing méthode populaire analyse structure ensemble données cependant certaines contraintes topologiques fixées avant prentissage peuvent pertinentes représentation struc données article proposons améliorer perfor mances nouvel algorithme apprend contraintes topolo giques carte partir données expériences bases données artificielles réelles montrent algorithme proposé produit meilleurs sultats classique relaxation triviale contraintes topologiques résulte forte augmentation erreur logique carte introduction carte organisatrice organizing kohonen ensemble neurones artificiels représentent structure données rones connectés connexions topologiques former grille dimen sions neurones connectés devraient représenter données neurones distants carte doivent représenter données différentes propriétés assurées pendant processus apprentissage grâce informations voisinage imposent contraintes topologiques toutefois algorithme information topologique fixée avant processus apprentissage pertinente rapport structure données résoudre problème certains travaux réalisés adapter nombre neurones cours processus apprentissage fonction données analyser fritzke résultats montré qualité modèle améliorée lorsque nombre neurones appris partir données dépit résultats travaux abordent problème appren tissage contraintes topologiques fonction structure données pourtant processus apprentissage neurones voisins peuvent représenter données similaires cabanes bennani matsushita nishio algorithme false neighbor matsushita nishio auteurs proposent conser topologie bidimensionnelle associant chaque ligne colonne apprendre contraintes topologiques cartes organisatrices connexions indice voisinage validité globale ligne lonne algorithme chaque neurone limité seulement quatre voisins comme valeur utilisée toutes connexions appartenant ligne colonne méthode conduire structure topologie incorrecte particulier bases données grande dimension valeurs associées chaque ligne colonne topologie rectangulaire proposons cette partie améliorer performances nouvel algorithme driven relaxation capable apprendre topologie carte partir structure données algorithme permet visualisation dimensions résultats nombre voisins limité voisinage couramment utilisé principale utiliser valeurs associées connexions réduire certaines contraintes topologiques entre neurones repré sentent données différentes relâchements contraintes censés améliorer notamment réduisant erreur quantification carte augmentation nombre neurones participent réellement représentation données reste article organisé comme section présente algorithme organisation carte section décrit algorithme section montre validations expérimentales avons effectuées résultats obtenus enfin conclusion donnée section cabanes bennani algorithme cartes organisatrices consiste carte neurones dimensions neurones connectés leurs voisins selon connexions topologiques connexions voisinage kohonen ensemble données analyser utilisé organiser contraintes topologiques espace entrée ainsi correspondance entre entrée espace carte construit observations proches espace entrée doivent activer neurone neurones voisins vérifier contraintes neurones voisins neurone représentatif donnée mettent prototype meilleure représentation cette donnée cette autant importante neurones proches voisins meilleur neurone apprentissage comme minimisation fonction nombre données nombre neurones carte neurone vecteur prototype proche donnée minimiser distance entre données prototypes selon pondération donnée fonction noyau représente contraintes topologiques respecter exemple fonction température diminuer contraintes topologiques temps façon assurer convergence apprentissage rayon voisinage cheng distance voisinage entre neurones calculée distance manhattan nombre minimal connexions topologiques entre exemple distance manhattan pondérée entre neurones topologie gonale nombre minimal connexions topologiques entre notre algorithme adapté version batch présente façon vante apprendre contraintes topologiques cartes organisatrices phase initialisation choisir topologie initialiser prototypes carte phase affectation déterminer prototype représentatif chaque donnée entrée argmin phase représentation mettre prototypes carte chaque neurone façon minimiser fonction répéter phases jusqu relâchement contraintes topologiques guidé données principe algorithme proposons associer chaque connexion voisi valeur réelle indique pertinence neurones connectés compte contrainte organisation neurones représentatifs ensemble données doivent reliés liaison voisinage ainsi paire neurones représentants ensemble données devraient fortement connectés tandis paire neurones voisins représentent données faiblement connectés associe chaque connexion sinage valeur réelle variant faible utilise fonction logistique dépend nombre données représentées chacun neurones connectés valeurs ensuite utilisées estimer distance manhattan pondérée entre chaque paire neurones voisins notée cette distance nombre connexions entre pondéré valeurs associées chaque connexion exemple utilisons algorithme johnson johnson calculer chemin court connexions voisinages entre chaque paire rones longueur chemin fonction valeurs associées chaque connexion cette manière distance entre neurones supposée refléter voisinage neurones valeurs connexion distances pondérées entre neurones mises cours apprentissage cette façon obtenue devrait meilleur représentant données algorithme classique cabanes bennani distance manhattan pondérée entre neurones topologie gonale court chemin entre fonction valeurs connexion nouvel algorithme algorithme procède trois phases phase initialisation choisir topologie initialiser prototypes carte initialiser toutes valeurs connexions voisinage phase compétition déterminer premier second chaque donnée entrée argmin argmin mettre valeurs connexions voisinage selon règle suivante fonction logistique variant entre lorsque représentent mêmes données lorsque représentent données différentes nombre données ayant valeur rique hypothèse homogénéité autrement moyenne toutes connexions voisinage finir paramètre fonction logistique choisi utilisateur phase adaptation apprendre contraintes topologiques cartes organisatrices mettre prototypes carte chaque neurone notez valeurs dépendent valeurs répéter phases jusqu résultats expérimentaux description bases données utilisées tester validité nouvel algorithme avons utilisé bases données artifi cielles réelles tailles dimensions variables caractéristiques chaque décrites table description bases données utilisées données taille dimension target artificielle twodiamonds artificielle hepta artificielle tetra artificielle réelle harot réelle housing réelle réelle cockroach réelle chromato réelle bases données target twodiamonds tetra hepta viennent funda mental clustering problem suite ultsch données artificielles faible dimension structure parfaitement connue elles souvent utilisées comme bases algorithmes clustering cabanes bennani matsushita ultsch bases données housing harot données réelles connues repository frank asuncion elles tailles dimensions variables finir cockroach chromato bases données réelles bruitées provenant expériences domaine biologie bases données représentent partie diversité problèmes pouvant contrés utilisateurs analyse bases données cabanes bennani target tetra twodiamonds hepta visualisation bases données estimation qualité évaluer performances apprentissage notre algorithme utilisons trois indices classiques qualité algorithmes erreur quantification indice mesure distance moyenne entre chaque donnée kohonen valeur petite qualité algorithme grande erreur topographique décrit façon préserve topologie ensemble données étudiées kiviluoto mesure proportion données ayant premiers adjacents reliés connexion voisinage faible valeur signe qualité contrairement erreur quantification prend compte structure utilisation neurones mesure pourcentage neurones jamais donnée cheung bonne devrait avoir faible chaque neurone devrait utilisé représentation données gaspiller apprendre contraintes topologiques cartes organisatrices ressources calcul particulier analyse grandes bases données applications réelles toutes expériences suivantes indices normalisés pouvoir comparer efficacement résultats différentes bases données représenter perte rapport algorithme classique chaque indice divisé valeur obtenue algorithme erreur toujours égale toutes expériences cette section avons utilisé paquet matlab toolbox vesanto paramètres fixés valeurs défaut particulier utilisons grille hexagonale comme topologie initiale carte effet relâchement trivial contraintes topologiques principe essentiel nouvel algorithme réduire contrainte topologique augmentant distance entre neurones modifications guidées données optimiser qualité finale première étape notre expérimentation analyser comment comporte diminue façon triviale contraintes topologiques prévoyons faible contrainte conduit meilleur modèle faibles erreurs quantification utilisation neurones conduit également augmentation erreur topolo gique puisque fonction contraintes topologiques réduire cette erreur vérifier avons calculé erreurs quantification utilisation neurones topologique chaque données partir résultats différents relâchements contraintes topologiques algorithme chaque distance entre neurones multipliée valeur constante cette constante grande contraintes topologiques faibles exemple similaire algorithme avons testé différentes valeurs cette constante similaire neurones pratiquement indépendants comportement algorithme similaire celui moyennes résultats résumés cette figure montre valeur moyenne toutes bases données différents relâchements contraintes topologiques comme prévu diminuent lorsque contraintes topologiques affaiblissent tandis augmente fortement puisque associé perte proposons définir erreur générale reflète compromis entre produit compromis valeur faible lorsque supérieur perte rapport algorithme grande contraire valeurs moyennes toutes bases données différentes valeurs représentées résultats montrent effet diminution triviale contraintes topolo giques compenser perte ainsi meilleur compromis utiliser algorithme classique maintenant question utiliser données apprentissage trouver relaxation contraintes topologiques meilleur compromis cabanes bennani visualisation valeur moyenne ensemble bases données différents relâchements contraintes topologiques évaluation nouvel algorithme guidé données évaluer performances avons comparé différentes versions différentes valeurs paramètre montre valeurs moyennes toutes bases données valeurs chaque données chaque version indiquées table enfin illustre qualité comparaison algorithme valeurs obtenues chaque données diffé rentes valeurs target twodiamonds hepta tetra harot housing cockroach chromato apprendre contraintes topologiques cartes organisatrices visualisation valeur moyenne ensemble bases données différentes valeurs pouvons noter contraintes topologiques moyennes laires celles cependant comme pouvons constater performances algorithme meilleures celles autre supérieur perte rapport effet erreur quantification semblable celle faible semblable tandis erreur topologique semblable erreur obtenue algorithme classique pouvons faire commentaires partir résultats performances meilleures celles toutes valeurs cependant valeur semble donner meilleurs résultats bases données tendance élevé bases données grandes dimen sions exemple chromato article matsushita nishio constate cartes 10x10 topologie rectangulaire moyenne formant quelques bases données utilisées auteurs tetra hepta target conclusion article proposons nouvel algorithme adapté liorer qualité modèle obtenu apprentissage utilisons relâchement cabanes bennani données target données premières variables visualisation résultats obtenus bases données données rouge grille prototypes apprentissage représentée contraintes topologiques guidé données avons défini erreur globale représente compromis entre erreurs topologiques quantification lisation neurones expériences bases données artificielles réelles montrent algorithme obtient meilleurs résultats algorithme avons également cette amélioration obtenue relaxation triviale contraintes pologiques raison forte augmentation erreur topologique diminution contraintes guidée données semble bonne solution améliorer compro remerciements travail soutenu partie projet financé agence nationale recherche apprendre contraintes topologiques cartes organisatrices références cabanes bennani simultaneous level clustering algorithm automatic model selection proceedings international conference machine learning applications icmla cabanes bennani local density based simultaneous level algorithm topographic clustering proceeding international joint conference neural networks cheng convergence ordering kohonen batch neural comput cheung rival model penalized organizing trans neural networks frank asuncion machine learning repository fritzke growing organizing network constant neighborhood range adaptation strength neural processing letters johnson efficient algorithms shortest paths sparse networks journal kiviluoto topology preservation organizing international confe rence neural networks kohonen organization associative memory berlin springer verlag kohonen organizing berlin springer verlag matsushita nishio organizing false neighbor degree between neurons effective organization ieice transactions fundamentals ultsch clustering procedings workshop organizing vesanto himberg alhoniemi parhankangas organizing matlab toolbox proceedings matlab conference summary organizing popular algorithm analyze structure dataset however topological constraints fixed before learning relevant regarding structure paper propose improve performance algorithm which learn topological constraints using structure information experiments artificial databases rithm achieve better results trivial topological constraint relaxation because increase topological error