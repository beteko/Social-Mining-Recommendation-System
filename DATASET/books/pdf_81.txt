e33classification ascendante hiérarchique noyaux application données textuelles julien xinyu université université avenue pierre mendès france 69676 cedex france julien xinyu lyon2 résumé formule lance williams permet unifier plusieurs méthodes classification ascendante hiérarchique article suppo données représentées espace euclidien établis nouvelle expression cette formule utilisant similarités cosinus distances euclidiennes carré notre approche présente tages suivants permet étendre naturellement méthodes classiques fonctions noyau autre permet appliquer méthodes écrêtage permettant rendre matrice similarités creuse améliorer complexité application notre approche tâches classification automatique données textuelles montre passage échelle amélioré mémoire temps traitement autre qualité résultats préservée voire améliorée introduction ensemble objets constitué éléments matrice dissimilarités entre chaque paire objets procédure classique classification ascendante hiérarchique initialise arbre feuilles chaque itération regroupe couple groupes objets distance petite argmin nouveau groupe ajouté arbre binaire ensuite calculer dissimilarités entre groupes existants existe plusieurs niques selon façon définit dissimilarité entre groupes cependant lance williams lance williams montré majorité entre elles pouvaient généralisées formule formule suivante présentons tableau définition méthodes étudions cadre formule objets comme singletons classification ascendante hiérarchique noyaux méthodes single complete average mcquitty centroid median formule lance williams méthodes paramètres procédure bottom décrite dessus formule forme approche sique cette dernière simple flexible coûteuse mémoire temps traitement passe échelle grandes masses données effet matrice dissimilarité nécessairement dense coûte mémoire tandis procédure bottom complexité article proposons reformulation approche classique rappeler permet outrepasser limites évoquées dessus notre principale définir expression équivalente formule termes similarités plutôt termes dissimilarités cette perspective supposons objets représentés espace euclidien dissimilarité mesurée carré distance euclidienne entre vecteurs normés produit scalaire associé cosinus angle formé vecteurs ainsi comme mesure similarité notre approche présente double avantage permet étendre naturelle fonctions noyau permettant ainsi traiter efficacement données linéairement séparables espace description initial autre permet définir stratégies écrêtage matrice similarité rendu creuse cette dernière alors légère mémoire pouvons également améliorer temps traitement comme expliquerons suite illustrer propriétés notre approche appliquons celle tâches classification automatique données tuelles résultats montrent notre méthode permet réduire complexité obtenir meilleurs résultats nombreux suite article organisée façon suivante section introduisons différents ingrédients notre approche section présentons résultats expériences avons menées trois données classiques concluons section discussion esquisse travaux futurs notre approche supposons objets représentés espace euclidien dimension produit scalaire vecteurs similarité définie remarquons ainsi vecteurs norme constitue condition importante notre matrice produits scalaires associée matrice carrés distances euclidiennes comme cette condition présentons procédure bottom fondée formules récurrence produisent arbre binaire équivalent approche classique partant arbre feuilles regroupons itérativement couples groupes objets vérifiant argmax lorsque nouveau ajouté arbre similarités entre groupes existants ainsi obtenues formules suivantes notre méthodes définies paramètres listés tableaux paramètres mêmes énoncés tableau alors nouveaux paramètres introduits tableau notons exceptées méthodes median centroid peuvent choisis arbitrairement condition cette reformulation formule nécessaire avoir formules récurrence distinctes autre obtenir équivalence entre recherche minimum celle maximum preuve cette équivalence obtient montrant méthodes single complete average mcquitty centroid median formule basée similarités cosinus méthodes paramètres comme raisonnons désormais produits scalaires pouvons étendre turellement notre approche fonctions noyau matrice produits scalaires matrice taille objets classification ascendante hiérarchique noyaux espace hilbert dimension pouvant infini matrice contenant similarités cosinus espace alors facilement obtenue utilisant astuce noyau ensuite façon générale contenir valeurs négatives petite valeurs toujours possible transformer façon avoir valeurs positives comme cette application monotone croissante matrice transformée reste matrice supposons désormais valeurs comprises entre écrêtons selon paramètre seuillage ainsi toute valeur inférieure égale remplacée matrice devient creuse complexité mémoire réduite facto étant nombre paires objets similarité strictement positive améliorer complexité temps traitement proposons restreindre recherche couple groupes objets fusionner seules paires similarité strictement positive introduisons ensemble équation alors remplacée argmax complexité notre procédure bottom réduite expériences objectifs expériences démontrer condition énoncée précé demment notre méthode basée équivalente procédure classique fondée écrêtage notre approche utilisant permet réduire considérablement coûts mémoire temps traitement produire résultats bonne qualité comparaison méthode classique expérimentons tâches classification automatique données textuelles documents représentés espace vectoriel termes différentes dimension espace coordonnées vecteurs nombres occurrence termes documents notons termes étant apparus moins documents collection retirés différentes étude reuters groupes documents termes smart groupes documents termes groupes documents termes étant donné matrice documents termes déterminés résultat approche classique basée calculé résultat référence ainsi résultats notre approche basée différents niveaux seuillage paramètre choisi façon écrêtées notons devions appliquer principe partir matrice dissimilarité valeurs grandes dessus seuil aurait fallu écrêter raisonnable documents étant représentés vecteurs composantes positives cosinus valeurs positives mesurer proximité entre arbre obtenu méthode classique obtenus notre approche utilisons valeur absolue corrélation cophénétique évaluer qualité résultat coupons arbre obtenu obtenir partition nombre correct groupes comparons celle vérité terrain index corrigé mesure classique avons utilisé fonctions noyau linéaire gaussien raison restriction nombre pages pouvons montrer ensemble résultats expériences cependant voici observations importantes faire lorsque écrêté retrouvons résultat approche classique usage mémoire temps traitement diminuent lorsque creuse méthode single présente comportement particulier écrêtant valeurs obtenons résultat approche classique mesures instables selon méthodes données souvent améliorations average méthodes performantes général concernant derniers points donnons précisément tableau meilleures performances observées important noter celles souvent obtenues notre méthode matrice largement écrêtée méthode noyau temps reuters average gaussien average gaussien smart average linéaire average linéaire gaussien gaussien meilleures valeurs chaque collection quand dense quand écrêtée diminution relative mémoire temps traitement discussion travaux futurs notre méthode repose expression formule termes similarités sinus écrêtage matrice similarité correspondante théorie améliore passage échelle permet également extension celle fonctions noyau pratique avons constater améliorations classification automatique documents surcroît pouvons également observer notre approche permet aboutir plusieurs reprises meilleurs résultats terme qualité pouvons expliquer phénomène points écrêtage permet réduire bruit notre méthode telle sorte fermeture transitive repose principe paramètre noyau gaussien article version française écourtée classification ascendante hiérarchique noyaux permet mieux tenir compte géométrie intrinsèque données toutefois possible mauvais choix paramètre seuillage conduise dégradation qualité point critique travaux futurs concerne choix paramètre également application diverses autres stratégies écrêtage comme restriction proches voisins parmi travaux cours poursuivons effort amélioration passage échelle classification automatique implémentant notre approche architecture distribuée utilisant outil apache spark références similarity based hierarchical clustering application collections advances intelligent analysis international sympo stockholm sweden october proceedings cristianini shawe taylor introduction support vector machines other kernel based learning methods cambridge university press lance williams general theory classificatory sorting strategies clustering systems computer journal müllner modern hierarchical agglomerative clustering algorithms arxiv preprint arxiv murtagh contreras algorithms hierarchical clustering overview wiley interdisciplinary reviews mining knowledge discovery wunsch survey clustering algorithms neural networks transactions summary lance williams formula framework unifies seven schemes agglomerative archical clustering paper establish expression formula using cosine similarities instead distances state conditions under which formula equivalent original interest approach twofold firstly naturally extend agglomerative hierarchical clustering techniques kernel functions secondly reasoning terms similarities allows design thresholding strategies proximity values thereby propose sparsify similarity matrix making these clustering techniques efficient apply approach clustering tasks results sparsify inner product matrix considerably decreases memory usage shortens running while assuring clustering quality