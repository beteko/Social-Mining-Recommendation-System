 Contrôle du risque multiple pour la sélection de règles d'association signi�catives Stéphane Lallich Elie Prudhomme Olivier Teytaud Laboratoire E R I C Université Lumière Lyon 2 5 avenue Pierre Mendès France 69676 BRON Cedex � France stephane lallich univ lyon2 fr Elie Prudhomme etu univ lyon2 fr Artelys 215 avenue Jean Jacques Rousseau 92136 Issy les Moulineaux olivier teytaud artelys com Résumé Les algorithmes d'extraction de règles d'association parcourent e�cacement le treillis des itemsets pour constituer une base de règles ad missibles à des seuils de support et de con�ance mais donnent une mul titude de règles peu exploitables Nous suggérons d'épurer de telles bases en éliminant les règles non statistiquement signi�catives La multitude de tests pratiqués conduit mécaniquement à multiplier les règles sélection nées à tort Après avoir présenté des procédures issues de la biostatistique qui contrôlent non pas le risque mais le nombre de fausses découvertes nous proposons BS_FD un algorithme original fondé sur le bootstrap qui sélectionne les règles signi�catives en contrôlant le nombre de fausses découvertes Des expérimentations montrent l'e�cacité de ces procédures Mots clefs Règle d'association qualité contrôle du risque multiple 1 Admissibilité intérêt et signi�cation statistique La recherche des règles d'association intéressantes est un problème classique de l'Extraction des Connaissances à partir des Données à la suite des travaux de [Agrawal et al 1993] dans le cadre des bases de données transactionnelles Dans une telle base un enregistrement est une transaction et les champs correspondent aux articles disponibles On note n le nombre de transactions et p le nombre d'articles L'acte d'achat item associé à chaque article est une variable booléenne Sur l'ensemble des transactions on a une matrice booléenne X de dimensions n et p La conjonction des actes d'achat itemset associés à un ensemble d'articles est vue comme une variable booléenne A partir de la matrice booléenne X on veut extraire des règles du type "si un client achète du pain et du fromage alors probablement il achète aussi du vin" Une règle d'association est une expression r du type A → B où l'antécédent A et le conséquent B sont des itemsets qui n'ont pas d'items communs On note na et nb les nombres de transactions qui réalisent respectivement les items de A et de B nab le nombre de celles qui réalisent à la fois A et B Les proportions correspondantes sont désignées par pa pb et pab Ce formalisme se généralise à toute base de données dont on a extrait une table booléenne cas attributs Contrôle du risque et sélection de règles Les algorithmes d'extraction usuels reposent sur le support et la con�ance en particulier Apriori l'algorithme fondateur [Agrawal et Srikant 1994] et les amélio rations qui en ont été proposées Le support d'une règle est la proportion de tran sactions qui réalisent à la fois A et B Supp A → B = pab = nabn alors que sa con�ance est la proportion de transactions qui réalisent B parmi celles qui réalisent A Conf A → B = pabpa = nab na = 1− nabna Les algorithmes d'extraction "support con�ance" parcourent le treillis des itemsets pour rechercher les itemsets fréquents dont le support dépasse un seuil minsupp avec une e�cacité liée à l'antimonotonie du treillis On en déduit les règles dont la con�ance dépasse le seuil minconf obtenant la base de règles admissibles aux seuils choisis De telles bases comportent un grand nombre de règles pas toujours intéressantes La sélection des règles intéressantes à partir d'une base de règles admissibles néces site d'évaluer celles ci à l'aide de mesures ayant les qualités requises compte tenu de la nature des règles d'association et des attentes de l'utilisateur Nous avons recensé de telles mesures et proposé des critères pour les évaluer [Lallich et Teytaud 2003] ainsi qu'une procédure d'aide à la décision pour les choisir [Lenca et al 2003] Pour chaque mesure choisie on �xe le seuil minimal à partir duquel une règle est sélectionnée ou l'on retient un nombre �xé des meilleures règles Le critère "prise en compte du nombre d'observations" oppose les mesures statis tiques et les mesures descriptives Il est logique a priori de souhaiter qu'une mesure soit statistique les résultats observés étant d'autant plus �ables que n est grand Cependant compte tenu de la taille des bases sur lesquelles on recherche des règles d'association de telles mesures perdent leur pouvoir discriminant ainsi l'indice d'implication [Lerman et al 1981] et l'intensité d'implication [Gras 1979] Des solutions très intéressantes ont été proposées l'indice probabiliste discriminant [Lerman et Azé 2003] qui centre et réduit les valeurs de l'indice d'implication sur une base de règles et l'intensité d'impli cation entropique [Gras et al 2001] qui a�ecte l'intensité d'implication d'un facteur correctif tenant compte de l'entropie des expériences B A et A B Mais il s'ensuit un mélange des notions de signi�cation et d'intérêt une perte d'intelligibilité de la mesure et sa loi est plus di�cile à étudier Paradoxalement le nombre de cas n est le même pour toutes les règles de la base ce qui milite pour d'abord tester la signi�cation statistique des règles au sens de sa voir si elles renforcent réellement la probabilité du conséquent On testera l'hypothèse d'indépendance de A et B notée H0 en direction d'une dépendance positive hypo thèse alternative unilatérale notée H1 pour ensuite utiliser des mesures descriptives intelligibles et discriminantes de l'intérêt des règles sur la base �ltrée Nous proposons ainsi une nouvelle démarche qui dissocie la signi�cation statistique de l'évaluation de l'intérêt Elle comporte trois étapes � étape 1 application d'un algorithme "support con�ance" à la base de cas pour constituer une base de règles admissibles aux seuils choisis � étape 2 �ltrage de la base de règles par le test d'indépendance de A et B pour chaque règle soit une multitude de tests on pourrait tester tout à la fois l'indé pendance et le dépassement signi�catif des seuils de support et de con�ance � étape 3 analyse des règles �gurant dans la base �ltrée par des mesures descrip tives véri�ant les critères retenus par l'utilisateur [Lenca et al 2003] RNTI 1 Lallich et al Dans cet article nous approfondissons l'étape 2 qui pose le problème de contrôler la multiplicité de tests pour éviter l'in�ation de "faux positifs" En e�et si chaque test est pratiqué au risque de 1e espèce α on engendre mécaniquement des "faux positifs" ici des règles sélectionnées alors qu'elles ne renforcent pas réellement la probabilité du conséquent Nous avons proposé des méthodes de contrôle du risque utilisant la théorie de l'apprentissage statistique et la VC dimension [Teytaud et Lallich 2001] ou le bootstrap [Lallich et Teytaud 2003] Dans la pratique ces méthodes sont peu puissantes ignorant des règles signi�catives La section 2 détaille le test de signi�cation appliqué à chaque règle Le contrôle des faux positifs et l'idée de contrôler le nombre de fausses découvertes plutôt que le risque sont exposés en section 3 Les procédures de contrôles récemment développées en biostatistique sont présentées en section 4 et une méthode de contrôle originale fondée sur le bootstrap est proposée en section 5 En�n nous appliquons ces méthodes pour �ltrer quelques bases de règles section 6 et nous concluons section 7 2 Test de signi�cation d'une règle Considérons une règle A → B et une mesure de qualité µ croissante avec nab à marges �xées On dira que la règle A → B est signi�cative au sens de la mesure µ si la valeur µobs = µ A → B remet en cause H0 dans le sens de H1 au risque de 1e espèce α Pour décider de la signi�cation on calcule la p value de µobs qui est la probabilité d'obtenir une valeur aussi grande que µobs sous H0 et on sélectionne la règle si l'on a p value < α Cette démarche impose de connaître la loi de µ A → B sous H0 Dans le cadre d'une modélisation à marges �xées on suppose que sous H0 les "1" de A et les "1" de B sont répartis au hasard indépendamment en respectant les marges On démontre que le nombre d'exemples suit une loi hypergéométrique Nab ≡ H n npa pb ≡ H n npb pa par convention les variables aléatoires sont en majuscules Pour une valeur observée nab à marges �xées p− value = Pr Nab ≥ nab = Pr H n npa pb ≥ nab On peut opérer l'approximation normale de cette loi hypergéométrique sous des conditions peu contraignantes à savoir nanb ≥ 5n et nanb ≥ 5n où Φ est la fonction de répartition de la loi normale centrée réduite N 0 1 En notant tab = nabnanb la valeur attendue de Nab sous H0 r le coe�cient de corrélation entre A et B il vient p− value = 1− Φ nab−tab√ na n−1 tabpb u 1− Φ nab−tab√ npapbpapb = 1− Φ r √ n Le coe�cient de corrélation r est donc une mesure privilégiée pour tester l'indépen dance H0 face à une dépendance positive H1 3 Risque et erreurs de 1e espèce Pour rechercher les règles A → B statistiquement signi�catives parmi les m règles de la base de règles on répète m fois le test d'indépendance entre A et B face à une dépendance positive On rencontre ainsi un problème classique en fouille des données RNTI 1 Contrôle du risque et sélection de règles Réalité \ Décision Acceptation Rejet Total H0 est vraie U V m0 H1 est vraie T S m1 Total W R m Tab 1 � Synthèse des résultats de m tests le contrôle des erreurs de 1e espèce ou faux positifs Si l'on teste m règles non signi� catives au risque α mécaniquement on sélectionne à tort mα règles soit 500 règles si m = 10000 et α = 0 05 La correction de Bonferroni qui consiste à pratiquer chaque test au risque αm pour que le risque de rejeter au moins une fois à tort H0 noté par la suite FWER soit égal à α n'est pas une bonne solution pour deux raisons � le FWER est en fait non contrôlé compris entre αmet α ne valant α que si toutes les règles sont indépendantes � le FWER est très conservateur en faveur de H0 ce qui augmente considérable ment le risque de 2e espèce ou risque de ne pas sélectionner une règle pertinente Pour régler ce problème il faut évaluer les erreurs de 1e espèce par une quantité moins sévère que le FWER et contrôler celle ci notamment lorsque les tests ne sont pas indépendants Les règles ne sont pas indépendantes en raison des items qu'elles partagent et des dépendances entre items Di�érentes solutions ont été développées le plus récemment en sélection de gènes s'exprimant di�éremment suivant l'étiquette de la biopsie On trouvera une remarquable synthèse de ces travaux dans [Ge et al 2003] L'idée fondamentale [Benjamini et Hochberg 1995] est de considérer non pas le risque d'erreur de la procédure de test lorsqu'elle est pratiquée une fois mais le nombre d'erreurs commises lorsque l'on réitère m fois la procédure A partir du tableau 1 où les quantités en majuscules sont des variables aléatoires observables celles en minuscules étant �xes mais inconnues en ce qui concerne m0 et m1 on peut dé�nir di�érents indicateurs des erreurs commises Nous présentons ici les deux plus connus le FWER Family wise error rate ou taux d'erreur en famille complète et le FDR False discovery rate ou taux de fausses découvertes FWER est la probabilité de rejeter au moins une fois à tort H0 FWER = Pr V > 0 Il a l'inconvénient d'être bien trop sévère pour une multiplicité de tests mais il nous a suggéré une variante �exible originale où l'on s'autorise V0 fausses découvertes Nous l'appelons User Adjusted Family Wise Error Rate UAFWER = Pr V > V0 pour le contrôle duquel nous proposons un algorithme fondé sur le bootstrap section 5 Pour remédier aux inconvénients du FWER diverses quantités reposant sur l'es pérance de V le nombre de fausses découvertes éventuellement normalisée ont été proposées La plus connue est le FDR [Benjamini et Hochberg 1995] la proportion attendue de règles sélectionnées à tort parmi les règles sélectionnées Lorsque R = 0 on pose VR = 0 soit FDR = E Q où Q vaut V R si R > 0 0 sinon Il s'ensuit FDR = E VR | R > 0 P R > 0 [Storey 2001] a proposé le pFDR une variante du FDR adaptée à l'estimation du taux d'erreur sachant que l'hypothèse nulle a été refusée au moins une fois pFDR = E VR | R > 0 RNTI 1 Lallich et al Ces quantités ont l'intérêt d'être moins sévères sur le résultat des m tests au prix de l'acceptation de quelques sélections à tort dont on contrôle la proportion ce qui augmente pour chaque test la probabilité pour qu'une règle pertinente soit sélectionnée puissance On a FDR ≤ FWER et FDR ≤ pFDR d'où FDR ≤ pFDR ≤ FWER pour m grand car Pr R > 0 tend vers 1 quand m croît Une fois choisie une dé�nition du risque multiple de 1e espèce se pose le problème de son contrôle Nous allons examiner successivement le cas du FWER et celui du FDR 4 Procédures de contrôle 4 1 Contrôle du FWER Correction de Bonferroni La correction de Bonferroni consiste à calculer des p values ajustées a�n de prendre en compte la multiplicité des tests Etant données la statistique de test Tr relative à la règle r r = 1 2 m et la p value correspondante pr on dé�nit la p value ajustée notée p̃r par p̃r = min {mpr 1} On sélectionne toutes les règles ayant une p value ajustée inférieure au risque α0 On montre FWER = 1− Pr ⋂m r=1 Pr > α0 m | H0 Sous condition d'indépendance des règles il vient FWER = 1− 1− α0m m ≈ α0 A défaut d'indépendance on a α0m ≤ FWER ≤ α0 Procédure Step down de Holm Les procédures pas à pas examinent les p values par ordre croissant et font évoluer le seuil tout au long de la procédure [Holm 1979] considère qu'une variable sélectionnée correspond à une situation où H0 est fausse ce qui amène à ne prendre en compte pour le nouveau seuil que les variables restant à examiner Les p values étant rangées dans l'ordre croissant où p k désigne la ke p value on refuse H0 tant que p k < α0m−k+1 On accepte H0 pour toutes les p values qui suivent la première acceptation Cette procédure facile à mettre en oeuvre donne de bons résultats lorsque le nombre de tests est faible la correction du seuil ayant alors de l'importance Elle reste mal adaptée à un grand nombre de tests Procédure minP de Westfall et Young [Westfall et Young 1993] ont proposé minP une procédure d'ajustement des p values qui contrôle le FWER mais oblige à calculer p̃r = Pr mink=1 2 m Pk ≤ pr | H0 Ce calcul doit être opéré par randomi sation des étiquettes de cas si les variables ici des règles ne sont pas indépendantes Bien adapté à la recherche des gènes s'exprimant di�éremment suivant l'étiquette de la biopsie ce procédé ne convient pas à la recherche des règles d'association 4 2 Contrôle du FDR Procédure Benjamini Liu On doit à [Benjamini et Liu 1999] une méthode sé quentielle pour contrôler le FDR en cas d'indépendance Les p values sont prises dans l'ordre croissant et l'on rejette l'hypothèse nulle tant que la p value examinée p i est inférieure à iα0m Cette procédure assure un FDR égal à m0 m α0 en cas d'indépendance Elle est compatible avec des données positivement dépendantes RNTI 1 Contrôle du risque et sélection de règles Procédure SAM de Storey Cette procédure [Storey 2001] repose sur l'estimation de m0 puis celle de E V et en�n celle du FDR Nécessitant une randomisation des étiquettes elle n'est pas adaptée au cas des règles pFDR Pour estimer pFDR = E VR | R > 0 qui est la proportion de fausses détec tions on utilise l'approximation [Storey 2001] ˆpFDR δ = π̂0 m δ {pi≤δ i=1 m} m est le nombre de variables à tester ici le nombre de règles δ dé�nit la zone de rejet les hypothèses correspondant aux p value inférieures ou égales à δ sont rejetées pi est la ieme plus grande p value π0 = m0m est la proportion d'hypothèses nulles ici π0 est estimé par f̂ 1 où f̂ est une cubic spline avec 3 degrés de liberté de π̂0 λ sur λ On a π̂0 λ = {pi≥λ i=1 m}m 1−λ et λ désigne la zone d'acceptation dont les valeurs sont comprises entre 0 et 0 95 Le pFDR est donc dé�ni par rapport à une zone de rejet qu'il faut choisir par avance Une fois le pFDR global calculé les variables sont contrôlées par une procédure step down grâce aux q values dé�nies pour chaque p value par q̂ pm = π̂0 pm et q̂ pi = min π̂0 m pi i q̂ pi+1 i = m− 1 1 La q value est au pFDR ce que la p value est à l'erreur de 1e espèce ou ce que la p value ajustée est au FWER Pour toute règle dont la p value a une q value inférieure au pFDR on rejette H0 et l'on sélectionne la règle 5 Contrôle du UAFWER par l'algorithme BS_FD 5 1 Notations � on note T l'ensemble des transactions n = Card T p le nombre d'items � R une base de règles d'association valides au sens de critères prédé�nis par exemple le support et la con�ance m = Card R R un sous ensemble de R rassemblant les règles valides signi�catives au sens du critère c � c r désigne l'évaluation de la règle r selon le critère c c′ r l'évaluation empirique de la règle r selon le critère c sur l'ensemble T � V nombre de faux positifs δ risque de la procédure de contrôle des faux positifs avec V0 nombre de faux positifs que l'on ne souhaite pas dépasser au risque δ 5 2 But On veut sélectionner parmi les règles r de la base R celles qui sont statistiquement signi�catives pour le critère c au sens où leur évaluation c r est signi�cativement plus élevée que c0 r valeur attendue sous H0 l'hypothèse d'indépendance de A et B Nous avons suggéré [Lallich et Teytaud 2003] di�érents algorithmes qui utilisent les outils de l'apprentissage statistique pour garantir que 100% des règles trouvées sont signi�catives au risque α donné ainsi l'algorithme BS fondé sur le bootstrap RNTI 1 Lallich et al Les expérimentations ont con�rmé que cette approche était trop prudente et par là même peu puissante Prenant en compte l'idée d'accepter de façon contrôlée un certain nombre de fausses découvertes à l'instar des travaux de Benjamini section 3 nous proposons BS_FD qui adapte l'algorithme BS au contrôle du nombre de faux positifs L'algorithme BS_FD sélectionne les règles candidates de telle sorte que l'on contrôle UAFWER = P V > V0 assurant que le nombre de règles sélectionnées à tort faux positif ne dépasse pas V0 au risque δ Plus précisément on garantit que P V > V0 converge vers δ à la limite d'un grand échantillon de transactions 5 3 Algorithme BS_FD Pour garantir c r > c0 r on peut se limiter sans perte de généralité à garantir c r > 0 en remplaçant c r par le critère translaté c r − c0 r On note par la suite l'opérateur "cardinal" associant à un ensemble son cardinal i e E est le cardinal de l'ensemble E 1 Dé�nir c′ r l'évaluation empirique de c r sur l'ensemble T de transactions 2 Un grand nombre de fois pour i = 1 2 l Tirer au sort avec remise une liste T ′ d'éléments de T de même cardinal que T Dé�nir c” r l'évaluation de c r sur la liste T ′ de transactions Calculer ε V0 i minimal tel que {c” r > c′ r + ε V0 i } ≤ V0 3 On obtient l valeurs ε V0 i Calculer ε δ le quantile 1− δ des ε V0 i 4 Garder dans R toutes les règles r de R telles que c′ r > ε δ Pour réaliser la dernière partie de l'étape 2 de l'algorithme on applique la procédure "calculerEpsilon i δ V0 " dé�nie ci dessous Procédure "calculerEpsilon i δ V0 " tableauEpsilon = 0 0 0 tableau de taille m pour r variant de 0 à m− 1 tableauEpsilon r = c” r − c′ r Calculer le V0 + 1 e plus grand élément de tableauEpsilon 5 4 Justi�cation de la méthode Les méthodes de bootstrap [Efron 1979] ont d'abord un aspect très intuitif de par leur idée d'approcher l'écart entre la loi empirique et la loi réelle par l'écart entre la loi bootstrappée et la loi empirique En outre elles ont de profondes justi�cations mathématiques [Giné et Zinn 1984] qui nécessitent une formalisation précise de la question posée Formellement l'objectif est que la fonction de répartition du nombre de règles telles que c r < 0 malgré c′ r > � ait pour valeur au moins 1 − δ en V0 On a {c r ≤ 0 et c′ r > �} majoré par {c′ r ≥ c r + �} Les théorèmes sur le bootstrap appliqué à une famille de fonctions véri�ant des hypothèses minimales [Van der Waart et Wellner 1996] nous permettent d'approcher cette quantité par {c′′ r ≥ c′ r + �} RNTI 1 Contrôle du risque et sélection de règles 5 5 Cas de plusieurs critères En pratique on s'intéresse souvent à plusieurs critères dans notre cas à la con�ance au support plus un critère de non indépendance L'extension de l'algorithme BS_FD notée BS_FD_mc se fait simplement en utilisant comme critère unique le min des di�érents critères Ainsi pour un travail sur 3 critères c1 c2 et c3 considère t on c r = min {c1 r c2 r c3 r } Utiliser BS_FD_mc sur c au risque δ fournit bien des règles garanties à la fois pour les critères c1 c2 et c3 au risque δ A�n d'optimiser le risque de seconde espèce on gagnera à travailler sur des trans formations di�érentiables au sens de Hadamard des ci qui rendent ces critères ho mogènes par exemple des p values ou des réductions en divisant l'écart empirique de chaque critère à sa valeur de référence par l'estimation de l'écart type issue des c′ r 5 6 Complexité de BS_FD La complexité de BS_FD est proportionnelle à l × m × n en considérant que le générateur de nombres au hasard fonctionne en temps constant En e�et la complexité de recherche du ke plus grand élément d'un tableau est proportionnelle à la taille du tableau La valeur de l doit être assez grande pour que l'imprécision liée à la �nitude de l ne nuise pas à la con�ance globale mais elle ne dépend ni de m ni de n L'algorithme est donc globalement linéaire en m× n avec une forte constante l liée au bootstrap 6 Expérimentations 6 1 Description des données Les méthodes de �ltrage présentées ici ont été appliquées à cinq bases de règles disponibles sur la plateforme HERBS [Vaillant et al 2003] Celles ci ont été extraites à l'aide d'Apriori suivant l'implémentation de [Borgelt et Kruse 2002] de bases de cas du site UCI ics uci edu �mlearn MLSummary html Contraceptive Method Choice CMC Flags Flags Wisconsin breast Cancer WBC Solar Flare I SFI et Solar Flare II SFII Nous avons calculé pour chaque méthode le taux de réduction de chaque base après retrait des règles non signi�catives 6 2 Caractéristiques et résultats Le tableau ci dessous est composé de deux sous tableaux qui récapitulent les carac téristiques de chaque base et le nombre de règles sélectionnées suivant chaque méthode � Contrôle à 5% on sélectionne les règles ayant une p value ≤ 5% � Bonferroni la correction est appliquée sur un seuil de 5% � Holm la procédure est appliquée avec un seuil de 5% � BS_FD r risque de 5% avec un V0 égal au résultat du pFDR appliqué au coe�cient de corrélation r comparé à 0 unilatéralement à droite RNTI 1 Lallich et al Caractéristiques CMC Flags WBC SF I SF II Nb cas 1473 194 699 323 1066 Nb règles 2878 3329 3095 5402 3596 Tx couverture 100% 100% 96 2% 100% 100% Tx recouvrement 259 1848 646 1828 6 2277 Seuil support 5% 50% 10% 20% 20% Seuil con�ance 60% 90% 70% 85% 85% Résultats CMC Flags WBC SF I SF II contrôle à 5% 1401 2181 3094 2544 2558 pFDR 916 3 1200 3 3095 0 900 5 1625 4 FDR Benj 913 0 003 1198 0 0027 899 0 006 1626 0 0022 BS_FD r 794 3 1074 3 3093 0 604 5 738 4 Holm 742 564 3094 432 1020 Bonferroni 731 539 3042 427 1006 Tab 2 � Filtrage de quelques bases de règles � FDR la méthode décrite en section 4 est utilisée avec un seuil égal à celui de la dernière q value sélectionnée par le pFDR indiqué entre parenthèses pour pou voir comparer grossièrement le pFDR et le FDR il est nécessaire de sélectionner les règles à un degré de contrôle voisin � pFDR la méthode est utilisée avec une zone de rejet de 0 1% Entre parenthèses est indiqué le nombre moyen de rejets à tort La zone de rejet est choisie telle que ce nombre soit le plus acceptable possible � BS_FD m c identique à BS_FD r avec trois critères r comparé à 0 le support et la con�ance comparés aux seuils utilisés en extraction A partir du tableau 2 on opère di�érentes constatations � Sur l'une des bases WBC le �ltrage est totalement ine�cace y compris la cor rection de Bonferroni La raison en est qu'une seule règle de la base de départ a une p value supérieure à 0 05 à savoir 0 184 une autre est à 0 023 les autres p values sont inférieures à 0 01 3036 d'entre elles valent 0 0000 � Pour les 4 autres bases le �ltrage par la simple répétition du test d'indépendance réduit notablement la base 51% 34% 53% 39% � Dans la base ainsi �ltrée il reste encore beaucoup de faux positifs que les di�é rentes méthodes de contrôle du risque permettent encore d'éliminer � Comme prévu la procédure de contrôle du risque la plus sévère est la correction de Bonferroni donnant sur les 4 bases des taux de réduction de 75% 81% 65% et 60% Par sa sévérité même cette procédure est peu puissante limitant certes les faux positifs mais au prix d'une augmentation des faux négatifs La procédure de Holm donne des résultats voisins son ine�cacité venant du très grand nombre de règles qui rend inutile la correction du seuil pas à pas � Les méthodes pFDR FDR Benjamini et notre méthode BS_FD donnent des ré sultats intermédiaires correspondant à ce que l'on pouvait attendre La méthode BS_FD apparaît comme la plus sévère des 3 plus particulièrement sur Solar RNTI 1 Contrôle du risque et sélection de règles Résultats CMC_app CMC_val croisement Taux sans contrôle 3391 2742 2742 0 81 contrôle à 5% 1835 1462 1238 0 68 pFDR 1347 3 996 4 938 0 70 BS_FD r 1302 3 955 4 903 0 69 Holm 1149 858 795 0 69 BS_FD m c 159 3 276 4 125 0 79 Tab 3 � Résultats de la base CMC en validation Flare II mais la raison est que le paramétrage de pFDR et FDR Benjamini as sure un nombre moyen de fausses découvertes égal à V0 alors que BS_FD assure que V0 n'est dépassé qu'avec le risque 0 05 ce qui est plus exigeant � La procédure de �ltrage est d'autant plus nécessaire qu'elle permet d'éliminer des règles qui seraient sélectionnées avec bon nombre de mesures de qualité Ainsi les règles logiques dont le conséquent est très fréquent e g Solar Flare II ont elles une valeur maximale pour toutes les mesures qui donnent une valeur �xe maximale aux règles logiques alors qu'elles n'ont aucune espèce d'intérêt et que leur p value est non signi�cative A l'inverse le calcul des p values ne préjuge pas du classement ultérieur des règles par des mesures descriptives favorisant les règles les plus intéressantes par exemple des mesures à la fois dissymétriques et qui avantagent les règles dont le conséquent est rare 6 3 Validation des résultats Alors que le découpage de la base de cas en base d'apprentissage et base de validation est courant en apprentissage supervisé il n'est jamais pratiqué dans le domaine des règles d'association La raison est sans doute que les règles d'association ressortent de l'apprentissage non supervisé et que leur extraction suivant le support et la con�ance est une tâche déterministe clairement dé�nie comme le souligne [Freitas 2000] Dans la mesure où nous avons introduit un test de signi�cation il est logique de distinguer base d'apprentissage et base de validation pour étudier la pertinence sur la base de test des règles sélectionnées sur la base d'apprentissage La base CMC a été séparée au hasard pur en 2 bases de même taille Sur CMC_app nous avons extrait une base de règles admissibles au sens de l'algorithme Apriori et nous avons �ltré ces règles à l'aide des di�érentes méthodes présentées Ensuite tableau 3 nous avons examiné comment se comportaient les règles admissibles issues de la base CMC_app col 1 lorsqu'elles étaient appliquées à la base CMC_val col 2 et nous avons calculé parmi les règles sélectionnées par chaque méthode sur CMC_app combien étaient encore sélectionnées par la même méthode sur CMC_val col 3 On constate ainsi que 80% des règles admissibles pour CMC_val le sont encore pour CMC_app alors que ce taux descend à 70% lorsque l'on �ltre la base de règle pour ne garder que les règles signi�catives quelle que soit la méthode En revanche l'application de BS_FD m c avec les trois critères support con�ance et r permet de revenir à ce taux de 80% au prix il est vrai d'une sérieuse diminution du nombre de règles RNTI 1 Lallich et al 7 Conclusion et travaux futurs Nous partons d'un principe simple il ne faut garder dans une base de règles que celles qui renforcent la conclusion sur le conséquent Pour ce faire nous proposons une stratégie de �ltrage des bases de règles qui conduit à pratiquer une multitude de tests unilatéraux à droite sur le coe�cient de corrélation entre A et B Cette stratégie est fondée sur le contrôle du nombre de règles sélectionnées à tort et non pas du risque assurant par là une plus grande puissance tout en permettant à l'utilisateur d'arbi trer entre "règles sélectionnées à tort" et "règles pertinentes non sélectionnées" Nous proposons un algorithme original BS_FD qui a l'avantage de contrôler directement le nombre de "faux positifs" et non pas leur moyenne en tenant compte de la dépendance des règles tout en permettant de tester plusieurs critères à la fois Les expérimenta tions montrent l'e�cacité de la stratégie proposée qui permet de réduire sensiblement la taille de la base facilitant le recours ultérieur à des mesures de qualité descriptives qui apportent un point de vue complémentaire sur la pertinence des règles Une extension de travail est prévue en fouille des données génomiques pour re chercher des règles discriminantes Les procédures de contrôle du risque intéressent l'ensemble des méthodes de fouille des données où l'on multiplie les tests Références [Agrawal et Srikant 1994] R Agrawal et R Srikant Fast algorithms for mining asso ciation rules Proc of the 20th VLDB Conference Santiago Chile 1994 [Agrawal et al 1993] R Agrawal T Imielinski et A Swami Mining associations between sets of items in large databases Proc of the ACM SIGMOD Conf Wa shington DC USA 1993 [Benjamini et Hochberg 1995] Y Benjamini Y Hochberg Controlling the false dis covery rate a practical and powerful approach to multiple testing J R Statisc Soc B 57 289 300 1995 [Benjamini et Liu 1999] Y Benjamini et W Liu A step down multiple hypothesis procedure that controls the false discovery rate under independance J Stat Planng Inf 82 163 170 1999 [Borgelt et Kruse 2002] C Borgelt et R Kruse Induction of association rules Apriori implementation Proc 15th Conf on Comp Stat Physika Verlag Germany 2002 [Efron 1979] B Efron Bootstrap methods Another look at the jacknkife Annals of statistics 7 1 26 1979 [Freitas 2000] A Freitas Understanding the crucial di�erence between classi�cation and discovery of association rules SIGKDD Explorations vol 2 1 65 69 2000 [Ge et al 2003] Y Ge S Dudoit et T P Speed Resampling based multiple testing for microarray data analysis Tech Rep 663 Univ of California Berkeley 2003 [Giné et Zinn 1984] E Giné et J Zinn Bootstrapping general empirical measures Annals of probability 18 851 869 1984 [Gras 1979] R Gras Contribution à l'étude expérimentale et à l'analyse de certaines acquisitions cognitives et de certains objectifs didactiques en mathematiques Thèse RNTI 1 Contrôle du risque et sélection de règles d'Etat Rennes 1 1979 [Gras et al 2001] R Gras P Kuntz R Couturier et F Guillet Une version entropique de l'intensité d'implication pour les corpus volumineux Revue ECA Extraction des Connaissances et Apprentissage Hermès 1 69 80 2001 [Holm 1979] S Holm A simple sequentially rejective multiple test procedure Scand J Statistic 6 65 70 1979 [Lallich et Teytaud 2003] S Lallich et O Teytaud Evaluation et validation de l'intérêt des règles d'association à paraître RNTI Revue des Nouvelles Technologies de l'Information Cépaduès Toulouse [Lenca et al 2003] P Lenca P Meyer P Picouet B Vaillant et S Lallich Cri tères d'évaluation des mesures de qualité des règles d'association RNTI Revue des Nouvelles Technologies de l'Information 1 123 134 Cépaduès Toulouse [Lerman et Azé 2003] I C Lerman et J Azé Une mesure contextuelle dicriminante de qualité des règles d'association EGC'03 RIA ECA 17 1 2 3 247 262 2002 [Lerman et al 1981] I C Lerman R Gras et H Rostam Elaboration et évalua tion d'un indice d'implication pour des données binaires I et II Mathématiques et Sciences Humaines 74 5 35 75 5 47 1981 [Storey 2001] J D Storey The positive false discovery rate and the q value écrit en 2001 à paraître Annals of Statistics 2003 [Teytaud et Lallich 2001] O Teytaud et S Lallich Bornes uniformes en extraction de règles d'association Actes Colloque CAp'01 Grenoble 133 148 2001 [Vaillant et al 2003] B Vaillant P Picouet et P Lenca An extensible platform for rule quality measure benchmarking R Bisdor� Ed Human Centered Processes 187 191 2003 [Van Der Vaart et Wellner 1996] A Van Der Vaart et J A Wellner Weak Convergence and Empirical Processes Springer Series in Statistics 1996 [Vapnik 1995] V N Vapnik The nature of statistical learning Springer 1995 [Westfall et Young 1993] P H Westfall et S S Young Resampling based multiple testing examples and methods for p values adjustment John Wiley Sons 1993 Summary Association rules extraction algorithms allow to e�ciently go through the item sets lattice in order to constitute a base of rules acceptable at prede�ned support and con�dence levels However they result in a multitude of rules hardly exploitable We suggest to re�ne such bases by eliminating non statistically signi�cant rules The mul titude of performed tests mechanically leads to a multiplication in false discoveries We �rst present procedures issued from biostatistic which aim at controlling not the risk but the number of false discoveries Then we propose BS_FD an original algorithm based on bootstrap which selects signi�cant rules while controlling the number of false discoveries By experimenting these procedures on di�erent rule bases we show their ability to re�ne rules bases Keywords Association rule quality multiple testing risk control RNTI 1