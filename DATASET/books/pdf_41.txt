Mean-Shift: Clustering évolutive et Beck Gaël Distribué, Hanane Azzag, Mustapha Lebbah, Tarn Duong, Christophe Cerin Laboratoire d'Informatique de Paris Nord (LIPN) Université Paris Nord - Paris 13, F-93430 Villetaneuse, France Email: {beck, prénom.nom}@lipn.univ-paris13.fr CV. CE papier de un de nous Présentons Mean-nouvel algorithme Maj les K uti- lisant-plus PROCHES Pour la voisins Montee du gradient (NNMS: les plus proches voisins de décalage moyenne). Le computationnel Coût Intensif de CE un long un dernier partition- en grappes temps nement utilisation de fils Limité sur des jeux de complexes where non Données ellipsoïdaux Bénéfique Serait. Ou, Une évolutive de l'Implémentation ne COMPENSE pas Algorithme l'augmentation du temps d'exécution en fonction de la taille du jeu de en raison de Données sa QUADRA Complexité tique. De Pallier AFIN, nous CE Problème le Avons introduit "Localité Hashage sensible" (LSH) Qui est Une approximation de la recherche des K-plus proches voisins AINSI répandrai Qu'une règle empirique le choix du K. La combinaison de bureaux au sein Améliorations du NNMS l'offre d'un treatment Opportunité pertinente aux Problématiques du regroupement aux Données massifs appliquée. 1 Introduction L'Objectif de la recherche non supervisee is d'un label Affecter à des points de non labélisés where le et l'emplacement Nombre des grappes Sont Inconnus. Nous nous Concentrés sur un Sommes de regroupement modale algorithme where le Nombre de grappes en defini is modes de terme de la fonction Locaux de density de genere les Qui Probabilité Données. Le plus CONNU des grappes de modales Algorithmes Est Le k-means. Comme CE sur basons is dernier la répartition de chiné normalien, il EST à Pressothérapie des contraint groupes ellipsoidaux Ce Qui can be verser des jeux inapproprié de complexes Données. Le Mean-shift is Une du k-généralisation des moyens en raison de sa capacity à des grappes de Calculer topologie Definis Comme les aléatoire d'attractions des bassins modes par la Generes Locaux de gradient de montée (Fukunaga et Hostetler, 1975). De les AFIN Calculer Chemins de la montée de gradient, les k, plus proches voisins voiture Sont appropriés s'adaptent à la NIT locale des Données topologie. La Version actuelle des K plus proches de Mean-Shift contains des Goulots d'étranglement par Poses Une grille de recherche multiple for the d'un choix Nombre de et par optimal Voisins le calcul des k exactes, plus PROCHES voisins. Nous proposons ici un nouvel algorithme Qui résout SCÉ Gouffres computationnels: (a) Une échelle normale Efficace du choix du Nombre des plus de proches voisins Qui Evite la recherche en grille, (b) le lieu hash sensible (LSH) Qui est juin la version approximée des k, plus proches voisins et (c) Une Implémentation MapReduce distribuée. - 415 - Mean-shift: clustering évolutives AVEC Les plus proches voisins approximés 2 Méthode 2.1 Le moyen de décalage Le moyen par Shift introduit (Fukunaga et Hostetler, 1975), genere point d'écoulement non x de dimension d une séquence de points de Qui suivent le chemin en montée de la densité de gradient en Utilisant la relation de récurrence: xj + 1 = 1 k Σ Xi∈k-nn (xj) Xi (1) where X1,. . . , Xn is un échantillon aléatoire Obtenu D'une fonction de densité f commune, les k voisins, plus proches de x k-nn are (x) = {Xi: ‖x-Xi‖ ≤ δ (k) (x)} tel Que δ (k) (x) is the Distance du k-ème, plus proche voisin. x0 = x. L'équation (1) au moyen fils donne Shift NOM because DU DEPLACEMENT des ITERATIONS de successif xj Vers la moyenne de Ses K plus PROCHES Pour la Voisins xj + Prochaine 1 itération. La convergence de la SEQUENCE {x0, x1,. . .} vers un mode Locale pour la Version à de l'équation noyau (1) par un etablie Été (Comaniciu et Meer, 2002) pour Une grande classe de sur des noyaux fenêtres fixées. Convergence This Validé Reste Quand la fenêtre fixe is Replaced par la distance la plus de des décroit Qui proches voisins Avec l'augmentation du Nombre d'Itérations. Le chemin de gradient v Montée de ers les modes Locaux produit par l'équation (1) forme les bases de l'Algorithme 2.1 (NNMS), ainsi que notre méthode des moyennes proches de décalage Voisins. Les plats principaux du NNMS les Échantillons de are X1 Données,. . . , Xn et les points de nous Candidats Que clusteriser x1 souhaitons,. . . , Xm (ILS PEUVENT Être X1,..., Xn Mais CE Ne EST PAS UN prérequis). Les paramêtres de règlage Sont les Suivants: - le Nombre de plus de proches de k - le seuil sous Lequel la convergence des Itérations is considérée Comme being sufisante ε1 - le Nombre maximal d'Itérations jmax - le seuil sous Lequel deux itérés finaux Sont considérés COMME being du same groupe Membres ε2 - la cardinalité MINIMALE des grappes Cmin Les formes les étiquettes Sorties des Sont grappes Punti {c Candidats (x1),. . . , C (xm)}. Il y a trois sous-routines à L'Algorithme 2.1. Les 1-6 correspondant à lignes la formation des Chemins de la montée de gradient Dans l'équation 1 Qui sont itérés Jusqu'a Ce que la distance de de la Dernière INFERIEURE à itération ε1 Soit ous le Qué d'un maximum Itérations Nombre jmax reached Soit . Les sorties de bureaux les itérés lignes Sont x * 1 finaux,. . . , X * m. Les 7-8 lignes la fusion des concernent itérés Dans le same finaux groupe when la distance de les séparantes is sous le seuil ε2, creant un CECI initial des x Regroupement * 1,. . . , X * m. Les lignes 9-13, plus il un facteur déterminant SI les petits amas Ont Une cardinalité supérieure à smin SINON sur fusionné les grappes Concernés with their voisin Le plus proche verser Produire c (x * 1). . . , C (x * m). La ligne 14 les étiquettes de assigne grappes aux bureaux x1 originalExcellent Données,. . . , Xm. 2.2 Choix du Nombre de plus de proches voisins suivant Une échelle de normalien Le règlage critique Paramètre for the mean shift is le Choix du Nombre de plus de k de proches. . Le de travaux pionniers (Loftsgaarden et Quesenberry, 1965), (Fukunaga et Hostetler, - 416 - G. Beck et al algorithme 1 NNMS - Plus de Mean-proches changement Avec les K plus proches exactes de {X1: Entrées,.. ., Xn}, {x1, xm}, k, ε1, ε2, jmax, smin Sorties:...... {c (x1), c (Xm)} / * Calcul du chemin de montee de gradient * / 1: pour `: = 1 à m faire 2: j: = 0; x`, 0: = x`; 3: x`, 1: = moyenne de k-nn de x`, 0; 4: tandis que ‖x`, j + 1, x`, j‖> ε1 ou j <jmax faire 5: j: = j + 1; x`, j + 1: = moyenne de k-nn de x`, j; 6: x * `: = x`, j; / * Création des grappes fusions nominale des itérés finaux * / 7: pour` 1, '2: = 1 à m faire 8: si ‖x * `1 - x *` 2‖ ≤ ε2 puis c (x * `1) = c (x *` 2); / * Fusion des petits amas * / 9: C *: = grappe with a cardinalité minimale; 10: alors que la carte (C *) <smin faire 11: C ': = plus proche grappe à C *; 12: pour x * `∈ C * au c (x *`): = c (C'); 13: C *: = grappe with a cardinalité minimale; 14: pour `: = 1 à m do c (x`) : = C (x * `); 1973) l'erreur établissent des Sélecteurs quadratique optimale et Globaux Locaux Pour Les estimateurs de density des plus de PROCHES voisins, Qué bureaux Sachant ne considèrent pas auteurs les Sélecteurs Basés sur les Données. Une grille de recherche sur les basée à minimiseur Données Cherche les indices de qualité de recherche non supervisee Silhouette Comme l'indice par Considéré (Wang et al., 2007). Notre proposition d'échelle normale verser le sélecteur is: kns = v0 [4 / (d + 4)] d / (d + 6) n6 / (d + 6) (2) where v0 = nD / 2Γ ((d + 2 ) / d)) is l'hyper-sphère de volume d'juin unitaire d-dimentionnel. La dérivation de l'équation (2) is Dans Donnée (Duong et al., 2016). Elle suit l'affirmation Que la Sélection des paramêtres de reglages Basés sur le gradient de densité sur la Que Plutôt density elle-same is, plus verser le quart de travail moyen adéquat (Chacón et Duong, 2013). La Complexité de KNS is O (1) Ce Qui contraste Avec le O (n) de la grille de Recherche pour selectionner le Nombre optimal de Plus proches voisins k Sachant Que le Nombre de recherches de facts possibles is usuellement régle Pour Etre proportional à n . 2.3 De plus proches approximés with the voisins Localité sensible Hashage La Tache calculatoire la, plus intensive Dans NNMS is le calcul des k, plus proches Plutot Que la Voisins Sélection du Nombre de plus de PROCHES voisins. En effet, le point d'écoulement each candidat, ACDE le requiert et le tri calcul de la distance de ‖xi - xj‖, i = 1,. . . , N, j = 1,. . . , M, Qui est O (mn log n). Dans les m where No CAS EST usuels du same de grandeur ordre Qué n, ACDE application fils empèche verser des jeux de Donnees Importants. Une approche de réduction de Complexité - 417 - moyenne équipe: Clustering évolutive Avec les plus de approximés proches voisins sur le TIENT prometteuse des approximés plus de calcul proches voisins sur les Qué Plutôt, plus PROCHES exactes voisins. Parmi CELLES to vary, le lieu pair introduit de hachage sensible, EST Une approche probabiliste basée sur la projection Une scalaire aléatoire de points de multivariés x L (x, w) (Datar et al., 2004) (Datar et al., 2004) = (ZTX + U) / w Où Z ~ N (0, Id) Variable is Une aléatoire normale d-variée et U ~ Unif (0, w) la variable aléatoire is juin prix uniforme sur [0, w), w> 0. Une table de hashage Dont les blocs Sont Basés sur des Valeurs Entières bL (Xi; w) c, i = 1,. . . , N is Alors Construite. En raison de la de Propriétés statistiques de distribution normale, les points de dans l'Espace proches de départ multidimensionnel à tomber auront tendance Dans les blocs scalaires et Mêmes les Points des Dans distants blocs tomberont Comme verifie Dans Différents (Slaney et Casey, 2008). D'facts de w IMPORTANTES impliqueront de blocs Avec Moins, plus de la précision Dans des characteristics de préservation Xi, TANDIS Que de facts de w petites entraineront en plus de blocs Avec Moins de précision. Nous paramétriser le Avons préféré LSH par le M blocs Nombre de de la Table de hashage. NOUS Avons fixe w = 1 sans perte de généralité Li ≡ L (Xi; 1). projections CES scalaires Sont bains triées Dans Leur ordre statistique w = (L (n) - L (1)) / M ou Ij = [L (1) + w (j - 1), L (1) + wj], j = 1,. . . , M. La Valeur de hashée x is l'indice de l'intervalle Dans Lequel L (x, 1) tombe H (x) = j1 {L (x, 1) ∈ Ij} (3) ou 1 {·} is la fonction d 'indication. De les AFIN approximés, plus Chercher PROCHES Voisins, le réservoir des Potentiels, plus proches voisins à la régle HNE du bloc Valeur la Valeur de Contenant hashage. Ce Élargi si is réservoir par concaténation Avec Nécessaire les blocs voisins. Les approximés k, plus proches voisins de x k, plus Sont Les proches voisins Contenus Dans le réservoir Réduit R (x): k-NN (x) = {Xi ∈ R (x): ‖x -Xi‖ ≤ δ (k) ( x)} where δ (k) (x) la distance de l'Est, plus des SEUIL à x proches voisins. L'erreur d'approximation DANS LES comprises proches voisins à x induite par la recherche Dans R (x) Plutôt Que Dans Toutes Les Données is probabilistiquement Contrôlée, voir (Slaney et Casey, 2008). L'Algorithme 2 NNLSH is Une approximation de la recherche des plus de proches voisins Avec le LSH et la fonction de hashage par l'équation Fourni (3). Les plats principaux les Échantillons de are X1 Données,. . . , Xn. Dans les lignes 2 à 6, point d'écoulement each candidat x`, les approximés k-plus proches de k-NN (x`) Sont plus de calculs à partir du réservoir R (x`). La proposition where le NNLSH au NNMS is intégrée was faité par (Cui et al., 2011), Ce Qui la Complexité à Réduit O ((mn / M) log (n / M)). Le M Nombre de blocs de l'Est un reglage essentiel Paramètre. Un fort intêret du Malgré le verser LSH (Har-Peled et al., 2012), il Ne existe pas de verser selectionner méthode optimale le de blocs Nombre, nous heuristiques des examinerons de Fait des performances Dans la section Prochaine. Implementer les plus les proches voisins approximatifs NNMS de Manière un Avec distribuée et N Processus maître Processus la Complexité Esclaves à Réduit O (mn / (MN) log (n / (MN))). C'est notre proposition, le DNNMS Dans l'3. Les plats principaux Algorithme et les sorties are For the Que Mêmes 1. Pour la Algorithme j-ème iteration, les Chemins de gradient de montée xj = [x1, j; . . . ; Xm, j] are Collectes Dans Une matrice m × d. Aux lignes 1 à 6, sur itère JUSQU'A Une convergence globale ‖xj + 1 - xj‖≤ 2 ≡ ‖x1, j + 1 - x1, j ‖,. . . , ‖Xm, j + 1 - xm, 2 ou j‖≤ Nombre maximal d'Jusqu'au Itérations jmax. CERTAINS Calculs when redondants Sont effectués des x` CERTAINS, j Converge Déjà en Ontario, Mais this forme calculation is Nécessaire POUR UNE - 418 - Beck G. et al. parallélisation efficace en MapReduce (Dean et Ghemawat, 2008). Le Paradigme MapReduce is, plus si les algorithmes Efficace en repensés série are, D'une passant sur each itération à Une candidat sur l'ensemble itération des Candidats simultanément. Les 7-14 lignes la fusion des décrivent reprise de l'regroupements 1 sans modification Algorithme majeure being Donné Que le MapReduce Ne est pas ici Requis. Algorithme 2 NNLSH - Approximés k, plus proches voisins Avec LSH Entrées: {X1,. . . , Xn}, {x1,. . . ,} Xm, k, M: {k Sorties-ppv (x1),. . . , K-NN (xm)} / * Création des tables de hashage with blocs M * / 1: pour i: = 1 à n faire Salut: H = (Xi); / * Recherche des approximés en plus proches voisins Dans les blocs adjacents * / 2: pour `: = 1 à m faire 3: R (x`): = {Xi: Salut = H (x`), i ∈ {1,. . . , N}} 4: tandis que la carte (R (x`)) <k do 5: R (x`): = R (x`) ∪ adjacent bloc; 6: k-NN (x`): = k-nn de R (x`) à x`; Algorithme 3 DNNMS - Plus DISTRIBUE moyenne Voisins-proches quart de travail, approximés k Avec en plus en proches voisins le LSH Utilisant Entrées: {X1,. . . , Xn}, {x1,. . . , Xm}, k, ε1, jmax, ε2, smin, M Sorties: {c (x1). . . , C (xm)} / * Calcul des chemins de montée de gradient * / 1: j: = 0; x0: = [x1,0; . . . ; Xm, 0]; 2: x1: = moyenne de k-ppv de {X1,. . . , Xn} pour x0 3: tandis que ‖xj + 1 - xj‖> ε1 ou j <jmax do 4: xj + 1: = moyenne des k-NN de {X1,. . . , Xn} pour xj; / * Cf Algorithme 2 * / 5: x *: = [x1, j; . . . ; Xm, j]; 6: aux lignes 7-14 Identique Dans l'Algorithme 2.1; 3 Influence 3.1 Résultats des expérimentaux de paramêtres parallélisation 3.1.1 Nombre de Sur observateur noeuds sur la may figure 1a Que notre Scala / Spark de la montée Implémentation de gradient Avec les k-plus proches voisins is évolutive, le temps d'exécution diminue Efficace - ment les Avec Nombre de noeuds esclaves. Des enquêtes après Lacunes de notre première mise en œuvre, nous observateur de l'étape Que Avons de LABELISATION ne etait pas extensible sur la Montré Comme la figure 1b. Cependant, en réutilisant les LSH de segmenteur les AFIN Comme Tâches - 419 - Mean-Shift: clustering évolutives AVEC Les plus proches voisins approximés algorithme 4 LABELISATION les k AVEC distribuée, plus PROCHES approximés Entrées voisins: {x * 1,. . . , X * m}, M2, 2, 3 Sorties: {c (x1),. . . , C (xm)} / * Création des tables de hashages Ë blocs M2 * / 1: pour i: = 1 à m faire Salut: = H (x * i); / * Labelisation DES DONNEES * / 2: pour `: = 1 à m faire 3: R (x *`): = {x * i: Salut = H (x * `), i ∈ {1,. . . , M}} 4: 1 pour `,` 2: = 1 à la carte (R (x * `)) do 5: si ‖x *` 1 - x * `2‖≤ ε2 puis c (x *` 1) : = c (x * `2); / * Calcul des barycentres * / C1, ..., Cj: = barycentre des grappes / * Fusion Des plus proches se regroupent * / 6: pour C1, C2: = 1 à j 7 faire: si ‖C1 - C2‖≤ 3 puis Fusion de C1 et C2 decrit Dans l'algorithme 4, nous observons Avons juin Scalabilité de la solution representee sur la figure 1c. La third à fusionner étape les cohérente petits groupes, plus their Avec se deroule proches voisins sur le noeud localement maître, les coordonnées Elle des Prend barycentres et la cardinalité du groupe qu'associé verser LABELISATION Une finale sortir Qui sera appliquée en parallèle. En si les ValeurS pratique 2 et 3, bien Sont choisies this is étape IMMEDIAT, un mauvais choix de bureaux facts may la génération d'agent d'entraînement un grand amas de et Nombre faire le temps d'exploser exécution. 5 10 15 20 25 500 1000 1500 Nombre d'esclaves Te m ps d 'ex a c © cu tio n (s) (a) les plus proches voisins de remontée gradient 2 3 4 5 6 7 8 20 40 60 80 100 120 Nombre d' Te m ps esclaves d » ex éc ut io n (s) (b) Première Implémentation de labelisation 2 = V 1 = V 2 2 2 3 4 5 6 7 8 6 8 10 12 Nombre d'esclaves Te m ps d 'ex éc ut io n (s) (c) Nouvelle labelisation 2 = V 1 = V 2 2 FIGUE. 1 - de Scalabilité Amélioration. (A) Montée de gradient KNN. (B) Première labellisation. (C) Nouvelle mise en œuvre. 3.1.2 Influence du blocs Nombre de LSH du Notre pneu nouvelle Implémentation du LSH Avantage la phase de de Durant de gradient montée pendant l'Mais aussi de LABELISATION étape. Un sérum clé Paramètre du Nombre de Celui blocs M1, M2 Dans le LSH. Si on laisse CE constante dernier sur la presented Comme la figure 2, sur CONSTATE Que la Complexité de la montée quadratique de gradient par les k plus de Persiste tout proches voisins Comme l'répandrai de LABELISATION étape. Cependant, sur observateur may sur la Figure 3 - 420 - Beck G. et al. 0,2 0,4 0,6 0,8 1 · 107 0 1000 2000 Taille du jeu de Donné Te m ps d » ex éc ut io n (s) (a) Montée de gradient with 1000 blocs 0 1 2 3 4 · 106 0 500 1000 Taille du jeu Te de Donné m ps d » ex éc ut io n (s) (b) labelisation with 13 blocs FIG. 2 - Influence de la taille du jeu de à Données de constante Nombre bloc. (A) Sur la Mon- de gradient tée. (B) Sur la LABELISATION. 0 2000 4000 6000 8000 0 2000 4000 6000 Nombre de blocs M1 Te m ps d » ex éc ut io n (s) (a) Montée de gradient Avec les K plus de proches 4 Esclaves 6 esclaves 8 esclaves 0 50 100 150 200 250 20 40 60 80 100 Nombre de blocs M2 Te m ps d » ex éc ut io n (s) (b) labelisation FIG. 3 - Influence du blocs Nb de verser un jeu banque de points taille fixe. (A) Sur la Mon- de gradient tée. (B) Sur la LABELISATION. le temps d'exécution decrease RAPIDEMENT, en fonction du blocs Nombre de, verser Puis ralentir un SEUIL Qui Atteindre du dépendra Nombre de d'entrées Données. Une observation Intéressante concerns l'augmentation du temps d'linéaire exécution un fixe lorsqu'on d'élément constant Nombre par bloc l'augmentation de Avec la taille du jeu de sur Données Comme illustré la figure 4. Ce de Përmet Résultat Version notre conforter du décalage en moyenne qu'algorithme Tant évolutive pleinement. 3.2 Application à la segmentation d'images La Résurgence d'Intérêt Dans l'algorithme Mean-Shift à l'application is dûe fils à la segmentation d'image (Comaniciu, 2003) where l'image Une is transformée Dans un espace colorimétrique each Dans Lequel correspondent à des grappes des régions segmentées de l'image originale. de L'allure 3 dimentionnel de L * u * v * de couleur (Pratt, 2001) is a choix commun. Sachant image Qu'une is un tableau de pixels bidimentionnel, disons QUE (x, y) Sont Les indices de lignes et de colonnes d'un pixel. Les informations spatiales et colorimétriques d'pixel d'un PEUVENT Fait Être concaténées en un vecteur 5-dimensionnel (x, y, L *, u *, v *) Dans la jointure des Domaines - 421 - Mean-shift: Clustering évolutive with Les plus proches voisins approximés 0 0,2 0,4 0,6 0,8 1 1,2 1,4 · 108 0 0,5 1 1,5 · 104 Taille du jeu de Données Te m ps d » ex éc ut io n (s) (a) Montée de gradient 1000 éléments blocs nominale de 2000 éléments par blocs 5000 éléments blocs pair 0 0,4 0,6 0,8 0,2 1 1,2 1,4 · 108 0 500 1000 Taille du jeu de Données Te m ps d » ex éc ut io n (s) (b) blocs nominale labelisation with 20k éléments Fig. 4 - Influence de la taille du jeu de Données. (A) Sur la montée de gradient. (B) Sur la LABELISATION. Spatio-. colorimétriques D'ACDE AFIN illustrer, l'image de l'# NOUS prenons 171 du jeu d'entrainement de Berkeley Segmentation coloré DataSet et référence 1. Dans la Fig.3 (ab) sur les pixels Retrouvé 481 × RGB originaux 321 de l'image JPEG et le diagramme de dispersion des n = 154401 jointure des coordonnées spatio-colorimétriques (X, Y, L *, u *, v *). (A) RGB (b) FIG distance spatiale. 5 - Représentations des couleurs de l'image. (a) l'image RVB de 481 × 321 pixels. (B) nuage de points Avec n = 154401 Transformé en espace (x, y, L *, u *, v *). Un algorithme de segmentation d'images basons sur le moyen-quart à noyau Été introduit Dans Avait [2] nous Qué Pour un adapted Avons utilisation NNMS AVEC. Les paramêtres de reglages verser NNMS et DNNMS KNS = 2463 are, ε1 = 0,005 Fois la marge maximale de la portée des Données, jmax = 100, ε2 = 10ε1, smin = 1544. Nous exécutons le DNNMS-M = M Avec 200, 500, 1000 blocs. Les temps d'exécution 20 respectively Sont, 13 et 10 minutes, en juin improvement significative à la nuit comparaison calculation for the NNMS Nécessaire sur un ordinateur de bureau standard. Quants choix à la Qualité de La segmentation d'images Dans la Fig.4 (a) Pour le NNMS with the PROCHES, plus exactes voisins. This l'image segmentée Une offre de la réduction considérable de l'image Complexité, tout en les centres des agent de détection Fleurs de détails Incluant CERTAINS granularité bien. Les contours, les bords des Pétales, 1. http CERTAINES: Ombres //www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds - 422 - G. Beck et al. Que Feuillages AINSI d'arrière Différents plan en la preuve are. En raison de la nature de la projection aléatoire du LSH verser approximer Les plus proches voisins in the DNNMS, les clusters compacts bureaux et Sont en plus Moins Diffus du NNMS Que Ceux where les projections LSH ne pas Sont utilisées. Pour le DNNMS-200 Dans la Fig.4 (b) avec les approximés, plus proches voisins M = 200 Avec blocs, CERTAINS détails Plus ou moins Sont en l'absence visibles du LSH. Pour le DNNMS-500 et le DNNMS-1000 Dans les Fig.4 (c-d), les répandrai les centres des fleurs jaunes reVu delimités are Pétales Moins, et il y a de considerables épanchements des Pétales sur le feuillage. Nous observons Que M = 200 blocs is un choix fortuit Comme c'est le also in the Fig.2 No CAS et de Plus amples Requises les enquêtes de verser un optimal choix général. (A) NNMS (1 nuit) (b) DNNMS-200 (20 min) (c) DNNMS-500 (13 min) (d) DNNMS-1000 (10 min) FIG. 6 - Segmentation d'images par l'intermédiaire colorée Les plus Mean-shift Voisins proches. (A) NNMS Avec le plus de moyen exact proche voisin de décalage en série. (B-d) DNNMS-M Avec les approximatifs, plus de proches l'Distribué Mean-shift with M = 200, 500 et 1000 blocs. Les temps d'exécutions are respectively une nuit, 20, 13 et 10 minutes. Le Berkeley Segmentation Dataset et Benchmark Une segmentation d'fournit l'image images de humaine their Ë des nageoires de Comparaisons. Dans la Fig.5 (a-b) se trouvent deux Détections de par l'bordures Faites # 1107 et Utilisateur # 1123. L'# 1107 se Utilisateur CONCENTRE sur la segmentation du plan de feuillage d'arrière et le contour de la forme des fleurs, tout en ignorant les détails des Pétales des fleurs. L'# 1123 Utilisateur Ã lui se Quant CONCENTRE sur la segmentation des Pétales Dans le plan de individuelles de premier ordre. Nous portons notre attention ici sur le NNMS et le DNNMS-200 (Fig. 5 (c-d)). Le DNNMS-500 et le DNNMS-1000 (Fig. 5 (e-f)) donne Une qualité insuffisante de la détection des bords. Le NNMS et le DNNMS-200 de segmenteur Sont en capables Une Seule un unique, with exécution de jeu paramêtres de règlage, le feuillage d'simultanément le plan et la arrière des Pétales de forme plan de premier ordre. Sur une zone Une segmentation automatique AINSI Combinant le de deux experts Résultat se focalisant sur Humains de l'image Différentes. - 423 - Mean-shift: Clustering évolutive with Les plus de proches les approximés (a) Utilisateur # 1107 (b) Utilisateur # 1123 (c) NNMS (1 nuit) (d) DNNMS-200 (20 min) (e) DNNMS- 500 (13 min) (f) DNNMS-1000 (10 min) FIG. 7 - Détection de l'image d'segmentée bordure. (A-b) Deux experts Humains: # 1107 et Utilisateur # 1123. (C) NNMS en les Avec série, plus PROCHES exactes voisins. (D-f) DNNMS-M Distribué Avec les approximatifs, plus proches voisins LSH with M = 200, 500 et 1000 blocs. 4 Conclusion Nous Avons several introduit à l'algorithme Améliorations des plus de changement de dire- proches. La première du EST Une heuristique OFFERTE Choix d'une valeur optimale du Nombre de plus de PROCHES voisins. La seconde is l'emploi d'approximation Une des plus de par le proches voisins localité hash sensible Pour la phase de gradient de montée de Mais aussi la phase de verser de LABELISATION. La third is juin Dans un ecosystem Implémentation Distribué. Nous Avons c Que démontré es le drastiquement Améliorations temps diminuent d'exécution tout en la qualité du Maintenant vis-à-vis du Regroupement des PROCHES, plus exactes voisins. CÉS application possible Améliorations de rendent l'dU Mean-Shift verser le regroupement au Appliqué Big Data Dans un futur proche. RESTENT CERTAINES Améliorations à faire sur cependant les paramêtres de reglages cruciaux, i.e. le plus de Nombre de proches voisins that the AINSI Nombre de blocs Dans le localité hash sensible les approximés plus de verser proches voisins is Requis. Références Chacón, J. E. et T. Duong (2013). estimation de la densité axée sur les données, les applications de cluster non paramétrique et la chasse bosse. Revue électronique de la statistique 7, 499-532. Comaniciu, D. (2003). Un algorithme de sélection de bande passante guidée par les données. IEEE Transactions sur le modèle d'analyse et de l'intelligence artificielle 25, 281-288. Comaniciu, D. et P. Meer (2002). mean shift: une approche solide vers l'analyse de l'espace caractéristique. IEEE Transactions on analyse et de renseignement machine modèle 24, 603-619. - 424 - Beck G. et al. Cui, Y., K. Cao, G. Zheng, et F. Zhang (2011). Un algorithme de changement de vitesse moyenne adaptatif basé sur lsh. Procedia ingénierie 23, 265-269. Datar, M., N. Immorlica, P. Indyk,, et V. S. Mirrokni (2004). des tables de hachage Localité sensibles en fonction des distributions stables p. Actes du XX e colloque annuel sur la géométrie computationnelle, 253-262. Dean, J. et S. Ghemawat (2008). MapReduce: traitement des données simplifiées sur les grands groupes. Communications de l'ACM 51, 107-113. Duong, T., G. B. H. Azzag, et M. Lebbah (2016). Les plus proches voisins des estimateurs de dérivés de densité, avec application à dire le regroupement de changement de vitesse. Motif Lettres de reconnaissance 80, 224-230. Fukunaga, K. et L. Hostetler (1973). Optimisation des estimations de densité k-plus proche voisin. IEEE Transactions on Théorie de l'information 19, 320-326. Fukunaga, K. et L. Hostetler (1975). L'estimation du gradient d'une fonction de densité, avec des applications dans la reconnaissance des formes. IEEE Transactions on Théorie de l'information 21, 32-40. Har-Peled, S., P. Indyk, et R. Motwani (2012). Approximer voisin le plus proche: vers la suppression de la malédiction de la dimensionnalité. Théorie de l'informatique 8, 321-350. Loftsgaarden, D. O. et C. P. Quesenberry (1965). Une estimation non paramétrique d'une fonction de densité multivariable. Annale de la statistique mathématique 36, 1049-1051. Pratt, W. K. (2001). Traitement de l'image numérique: PIKS intérieur. Slaney, M. et M. Casey (2008). hashing Localité sensible pour trouver les plus proches voisins. IEEE Signal Processing Magazine, 128-131. Wang, X., W. Qiu, et R. H. Zamar (2007). Une méthode de classification non paramétrique sur la base de rétrécissement local. Informatique Statistiques et analyse des données 52, 286-298. Résumé Nous présentons une mise en œuvre efficace de distribution voisin le plus proche regroupement de décalage moyenne (NNMS). La nature intensive de NNMS informatiquement a jusqu'à présent restreint son application aux ensembles de données complexes où un regroupement flexible avec des groupes non-ellipsoïdales serait bénéfique. Une mise en œuvre parallèle des NNMS série algorithme standard sur ses propres apporte des gains de performances insuffisantes pour que nous introduisons deux nouvelles améliorations algorithmiques: un choix échelle normale (NS) du nombre optimal de voisins les plus proches, et la localité hash sensible (LSH) pour se rapprocher le plus proche voisin recherches. La combinaison de ces améliorations dans un algorithme unique distribué DNNMS offre la possibilité d'une méthode efficace pour Big Data Clustering. - 425 -