articles assemblage.pdf estimation de densité sur les flux de données: une application de détection de changement de Alexis Bondu *, Benoît Grossin *, Marie-Luce Picard * * EDF R & D ICAME / SOAD, 1 avenue du Général de Gaulle, 92140 Clamart. firstname.name@edf.fr Résumé. Ces dernières années, la quantité de données à traiter a augmenté dans de nombreux domaines d'application tels que la surveillance du réseau, cliquez sur Web et la yse de données du capteur. réponses minières de flux de données au défi du traitement de données massives, ce paradigme permet de traiter des morceaux de données à la volée et de surmonter le stockage des données. La détection de changements dans une distribution de flux de données est une question importante. Cet article propose un nouveau schéma de détection de changement: i) le risation Résumés d'du flux de données d'entrée par un ensemble de micro-agrégats; ii) l'estimation de la distribution de flux de données exploitant des micro-amas; iii) l'estimation de la convergence entre la distribution di- et une répartition de référence de courant estimé; iv) l'étape de diagnostic grâce à la contribution de chaque variable prédictive à l'écart global entre les deux distributions. Notre schéma de détection de changement est appliqué et évalué sur les flux de données artificielles. 1 Introduction Ces dernières années, la quantité de données à traiter a augmenté dans de nombreux domaines d'application tels que les flux réseau, cliquez sur Web et d'analyse de données capteur. l'extraction de flux de données indique les algorithmes qui traitent tuples 1 à la volée: quand ils sont émis, sans les stocker. Le traitement des tuples doit être aussi rapide que possible, ce qui permet de gérer les flux de données à haut débit. Un problème important dans le traitement des flux de données détecte des changements dans la distribution sous-jacent qui est généré par tuples. La conception des systèmes de détection de changement qui sont d'ordre général, évolutive et statistiquement pertinente est un grand défi. Un changement dans la distribution sous-jacente peut être interprétée en différentes façons: i) le phénomène a servi ob- dérive naturellement en raison d'un changement dans un contexte caché (Widmer et Kubat, 1996) qui n'est pas donnée explicitement par facteur prédictif; ii) un changement anormal a lieu dans le système observé. Distinguer les deux cas est une question très difficile qui nécessite une expertise sur l'application. Dans cet article, nous supposons un expert, qui connaît bien le flux de données observées, peut se prononcer sur l'interprétation des changements détectés. Une vue d'ensemble des principales approches de détection de changement est donnée par A. Dries (Dries et Rückert, 2009): Détection des changements dans la distribution des tuples peut être considérée comme un test d'hypothèse statistique qui implique deux échantillons de tuples multidimensionnels. De tels problèmes sont étudiés dans la littérature statistique. Les tests de Wald-Wolfowitz et Smirnov a été généralisée 1. Le terme « tuple » fait référence à une donnée qui est émise à partir du flux d'entrée. RNTI-E-19- 229 - Estimation de densité sur flux de données à des ensembles de données multidimensionnelles dans (Friedman et Rafsky, 2006). Par la suite, des approches fondées sur des analyses plus proches voisins (Hall, 2002) ou la distance entre les estimations de densité (Anderson et al., 1994) ont été mis au point. Plus récemment, les statistiques fondées sur un écart moyen maximal pour les noyaux universels sont devenus populaires (Gretton et al., 2006). Une gamme de travaux statistiques sur la détection de changement brusque ont été fait (Basseville et Nikiforov, 1993, Desobry et Davy, 2003). Dans cet article, un nouveau schéma de détection de changement est proposé (voir la figure 1). Ce schéma est composé de quatre étapes successives. Dans la section 2 du flux de données d'entrée est résumée par un algorithme de micro-clusters. Cette première étape est nécessaire en raison du taux élevé de flux de données d'entrée, dans la pratique toutes les lignes ne peuvent pas être traitées en temps réel. L'algorithme « Denstream » (Feng Cao et al., 2006) a la capacité de résumer les zones denses de l'espace d'entrée et d'oublier les anciens tuples par une pondération en fonction du temps. Nous vous proposons un moyen simple de régler cet algorithme en termes de durées. Dans la section 3 une nouvelle variante de la fenêtre de Parzen est proposé et il est utilisé pour estimer la distribution sous-jacente du flux de données. Cet estimateur de densité exploite le résumé du flux de données d'entrée au lieu de tuples. Cette étape est répétée périodiquement avec un taux inférieur à celui de l'émission de tuples à partir du flux de données. L'article 4 montre comment la distance entre la répartition actuelle estimée et une distribution de référence peut être évaluée par la divergence de Kullback-Leibler. Cette mesure permet d'envoyer une alarme à l'expert lorsque les deux distributions sont significativement différentes. La dernière étape de notre schéma consiste en un diagnostic qui est donnée à l'expert pour l'aider à comprendre les causes de l'anomalie détectée. La contribution de chaque variable à la distance globale entre les deux distributions est évaluée en raison d'un nouveau critère proposé. Synopsis de données d'entrée courant basée sur l'estimation de détection de DiagnosticEvents synopsis Densité Densité à base Clustering (DenStream) Addapted Parzen fenêtre estimateur divergence de Kullback-Leibler contribution de chaque variable de l'étape de divergence 1 (voir la section 2) étape 2 (voir la section 3) l'étape 3 (voir la section 4) l'étape 4 (voir la section 4) FIG. 1 - schéma global pour la détection de changement dans la distribution de flux de données d'entrée. Enfin, notre approche est appliquée et évaluée sur deux flux de données artificielle dans la section 5. applications industrielles possibles de nos travaux de schéma et futurs sont discutés dans la section 6. 2 Summarization du flux de données d'entrée Dans le paradigme de flux de données, tuples émis ne peuvent pas être exhaustive stockées et traitées en raison du taux élevé de flux d'entrée. Cette section présente la récapitulation de flux de données d'entrée qui est une étape préliminaire dans le traitement de détection de changement. Notre approche exploite l'algorithme « Denstream » (. Feng Cao et al, 2006) pour résumer le flux de données: une pondération en fonction du temps est appliquée sur un ensemble de micro-clusters. RNTI-E-19 - 230 - A. Bondu et al 2.1 flux de données pondérées sont progressivement Tuples émis à partir du flux de données et sont pondérées en fonction de leur âge. Plus précisément, tuples (xi) dénotées par sont définis dans Rk et sont caractérisés par le vecteur {x1i, X2i ... xki}. Chaque tuple xi est émise à l'instant (Tcurrent - αi) avec αi indiquant l'âge de xi. A Tcurrent de l'heure chaque tuple est pondérée par wi = 2-λ.αi, où λ est un paramètre d'affaiblissement appartenant à l'intervalle] 0, + ∞]. Plus la valeur de λ, plus l'importance des données historiques par rapport aux données les plus récentes. Dans cet article, N représente le nombre total de tuples émis à Tcurrent. Que WN soit le poids total du flux de données à l'instant où Tcurrent de N tuples ont été émis. Nous avons WN = ΣN i = 1 2 -λ.αi et WN + 1 = ΣN i = 1 -λ 2 (αi + At) + 1, avec At correspondant au temps écoulé entre l'émission des deux dernières tuples . Sous l'hypothèse que le taux de flux de données est constante, le poids total est défini comme récursivement WN + 1 = 1 + 2 × λ.Δt WN. Cette série « géométrique arithmétique » converge 2 à limN → + ∞ WN = 11-2-λΔt. 2.2 Les micro-amas Un ensemble d'objectifs micro-amas de résumer le flux de données d'entrée des informations de maintien sur la distribution de densité. Ce synopsis est maintenu en mémoire à tout moment. Le « jème » de MCJ de cluster micro (cj, rj, wj) est défini par: i) un wj de poids qui correspond à la somme des poids des tuples appartenant au cluster (désignés par x1j, x2j ... xnjj) avec wj = Σnj i = 1 2 -λαij; ii) la cj centrale qui est un vecteur correspondant au barycentre pondéré d'exemples avec cj = 1 wj Σnj i = 1 wijxij; iii) le rj rayon qui est un vecteur qui correspond à l'écart type pondéré avec rj = 1wj √Σnj i = 1 .d wij (xij, cj) 2 avec la distance euclidienne notée d (). L ' « âge » de tuples augmente quand une nouvelle ligne est émise, comme αi ← αi + Dt, le temps écoulé entre deux tuple est considéré comme At constant. Le nouveau tuple est affecté au micro-cluster plus proche. L'ensemble de micro-agrégats est maintenu en raison d'un iter Procédé ative. Les poids des micro-amas sont maintenus par les étapes successives suivantes: 1. le vieillissement de tous les micro-agrégats tels que w (1) j ← wj 0,2-λΔt ∀j ∈ [1, C]; 2. L'augmentation du poids de la micro-cluster j * où le tuple émis est affecté tel que w (2) j * w ← (1) j + 1. Deux amas caractéristiques sont nécessaires pour maintenir le centre et le rayon des micro-amas (Zhang et al., 1996). Soit CF 1j [respectivement CF 2 j] est un vecteur k-dimensionnel de mémorisation, pour chaque variable, la somme pondérée des coordonnées [respectivement la somme des coordonnées au carré] des exemples appartenant à la micro-cluster « jème »: CF 1j = Σ nj i = 1 wijxij [respectivement CF 2 j = i = 1 Σnj wijx 2 ij]. cj et rj sont maintenus comme suit: cj = CF 1 et rj = wj √ | CF 2J | wj - (| CF 1j | wj) 2 2.3 L'approche Denstream Les poignées d'approche Denstream deux types de micro-cluster correspondant à différentes fonctions. L'ensemble de "potentiel micro-clusters", désignés par mcp, résume 2. significative in- Dans ce cas, la condition | 2-λΔt | ≤ 1 est toujours satisfaite. RNTI-E-19- 231 - Estimation de densité sur la formation de flux de données à partir du flux de données. Les micro-grappes dépassant un poids minimal sont considérés comme représentant des informations importantes. L'ensemble de « aberrantes-micro-clusters », désignés par AGC, consiste en un tampon de conservation des informations négligeable du flux de données. L'intuition est la suivante: une légère micro-cluster (sous le poids minimal) peut se développer si la densité distri- bution du flux de données est en train de changer. L'objectif est de conserver des informations insignifiantes au début détecter de nouvelles zones denses dans le flux de données. Deux contraintes sont appliquées sur les micro-groupes: i) les micro-amas dont le poids diminue en dessous d'un poids minimum (notée μ) sont supprimés; ii) un nouveau tuple est fusionné dans son micro-groupe le plus proche si son rayon de mise à jour r * j est inférieur à un écart-type maximum (notée). Ces contraintes assurent les micro-clusters représentent des zones denses de l'espace Rk où tuples sont apparus récemment. Une stratégie est mise en œuvre par la taille de l'algorithme « Denstream ». Cette stratégie vise à réglementer de l'espace mémoire nécessaire pour stocker les deux ensembles de micro-clusters et mco. Mcp Comment nous paramètres tune en termes de durées: Nous supposons notre système de détection de changement ap- proche est exploitée par un expert qui connaît bien les phénomènes intégrés dans le flux de données. L'algorithme « Denstream » implique plusieurs paramètres (λ, u et) qui peuvent être difficiles à régler par l'expert. Dans ce paragraphe, l'examen se fait sur la façon de régler les paramètres d'une manière compréhensible. L'expert connaît la durée de validité de tuples émis et il est capable de mettre en place une période de demi-vie 3 (notée ΔHalfLivet). Le paramètre d'affaiblissement peut être déterminé dans un second temps tel que λ = - log2 (12) / Δ t HalfLive. Nous démontrons que le paramètre μ qui représente le poids minimal de grappes est délimitée comme suit: 2-λΔ ClusMin t 1 - 2-λΔt> μ ≥ 1 au 2 janvier-λΔClusMaxt Soit ΔClusMaxt soit le laps de temps au-delà de l'arrivée d'un nouveau tuple dans un p-micro-cluster qui n'est pas reasonable de garder le statut « potentiel ». Pour tout p-micro-cluster, nous avons wj .2-λΔ ClusMax t + 1 <μ et wj <μ. A la fin, on obtient μ> 1 / (1 - 2 λΔ ClusMax t). Que ΔClusMint soit la durée minimum de temps qui doit être maintenu un non mis à jour p-micro-cluster dans le synopsis. Pour tout p-micro-cluster, nous avons wj .2-λΔ ClusMin t> μ. Le poids d'un p-micro-cluster est inférieure ou égale au poids total du flux de données, donc nous avons ClusMin W.2-t λΔ> μ. A la fin on obtient (2-λΔ ClusMin t) / (1 - 2-λΔt)> μ. Dans cet article, nous adoptons le même choix que dans (Feng Cao et al., 2006) où les auteurs définissent la période de taille T comme le minimum de ΔClusMaxt. Nous considérons que T et Δ t HalfLive sont donnés par l'expert. Dans ces conditions μ peut être exprimée comme suit: μ = 1 1 - 2 - T ΔHalfLivet Un nouveau micro-cluster est créé lorsque l'écart-type maximal est re dans le plus proche faisait mal micro-cluster d'un tuple émis. Intuitivement, la valeur des influences du nombre de micro-agrégats potentiels qui sont maintenues dans la mémoire. L'accord est une question de l'écart parce que l'ensemble standard du flux de données d'entrée est inconnue dans le cas général. Dans cet article, nous supposons que l'écart-type global à être connu par l'expert et est ajustée en proportion de l'écart-type global. 3. La valeur de wi est périodiquement divisé par 2. RNTI-E-19 - 232 - A. Bondu et al Notations: • l'ensemble des MCP-micro-cluster potentiel; • mco l'ensemble des micro-cluster-aberrant; • u le poids minimum d'un micro-cluster potentiel; • l'écart-type maximum de un micro-cluster potentiel; • T la période d'élagage. Répétez Obtenez le point suivant xi + 1 à partir du flux de données. / * Procédure de fusion * / Essayez de fusion xi + 1 à son plus proche p-micro-cluster, noté mcp (cp, rp, wp). Soit r * p le nouveau rayon de mcp. Si r * p ≤ puis fusionner xi + 1 dans mcp, et mettre à jour cp, rp, wp. Essayez de fusion autre xi + 1 à son plus proche o-micro-cluster, noté mco (co, ro, wo). Soit r * o être le nouveau rayon de mco. Si r * o ≤ puis fusionner xi + 1 dans mco, et mise à jour co, ro, wo. Si wo> μ puis retirez mco du tampon des valeurs aberrantes et de créer un nouveau p-Microcluster par mco. end Si d'autre Créer une nouvelle o-micro-cluster par xi + 1 et l'insérer dans la mémoire tampon des valeurs aberrantes. end if end Si / * Procédure d'élagage * / Si la taille Période T est écoulée alors pour chaque p-micro-cluster mcp (cp, rp, wp) faire Si wp <μ puis Supprimer fin mcp Si end Pour Pour chaque o-micro -cluster mco (co, ro, wo) faire Si wo <2-λ (à + T) -1 2 λT -1 puis Supprimer fin de mco Si end pour fin Si jusqu'à ce que le flux de données existe algorithme 1: synthèse de flux de données par approche Denstream RNTI-E-19- 233 - estimation de densité sur les données flux 3 estimation de densité exploitant la synopsis Cette section montre comment le synopsis du flux de données d'entrée est exploitée pour estimer la densité des données. On modifie l'estimateur de densité de fenêtre de Parzen (Parzen, 1962) pour exploiter les micro-agrégats à la place de tuples. Le paragraphe 3.1 présente la fenêtre de Parzen classique dans le cas du noyau gaussien, au paragraphe 3.2 cet estimateur de densité est adaptée pour les micro-amas. 3.1 Parzen de Windows Parmi les large gamme de modèles capables d'estimer la densité de données à partir d'un ensemble de tuples, fenêtre Parzen muni d'un noyau gaussienne (Shawe-Taylor et Cristianini, 2004) a l'avantage de nécessiter quelques paramètres. L'équation 1 correspond à la « sortie » de ce modèle prédictif qui est une estimation de la probabilité d'observer le tuple x ∈ Rk. K (x - xi) est une fonction noyau évaluer la proximité entre les tuples x et xi, cette fonction est additionnée sur tous les tuples émis. P (x) = 1 N NΣ i = 1 K (x - xi) (1) Dans la pratique, la fonction de noyau doit être spécifié. L'équation 2 correspond à la « sortie » d'une fenêtre de Parzen pourvu d'un noyau gaussien 4. Dans ce cas, la fenêtre de Parzen implique un seul paramètre qui est σ: écart type du noyau Gaussien. K (x - xi) = 1 (σ √ 2π) k EXP- d (x, xi) 22.σ2 (2) La figure 2 illustre l'estimation de P (x) par un estimateur de fenêtres de Parzen. gaussiennes sont positionnés sur chaque tuple, à côté, ils sont additionnés et normalisés. Dans ce cas, chaque tuple contribue à l'estimation de P (x). x P (x) estimation densité de la fenêtre de Parzen Contribution de chaque exemple de formation des exemples de formation figure. 2 - Estimation de la distribution des flux de données en raison d'une fenêtre de Parzen. 4. Nous considérons l'écart-type du noyau gaussienne est constante sur toutes les dimensions de l'espace d'entrée Rk. RNTI-E-19 - 234 - A. Bondu et al 3.2 Notre modifiées fenêtres Parzen Dans ce paragraphe, l'estimateur de densité de fenêtre de Parzen est adaptée pour exploiter l'ensemble des micro-amas po- tentiel au lieu de tuples. La distribution de P (x) est approchée par l'équation 3: P * (x) = 1 CW CΣ j = 1 2π ωj√ (δ2 + R2J) k EXP- d (x, cj) 2 2 (δ2 + R2J) (3) • W représente le poids total du flux de données; • C représente le nombre de micro-agrégats potentiels résumant le flux de données; • ωj représente le poids du micro groupe jème; • cj représente le barycentre des points pondérés appartenant au micro groupe jème; • rj désigne l'écart-type des points pondérés appartenant à la jème micro grappe; • δ représente un paramètre planéité joue le même rôle que σ dans l'équation 2. Chaque tuple observée est censé être le plus probable d'un ensemble de tuples qui inobservée est normalement distribué avec un écart-type égal à δ. Dans cette hypothèse, la loi de la variance totale donne la variance du micro-cluster potentiel « jeme » comme la somme de la variance intra-δ2 et la R2J entre-variance. Dans l'équation 3 noyaux gaussiens sont positionnés sur le centre de chaque micro-cluster potentiel. Ensuite gaussiennes sont additionnés et normalisés en ce qui concerne le nombre de micro-clusters potentiels et le poids total du flux de données. L'écart-type ot exemples de formation x P (x) la position du centre et la valeur des exemples de formation de poids estimation de la densité de la fenêtre de Parzen Micro Cluster FIG. 3 - Influence du poids des micro-amas sur l'estimation de la densité La figure 3 illustre l'estimation de P (x) par nos fenêtres Parzen modifiés. Sur cette figure, l'ensemble des tuples est divisé en deux micro-agrégats potentiels dont le rayon est symbolisé par une ligne pleine horizontale et les poids sont symbolisés par une ligne pointillée verticale. D'une part, l'RNTI-E-19- 235 - Estimation de densité sur une estimation de flux de données de P (x) est moins précis que sur la figure 2 en raison de la perte de chaque emplacement de tuple. D'autre part, cette estimation prend en compte le poids de chaque groupe. L'estimation de la distribution de P (x) change au fil du temps en raison du vieillissement des micro-amas. Si aucun changement se produit dans la distribution sous-jacente, les tuples dont une diminution du poids sont remplacés par de nouveaux émis: dans ce cas, l'estimation de P (x) ne changeront pas. Dans le cas contraire, les tuples non remplacés dans un micro-cluster engendrer une diminution de son poids: alors un changement de l'estimation de P (x) est observée. 4 Changement de détection et de diagnostic On suppose qu'une anomalie qui se produit dans les résultats de flux de données d'entrée dans un changement de dis- tribution de P (x). Une distribution de référence est mis en place après une période d'apprentissage sans anomalies à détecter. L'expert examine le flux de données d'entrée pour assurer aucune anomalie n'a eu lieu au cours de cette période. Ensuite, l'estimation actuelle de la distribution de P (x) est comparée à la distribution de référence en raison de la divergence de Kullback-Leibler (Hershey et Olsen, 2007) montre l'équation 4. La divergence Kullback-Leibler a des propriétés statistiques intéressantes, en parti- paramètres LAR de constatation d'un modèle statistique maximisant la probabilité est analogue à la recherche de paramètres réduisant au minimum la divergence (Eguchi et Copas, 2006). La divergence de Kullback-Leibler généralise des tests statistiques classiques comme le t-test et le χ2: i) le test t est équivalente à la divergence de Kullback-Leigler entre deux distributions normales; ii) la fonction χ2 est le premier terme dans le développement de Taylor de la divergence de Kullback-Leigler. Dans notre schéma de détection de changement, une alarme est envoyée à l'expert lorsque la divergence entre les distributions et Préf P * atteint un seuil fixe. KL <Pref (x) ‖P * (x)> = - ∫ Rk Pref (x) log Pref (x) * P (x) dx (4) Le diagnostic est requis par l'expert afin de donner une réponse appropriée à l'alarme. Les objectifs de la phase de diagnostic à l'évaluation de la contribution de chaque variable de l'écart entre Pref et P *. Ainsi, l'expert est informé que facteur prédictif sont impliqués dans le changement détecté. La contribution des variables est évaluée par l'équation 5. Soit KLiminus soit la divergence Kullback-Leigler évalué dans un (k - 1) après l'exclusion espace de dimension de la variable « ième ». Lorsque la contribution de la variable « LTH » est évaluée, KLlminus est comparée à la somme des KLminus sur tout les variables, la contribution est normalisée. Contrib (l) = (rk i = 1 i KL moins) - KLlminusΣk i = 1 i KL moins (5) la contribution de chacun des objectifs variables à aider l'expert de se prononcer sur l'interprétation de la variation détectée. Dans la pratique, l'expert peut être autorisé à mettre à jour la distribution de référence avec la distribution actuelle si le changement détecté n'est pas une anomalie. Cette mise à jour constitue un moyen possible de prendre en compte la dérive naturelle du phénomène observé. RNTI-E-19 - 236 - A. Bondu et al 5 Expériences Dans cette section, notre schéma de détection de changement est appliqué sur deux flux de données d'artificiel. L'objectif est d'évaluer la capacité de notre schéma pour détecter deux types de changements différents: i) un changement dans la moyenne d'une distribution normale; i) un changement de l'écart type d'une distribution normale. 5.1 Protocole expérimental Les deux flux de données artificielle considérés partagent la même structure temporelle. Chaque seconde, un tuple est tiré d'une distribution sous-jacente qui change au fil du temps. La figure 4 montre comment la distribution sous-jacente évolue. Les 2000 premiers tuples sont émis à partir de la « distribution initiale » qui représente l'opération habituelle. En ce moment, la distribution de référence est mis en place: notre schéma de détection de changement commence. Entre 4000 et 6000 secondes, la distribution sous de couchage se déplace progressivement à partir de la « initial » pour la distribution « modifié ». Ensuite, 2000 tuples sont émis de la distribution « modifiée ». Entre 8000 et 10000 secondes, la distribution sous-jacente progressivement de retour à son état « initial ». Enfin, 2000 tuples sont émis par la distribution « initiale ». Répartition des données flux 0 4000 8000 Temps (s) 12000 Distribution initiale de référence mis en place la distribution modifiée Fig. 4 - Structure temporelle des deux flux de données artificielles. Dans nos expériences tuples sont définies dans R2. Les distributions « initial » et « modifié » sont définis dans le Tableau 1 pour les deux flux de données artificielles. Ces distributions normales sont désignées par N (m, v), où m est un vecteur à deux dimensions correspondant à la moyenne et v est la matrice de covariance. distribution initiale distribution modifiée flux de données 1: modification moyenne N « 0 0, 1 0 0 1« N « 4 8, 1 0 0 1« flux de données 2: changement de l'écart type N « 0 0, 1 0 0 1« N « 0 0 4 0 0 9« TAB. 1 - Définition des distributions « initial » et « modifiés » pour les deux flux de données artificielles. Notre schéma de détection de changement implique plusieurs paramètres qui doivent être fixés avant les expériences. L'algorithme « Denstream » qui résument le flux de données d'entrée (voir la section 2.3) est paramétrée par = 0,1, ΔHalfLivet = 300s et T = 1000s. Le paramètre de planéité de notre estimateur de densité (voir section 3.2) est fixé par δ = 1. RNTI-E-19- 237 - Estimation de densité sur flux de données 5.2 Résultats La figure 5 présente les résultats de nos expériences, le graphique gauche [respectivement le droit tableau] indique la détection d'un changement dans la moyenne [respectivement de l'écart type] de la distribution sous-jacente (décrite dans le tableau 1). Sur les deux cartes, les correspond à axe horizontal au temps et commence quand est Répartissez la distribution « de référence » vers le haut (à t = 2000). Les correspond à axe vertical à l'écart entre la « référence » et les distributions « courant ». La contribution de chaque variable à la divergence est également symbolisée par des couleurs. FIGUE. 5 - détection des changements dans la distribution des deux flux de données artificiels. Le premier flux de données artificielle implique un changement dans la moyenne d'une distribution normale (graphique de gauche sur la figure 5). Dans ce cas, le changement qui se produit lorsque t ∈ [4000, 6000] est détecté tôt, en effet l'augmentation de la divergence significative une fois que t = 4500. Entre 6000 et 8000 secondes, l'augmentation de la divergence à son maximum (KL = 25) et les contributions bien estimer le mouvement de la distribution sous-jacente des deux dimensions. Le retour à la distribution initiale sous-jacente (t ∈ [8000, 10000]) est dete CTED relativement tard. La divergence maintient des valeurs élevées jusqu'à ce que t = 9000 et diminue fortement après. Ce comportement peut être expliqué soit le laps de temps nécessaire pour supprimer les micro-clusters potentiels inutiles dans le résumé du flux de données d'entrée. Le second flux de données artificiel implique un changement dans l'écart type d'une distribution normale (graphique de droite sur la figure 5). Dans ce cas, la détection de changement est moins nette que pré viously: i) la divergence varie fortement au fil du temps et ne se stabilisent pas; ii) la divergence atteint une faible valeur maximale (KL = 0,6). Cependant, la première modification de la dis- tribution sous-jacent est détecté: la divergence augmente à partir de t = 4800 à t = 6000. Entre 6000 et 8000 secondes, la divergence atteint sa valeur maximale qui est compatible avec la structure du flux de données d'entrée. Au cours de cette période, la contribution de la deuxième variable a tendance à être plus importante que la première variable. Enfin, le retour à la distribution initiale sous-jacente est détectée dans le temps. Ces expériences montrent l'intérêt de notre approche pour la détection de la dérive progressive de la distribution sous-jacente. D'autres tests concluants ont été effectués sur les changements brusques. Dans ce cas, un temps de latence très court est observée en raison de la durée de temps nécessaire pour créer de nouveaux micro-clusters potentiels. On remarque le réglage du paramètre d'affaiblissement λ est sensible et soulève le dilemme entre la réduction de la latence et d'assurer la détection de signification statistique de l'estimation de la distribution. Réglage du paramètre λ pourrait être moins sensible dans la pratique, si le rythme des changements est connu à l'avance par l'expert. RNTI-E-19 - 238 - A. Bondu et al 6 Conclusion et perspectives Cet article propose un nouveau schéma de détection de changement dans la distribution sous-jacente d'un flux de données. Notre approche se compose de quatre étapes successives. Tout d'abord, le flux de données d'entrée est résumée par un ensemble de micro-agrégats du fait de l'algorithme « Denstream » (Feng Cao et al., 2006), ainsi des flux de données à vitesse élevée peut être traitée. L'algorithme « Denstream » a la capacité de résumer les zones denses de l'espace d'entrée et d'oublier les anciens tuples par une pondération en fonction de temps. Nous vous proposons un moyen simple de régler les paramètres de cet algorithme en termes de durées. La deuxième étape consiste en une estimation de la distribution sous-jacente exploitant le résumé du flux de données: une nouvelle variante de l'estimateur de fenêtre de Parzen (Parzen, 1962) est proposé. Ensuite, la dérive de la distribution actuelle estimée est évaluée par rapport à une distribution de référence: la divergence Kullback-Leibler est exploitée (Hershey et Olsen, 2007). A la fin, un diagnostic est donné par un nouveau critère qui évalue la contribution de chaque variable de la distance totale entre les deux distributions. Dans la pratique, cette dernière étape pourrait être utile de comprendre les causes d'une anomalie détectée et d'y répondre d'une manière appropriée. Depuis notre schéma de détection de changement implique un estimateur de densité, la probabilité de chaque tuple émise peut être estimée par la fenêtre Parzen actuelle. Cette information devrait être exploitée pour détecter rapidement les changements brusques dans la distribution sous-jacente, en supposant qu'un changement provoque l'émission d'une séquence improbable de tuples. Dans ce cas, la principale difficulté est de gérer la dépendance temporelle des tuples émis, les travaux futurs étudieront ce point. Un autre aspect sur lequel nous travaillons est la quantification théorique des informations perdues en utilisant des micro-clusters au lieu de tuples, lorsque la distribution du flux de données est estimée. Les poignées algorithme « Denstream » la variance de chaque micro-cluster comme une seule valeur scalaire, cela représente une perte d'information importante. Par exemple, la matrice de covariance de tuples émis pourrait être maintenue en ligne pour chaque micro-cluster. Dans les œuvres FUTUR, nous allons étudier le maintien en ligne de la matrice de covariance et des moments statistiques plus élevés, et nous wil l utiliser ces nouveaux éléments d'information pour estimer plus précisément la répartition des tuples. Notre schéma de détection de changement a été favorablement évaluée sur deux flux de données artificielle. Dans nos travaux futurs, d'autres expériences vont évaluer l'influence de plus en plus la dimension de l'espace d'entrée sur la capacité de notre schéma pour détecter les changements. Enfin, notre schéma sera appliqué sur les flux de données réelles. En particulier, nous visons à améliorer la maintenance préventive dans les centrales électriques grâce à la détection d'événements inhabituels. De manière plus générale, notre schéma de détection de changement pourrait être exploitée dans de nombreux domaines d'applications. Par exemple, la NASA a lancé un vaste programme de recherche en gestion de la santé intégrée des véhicules dont le but est de détecter automatiquement, diagnostiquer, prédire et atténuer les effets indésirables pendant le vol d'un aéronef (Srivastava, 2009). La détection précoce des anomalies sur les flux de données de capteur représente un réel intérêt pour la communauté scientifique. Références Anderson, N. H., P. Hall et D. M. Titterington (1994). Les statistiques de test sur deux échantillons pour écarts entre mesurables ING deux fonctions de densité de probabilité à plusieurs variables en utilisant des estimations de densité à base de noyau. Journal of multivariée Analyse 50 (1), 41-54. Basseville, M. et I. V. Nikiforov (1993). La détection de changements abrupts: tion théorie et Applica-. Prentice Hall. RNTI-E-19- 239 - Estimation de densité sur flux de données Desobry, F. et M. Davy (2003). Support Vector-Based détection en ligne des changements abrupts. Dans Proc. IEEE ICASSP, Hong Kong, pp. 872-875. Dries, A. et U. Rückert (2009). Adaptive Concept de détection de dérive. Dans Conférence SIAM sur Data Mining, p. 233-244. Eguchi, S. et J. Copas (2006). L'interprétation divergence Kullback-Leibler avec le lemme Pearson Neyman. Journal of Multivariate Analysis 97 (9), 2034-2040. Feng Cao, F., M. Ester, W. Qian et Zhou A. (2006). à base de densité regroupement sur un flux de données avec le bruit evolv- ment. Dans Conférence SIAM sur Data Mining, p. 328-339. Friedman, J. et L. Rafsky (2006). généralisations multivariées des tests sur deux échantillons Wald-Wolfowitz et Smirnov. Annales de statistique 7 (4), 697-717. Gretton, A., K. M. Borgwardt, M. J. Rasch, B. Schölkopf et S. A. J. (2006). Une méthode du noyau pour les deux-Sample-problème. En NIPS, pp. 513-520. MIT Press. Hall, P. (2002). Permutation tests pour l'égalité des distributions en milieu de grande dimension. Biometrika 89 (2), 359-374. Hershey, J. R. et P. A. Olsen (2007). Approximation les Kullback Leibler tween gaussiennes modèles Be- Mélange. En ICASSP: IEEE Conférence internationale sur l'acoustique, Speech and Signal Processing, Volume 4, pp 317-320... Parzen, E. (1962). Sur l'estimation d'une fonction et d'un mode de densité de probabilité. Annales de la statistique mathématique 33, 1065-1076. Shawe-Taylor, J. et N. Cristianini (2004). Méthodes du noyau pour l'analyse de modèle. La presse de l'Universite de Cambridge. Srivastava, A. (2009). L'extraction de données de la NASA: de la théorie à des applications. Dans Proc. KDD, Paris, pp. 7-8. Widmer, G. et M. Kubat (1996). L'apprentissage en présence de Concept Drift et caché Contextes. Machine Learning 23’ (1), 69-101. Zhang, T., R. Ramakrishan, et M. Livny (1996). BIRCH: Un clustering efficace des données Méthode de très grandes bases de données. Sigmod Rec. 25 (2), 103-114. Résumé CÉS Dernieres, La QUANTITE Années de Donnees Ë Ë Traiter de la DANS Augmentée considérablement les applications Nombreuses. La fouille de flux de Données au défi des Répond Mas- Sives par Données des Traitements à la volée Qui de capacity Une requièrent reasonable stockage. Detection of in the density Changements de probabilité d'un flux is Une question importantes. article propose un nouveau this schéma de Detection of Qui se Compose changement de Suivantes Quatre ÉTAPES: i) le résumé du flux ensemble par un de micro-clusters; ii) L'estimation La densité de probabilité de flux GRÂCE aux DU micro-grappes; iii) l'estimation de la divergence between Estimée à l'density instant et juin courant de density référence; iv) un diagnos la contribution tic estimant de variables each descriptif à la divergence separé les Qui globale deux densi- Tés. Notre schéma de détection de changement is FINALEMENT et Evalue sur Appliqué deux flux de Données artificiels. RNTI-E-19 - 240 -