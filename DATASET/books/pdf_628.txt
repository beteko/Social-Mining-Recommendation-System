articles assemblage pdfclassification supervisée grands nombres classes prédire approche partitionnement variables explicatives expliquer boullé orange avenue pierre marzin 22300 lannion boulle orange ftgroup perso francetelecom boulle résumé phase préparation données mining thodes discrétisation groupement valeurs supervisé possèdent nombreuses applications interprétation estimation densité conditionnelle sélection filtre variables recodage variables amont sifieurs méthodes supposent habituellement faible nombre valeur expliquer classes typiquement moins dizaine trouvent limite quand nombre augmente article introduisons exten méthodes discrétisation groupement valeurs consistant titionner variable explicative autre variable expliquer meilleur partitionnement recherché moyen approche sienne sélection modèle présentons ensuite comment utiliser cette méthode prétraitement préparation classifieur bayesien expérimentations intensives démontrent apport méthode centaines classes introduction objectif classification supervisée prédire valeur variable catégo rielle expliquer connaissant ensemble valeurs variables explicatives numériques catégorielles plupart problèmes classification considérés usuellement limitent prédiction valeur booléenne variable comportant nombre faible valeurs typiquement moins dizaine rencontre néanmoins problèmes nombre valeurs expliquer important comme exemple reconnaissance chiffres manuscrits reconnaissance caractères classification textes appli cations émergentes ciblage publicitaire internet amenées considérer choix bandeau publicitaire parmi plusieurs centaines maximiser navigation internautes méthodes existantes supposent moins implicitement faible nombre classes potentiellement moins performantes grands nombres classes individus classe envisager problème classification cadre général faire hypothèse nombre restreint classification supervisée grands nombres classes prédire classes intéresse préparation données univariée consistant discrétiser variable numériques grouper valeurs variables catégorielles méthodes largement traitées bibliographie prétraitement arbres décision breiman quinlan zighed rakotomalala classifieur bayesien dougherty objectif article étendre préparation données nombreuses classes méthodes prétraitement discrétisation consistent partitionner variable expli cative intervalles numériques groupes valeurs catégoriel façon obtenir estimation probabilité conditionnelle classes prédire quand nombre classes faible méthodes fournissent estimation robuste quand nombre augmente méthodes sujettes apprentissage contraintes partitionner variable explicative solution alors considérer partitionnement joint variable explicative variable expliquer méthode heuristique présentée ritschard intéresse problème groupement lignes colonnes tableau contingence maximise critère association coefficient cramer tschuprow pearson algorithme proposé nombre dividus limite variables ayant faibles nombres valeurs nadif govaert problème forme modèle mélange moyen algorithme expectation maximisation cette approche adaptée analyse exploratoire cadre coclustering individus variables raison temps calcul adaptée préparation données potentiellement nombreuses variables explicatives traiter parmi méthodes apparentées égale citer approches error correcting output codes dietterich bakiri permettent appliquer classifieur binaire multi classes réduisant problème multi classes série problèmes binaires basées partitions classes objectif notre approche permettre utilisation classifieurs naires multi classes améliorer précision robustesse estimateurs densité conditionnelle univariés recherchant chaque variable explicative partition classes adaptée potentiellement différente variable explicative article étendons approche utilisée discrétisa supervisée boullé groupement valeurs supervisé boullé cette approche problème prétraitement univarié comme problème sélection dèles modèle étant défini partition valeurs explicatives intervalles groupes valeurs distribution multinomiale classes chaque partie modèle traitement étendu partitionnant conjointement classes limitant distribution multinomiale groupes classes chaque partie explicative alors trouver compromis entre modèles fortement discriminants basés groupes classes faible cardinalité modèles robustes moins discriminants exploitant groupes classes forte cardinalité compromis trouvé recherchant meilleur modèle selon approche bayesienne article organisé façon suivante partie rappelle approche utilisée méthodes prétraitement supervisé univarié partie introduit extension cette approche variables expliquer comportant grands nombres classes partie présente impact prétraitements étendus classifieur bayesien partie évalue performances méthode enfin partie conclut article boullé prétraitements supervisés cette section rappelle principes approche modl1 discrétisation supervisée boullé groupement valeurs supervisé boullé discrétisation supervisée discrétisation supervisée traite variables explicatives numériques consiste partitionner variable explicative intervalles conservant maximum information relative classes compromis trouvé entre finesse information prédic permet discrimination efficace classes fiabilité statistique permet généralisation modèle discrétisation approche discrétisation pervisée formulée problème sélection modèles approche bayésienne appliquée choisir meilleur modèle discrétisation recherché maximisant probabilité model modèle sachant données utilisant règle bayes puisque quantité constante données apprentissage alors maximiser model model terme priori modèles terme vraisemblance données connaissant modèle premier temps famille modèles discrétisation explicitement définie paramètres discrétisation particulière nombre intervalles bornes intervalles effectifs classes intervalle second temps distribution priori proposée cette famille modèles cette distribution priori exploite hiérar paramètres nombre intervalles abord choisi bornes intervalles enfin effectifs classe choix uniforme chaque étage cette hiérarchie distributions classes intervalle supposées indépendantes entre elles soient nombre individus nombre classes nombre intervalles nombre individus intervalle nombre individus classe intervalle contexte classification supervisée nombre individus classes supposés connus modèle discrétisation supervisée entièrement caractérisé paramètres utilisant définition famille modèles discrétisation distribution priori formule bayes permet calculer explicitement probabilités posteriori modèles connaissant données prenant négatif probabilités conduit critère évaluation fourni formule trois premiers termes représentent probabilité priori modèle choix nombre intervalles bornes intervalles distribution classes chaque intervalle dernier terme représente vraisemblance probabilité observer classes connaissant modèle discrétisation discrétisation quasi optimale recherchée optimisant critère évaluation moyen heuristique gloutonne ascendante décrite boullé issue 1outil disponible shareware perso francetelecom boulle classification supervisée grands nombres classes prédire algorithme optimisation optimisations effectuées voisinage meilleure solution évaluant combinaisons coupures fusions intervalles algorithme exploite décomposabilité critère intervalles permettre après optimisation ramener complexité algorithmique groupement valeurs supervisé variables explicatives catégorielles traité moyen approche laire évaluant modèles groupement valeurs numérique partitionner valeurs explicatives contrainte adjacence entre valeurs partition nement intervalles catégoriel toujours partitionner valeurs explicatives cette aucune contrainte partitionnement groupes valeurs soient nombre individus nombre valeurs explicatives nombre classes nombre groupes valeurs nombre individus groupe valeur nombre individus classe groupe application approche bayesienne sélection modèle conduit critère évaluation groupement valeurs fourni formule cette formule possède structure similaire celle formule remplaçant premiers termes probabilité priori partition intervalles celle partition groupes valeurs nombre répartitions valeurs explicatives groupes éventuellement vides correspond nombre général écrire comme somme nombre stirling deuxième espèce nombre partitions valeur groupes vides critère évaluation groupements valeurs optimisé moyen heuristique gloutonne ascendante décrite boullé étapes optimisation optimisation utilisées garantir complexité algorithmique sacrifier performances méthode généralisation grands nombres classes prédire présence grand nombre classes raisonnable modéliser directe distribution classes raison effectifs classe potentiellement faibles propose partitionner classes groupes classes permet ramener problème classification supervisée standard portant faible nombre groupes classes décrire classe effective chaque individu localement groupe classes variable catégorielle expliquer comportant classes étendre modèles prétraitement supervisé incorporant groupement classes groupes standard considéré comme particulier lequel supposé connu avance alors nombre groupes paramètre estimer notations boullé nombre individus échantillon variable catégorielle expliquer nombre classes variable expliquer connu nombre groupes classes variable expliquer inconnu index groupe auquel rattaché valeur expliquer nombre individus groupe expliquer nombre classes groupe nombre individus classe nombre groupes décrire partition classes groupes revient spécifier façon similaire groupement valeurs variable explicative décrit section spécification groupement classes aboutit ajout nouveaux termes priori suivants notons cette partition spécifiée nombres valeurs groupe déduisent partie paramétrage modélisation ramène ensuite classique prétraitement univarié supervisé présenté section modèles partitionnement variable explicative exploités finir chaque partie explicative distribution individus groupes classes effectif groupe déduit sommation effectifs groupe classes ensemble parties explicatives selon chaque individu étant associé groupe classes désormais préciser quelle classe spécifique associé faire décrit localement chaque groupe distribution individus groupe classes groupe moyen modèle multi nomial distribution individus groupe classes comme précédemment utilise priori uniforme paramétrage modèle multinomial conduit chaque groupe ajout nouveau terme priori suivant vraisemblance distribution individus groupes gérée modèle prétraitement standard ajouter terme vraisemblance localement chaque groupe distribution individus groupe classes groupe moyen terme multinôme sommant ensemble groupes expliquer obtient termes priori classification supervisée grands nombres classes prédire termes vraisemblance maintenant rajouter nouveaux termes priori vraisemblance critères prétraitement présentés section discrétisation supervisée exemple formule étendue problème partitionnement joint variables explicatives expliquer apparente celui discrétisation supervisée variables explicatives permet réutiliser mêmes algorithmes critères soient sensiblement différent possèdent structure article avons utilisé heuristiques décrites boullé présentent avantage tenue charge complexité algorithmique indépendante nombre valeurs variable impact classifieur bayesien cette partie rappelle principes classifieur bayesien détaille prise compte prétraitements introduits section calcul scores prédiction classifieur bayesien soient ensemble variables explicatives numériques catégorielles variable catégorielle expliquer comportant classes valeurs explicatives nouvel individu classer classifieur bayesien associe chaque individu classe maximisant probabilité condi tionnelle posteriori classifieur bayesien optimal calculable pratique puisqu suppose connaisse parfaitement distribution jointe probabilité conditionnelle modèle bayesien langley relâche fortement contrainte estimation multiva probabilité conditionnelle faisant hypothèse naïve indépendance variables explicatives conditionnellement variable expliquer parfois qualifié idiot bayes littérature classifieur bayesien souvent performant pratique nombreux données réels simple mettre œuvre rapide apprendre déployer sujet apprentissage puisque espace modèles réduit singleton appliquant cette hypothèse naïve indépendance conditionelle variables plicatives aboutit boullé équation suffisante obtenir classe probable connaissant riables descriptives problèmes score prédiction nécessaire probabilité conditionnelle classe obtenue sommation dénominateur classes prise compte prétraitements univariés issue prétraitements chaque variable partitionnée parties explicatives intervalles groupes valeurs estimation conditionnelle partition groupes classes soient effectif apprentissage partie celui partie celui cellule exploitant prétraitement partitionnement joint soient partie laquelle valeur explicative appartient groupe associé classe modèle prétraitement permet estimer probabilités conditionnelles façon constante morceau conduit nkikjk probabilités priori classes peuvent elles estimées probabilité pirique effectif classe taille échantillon apprentissage exploitant estimations empiriques probabilité formule calcule façon suivante nkikjk nkikjk éviter probabilités nulles probabilités conditionnelles estimées utili estimate support coverage expérimentation cette section évaluons impact notre méthode prétraitement prépa ration données modélisation classifieur bayesien exemple illustratif titre illustratif utilisons données letter asuncion newman consiste prédire lettre capitale parmi partir matrice pixels blanc variables numériques descriptives mesures concernant taille classification supervisée grands nombres classes prédire boîte contenant lettre moments statistiques résumant position pixels noirs boîte titre exemple largeur boîte mesures figure présente forme histogramme bivarié résultats prétraitement partitionnement joint variable explicative width intervalles largeur variable expliquer groupes lettres hauteur histogrammes représente probabilité conditionnelle observer lettre appartenant groupe lettres sachant largeur tervalle valeurs exemple petites largeurs width probabilité conditionnelle observer lettre inverse grandes largeurs lettre width observer lettre parmi autres situations correspondent intermédiaires globalement prétraitement permet obtenir estimation constante morceau probabilités conditionnelles cette estimation meilleure connaissant données selon approche bayesienne sélection modèles utilisée letter width width letter estimation probabilités conditionnelles lettre expliquer letter connais largeur width letter expérimentation bases étudier impact méthode prétraitement groupement classes bonne prédiction classifieur bayesien avons étudié trois types prétraitement boullé partitionnement supervisé variables explicatives expliquer section partitionnement supervisé variables explicatives uniquement section version standard discrétisation supervisée intervalles effectif numérique regroupement valeurs catégoriel noter alors chaque prétraitement univarié permet obtenir estimations probabilité conditionnelles groupe classes section prédicteur bayesien combine estimations correspondant partitions classes potentiellement diffé rentes obtenir estimations probabilité conditionnelle classe formule expérimentations menées utilisant données asuncion newman décrits table représentant grande diversité domaines nombres individus variables explicatives numériques catégorielles comportant moins trois classes distributions classes parfois déséquilibrées rappelle fréquence classe majoritaire bonne prédiction évalué moyen validation croisée stratifiée niveaux dataset abalone glass led17 10000 letter 20000 pendigits 10992 phoneme satimage segmentation shuttle 58000 soybean thyroid vehicle waveform yeast moyenne bonne prédiction bases moyennes écart types résultats données présentés table ainsi moyenne ensemble données moyenne nombre victoires défaites significatives évaluées seuil moyen student résultats confirment apport important méthodes prétraitement supervisé victoires significative méthode rapport méthode standard revanche résultats méthodes équivalents bases comportant ordre dizaine classes méthode prétraitement groupement classes intéressante interprétation section impact bonne classification négligeable classification supervisée grands nombres classes prédire expérimentation nombreuses classes étudier impact notre méthode grands nombres classes avons utilisé mushroom construire série bases artificielles mushroom comporte individus variables catégorielles nombres valeurs rappelés figure nombre valeurs variable mushroom première variable classe première variable bruise autres variables étant explicatives seconde variable classe produit cartésien premières variables bruise capcolor autres variables étant explicatives bases suivantes construites façon similaire jusqu dixième ayant variable classe produit cartésien première variables dernières étant explicatives table présente caractéristiques artificielles nombres classes allant classes majoritaires target variable v1xv2 v1vv2xv3 v1xv2 v1xv2 v1xv2 v1xv2 v1xv2 v1xv2 v1xv2 bonne prédiction bases articicielles compare prédiction comme section moyen validation croisée stratifiée niveaux cette différences performance importantes significatives victoire significatives autant nombre classes augmente figure présente différences apprentissage méthodes obtiennent résultats similaires apprentissage performances boullé dégradent significativement atteint cinquantaine classes alors apprentissage reste faible classes nombre valeurs expliquer train train bonne prédiction apprentissage méthodes fonction nombre valeurs prédire conclusion méthode prétraitement univarié supervisé présentée article modèle partitionnement joint variable explicative intervalles groupes valeurs variable expliquer groupe classes partitionnement joint permet façon robuste distribution probabilité conditionnelle limité faibles nombres classes meilleur modèle partitionnement joint recherché moyen approche bayesienne sélection modèles évaluations intensives bases comportant ordre dizaine classes portant apport méthode prétraitement classifieur bayesien montrent méthode obtient performances équivalentes supérieures celles revanche quand nombre classes devient important typiquement plusieurs centaines expérimentation démontrent apport significatif méthode meilleure robustesse bonne prédiction accrus résultats permettent étendre champ application méthodes classifica problèmes comportant nombreuses classes travaux futurs envisagés appliquer approche ciblage publicitaire internet lequel classe prédire bandeau publicitaire susceptible entraîner internaute comporter plusieurs centaines valeurs références asuncion newman machine learning repository classification supervisée grands nombres classes prédire boullé bayes optimal approach partitioning values categorical attri butes journal machine learning research boullé bayes optimal discretization method continuous attributes machine learning boullé optimum simultaneous discretization models supervi classification bayesian model selection approach advances analysis classification breiman friedman olshen stone classification regression trees california wadsworth international dietterich bakiri solving multiclass learning problems error correcting output codes journal artificial intelligence research dougherty kohavi sahami supervised unsupervised discretization continuous features proceedings international conference machine learning morgan kaufmann francisco idiot bayes stupid after international statistical review langley thompson analysis bayesian classifiers national conference artificial intelligence press hussain discretization enabling technique mining knowledge discovery nadif govaert block clustering contingency table mixture model advances intelligent analysis volume springer quinlan programs machine learning morgan kaufmann ritschard zighed nicoloyannis maximisation association regroupement lignes colonnes tableau croisé mathématiques sciences humaines comparative study discretization methods naive bayes classifiers proceedings pacific knowledge acquisition workshop zighed rakotomalala graphes induction france hermes summary preparation phase mining supervised discretization value grouping methods numerous applications interpretation conditional density estimation filter lection input variables variable recoding classification methods these methods usually assume small number output values typically reach their limit their number increases paper extend discretization value grouping methods based partitioning input output variables joint partitioning searched maximizing bayesian model selection criterion exploit preprocessing method preparation naïve bayes classifier extensive experiments demonstrate benefits approach hundred output values