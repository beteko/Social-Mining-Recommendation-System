 Evaluating Bayesian Networks by Sampling with Simplified Assumptions Saaid Baraty Dan A Simovici University of Massachusetts Boston Computer Science Department Boston Massachusetts 02125 e mail{sbaraty dsim} cs umb edu Abstract The most common fitness evaluation for Bayesian networks in the presence of data is the Cooper Herskovitz criterion This technique involves massive amounts of data and therefore expansive computations We propose a cheaper alternative evaluation method using simplified assumptions which pro duces evaluations that are strongly correlated with the Cooper Herskovitz crite rion 1 Introduction We investigate the problem of constructing a Bayesian network for a composite phe nomenon U = {u1 u2 un} where ui for 1 ≤ i ≤ n are discrete random variables representing the state assignment of the attributes of U To accomplish this we start from a data multiset D = {t1 t2 tm} where an n ary tuple ti is an instance of the event U We refer to this multiset as evidence data set data set for short A number of assumptions are necessary for deriving a measure for evaluating the fitness of a Bayesian network structure BNS for a training data set Stronger hypotheses make the evaluationmoremanageable On the other hand the model obtained under weaker assumptions is better capable to be conforming with the underlying true distribution of the problem Let G = U E be a directed acyclic graph having U as its set of vertices and E as its set of edges which captures the direct probabilistic dependencies among these variables Let Θ be the collection of parameters which quantifies the joint probability distribution of U as specified by G We denote the set of possible assignments of a random variable ui by Dom ui = {u1i urii } The notion of domain can be extended to sets of variables V using Cartesian product If the set of parent nodes of ui is ParG ui then Dom ParG ui = {U1i U qii } The set of non descendants of ui ndG ui is the set of all nodes in U excluding ui and all its descendants When it is clear from the context we drop the subscript G The pair B = G Θ satisfies the local Markov condition if PB ui|nd ui = PB ui|Par ui for 1 ≤ i ≤ n where PB is the probability distribution on U specified by B The model B is a Bayesian network if it satisfies the local Markov condition By the chain rule we have PB u1 u2 un =∏n i=1 PB ui|Par ui Therefore if we let θijk = P ui = uki |Par ui = U ji and θij· = θij1 θijri for 1 ≤ i ≤ n 1 ≤ k ≤ ri and 1 ≤ j ≤ qi then the joint probability distribution on U is specified by Θ = {θij·|1 ≤ i ≤ n and 1 ≤ j ≤ qi} 11 Evaluating Bayesian Networks by Sampling 2 A Posterior based Score with A Reduced Assumptions Set Cooper and Herskovitz introduced the probability P G|D as a measure of assessing the fitness of G as a probabilistic model of D Since P D is constant across different networks we can work with P G D Let ΩG be the space of all probability distributions Θ for the structure G Then P G D = ∫ ΩG Θ P D|Θ G f Θ|G P G dΘ 1 Recall that Θ is a collection of distributions θij· = θij1 θij ri−1 1 − ∑ri−1 k=1 θijk for all i and j The vectors θij· for any i j ∈ [1 n] × [1 qi] must satisfy ∑ri−1 k=1 θijk ≤ 1 and θijk ≥ 0 for all k Also Θ itself the collection of these random vector variables can be treated as a random variable P D|Θ G is the conditional probability function of data given G Θ f Θ|G is the conditional density function of Θ given structure G and P G is the prior probability function of structure G To evaluate this integral a number of assumptions were introduced by Cooper and Herskovits 1993 The data independence assumes tuples of D are independent given the network structure The local and global independence LGI assumption requires that θij· is conditionally independent of θi′j′· for all i j 6= i′ j′ given the structure Based on the LGI assumption Ω Θ the space of possible collections Θ can be written as ΩG Θ = { n∏ i=1 qi∏ j=1 θij1 θij ri−1 ∈ R ri−1 | ri−1∑ k=1 θijk ≤ 1 and θij1 θij ri−1 ≥ 0 } and we have f Θ|G = ∏ni=1 ∏qi j=1 g θij·|G due to the LGI assumption Cooper and Herskovits 1993 replace f by the above product in Equality 1 Also they assume the distribution g θij·|G for each i and j is uniform We refer to this assumption as second order uniform probability SOUP Heckerman et al 1995 introduce the BDe metric which is a posterior based measure similar to CH metric They use the LGI assumption and three other assumptions the second order Dirichlet probability SODP suggested but not used in Cooper and Herskovits 1993 the parameter modularity and the multinomial sample MS assumption SODP is generalization of SOUP assumption which states that P θij·|G follows a Dirichlet distribution for all i and j The multinomial sample assumption asserts that if we define the ordered set Dl = {t1 tl−1} then P tl[ui] = u k i | tl[u1 ui−1] = uv11 u vi−1 i−1 Dl G Θ = θijk where t[V] denotes the restriction of V ⊆ U on tuple t ∈ D and we have the state assign ment ParG ui = U j i consistent with tl[u1 ui−1] = u v1 1 u vi−1 i−1 and θijk ∈ Θ Later the SODP assumption was replaced with two other assumptions likelihood equiva lence and structure possibility which imply the SODP assumption Note that every probabil ity function g θij·|G follows a Dirichlet distribution which requires ri parameters Thus for each BNS G we need to specify ∑n i=1 qiri parameters and this makes this approach impractical To overcome this difficulty Heckerman et al 1995 encoded the prior knowl edge into a single Bayesian network referred as a prior network Bpr = Gpr Θpr Then they set the Dirichlet parameter corresponding to probability distribution component θijk to αijk = N ′ ·PBpr ui = uki ParGpr ui = U ji whereN ′ is a user given parameter which they 12 S Baraty et D A Simovici refer as equivalent sample size The choice of a values of N ′ and the collection Θpr without observing data is arbitrary We use sampling which enable us to let data shape the distribution of the posterior probability on vectors θij· In the evaluation of the prior P G Cooper and Herskovits 1993 assumed an uniform prior distribution This and other assumptions are based on parameters that need to be arbitarily specified Sampling enables us to use data as a substitute for strong assumptions or domain knowledge in determining the parameters of the second order probability distribution and the prior probability P G Let S1 and S2 be two disjoint samples from D We evaluate P G|S1 S2 as a measure of fitness of BN structure G Since P S1 S2 does not depend on the specific BNS we can drop it and instead compute P G S1 S2 Note that by chain rule P G S1 S2 = P S1|G S2 · P G|S2 · P S2 If we sample consistently across different structures then P S2 is constant and can be dropped Therefore we adopt P S1|G S2 · P G|S2 as a relative measure of fitness of structures for a data set D If we repeat the process of sampling we can extend our measure to k∏ q=1 P S2q−1|G S2q · P G|S2q 1 k where S1 S2 S2k are samples fromD where S2q−1 ∩ S2q = ∅ for each q We refer to this measure as k sample validation of structure G for data setD and denote it by SAMPk G D Let S = {t1 ta} and S′ be two disjoint samples ofD The first term of SAMPk G D can be written as P S|G S′ = ∫ ΩG Θ P S|Θ G S′ f Θ|G S′ dΘ 2 Let d = u1 un be a topological order of nodes of G which represents expert prior knowledge of the domain Denote by nS t the number of occurrences of tuple t in S and let γijk S = ∣∣{t ∈ S | t[{ui}] = uki ∧ t[Par ui ] = U ji } ∣∣ and γij· S = ∑ri k=1 γijk S Since the attributes ofD are discrete we have P S|B = a∏ l=1 P tl|Sl Θ G = a∏ l=1 n∏ i=1 P ui = tl[ui]|Ui = tl[Ui] Sl Θ G = a∏ l=1 n∏ i=1 qi∏ j=1 ri∏ r=1 θ λlijr ijr where the first equality is by the chain rule and Sl = t1 tl−1 the second equality is by assuming MS assumption and Ui = u1 ui−1 and λlijr = 1 if tl[ui] = uri ∈ Dom ui and tl[ParG ui ] = U j i ∈ Dom ParG ui and λlijr = 0 otherwise Since ∑a l=1 λlijr = γijr S we have P S|Θ G = n∏ i=1 qi∏ j=1 ri∏ r=1 θ γijr S ijr 3 Then P S|Θ G S′ = P S ∪ S ′|Θ G P S′|Θ G = ∏n i=1 ∏qi j=1 ∏ri r=1 θ γijr S∪S′ ijr ∏n i=1 ∏qi j=1 ∏ri r=1 θ γijr S ′ ijr = n∏ i=1 qi∏ j=1 ri∏ r=1 θ γijr S ijr 4 For the second term of right hand side of Equality 2 we have f Θ|S′ G = P S ′|Θ G f Θ|G ∫ ΩG Θ P S′|Θ G f Θ|G dΘ 5 13 Evaluating Bayesian Networks by Sampling We assume theSOUP hypothesis and set each g θij·|G = ri−1 The posterior probability of Θ is conditioned on G in presence of sample S′ as shown in Equality 2 This approach is is different from the one used in Cooper and Herskovits 1993 where SOUP hypothesis has been applied directly without intervention of sample data Then we have ∫ ΩG Θ P S′|Θ G f Θ|G dΘ = n∏ i=1 qi∏ j=1 ri − 1 · ∏ri r=1 γijr S ′ γij· S′ + ri − 1 from Equality 3 and SOUP LGI and from a result from Jeffreys and Jeffreys 1988 see pages 468 470 of this reference Thus from the previous equalities and from 3 and 5 we have f Θ|S′ G = n∏ i=1 qi∏ j=1 Γ γij· S ′ + ri ri∏ r=1 θ γijr S ′ ijr Γ γijr S′ + 1 6 where Γ is Euler’s function Combining Equalities 2 4 and 6 we obtain P S|G S′ = n∏ i=1 qi∏ j=1 Γ γij· S′ + ri Γ γij· S ∪ S′ + ri · ri∏ r=1 Γ γijr S ∪ S′ + 1 Γ γijr S′ + 1 To approximate the quantity P G|S we use a slight variation of a measure called the distribution distortion introduced in Baraty and Simovici 2009 Here we want to evaluate the conditional independency captured by local Markov condition according to data that is we want to assess to what degree the conditions fS ui|nd ui = fS ui|Par ui holds for 1 ≤ i ≤ n where fS is the frequency function relative to sample S ⊆ D To achieve this we measure the divergence of the set of probability distributions fS ui|nd ui = U from the set of probability distributions fS ui|Par ui = U [Par ui ] for all i and U ∈ Dom nd ui Definition 2 1 The local Markov divergence of the fork structure at node ui of G according to sample S denoted by LMDGS ui is the number ∑ U fS nd ui = U · KL [ fS ui|nd ui = U fS ui|Par ui = U [Par ui ] ] where the sum extends over all U ∈ Dom nd ui Here KL[p q] is the Kullbach Leibler divergence between the probability distributions p = p1 pn and q = q1 qn Let HS π u be the Shannon entropy of the set S partitioned according to the values of u and let HS π u|πW be the conditional Shannon entropy of the set S partitioned according to the values of u conditioned by the partition of S according to the assignment of the set of attributesW see Baraty and Simovici 2009 Theorem 2 2 For 1 ≤ i ≤ n we have LMDGS ui = HS πui |πPar ui − HS πui |πnd ui Theorem 2 3 LMD G S ui = 0 if and only if fS ui|nd ui = fS ui|Par ui Theorem 2 2 implies that 0 ≤ LMDGS ui ≤ HS πui By Theorem 2 3 the smaller the value of LMD G S ui is the closer is the fork structure at node ui to satisfy the local Markov condition according to S Therefore the Markov condition is closer to be satisfied according to sample S On another hand the closer LMD G S ui is toHS π ui the more divergent the two 14 S Baraty et D A Simovici probability distributions fS ui|nd ui = U and fS ui|Par ui = U [Par ui ] are for every U ∈ Dom nd ui When LMDGS ui = HS πui we haveHS πui |πPar ui = HS πui and HS π ui |πnd ui = 0 This means that the set Par ui has no prediction capability at all at the node ui and the set nd ui has a perfect predication capability on ui Let BNS U be the set of all possible Bayesian structures on set of attributes U Define P G|S as P G|S = ∑n i=1 HS π ui − LMDGS ui ∑ G′∈BNS U ∑n i=1 HS πui − LMDG′S ui Using the previous evaluations SAMPk G D can be written as k∏ q=1 P S2q−1|G S2q · P G|S2q 1 k =   k∏ q=1 ∑n s=1 HS2q π us − LMDGS2q us ∑ G′∈BNS U ∑n s=1 HS2q π us − LMDG′S2q us · qi∏ j=1 Γ γij· S2q + ri Γ γij· S2q−1 ∪ S2q + ri ri∏ r=1 Γ γijr S2q−1 ∪ S2q + 1 Γ γijr S2q + 1 1 k If we consistently sample the data across different structures we can drop the constant entities with respect to BNS G and assuming P S2q G = ∑n s=1 HS2q π us − LMDGS2q us we set SAMPk G D = k∏ q=1 P S2q G n∏ i=1 qi∏ j=1 Γ γij· S2q + ri Γ γij· S2q−1 ∪ S2q + ri ri∏ r=1 Γ γijr S2q−1 ∪ S2q + 1 Γ γijr S2q + 1 1 k 3 Experimental Results and Conclusions We conducted experiments on three well known structures GAM GCAR and GNC for domains Alarm Car Diagnosis2 and Neapolitan Cancer with 37 18 and 5 nodes respec tively For the first two structures we randomly generated the corresponding probability tables ΘAM and ΘCAR Then based on probability distributions introduced by GAM ΘAM and GCAR ΘCAR we generated data sets of sizes 80000 and 100000 respectively For the GNC we used its corresponding data set in the literature with 7565 with no missing values For each data set we randomly generated a number of structures of different complexities The number of the edges for these structures ranged from 1 − 10 12 − 108 and 12 − 330 for NC CAR and AM data sets respectively Figures 1 a 1 b and 1 c show very strong correlations between the CH score and the SAMP score for various values for k The derived measure is cheaper to compute since it works with samples much smaller than the entire data We introduced a measure based on posterior probability for measuring the fitness of a Bayesian network structure based on data The conclusion of this work is that our sampling based scoring is a viable and much cheaper alternative to the CH score The fact that we use sampling to reduce the set of assumptions and we get a very strong correlation between two measure confirms that the SOUP and uniform distribution on P G are safe assumptions and do not distort the search 15 Evaluating Bayesian Networks by Sampling a Alarm data b Car Diagnosis2 data c Neapolitan Cancer data d time comparison diagram FIG 1 – Correlations between log SAMPk G D and log CH and time in ms needed for computing log SAMP1 G CAR and CH scores References Baraty S and D A Simovici 2009 Edge evaluation in Bayesian network structures In Proceedings of the 8th Australian Data Mining Conference Melbourne pp 193–201 Cooper G F and E Herskovits 1993 A Bayesian method for the induction of probabilistic networks from data Technical Report KSL 91 02 Stanford University Knowledge System Laboratory Heckerman D D Geiger and D M Chickering 1995 Learning Bayesian networks The combination of knowledge and statistical data InMachine Learning pp 197–243 Jeffreys H and B S Jeffreys 1988 Dirichlet Integrals Cambridge UK Cambridge Uni versity Press Résumé L’évaluation qualitative la plus connue des réseaux Bayesiens en présence de données est le critère Cooper Herskovitz Cette technique implique des quantités massives de données donc par conséquent des nombreux calculs Nous proposons une méthode d’évaluation plus efficace utilisant des suppositions simplifiées et qui produit des évaluations fortement corrélées avec le critère Cooper Herskovitz 16 