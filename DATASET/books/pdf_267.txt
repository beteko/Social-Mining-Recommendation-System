Classes minières par Multi-label Classification Yuichiro KASE *, Takao MIURA ** * Département des sciences avancées, yuichiro.kase.7n@stu.hosei.ac.jp ** Département des élus. De & Elect. Eng., Miurat@hosei.ac.jp HOSEI Université 3-7-2 KajinoCho, Koganei, Tokyo, Japon 184-8584 CV. Nous vous proposons une nouvelle approche de mes classes potentielles dans les documents d'information en examinant relation étroite entre les nouvelles classes et des vecteurs de probabilité de marquage multiple des documents. En utilisant l'algorithme EM pour obtenir la distribution par rapport aux modèles de mélange linéaires, nous faisons le regroupement des classes et des mines. 1 Motivation nuage Récemment systèmes par Internet ont été largement répandu pour que nous puissions arriver à énorme quantité d'informations complexes facilement et rapidement. Cependant, nous pouvons attraper à peine avec les changements à l'intérieur et la plupart des informations disparaissent immédiatement ce qu'ils sont précieux. Très souvent, nous aimons classer les informations dans les classes qui viennent de cours donnés à l'avance. Une classe peut être obtenue grâce à la reconnaissance humaine par laquelle on peut imaginer ce qui se passe en utilisant des classes. Étant donné que chaque classe correspond à de certain concept, on peut voir ce qu'un mot ne signifie une fois que nous savons que le mot appartient à la classe. Dans ce travail, nous discutons problème de classification multi-étiquette et comment trouver des cours potentiels. classification multi-classe signifie un processus pour mettre l'information dans l'une des catégories multiples. Toute information dans une catégorie part des aspects communs qui caractérisent la catégorie donnée à l'avance, a appelé une classe et son nom une étiquette. La classification automatique nous permet d'extraire les règles par apprentissage inductif. Nous examinons une collection d'histoires (valeurs d'attributs avec des étiquettes, des données de formation appelée), puis extrait caractéristiques spécifiques aux classes (Han et Kamber, 2011). La recherche de la classification multi-label a été initialement motivée par la difficulté d'ambiguïté concept rencontré dans la catégorisation de textes. En fait, chaque document peut appartenir à plusieurs thèmes (étiquettes) simultanément et quelques documents contient une seule partie. L'une des approches typiques est la classification probabiliste (Kita, 1995), étant donné que les de classification traditionnelle sultats dépendent fortement des données de formation. Le plus important est qu'il ya peu corpora, même si nous voyons un énorme quantité d'informations sans étiquette (données brutes). Ici, dans ce travail, nous adoptons une approche semi-supervisée dans un cadre de probabilité. Ici nous nous concentrons notre attention sur un fait que la façon dont les classes sont constituées. Tout article de nouvelles de contestation internationale des « navires de commerce » en Chine peut provenir de plusieurs labels de la politique, l'économie, ainsi que l'histoire et la culture. Chaque catégorie porte son propre sens, bien qu'il contienne al combinaison pondérée des concepts de labels comme l'une des caractéristiques (Han et Kamber, 2011). Cela nous permet de définir de nouvelles classes pour de nouvelles catégories en donnant du poids - 77 - vecteurs (0,1, 0,2, 0,3, 0,4) sur ces étiquettes. Pour de nouvelles classes de mines en combinant les étiquettes sur les espaces de probabilité, nous pourrions avoir un nombre infini de combinaisons d'étiquettes en raison de nombre infini de poids. La principale contribution de ce travail se résume comme suit: (1) Nous pouvons mes classes potentielles basées sur la classification label Muli. (2) au moyen d'un modèle de mélange linéaire, on obtient les probabilités d'appartenance sur les étiquettes fournies. Ensuite, nous faisons le regroupement des probabilités d'étiquettes. (3) Nos expériences montrent que de nouvelles classes d'identifier de nouveaux aspects des classes potentielles qui diffèrent des étiquettes constitutives. Le reste du papier est organisé comme suit. Dans la section 2, nous décrivons clas- sification multi-étiquettes pour les documents et l'approche probabiliste, ainsi que des travaux connexes. L'article 3 contient un cadre de notre approche, y compris l'algorithme EM et le regroupement. L'article 4 contient quelques résultats expérimentaux. Dans la section 5, nous concluons cette enquête. 2 multi-label Classification Il y a eu de nombreux algorithmes de classification multi-étiquettes proposées jusqu'à présent. Ils se composent de deux types d'approche, un classement lgorithms et l'estimation probabiliste (Han et ber Kam, 2011). Le premier constitue la combinaison de classification et l'étiquette-ensemble (Tsoumakas et Ka- Takis, 2007). approche de classification contient le regroupement, le classement (Elisseeff et Weston, 2002), l'entropie (arbre de décision) et la traduction des résultats binaires en multi-étiquettes (Rifkin et Klautau, 2004). Une difficulté se pose sur la dépendance entre les étiquettes comme indiqué dans (Zhang et Zhang, 2010) préoccupations d'estimation .Probabilistic sur la façon d'estimer les paramètres de certaines fonctions de distribution de probabilité. Fondamentalement, nous comptons des fréquences de documents et faire des classificateurs basés sur eux. Étant donné que nous estimons une étiquette c qui rend P (c | x) au maximum, nous devons avoir P (x | c) × P (c) par le théorème de Bayes. La plus simple est un classificateur Naive Bayes (NB). où à la fois P (x | c) et P (c) peut être obtenue rapidement par des fréquences. classificateurs Naive devrait dépendre des données de formation et nous supposons modèle de probabilité et l'apprentissage semi-supervisé. Algorithme Expectation Maximisation (EM) a été discuté pour le document basé sur la classification probabilité multinomiale. Au cours de chaque étape, nous appliquons une estimation MAP pour obtenir de nouveaux paramètres, mais ils ont examiné la classification multi-classe. Ensuite, pour la classification multi-étiquettes, il y a eu une approche étiquette ensemble de manière probabiliste. McCallum (McCallum, 1999) a discuté de la classification multi-étiquettes en utilisant EM algorithme basé sur la probabilité Gauss N (μ, σ). Ils ont examiné les mélanges de distributions normales sur toute la combinaison d'étiquettes et d'environ les étiquettes de la probabilité maximale. De toute évidence, il prend du temps beaucoup à cause de nombre exponentiel de la combinaison. Ueda (Ueda et Saito, 2003) a proposé une nouvelle approche pour décrire des documents par plusieurs étiquettes, compte tenu de toutes les étiquettes sous forme de mélanges de sujets et tous les sujets que la distribution de probabilité multinomiale sur les mots. Ils ont estimé les distributions de probabilité en utilisant l'algorithme EM et propose 2 modèles de relation d'étiquettes, PMM1 et PMM2 mais il reste encore quelques problèmes de dépendances d'étiquettes. En utilisant le modèle de sujet, Wang a proposé un certain modèle de relation inter entre les étiquettes (Wang et al., 2008). Bien que l'approche d'allocation Latent Dirichlet ne peut pas modéliser la situation directement, ils ont introduit l'étiquette-vecteurs qui peuvent être générés de manière multinomial et a examiné la performance. - 78 - 3 Estimating multi-étiquettes pour classer les documents d pour la classification multi-étiquettes sur les étiquettes L = {L1, .., Ln}, nous introduisons un vecteur de probabilité d⃗ = (c1, .., cn), Σ ck = 1 , cj ≥ 0 pour décrire la cj de probabilité de d sur une étiquette Lj, ​​j = 1, ..., n. Nous aimons obtenir le vecteur de probabilité d⃗ sur plusieurs étiquettes sur L au moyen de l'apprentissage semi-supervisé. Puisque P (d) = Σ j P (Lj) P (d | Lj) par la marginalisation, nous aimons estimer PLJ (d) = P (d | Lj) avec un poids λLj = P (Lj). Note cj = P (Lj | d) = P (Lj) P (d | Lj) / P (d). Formellement notre classement fonctionne bien par un modèle de mélange linéaire sur les étiquettes (Kita, 1995). Soit X une variable aléatoire qui correspond à un document et la probabilité d'événement est généré par un mélange aléatoire: P (X) = Σ c λcPc (X) où Cp (X) signifie une probabilité de X provenant de la distribution de probabilité multinomiale d'un étiquette c et Xc une probabilité de choix de c indépendant de X. Nous supposons un mot w passe temps XW dans un document d de c selon la distribution de probabilité multinomiale Pc (d) avec un mot probabilité pw d'une manière naïve Bayes: Pc (d) = e! Πwxw! Πwp XW w, nd = Σ w∈d XW. Pour estimer les probabilités Pc (X) et les coefficients Xc, nous améliorons plusieurs paramètres θ (des fonctions de distribution de probabilité) au cours de l'algorithme EM jusqu'à ce que la convergence de telle sorte que le PC (X) = p (X | c, θc) est un multinomial fonction avec des paramètres θc (Han et Kamber, 2011). En appliquant l'estimation du maximum de vraisemblance plusieurs fois, nous arrivons à l'état stable BE- la cause de chaque EM itération ne diminuera pas la probabilité. Finalement, nous devons avoir une collection de probabilités d'appartenance d'une manière cohérente. Doigt de pied Stimate eux à une itération, nous ob- TAIN nouveaux paramètres thetav de θ en maximisant une des probabilités a posteriori (MAP) des fonctions de distribution multinomiale. L'histoire va avec le processus probabiliste, un mélange linéaire des distributions multinomiales sur les mots en fonction des étiquettes données à l'avance. Chaque document conserve la probabilité en fonction de chaque étiquette, et nous supposons mot selon les probabilités se pose de mélange. Nous estimons toutes les probabilités postérieures des étiquettes constituant P (d | Lj), ainsi que les probabilités a priori P (Lj) au moyen de l'algorithme EM. Pour plus de détails, allez à (Han et Kamber, 2011). Notons que la classification multi-étiquettes fonctionne bien avec le concept d'identifier correctement dans les documents et un document peut contenir plusieurs thèmes sur plusieurs. Cela nous amène à grouper dans l'espace d'étiquettes et nous extrayons des grappes de documents en fonction des vecteurs de probabilité. C'est, chaque document porte certaine probabilité de chaque étiquette, qui décrit la répartition des thèmes qu'il contient dans une certaine mesure. Considérant un ensemble de probabilités comme un nouvel aspect, nous donnons une classe (éventuellement nouvelle) au document. Grâce à l'algorithme EM, on obtient la probabilité d'appartenance P (d | c) d'un document d et une étiquette c, ainsi que les probabilités de choix P (c) de c1, .., cann, Λ = (λ1, .., Xc ), Σ = 1 Ài, ≥ 0. Comme Ài tout document peut appartenir à plusieurs étiquettes en même temps, définissons d⃗ = (P (d | c1), ..., P (d | tcc)). Note P (d) = Σ λiP (d | ci) = Λ · d⃗ détient. Définissons la norme de Λ et d⃗ comme || Λ || · d⃗ = Λ · d⃗ / | Λ · d⃗ |. Étant donné une collection de documents d1, .., dN, nous faisons le regroupement de tous les documents en fonction des vecteurs de probabilité d⃗1, ..., d⃗N dans notre espace d'étiquettes. en K ensembles exclusifs de telle sorte que nous avons le minimum rk j Σ i || Λ · (d⃗ij - tj) ||, chaque groupe est marqué par les centres t1, .., tK. Dans cette enquête P (d | c) est générée au moyen de distribution de probabilité multinomiale sur les mots, tous les groupes décrivent le maximum de vraisemblance de l'adhésion de documents. - 79 - Résumons notre approche: Supposons multinomial fonction de distribution de probabilité de chaque étiquette. Étant donné les étiquettes {c1, ...,} cC, documents de formation L et documents de test T, nous générer des vecteurs de probabilité sur D = L ∪ T et des grappes de faire plus de marques pondérées. Nous prétraiter D à l'avance, comme l'élimination des mots vides et à endiguer. (1) En utilisant l'algorithme EM, nous estimons les probabilités de choix et les vecteurs de probabilité, (λ1, .., Xc et P (d | c1), ..., P (d | tcc)). (2) Nous faisons le regroupement de tous les documents exclusivement en fonction des probabilités de choix et les vecteurs de probabilité. (3) Nous avons mis une étiquette c, le centre, à chaque groupe avec une technique de marquage du cluster. 4 Les expériences Montrons des résultats expérimentaux dans deux aspects, la classification multi-étiquette et l'exploitation minière de classe. Nous examinons ModApte de Reuter sous-corpus-21578 Version 1.0 en sélectionnant le top 10 des étiquettes fréquentes, et le premier 1000 article en fin de test. Un tableau 1 contient les étiquettes. Après prétraiter les articles tels que et les mots vides égrappage, nous avons remplacé chaque numéro de chiffres par un mot spécial « * d ». Nous avons sélectionné 200 articles au hasard en formation avec 10 fois repeatition du processus EM. Comme mesure d'évaluation, nous examinons la précision et le rappel à l'appariement multi-étiquette (correspondant complet) et à la correspondance unique étiquette (seule). Les anciens moyens que nous disons correct que si toutes les étiquettes d'un article sont exactement estimés, alors que ce dernier moyen que nous disons correct si des étiquettes sont estimées. Nous appliquons Naive classification bayésienne (NB) comme référence. Nous examinons 200 articles et la fréquence des mots d'extrait par étiquette sous forme de données de formation. Nous faisons décision binaire par un certain seuil. Notez le résultat ne varient pas beaucoup avec plusieurs seuils. Dans un tableau 2, nous obtenons 669 articles au total et la précision est 0,669, tandis que NB montre seulement 8 articles (précision 0,008). Au Nouveau-Brunswick, les 8 articles appartiennent à une étiquette « commerce » aucun article correspondant à d'autres étiquettes. Un tableau 3 présente les résultats de rappel et de précision à chaque obtai étiquette défini par notre approche et la ligne de base (NB) où Ans, Corr, des moyens réponses avec Précipitation, Correcness et précision respectivement. Nous obtenons les moyennes 0,745 et 0,770 du rappel et la précision respectivement par notre approche, mieux que les moyennes et 0,674 0,103 (NB). Bien que les valeurs de rappel par NB sont mieux que notre approche, toutes les valeurs de précision de notre approche surperformer NB considérablement, disent 748% amélioré. Le résultat rappel montre 1,12 fois meilleurs et la précision 7.24 fois mieux. Au Nouveau-Brunswick, nous obtenons assez élevé rappel dans chaque étiquette qui provoque la précision plutôt pire. Dans un tableau 4, nous montrons tous les groupes (centres) et tout le nombre des articles dans chaque groupe. Nous obtenons 11 groupes non vides où (...) signifie que les parties dominantes. Il y a 10 groupes avec étiquette dominante unique et 1 groupe multi-étiquettes ( « gagner, acq » nouvelle classe). Une seule classe de « Earn, acq » se pose lorsque plusieurs étiquettes ( « gagner » et « ACQ ») sont dominants parmi les 11 groupes, mais nous obtenons la pire précision de correspondance complète. Notez que nous avons 76 articles corrects entre 162 articles affectés à la classe « gagner, acq ». En fait, nous obtenons 33 articles de l'étiquette « gagnent », 41 articles de « ACQ » et 2 articles de « gagner, acq ». En mains, nous voyons les articles de « gagner, acq » (contenant plusieurs étiquettes dominantes) diffèrent des articles dans les classes « gagnons » et « ACQ ». Ces articles d'une seule classe d'étiquettes ont un aspect de l'analyse économique, tandis que les articles des multilabel ont un aspect différent des tendances financières. Il semble préférable de définir une nouvelle classe. - 80 - Articles Étiquette Étiquette articles 507 gagnent 13 226 intérêt acq 11 grain, le blé, le maïs 55 brut 5 gagnent, acq 41 commerce 5 brut, navire 34 grain, de blé 2 gagnent, brut 29 fx d'argent, l'intérêt 2 grain, de blé, expédier 20 argent-argent-2 fx fx, le commerce 19 grain, maïs 1 acq, bateau 13 bateau 1 grain, le grain navire 13 1 blé, le maïs 1000 TAB. 1 - Les étiquettes des articles Étiquette articles appariées de précision (nôtre) gagnent 468 453 0,968 acq 172 153 0,890 gagnent / acq 162 2 0,012 commerce 30 17 0,567 navire 21 8 0,381 grain 17 3 0,176 brut 28 25 intérêt 0,893 31 4 0,129 argent-fx 19 4 0,211 blé 24 0 0,0 28 0 0,0 maïs (total) 1 000 669 0,669 (NB) commerce 30 8 0,267 (total) 1000 8 0,008 TAB. 2 - Multilabels Étiquette entièrement appariée Ans Corr rappel Ans Corr avec Précipitation Ans Corr rappel Ans Corr Prec (Ours) (NB) acq 232 198 0,853 334 232 0,695 232 165 0,711 708 165 0,233 maïs 31 15 0,484 28 15 0,536 31 21 0,677 750 21 0,028 brut 62 26 0,419 28 26 0,929 62 43 0,694 821 43 0,052 gagnent 514 508 0,988 630 530 0,841 514 350 0,681 466 350 0,751 grain 80 12 0,15 17 12 0,706 80 50 0,625 760 50 0,066 intérêt 42 22 0,523 31 22 0,710 42 28 0,667 759 28 0,037 d'argent fx 7 0,137 19 51 7 0,368 51 33 0,647 772 33 0,043 16 0,727 navire 21 22 16 0,762 22 14 0,636 713 14 0,0196 0,419 18 commerce 30 43 18 0,6 43 26 0,605 866 26 0,030 16 0,333 blé 48 24 17 48 28 0,583 0,708 760 28 0,037 (total) 1125 838 0,745 1162 895 0,770 1125 758 0,674 7375 758 0,103 TAB. 3 - Rappel / précision par étiquette 5 Conclusion Dans ce travail, nous avons proposé une nouvelle approche de la mine de classes potentielles. Nous avons introduit un modèle de mélange linéaire des fonctions de distribution multinomiale pour obtenir des probabilités d'adhésion sur les étiquettes, alors nous avons fait le regroupement des probabilités d'étiquettes. L'approche surclasse 1.12 fois mieux dans le rappel et 7,48 fois plus de précision pour l'adaptation d'une seule partie. Nous avons obtenu une nouvelle classe qui identifie de nouveaux aspects par rapport aux étiquettes constitutives. Elisseeff Références, A. et J. Weston (2002). Méthode du noyau pour la classification multi-étiquetés. Les progrès 14 NIPS, 681-687. Han, J. et M. Kamber (2011). Data mining: Concepts et techniques. Morgan Kauffman. Kita, K. (1995). modèles de langue probabilistes (en japonais). Université de Tokyo Press. McCallum, A. (1999). classification texte multi-étiquette avec un modèle de mélange formé par em. Atelier sur le texte d'apprentissage, AAAI. - 81 - Non / Articles gagner acq d'argent fx intérêt le commerce du grain brut maïs navire de blé 1/0 0,253 0,093 0,010 0,015 0,100 0,016 0,017 0,286 0,011 0,197 2/468 (1.000) 1.90E- 05 2.05E- 08 6.68E- 10 2.78E- 09 1.31E- 10 1.92E- 07 1.99E- 10 9.11E- 10 4.65E- 09 3/172 3.27E- 08 ( 1,000) 1.07E- 1.87E- 12 13 15 2.16E- 2.02E- 1.38E- 14 10 13 2.11E- 2.25E- 1.45E- 11 4/162 14 (0,558) (0,205) 0,023 0,033 0,020 0,036 0,037 0,029 0,025 0,033 24.5 30 1.84E- 2.99E- 1.35E- 47 48 48 1.95E- 1.27E- 34 2.10E- 2.47E- 48 46 (1,0) 48 1.47E- 4.97E- 1.58E- 12 6/0 58 5.82E- 6.43E- 59 60 60 9.48E- 5.75E- 1.02E- 60 59 59 1.05E- 8.12E- 9.48E- 60 1 7/0 0,313 0,115 60 0,013 0,019 0,011 0,435 0,021 0,016 0,038 0,019 0,215 8/0 0,009 0,013 0,008 0,079 0,014 0,014 0,011 0,625 0,013 0,088 0,085 9/0 0,004 0,005 0,056 0,006 0,006 0,110 0,004 0,637 21.10 7.54E- 2.77E- 60 60 61 3.06E- 4.51E- 61 2.74E- 4.83E- 61 61 5,00 E- 61 3.87E- 61 (1,0) 11/0 0,129 4.51E- 0,047 61 0,005 0,008 0,235 0,008 0,009 0,314 0,006 0,238 0,201 0,122 0,007 12/0 0,465 0,043 0,048 0,048 0,009 0,008 0,047 13/31 4.56E- 1.11E- 13 14 2.45E- 7.16E- 16 10 1.67E - 12 4.48E- 17 (1,000) 08 1.37E- 8.37E- 7.40E- 16 16 14/19 50 4.71E- 1.73E- 50 (1,0) 51 2.82E- 1.71E- 3.02E- 51 51 3.12E- 51 2.41E- 51 2.11E- 2.82E- 51 0,168 0,112 15/0 51 0,207 0,010 0,006 0,261 0,161 0,009 0,008 0,060 58 16/0 1.58E- 5.82E- 6.43E- 59 60 60 9.48E- 5.75E- 60 1,02 E- 59 1.05E- 8.12E- 59 60 1 9.48E- 5.07E- 60 17/28 11 36 1.20E- 1.10E- 39 (1,0) 36 1.22E- 6.38E- 1.35E- 34 38 36 1.40E- 5.19E- 1.87E- 35 37 18/17 59 2.58E- 9.48E- 60 1.05E- 1.54E- 60 60 (1,0) 60 1.65E- 1.71E- 1.32E- 60 60 60 1.16E- 6.65E- 27 19/28 46 1.93E- 7.10E- 7.84E- 47 48 47 1.16E- 1.55E- 1.24E- 23 47 47 1.28E- 1.21E- 8.66E- 21 48 (1,0) 41 1,78 20/30 4.85E- E- 41 1.97E- 42 2.90E- 1.76E- 42 42 (1,0) 42 3.21E- 2.49E- 42 2.18E- 2.90E- 42 42 TAB. 4 - Cluster Constituants Rifkin, R. et A. Klautau (2004). Dans la défense d'un contre-toute classification. Journal of Machine Learning Research 5, 101-141. Tsoumakas, G. et I. Katakis (2007). Classification multi-label: Une vue d'ensemble. J. de données maison et des Mines 3-3 entrepôt. Ueda, N. et K. Saito (2003). modèles de mélange pour le texte Parametric multi-étiquettes. Les progrès réalisés dans NIPS 15, 721-728. Wang, H., M. H. X., et Zhu (2008). Un modèle probabiliste pour générative classifi- cation multi-étiquette. Intn'l Conf. sur l'exploitation minière de données (CISM), 628-637. Zhang, M. et K. Zhang (2010). apprentissage multi-label en exploitant la dépendance à l'étiquette. connais- sances Discovery dans les bases de données (KDD). Résumé En examinant relation étroite entre les nouvelles classes et des vecteurs de probabilité de marquage multiple des documents, on obtient la fonction de distribution de probabilité de chaque étiquette de documents. Avec la prise en charge de la distribution multinomiale sur les mots, on applique l'algorithme EM pour obtenir la distribution. Ensuite, nous appliquons le regroupement des probabilités d'étiquettes aux classes de mines. - 82 - B - Classification, Clustering, Classes Similarité Mining par multi-label Classification Yuichiro Kase, Takao Miura