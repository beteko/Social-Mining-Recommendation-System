(Actes_non_num \ 351rotes.pdf) Une approche pour la gestion des risques et de l'incertitude dans les problèmes de bandit manchot Stefano Perabo *, Fabrice Clérot * * France Télécom Division Recherche & Développement 2, avenue Pierre Marzin, 22307 Lannion Cedex stefano.perabo@orange.fr, fabrice .clerot @ orange-ftgroup.com Résumé. Une approche est présentée pour faire face aux risques multiarmed bandit problè- mes. Plus précisément, le dilemme exploration-exploitation bien connu est résolu à partir du point de vue de maximiser une fonction d'utilité qui mesure l'attitude du décideur envers le risque et les résultats incertains. Un lien avec la théorie de la préférence est ainsi établie. les résultats des simulations sont prévues afin de soutenir les principales idées et de comparer l'approche des méthodes existantes, en mettant l'accent sur le comportement à court terme (petite taille de l'échantillon) de la méthode proposée. 1 Introduction Un « problème de bandit manchot » peut être formulé comme suit: donnée pour t = 1, 2. . . T une séquence de vecteurs aléatoires K dimensions R (t) = [r1 (t). . . rk (t)], appelés récompenses et dont la distribution probabilité est inconnue a priori, l'objectif est de déterminer en ligne une sé- quence d'actions a (t) (également appelée stratégie ou politique) où chacun un (t) est un mesure discrète Vari-aléatoire définie sur l'ensemble {1, 2,. . . , K}, qui maximise l'espérance du gain cumulatif, G (T) = E [ΣT t = 1 ra (t) (t)], en observant, pour chaque t un (et seulement un) réalisation ra (t) (t) 1. la principale difficulté du problème réside dans le fait que la fonction objective ne sait pas à l'avance. En fait, si les moyens pA (t) = E [ra (t)] étaient disponibles, la meilleure stratégie serait évidemment de jouer l'action a * (t) = arg Maxa pA (t). Par conséquent, à chaque fois instant t, le choix d'une action est le résultat d'un compromis pour tenter d'estimer (apprendre) la fonction objective (en explorant les actions dont les récompenses moyen n'a pas encore été déterminée avec confiance fiance assez) et, en même temps, à la maximiser (en exploitant ceux qui, sur la base des observations précédentes, sont estimés à fournir les meilleures récompenses). Cela représente un problème de décision prototype où le décideur doit faire face à ce qu'on appelle dilemme exploration / exploitation: tout en poursuivant le deuxième objectif (exploitation) en utilisant, inéluctablement, une stratégie de suboptimale, il pourrait subir des pertes qui pourraient être évités si une meilleure esti- accouple des moyens de récompenses étaient disponibles; au contraire, tout en poursuivant le premier objectif (exploration) en utilisant une autre stratégie de suboptimale, il pourrait renoncer à jouer les supposés 1. Les caractères italiques comme r et représentent des réalisations des variables aléatoires correspondantes qui sont désignés en utilisant des caractères romains comme r et a. risque de manipulation meilleure action dans les problèmes de bandit trouvé jusqu'à présent. Ceci est un problème qui, avec de nombreuses variantes, peut modéliser des tâches de décision de base communément rencontrées dans l'allocation des ressources et à la commercialisation. Un exemple très populaire est emprunté à la communauté de la publicité sur Internet: le décideur doit afficher périodiquement une annonce sur une page web (l'action) en le choisissant à partir d'un ensemble connu d'annonces, l'objectif étant la tion maximisation du nombre de visiteurs clique sur l'annonce affichée (les récompenses), ce qui se traduit par la maximisation des revenus du décideur. Le dilemme exploration-exploitation provient du fait que les utilisateurs les intérêts ne sont pas connus à l'avance et donc, afin de juger quelle annonce attire le plus grand nombre de clics, chaque annonce doit être affichée (testée) un certain nombre de fois. Dans les scénarios plus réalistes, avant de prendre une décision, le décideur pourrait avoir accès à une sorte d'information latérale (contexte souvent appelé), comme dans l'exemple ci-dessus, un profil de l'utilisateur actuellement en visite la page Web ou une liste de mots-clés extrait de la page Web. Comment ces informations pourraient être utilisées afin de mieux choisir l'annonce à l'affichage représente une extension de le problème de bandit qui est généralement appelé un « problème contextuel de bandit ». Il peut être formalisé par l'introduction d'une séquence supplémentaire de quantités aléatoires, les contextes x (t), et en supposant qu'il existe une corrélation entre les contextes et les récompenses r (t). La question de ploration ex consiste donc à estimer cette corrélation afin de mieux prédire quelle est la meilleure action à effectuer conditionnellement à un contexte donné. Dans cet article, une approche est proposée pour faire face au dilemme exploration-exploitation des problèmes de bandit manchot. Son extension aux problèmes de bandit contextuel plus intéressant n'est pas poursuivi. Cependant, il devrait être clair pour les lecteurs intéressés par une telle extension que le même raisonnement pourrait être appliqué sans changements aussi en présence d'un contexte, les principales difficultés étant plus technique plutôt que la nature conceptuelle (ces difficultés, en notamment pour le traitement d'énormes ensembles de données nécessitant simultanément l'application des techniques de classification). De plus, l'accent est mis ici sur le cadre classique, où les récompenses r (t) sont modélisés comme indépendants et identiquement distribuées (IID) variables aléatoires, indépendamment de la stratégie de décideur a (t). En dehors de ces hypothèses, sont considérées comme des récompenses que bornées, qui est le cas ra (t) ∈ [0, 1] sans perte de généralité (cette hypothèse est pas vraiment nécessaire pour la méthode de travail, mais il est dit dans l'intérêt de comparer avec deux autres existants). 2 rapport à l'état de l'art La littérature sur les problèmes de banditisme depuis les travaux fondateurs de Robbins (1952) est abondante. Les principales contributions récentes à la solution du problème de bandit manchot avec IID et récompenses bornées, notamment les stratégies UCB1 (Auer et al., 2002) et UCB-V (Audibert et al., 2007), la garantie que le gain cumulé attendu G ( T) est délimitée inférieure par une fonction de la forme G * (T) - c log T, où c est une constante et G * (T) est la valeur maximale de la fonction objective. Comme le montre la Lai et Robbins (1985), l'expression logarithmique log T c (appelé regret) ne peut pas être améliorée plus loin dans le sens suivant: toute stratégie possible subit un regret qui est délimitée inférieure par une fonction O (log T). D'un autre point de vue, la stratégie SUCCESSIVEELIMINATION (Even-Dar et al., 2006) a été prouvé pour trouver l'action optimale S. Perabo et F. Clérot 10 1 10 2 10 3 10 4 10 5 0 0,2 0,4 0,6 T av . e x p é c é e c u m. g a la Fig. 1 - Un exemple de limites inférieures sur le gain cumulatif moyen attendu pour l'algorithmes de UCB1 (trait plein) et UCB-V (ligne en pointillés). Ces limites sont applicables au cas où des points sont distribués comme décrit dans la section §4 (le deuxième exemple avec K = 5, les récompenses ont support [0, 1] et G * (T) / T = 0,6). a * avec une probabilité prédéfinie arbitraire en un nombre fini d'étapes. Toutes ces stratégies ont été obtenues par le traitement (avec un aperçu mathématique considérable) les inégalités de concentration des sommes de variables aléatoires. Plus précisément, ces inégalités indiquent ce qui suit: si, à l'instant t l'action un a été joué na (t) fois, sa moyenne empirique, définie par pA (t) = Σt τ = 1 1 (un (τ) = a ) ra (τ) / na (t), est à une distance ε à partir de la moyenne vraie uA avec une plus grande probabilité de ou égale à une certaine valeur de seuil α-1. La forme fonctionnelle de ε et α, dépend de la nature de l'inégalité utilisée; cependant, il est généralement vrai que si α est maintenue fixe, alors ε diminue pour augmenter na, alors que si ε est maintenu fixe, puis augmente pour augmenter les a na. Par conséquent, par l'action de jeu une fois de plus, on est sûr d'augmenter la confiance avec laquelle est connu le vrai pA moyenne. L'art de la conception de la stratégie consiste donc à trouver le meilleur compromis entre deux objectifs contradictoires: l'échantillonnage afin de maximiser le gain et l'échantillonnage afin d'augmenter la confiance sur les moyens de récompenses. Le regret des stratégies mentionnées ci-dessus a été prouvé être très proche du mal pour opti- ar distributions de récompenses bitrary, parce qu'ils comptent sur les inégalités de concentration qui sont valables pour les distributions de probabilité arbitraires. Ceci est cependant aussi l'inconvénient principal de ces approches, parce que les régions de confiance provenant de ces inégalités peuvent être, dans certains cas, pratique, très conservatrice, ce qui dans les grandes constantes devant le tor T fac- journal du terme de regret. En conséquence, les bornes inférieures sur le gain deviennent utiles seulement à long terme, qui est la quantité G * (T) - c log T est seulement positif pour un horizon de temps suffisamment grand T. A titre d'exemple, la figure 1 représente graphiquement les moyennes des bornes inférieures (soit la quantité (G * (T) -c log T) / T) pour les algorithmes UCB1 et UCB-V dans le cas particulier où K = 5, et les récompenses sont répartis comme décrit dans la section §4, où seront présentés les résultats de certaines simulations numériques. Il est clair que, pour T <200 ne peut rien dire. La principale critique qui peut être adressée aux approches actuelles pour résoudre les problèmes de bandit est de se concentrer tous les efforts dans la limite supérieure du regret d'un algorithme aussi serré que possi- ble, qui dans les feuilles de fait pas de place pour prendre en compte les préférences de l'utilisateur et les contraintes. Tenez compte, risque de manipulation des problèmes de bandit en particulier, ce qui se passe au cours de la première période suivant le début d'un algorithme de bandit, où l'exploration est l'activité principale. La question est de savoir quand et comment passer de l'exploration à l'exploitation compte tenu des informations fournies par un nombre fini d'échantillons. La réponse dépend clairement de nombreux facteurs, les plus influents, peut-être, être: l'horizon de temps disponible (qui est combien d'autres échantillons peuvent tirer) et l'attitude du décideur à l'incertitude et des choix risqués. L'objectif de cet article est donc de proposer une procédure de conception de stratégie qui permet à ces Ørences et contraintes de préférences avoir un impact sur le comportement de l'algorithme et à prendre en compte en quelque sorte automatiquement. En particulier, le dilemme exploration-exploitation sera lated refor- que le problème de maximiser une fonction d'utilité qui quantifie les préférences du décideur sur un ensemble d'intervalles de confiance appropriés. Plus précisément, chaque intervalle de confiance de remplacement est associée à une estimation du gain futur qui pourrait être obtenu en jouant une certaine stratégie. Le rôle de la fonction d'utilité est donc de mesurer l'attitude face au risque et à l'incertitude, l'attitude qui entraîne le choix d'un intervalle de confiance. Une fonction d'utilité spécifique qui est conçu pour exprimer l'aversion pour les stratégies à risque est prévue. L'algorithme connexe sera appelé LCB1. Les résultats de certaines simulations numériques sont présentées afin de comparer le court terme (petit horizon T) performance de ce nouvel algorithme avec celui des algorithmes UCB1 et UCB-V précité. 3 Description de l'approche Pretend pendant un certain temps que les récompenses des distributions de probabilités sont connues. Il est possible de voir le problème de maximiser le gain G (T) comme un problème de planification d'horizon fini dans un processus de décision dégénéré MARKOV (MDP), qui est le problème de la détermination de la stratégie optimale (souvent appelée politique) dans un PEAD consistant un seul état déterministe 2. Il est connu (Puterman, 1987) que la stratégie optimale peut être trouvée par un algorithme d'induction en arrière. Si elle est appliquée ici, sous les hypothèses d'indépendance énoncés dans l'introduction, il revient simplement de trouver pour chaque instant t les fonctions Q * (a, t) (appelées fonctions d'action valeur optimale dans le jargon MDP) qui permettent de résoudre Q * (a, t) = E [ra (t) (t) + max b Q * (b, t + 1) ‖ a (t) = a] Q * (a, t + 1) = 0 (1) Le un fonctionnement optimal de la valeur d'action Q * (a, t) est égal au gain maximal attendu qui peut être ob- contenue dans l'intervalle [t, T], conditionnée par le fait que, à l'action de temps t a est joué. La formule ci-dessus peut donc se lire comme suit: afin de maximiser le gain futur sur l'intervalle [t, T], il suffit de jouer à l'instant t l'acte ion maximiser la récompense attendue E [ra (t)], puis de suivre la meilleure stratégie dans l'intervalle [t + 1, T]. En d'autres termes, la meilleure stratégie consiste à jouer avec la probabilité d'une action a * (t) = arg maxa Q * (a, t), pour tout t ∈ [1, T]. Dans ce cas, la solution de l'induction arrière donne évidemment Q * (a, t) = pA + (T - t) μ *, où μ * = Maxa pA, et confirme l'interprétation ci-dessus. 2. Dans un MDP il existe un processus stochastique x (t) défini par la probabilité de transition P [x (t + 1) = w‖x (t) = z, a (t) = a] (ce qui donne la probabilité d'atteindre un état suivant possible dans l'état actuel et de l'action) et le vecteur de récompenses dépendent de l'état actuel par la probabilité conditionnelle P [r (t) = r‖x (t) = z]. Le cadre de IID considéré ici est donc obtenue lorsque la transition est autorisée seulement une forme déterministe état x (t) ≡ z à lui-même, quelle que soit la mesure est prise. S. Perabo et F. Clérot Maintenant, supposons que le décideur est autorisé à jouer une stratégie aléatoire, qui est l'action a (t) est tiré d'une variable aléatoire prenant des valeurs dans l'ensemble {1, 2,. . . , K}. Définir pa (t) = P [a (t) = a] pour la probabilité de prendre des mesures une à l'instant t, ainsi que la quantité V * (t, p (t)) = Σ a Q * (a, t) P [a (t) = a] = Σ un μapa (t) + (T - t) * μ (2), où p (t) = [p1 (t). . . Pk (t)]. Il est clair que V * (t, p (t)) représente le maximum de gain attendu que l'on peut obtenir dans l'intervalle [t, T] à chaque fois qu'une stratégie randomisée est lue à l'instant t. En outre, V * (t, p (t)) ≤ maxa Q * (a, t) pour le choix de p (t), le maintien de l'égalité lorsque pa * (t) (t) = 1 (cas où V * (t, p (t)) = Q * (a * (t), t), quantité que l'on appelle souvent fonction de valeur optimale). Par conséquent, dans un problème de planification (qui est lorsque la distribution de récompenses sont connus) il n'y a aucun avantage à adopter une stratégie aléatoire par rapport à la valeur optimale obtenue istic un détermi- par l'induction en arrière décrit ci-dessus. Considérons alors le cas d'aucune connaissance préalable sur la distribution des récompenses. Ni les fonctions Q * (a, t) ni V * (t, p (t)) peuvent être évalués parce que les moyens pA ne sont pas connus. Cependant, on suppose que, pour une donnée α> 0 et pour chaque t, 100 (1 - α)% intervalle de confiance pour V * (t, p (t)), par exemple C (t, p (t)) = [ V * lo (t, p (t)), V * jusqu'à (t, p (t))] (3) peut être calculée à partir de l'ensemble d'observation {ra (s) (s)} sur l'intervalle [1 , t - 1]. Re- appeler le sens d'un intervalle de confiance: il existe un intervalle dont les extrémités sont des fonctions des données empiriques et qui contient avec une probabilité (1 - α) de la valeur réelle. En faisant varier p (t), différents intervalles de confiance sont obtenus. Il est donc raisonnable de définir la stratégie optimale p * (t) que la stratégie telle que la intervalle de confiance C (t, p * (t)) est préférable à tous les intervalles C (t, p (t)) obtenu pour p (t) = 6 * p (t). Par exemple, un intervalle [a + Aa, b + Ab] doit être préféré [a, b] chaque fois que Aa, Ab> 0 parce que le gain maximal prévu de la stratégie correspondante est susceptible d'être plus grande. Il est donc nécessaire de montrer comment ces intervalles peuvent être construits dans la pratique et de définir une relation de préférence sur des intervalles de confiance. En ce qui concerne la première question, ici une approche heuristique est adoptée. On sait que, comme na (t) → ∞, les moyens empiriques pA (t) = Σt τ = 1 1 (un (τ) = a) ra (τ) / na (t) ont tendance à être dis- tribué en tant que N (uA, σ 2 a / na (t)) variable aléatoire, qui est une gaussienne dont la moyenne et la variance sont respectivement le vrai uA moyenne et la variance σ 2 vraie divisée par le nombre d'échan- ples. La variance réelle peut aussi être remplacé par le σ2a empirique de la variance non biaisée (t) = Σt τ = 1 1 (un (τ) = a) (ra (τ) - pA (t)) 2 / (na (t) - 1). Même si ceux-ci sont ré- sultats asymptotiques, prétendre qu'ils représentent des approximations acceptables aussi pour un nombre fini d'échantillons na (t). Par conséquent, V * (t, p (t)) en (2) est sensiblement distribué comme N (μ (t), σ2 (t)) avec μ (t) = Σ un pA (t) pa (t) + ( T - t) μ * (t) σ 2 (t) = σ un σ2a (t) na (t) pa (t) 2 + (T - t) σ2 * (t) n * (t) (4) et où μ * (t) est le plus grand mea empirique n et σ 2 * (t) et n * (t) sont la variance empirique et le nombre d'échantillons de l'action correspondante. Pour un exemple numérique voir la figure 2. Les extrémités d'un 100 (1 - α)% intervalle de confiance sont donc V * lo (t, p (t)) = μ (t) - fσ (t) V * jusqu'à (t, p (t)) = μ (t) + fσ (t) (5) des risques de manipulation dans des problèmes de bandits 50,3 50,4 50,5 20,1 20,15 20,2 td s moyenne. d e v. FIGUE. 2 - Dans le graphique, les croix indiquent les couples (μ (t)), σ (t)) obtenus en faisant varier chaque probabilité de sélection d'action pa (t) dans l'intervalle [0, 1] en étapes de 0,1 sous la con- straint Σ une pa (t) = 1 et en utilisant les données suivantes: K = 3, [pA (t)] = [0,5, 0,4, 0,3], [oa (t) / √ na (t)] = [0,2 , 0,15, 0,1] et (T - t) = 100. les cercles représentent les valeurs ob- CONTENUES lorsque pa (t) = 1 pour un certain a ∈ {1, 2, 3}. où σ (t) = √ σ2 (t) et f est la 100 (1 - α)% quantile d'une gaussienne standard. Ensuite, une relation de préférence est nécessaire qui est en mesure de représenter l'attitude du décideur à l'incertitude représentée par le fait que, pour une stratégie donnée, le gain correspondant ne peut pas être évalué avec précision. Une relation de préférence est une relation binaire qui sera notée, de sorte que si l'intervalle D est préféré à intervalle C, on écrit D C. Le résultat classique est que, sous des hypothèses appropriées, il existe une représentation quantitative d'une relation de préférence en termes d'une fonction d'utilité réelle u définie sur l'ensemble des intervalles, par exemple I, tel que D C ⇐⇒ u (D)> u (C) (6) Outre les hypothèses techniques qui sont nécessaires pour tenir compte du fait que I est dénombrable ONU- (voir par exemple Fishburn (1999) pour plus de détails), il peut être prouvé que ce qui précède repré- sentation si et seulement si la relation sur I est un ordre faible. Cela signifie que est transitif (E D et D C ⇒ E C), irréflexive (C C) et ne détient que la relation ~ (définie par C ~ D si ni C, ni D D C) est aussi transitive. Dans la soi-disant approche normative de la modélisation des préférences (Tsoukias, 2008), qui sera suivie ici, l'existence d'une relation de préférence avec les propriétés ci-dessus est postulée (un décideur adhérant à une telle relation est appelée rationnelle). Ce montant, dans la pratique, pour définir la fonction d'utilité à une forme appropriée, en fonction de l'application à portée de main, en choisissant peut-être dans un ensemble de plusieurs bons choix possibles. En d'autres termes, l'existence d'une fonction d'utilité optimale n'est pas indiqué ici: différentes formes pourraient être raisonnable et se comporter aussi bien dans la pratique. Dans cet article, la suite fonction d'utilité très simple est proposé et testé par simulation numérique: u (t, p (t)) = exp (μ (t) - σ (t) T - t + 1) (7) S. perabo et F. CLEROT une α β uA oa 0,333 0,222 0,600 1 0,393 2 1,200 0,800 0,600 0,283 17,000 12,000 3 0,586 0,090 4 0,222 0,333 0,400 0,393 5 2,000 18,000 0,100 0,066 TAB. 1 - Les paramètres des distributions bêta qui ont été utilisées pour modéliser les résultats de récompenses dans les simulations numériques, ainsi que les attentes correspondantes et des écarts stan- dard Notez qu'il est une fonction paramétrique des intervalles de confiance pour V * (t, p (t)) par μ (t) et σ (t) qui sont les quantités utilisées pour définir l'intervalle lui-même. La raison de ce choix est le suivant. La tangente en un point sur une courbe de niveau de u est égal à 1 partout dans le plan (μ, σ). Par conséquent, le décideur est indifférent entre un intervalle de confiance spécifié par le couple (μ, σ) et un autre spécifié par (μ + Δμ, σ + Δσ), où Δμ et Δσ sont deux incréments liés par Δσ = Δμ. Étant donné que les limites inférieures des intervalles de confiance sont égaux, σ = u- (μ + Δμ) - (σ + Δσ), cela signifie que le décideur accepte de négocier une stratégie avec une autre (autrement dit, ils ont la même utilité) que si les intervalles de confiance partagent la même limite inférieure. limites plus bas sont préférés tandis que les limites supérieures sont ignorées, indiquant ainsi l'aversion aux stratégies risquées. En conclusion, le Propo stratégie sed, qui sera appelée LCB1 (pour « Lower Confidence Bound »), est le suivant: 1. pour 1 ≤ t ≤ 2K échantillon chaque action deux fois, de sorte que les variances empiriques authentiques peuvent être réglées à leur valeur initiale; 2. Pour chaque temps t> 2K choisir l'un d'action avec une probabilité p * a (t), où p * (t) = arg maxp u (t, p), et u (t, p) est la fonction d'utilité dans ( 7). 4 Quelques simulations numériques Dans cette section, les résultats de deux simulations numériques sont prévues. Cinq sont considérés comme des distributions de récompenses, qui appartiennent à la famille des distributions bêta Be (α, β). Les paramètres et les tracés des densités de probabilité correspondants sont présentés dans le tableau 1 et la figure 3 respectivement. L'horizon de temps est T = 200 dans les deux tests et f = 1,96 (ce qui donne un intervalle de confiance de 95% pour une variable aléatoire gaussienne norme). Dans le premier test K = 3, ce qui correspond à la récompense distributions 1 à 3. La figure 4 Les emplacements d'exposition de la distribution de probabilité FG (t) (g) = P [G (t) ≤ g] du gain obtenu après t = 50 , 100, 150 et 200 pas par les algorithmes UCB1 (ligne bleue), UCB-V (ligne noire) et le LCB1 algorithme proposé (ligne rouge). Ces distributions ont été obtenues par l'exécution de ces algorithmes plus de 1000 réalisations différentes des séquences de récompenses. Il n'y a pas de différences remarquables entre les trois algorithmes. En fait, des moyens de récompense sont comparables et la réduction du risque qui pourrait être obtenu par une action d'échantillonnage 3 plus sont souvent contrebalancés par une réduction du risque de manipulation des problèmes de bandits 0 0,2 0,4 0,6 0,8 1 0 2 4 6 0 0,2 0,4 0,6 0,8 1 0 2 4 6 8 FIG. 3 - Emplacements des récompenses densités de probabilité, sur la gauche pour a = 1, 2, 3 (lignes bleues, vertes et rouges, respectivement), et sur la droite pour a = 4, 5 (lignes cyan et magenta, respectivement). gain attendu. Dans le second test K = 5 et actions 4 et 5 ont été ajoutés à des actions 1, 2 et 3 du premier test. Le test a été effectué de la même manière que décrit ci-dessus. Parcelles de la distribution de probabilité du gain sont présentés à la figure 5. Contrairement au cas considéré dans le premier test, ici la performance de LCB1 est supérieure. En fait, comme il peut être vérifié dans la figure 6, LCB1 se débarrasse des actions inférieures 4 et 5 plus rapide que les deux autres algorithmes. 5 Discussion Une approche pour la solution du problème de bandit manchot dans le cadre IID a été proposé. Contrairement aux méthodes existantes bien connues, l'approche est quelque peu heuristique en ce sens qu'elle ne repose pas sur des preuves de convergence rigoureuses. Cependant, les simulations numériques montrent une très bonne performance à court terme, au moins dans les cas considérés, ainsi des analyses plus poussées motivat- ing. Dans les applications pratiques, le comportement à court terme est particulièrement important parce que l'hypothèse d'une répartition identique dans le temps peut être violé à long terme. Deux marges d'améliorations convient d'indiquer. D'une part, les intervalles de confiance de manière sont calculés: mieux établis échantillons techniques statistiques finies telles que le bootstrap, par exemple, pourraient prévoir des intervalles plus corrects et même asymétriques. D'autre part, les fonctions d'utilité peuvent exister que représenterait différents équilibres entre le risque et d'éviter les comportements à la recherche des risques, qui est différentes façons de gérer le dilemme exploration-exploitation. La principale contribution de cet article, cependant, est le lien établi avec la théorie des préférences qui a été et est actuellement un grand sous-champ de recherche en économie. En fait, l'approche présentée ici partage de nombreuses similitudes avec les méthodes de sélection très portefeuille de base (Markowitz, 1952) et la possibilité d'introduire des techniques plus avancées (le soi-disant ou aversion pour le risque sensible au risque) dans le contexte des problèmes de bandit devraient être considérés pour les enquêtes futures. Il convient également de mentionner la possibilité d'étendre le cadre IID pour permettre des récompenses à prix réduit, c'est-à résoudre un dilemme exploration-exploitation où, à chaque instant t, un temps de- S. Perabo et F. CLEROT 25 30 35 0,2 0,4 0,6 0,8 1 Gain G après 50 étapes P ro bab ili té d est tr ib u ti on FG 55 60 65 70 0,2 0,4 0,6 0,8 1 Gain G après 100 pas P ro bab ili té d est tr ib u ti on FG 80 85 90 95 100 0,2 0,4 0,6 0,8 1 Gain G après 150 marches P ro bab ili té d est tr ib u ti on FG 110 120 130 0,2 0,4 0,6 0,8 1 Gain G après 200 pas P ro bab ili té d est tr ib u ti on FG Fig. 4 - Les résultats du premier test (K = 3, les récompenses distributions 1 à 3): distribution du gain obtenu par le UCB1 d'algorithme (ligne bleue), UCB-V (ligne noire) et LCB1 (ligne rouge), après 50 , 100, 150 et 200 pas de temps. pendants fonction objective de la forme G (t) = E [ΣT (t) S = tw (t, s) · ra (s) (s)] doit être maximisée, où T (t) ≥ t et le réel le nombre w (t, s) sont des poids donné (seul le cas T (t) ≡ T et W (t, s) ≡ 1 a été discuté dans le présent document). En fait, les décisions impliquent très souvent des compromis entre les coûts et les avantages incertains qui se produisent à différents points dans le temps sous la contrainte des horizons temporels différents. Dans de tels cas, il est connu (Frederick et al., 2002) que le choix d'une séquence de pondération est liée aux préférences temporelles du décideur: différentes préférences exigent différentes formes de pondération. Il devrait être clair que, chaque fois que ces préférences sont connus (ainsi que la fonction d'utilité sur des intervalles de confiance), l'approche présentée peut être appliquée tout à fait sans détour. Références Audibert, J.-Y., R. Munos et C. Szepesvári (2007). Estimations de la variance et la fonction exploration bandit à plusieurs bras. Rapport technique 07-31, CERTIS, France. Auer, P., N. Cesa-Bianchi et P. Fischer (2002). Analyse finie temps du problème de bandit manchot. Machine Learning 47, 235-256. risque de manutention des problèmes de bandits 20 25 30 0,2 0,4 0,6 0,8 1 Gain G après 50 étapes P ro bab ili té d est tr ib u ti on FG 40 50 60 0,2 0,4 0,6 0,8 1 Gain G après 100 pas P ro bab ili té d est tr ib u ti on FG 70 80 90 0,2 0,4 0,6 0,8 1 Gain G après 150 marches P ro bab ili té d est tr ib u ti on FG 90 100 110 120 0,2 0,4 0,6 0,8 1 Gain G après 200 pas P ro bab ili té d est tr ib u ti on FG Fig. 5 - Les résultats du deuxième test (K = 5, les récompenses distributions 1 à 5): répartition du gain obtenu par le UCB1 d'algorithme (ligne bleue), UCB-V (ligne noire) et LCB1 (ligne rouge), après 50 , 100, 150 et 200 pas de temps. Même-Dar, E., S. Mannor et Y. Mansour (2006). l'élimination d'action et arrêter les condi- tions pour des problèmes d'apprentissage bandit à plusieurs bras et le renforcement. Journal of Machine Learning Research 7, 1079-1105. Fishburn, P. (1999). structures de préférence et leur représentation numérique. Theoretical Computer Science (217), 359-383. Frederick, S., G. Loewenstein et T. O'donoghue (2002). actualisation du temps et le temps la préférence: un examen critique. Journal of Economic Literature 40 (2), 351-401. Lai, T. et H. Robbins (1985). règles d'allocation adaptative asymptotiquement efficace. Adv. Appl. Math. 6, 4-22. Markowitz, H. (1952). sélection du portefeuille. Journal des Finances 7 (1), 77-91. Puterman, M. (1987). Programmation dynamique. Dans l'affaire R. Meyers (Ed.), Encyclopédie des sciences physiques et de la technologie, volume 4, pp. 438-463. Academic Press. Robbins, H. (1952). Certains aspects de la conception séquentielle d'expériences. Taureau. Amer. Math. Soc. 58 (5), 527-535. Tsoukias, A. (2008). De la théorie de la décision à la décision de la méthodologie Complicité. Revue européenne de recherche opérationnelle 187, 138-161. S. perabo et F. CLEROT 20 40 60 80 100 120 140 160 180 200 1 2 3 4 5 A c ti ona UCB1 20 40 60 80 100 120 140 160 180 200 1 2 3 4 5 A c ti ona UCB-V 20 40 60 80 100 120 140 160 180 200 1 2 3 4 5 Temps t A c ti ona LCB1 FIG. 6 - comportements typiques des algorithmes UCB1, UCB-V et LCB1 (de haut en bas) obtenu dans le deuxième test (K = 5, les distributions de récompenses 1 à 5). Un symbole « + » indic ates que l'action correspondante a été sélectionnée à l'instant t. Une approach is résumé présentéisme Pour la prise en considération du Erotisme Dans le Problème du « bandit manchot ». De plus précisement, le dit d'exploration Dilemme-exploitation is reformulée Comme un de maximisation d'Problème Une fonction d'utilité Qui l'attitude de du mesure Décideur le Envers et osée l'incertitude. Un lien Avec la théorie de la ÉTABLI Préférence is Fait. La méthode proposed is Testée et par simulation numérique comparée Avec d'Autres bien connues Dans la littérature, un with interest for the Particulier à comportement de cour terme (petit d'Échantillons Nombre).