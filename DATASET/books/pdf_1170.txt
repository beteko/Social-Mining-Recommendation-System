Semi-Supervisé incrémental Clustering de données catégorielles Dan Simovici * Namita Singla ** * Université du Massachusetts Department of Computer Science de Boston, Boston, MA 02125, USA dsim@cs.umb.edu ** Université du Massachusetts Department of Computer Science Boston, Boston , MA 02125, Etats-Unis de CV. Le semi-supervisez la mise en grappes combinent l'apprentissage et supervisez non supervisez verser clusterings PRODUIRE Meilleurs. Dans la phase de ini- tiale supervisee de l'algorithme, un échantillon d'apprentissage pro- is Duit de sélection par aléatoire. On suppose Que les exemples de l'échantillon d'apprentissage par un Sont étiquetés de classe attribut. Puis, un algo- rithme incrémentiel Développé répandrai les Données catégoriques is used Ensemble pour un atelier Produire de grappes pur (tells Que les exemple de each groupe have the same étiquette), Qui Servent de « grappes d'ensemencement » pour la phase deuxiéme non supervisee de l'algorithme. Dans la phase this, l'al gorithme incrémentiel is aux Appliqué non Données étiquetées. La qua- lité du regroupement par l'EST évaluée indice de Gini des grappes des Moyen. Les expériences des très Que démontrent bures clusterings PEUVENT Être des petits Avec obtenus d'apprentissage Échantillons. 1 Introduction Le regroupement est un procédé qui vise à des données de partition en groupes qui se compose d'objets similaires. Similarité entre objets est mesurée en utilisant une métrique définie sur l'ensemble des objets ou, lorsque cela est possible, en utilisant classifications préexistantes des objets. En général, le regroupement est une activité non supervisée. En d'autres termes, le regroupement a lieu sans aucune intervention d'un opérateur extérieur qui attribue des objets classes. En supposant que la classe d'un objet est déterminé par les autres caractéristiques de l'objet, un bon algorithme de regroupement devrait générer des grappes aussi homogènes que possible. Le noyau de l'algorithme de regroupement est la construction progressive d'une partition de regroupement de l'ensemble des objets de telle sorte que que la distance totale à partir de cette partition pour les partitions déterminées par les attributs est minime. Un défi particulier du regroupement des données catégoriques découle du fait qu'aucun ordre naturel existe sur les domaines des attributs des objets. Cela ne laisse que la distance de Hamming comme une mesure de dissemblance, un mauvais choix pour établir une discrimination entre les attributs à valeurs multiples d'objets. regroupement semi-supervisée de données catégoriques comporte deux phases: la première phase consiste en un procédé supervisé qui est appliqué à un ensemble d'apprentissage obtenu échantillonnage aléatoire du jeu de données. Les clusters sont formés en utilisant un algorithme de classification incrémentale RNTI-E-3189 Clustering semi-supervisée qui est appropriée pour les données catégoriques. Ensuite, ces grappes sont réparties en groupes homogènes, qui forment les pôles d'ensemencement pour la deuxième phase de l'algorithme. Dans la deuxième phase sans supervision, les objets sont progressivement ajoutés aux clusters existants sans utiliser l'étiquette de classe. Enfin, clusterings sont évaluées en utilisant l'indice de Gini. regroupement incrémentale peut être attribuée à (Hartigan 1975) et (Carpenter et al., 1990). Cela a été suivi d'un article fondateur de Fisher (Fisher 1987) qui a créé COBWEB, un algorithme de clustering incrémental que les restructurations impliqués des groupes en plus des ajouts supplémentaires d'objets. clusters supplémentaires liés aux aspects dynamiques de bases de données ont été discutées dans (Can 1993) et (Can et al., 1995). Il est également à noter que le regroupement progressif a été utilisé dans une variété d'applications (Langford et al., 2001), (Lin et al., 2004), (Charikar et al., 2997), (Ester et al., 1998) . L'autre paradigme principal appliqué ici, le regroupement semi-supervisé, a récemment reçu beaucoup d'attention (Cheung et Yeung 2004), (Bilenko et al., 2004), (Cohn et al., 2003), (Zhu et al., 2002 ), principalement liés à des données numériques. Nous mettons l'accent est mis ici sur les données catégoriques qui nécessite une ap spécifique proach. regroupement supplémentaire assure que la principale utilisation de la mémoire est minime, car il n'y a pas besoin de garder en mémoire les distances mutuelles entre les objets; Par conséquent, les algorithmes sont très évolutive par rapport à la taille de l'ensemble des objets et le nombre d'attributs. regroupement semi-supervisé, agissant comme une enveloppe pour le regroupement progressif sous-jacente améliore la qualité de la mise en grappes. 2 partitions et Clusterings Soit S un ensemble. Une partition sur S est une collection non vide de sous-ensembles non vides de S indexé par un ensemble I, π = {Bi | i ∈ I} tel que ⋃ i∈I Bi = S i et 6 = j implique Bi ∩ Bj = ∅. Les ensembles Bi sont les blocs de la partition π. L'ensemble des partitions sur S est notée par une partie (S). Pour π, σ ∈ PARTIE (S) nous écrivons π ≤ σ si chaque bloc B de π est inclus dans un bloc de σ, ou de façon équivalente, si chaque bloc de σ est une union exacte des blocs de π. Cette commande partielle génère une structure en treillis sur une partie (S); Cela signifie que pour tous les deux partitions n, π '∈ PARTIE (S), il existe au moins une partition π1 telle que π ≤ π1 et π' ≤ π1 et il y a une plus grande partition π2 de telle sorte que π2 ≤ π et π2 ≤ π. La première partition est désignée par π ∨ π ', tandis que la seconde est notée π ∧ π. Pour introduire une métrique sur l'ensemble des partitions d'un ensemble fini, nous définissons l'application v: PARTIE (S) - → R v (π) = Σni = 1 | Bi | 2, où π = {B1,. . . , Bn}. L'application v est une valeur plus faible sur une partie (S), qui est, v (π ∨ σ) + v (π ∧ σ) ≥ v (π) + v (σ) (1) pour π, σ ∈ PARTIE (S ) (voir l'annexe 5 pour une preuve). Pour chaque évaluation inférieur v la cartographie d: (PARTIE (S)) 2 - → R définie par d (π, σ) = v (π) + v (σ) - 2v (π ∧ σ) est une mesure sur une partie ( S) (voir (JP et B. Leclerc Barthélemy, 1995), (JP Barthélemy, 1978), (Monjardet, 1981) Une propriété particulière de cette mesure RNTI -. 1 RNTI-E-3 190 Dan Simovici et Namita Singla permet formulation d'un algorithme de classification supplémentaire qui est utilisée comme une partie de la grappe semi-supervisée. système d'objet An est une paire S = (S, H), où S est défini appelé l'ensemble des objets de S, H = {A1, ..., Am} est un ensemble de correspondances définies sur S. pour chaque mappage Ai (désigné comme attribut de S), il existe un ensemble non vide Ei appelé le domaine de telle sorte que Ai Ai: S - → Ei pour 1 ≤ . i ≤ m la valeur d'un attribut Ai sur un t objet est désigné par t [Ai] Ceci est cohérent avec la terminologie utilisée dans les bases de données relationnelles, où un tableau peut être considéré comme un système d'objet;. cependant, la notion d'objet système est plus général parce que obje CTS ont une identité en tant que membres de l'ensemble S, au lieu d'être considérée comme seulement m-uplets de valeurs. Dans cet esprit, nous appellerons t [Ai] comme projection de t sur Ai. Un attribut A d'un système d'objet S = (S, H) génère une partition πA de l'ensemble des objets S, où deux objets appartiennent au même bloc de πA si elles ont la même projection sur A. On note par BAA le bloc de π a qui se compose de l'ensemble des tuples de S dont l'un est a-composant. Notez que pour les bases de données relationnelles, πA est la partition de l'ensemble des lignes d'une table qui est obtenue en utilisant le groupe par une option de sélection dans la norme SQL. Un regroupement d'un système d'objet S = (S, H) est définie comme étant une partition de κ de S. Les blocs de la partition κ sont les grappes de κ. 3 A semi-supervisée incrémental Clustering algo- rithme Un regroupement semi-supervisée d'un système d'objet S = (S, H) commence par la consommation as- qu'un oracle fournit la valeur d'un attribut spécial K d'objets appelée classe de l'objet pour un sous-ensemble T de l'objet mis en S. Dans la première phase de l'algorithme un algorithme de classification incrémentale a est appliqué à l'objet ensemble T qui produit une agrégation initiale σ de cet ensemble. En général, ces groupes ne sont pas pures par rapport à la classe K, qui est, nous pouvons trouver dans les mêmes objets de classe qui ont des valeurs distinctes de l'attribut K. Ensuite, chacun des groupes de T est divisé en grappes pures. La cloison κ0 de T obtenue de la manière contient les grappes d'ensemencement pour le regroupement de l'ensemble des objets. le deuxième phase, sans surveillance de l'algorithme commence avec la partition de l'ensemble κ0 T. En utilisant l'algorithme de clustering incrémental, objets de l'ensemble S - T sont ajoutés aux clusters existants ou former de nouveaux groupes. L'attribut class (si existant) ne joue aucun rôle dans cette phase. La classification finale prolonge la cloison κ0 de T à une κ de séparation de regroupement de l'ensemble des objets. Nous commençons par discuter de notre algorithme de classification incrémentale. Pour un système d'objet S = (S, H) nous cherchons un groupement κ = {C1,. . . , Cn} ∈ partie (S) de telle sorte que la distance totale de κ aux cloisons des attributs: D (κ) = nΣ i = 1 d (κ, πAi) RNTI - 1 RNTI-E-3191 Clustering semi-supervisée a un minimum local. La définition de d nous permet d'écrire: D (κ) = nΣ i = 1 | Ci | 2 + mAΣ j = 1 | BAAJ | 2 - 2 nΣ i = 1 mAΣ j = 1 | Ci ∩BAaj | 2, Soit t un nouvel objet, t 6∈ S, et nous allons laisser Z = S ∪ {t}. Pour former un regroupement de l'ensemble Z l'objet t peut ajouter à un cluster Ck existant ou un nouveau cluster + 1 Cn, peut être créé qui se compose uniquement de t. Si t est ajouté à un cluster Ck existant, le nouveau cluster est κ (k) = {C1,. . . , Ck-1, Ck ∪ {t}, Ck + 1,. . . ,} Cn, et la nouvelle partition d'attribut est πA '= {Baa1,. . . , BAt [A] ∪ {t},. . . , BAamA} Maintenant, nous avons: d (κ (k), πA ') - d (κ, πA) = (| Ck | + 1) 2 - | Ck | 2 + (| BAt [A] | + 1) 2 - | BAt [A] | 2 - 2 (2 | Ck ∩BAt [A] | + 1) = 2 | Ck | + 1 + 2 | BAt [A] | + 1- 4 | Ck ∩BAt [A] | - 2 = 2 | Ck ⊕BAt [A] |, où ⊕ est la différence symétrique des ensembles donnés par X ⊕ Y = (X ∪ Y) - (X ∩ Y) pour tous ensembles X, Y. Lorsque t est la formation d'un nouveau cluster, nous avons les partitions de '= {C1,. . . ,. . . , Cn, {t}} πA '= {Baa1,. . . , BAt [A] ∪ {t},. . . , BAamA} qui rendement d (κ ', πA') - d (κ, πA) = 2 | BAt [A] |. Par conséquent, D (κ ') - D (κ) = {Σ A 2 · | Ck ⊕BAt [A] | dans l'affaire 1Σ A 2 · | BAt [A] | dans l'affaire 2. Ainsi, le choix entre ajouter un objet à un cluster existant et la création d'un nouveau cluster est basée sur la comparaison des nombres min k Σ A | Ck ⊕BAt [A] | et Σ A | BAt [A] |. Si le premier nombre est plus petit, on ajoute t à un cluster Ck pour lequel Σ A | Ck ⊕ BAt [A] | est minime; autrement, nous créons un nouveau groupe d'un objet. RNTI - 1 RNTI-E-3 192 Dan Simovici et Namita Singla entrée: ensemble de données S, fraction de p de consigne surveillée, « « ne sont pas encore » » seuil α sortie: regroupement C1,. . . , Cn Méthode: obtenir un échantillon aléatoire d'objets T de l'ensemble des objets S telle que | T || S | = P; calculer le regroupement des semences de l'ensemble T κ0 = {D1,. . . , D`} = A (T, α) calculer la classification finale κ = C (S, T, κ0, α) Fig. 1 - Pseudocode de l'algorithme de classification semi-supervisée Pour les algorithmes de classification incrémentielles certains ordres d'objets peut entraîner plutôt pauvres clusterings. Pour diminuer le problème de l'effet de commande nous élargissons la rithme algo- initiale en adoptant la technique « non encore » introduit dans (Roure et Talavera, 1998). Un nouveau cluster est créé uniquement lorsque l'effet d'ajouter l'objet t sur la distance totale est assez importante. Tel est le cas lorsque P A | BAt [A] | vison P A | Ck⊕BAt [A] | <Α, où α ≤ 1 est un paramètre fourni par l'utilisateur. Dans le cas contraire, le t objet placé dans un tampon non encore. Toutes les expériences décrites à la section 4 utilisées α = 0,95. Lorsque P A | BAt [A] | vison P A | Ck⊕BAt [A] | > 1, le t objet est placé dans un cluster Ck existant qui minimise Σ A | Ck ⊕BAt [A] |. Cette approche limite le nombre de nouveaux clusters singleton qui seraient autrement créés. Après que tous les objets de l'ensemble S ont été examinés, les objets contenus par les α = 1 avec un tampon non encore sont traités Cela empêche de nouvelles insertions dans la mémoire tampon et les résultats dans les deux placer ces objets dans des groupes existants ou de créer de nouveaux clusters. Ainsi, la construction de la classification finale de κ S commence par une cloison de classification initiale κ0 d'un sous-ensemble T et avec un paramètre α. On note la classification finale par κ C (S, T, κ0, α). La partition créée sur le premier ensemble d'objets T est désigné par κ0 = A (T, α), et il utilise le même algorithme que ci-dessus. L'algorithme est donnée suivante: 4 Résultats expérimentaux Nous avons appliqué le regroupement semi-supervisé à plusieurs bases de données catégoriques obtenues à partir de l'ensemble de données UCI (C. L. Blake et C. J. Merz, 1998). Chaque expérience a été appliquée à l'aide d'une série de pourcentages de plus en plus pour l'ensemble de données semi-supervisé, en moyenne sur cinq échantillons aléatoires. La qualité du regroupement des données catégoriques nécessite un spécialiste Treatmet RNTI - 1 RNTI-E-3193 clustering semi-supervisé car les distances entre les objets ne peuvent pas être définis naturellement. Nous avons évalué clusterings en utilisant l'indice de Gini moyenne des grappes (Demiriz et al., 1999). Soit K l'attribut class et laisser {BKK1,. . . , BKkp} la partition de l'ensemble d'objets S. La classe-impureté d'un ensemble d'objets U est défini comme l'indice de Gini de la « partition de trace » {U ∩BKkj | 1 ≤ j ≤ p}: giniK (U) = 1- j = 1 pΣ (| U ∩BKkj | | U |) 2. Notez que si un groupe U est pur, qui est, il contient des objets qui appartiennent à une seule classe, puis giniK (U) = 0. Pour un regroupement κ = {U1,. . . , U`} de l'ensemble des objets S l'indice de Gini est donnée par impK (κ) = Σ i = 1 | Ui | | S | giniK (Ui). De toute évidence, les faibles valeurs de impK (κ) indiquent une bonne clusterings. L'algorithme a été appliqué à l'ensemble de données CHAMPIGNONS. Cet ensemble de données contient 8124 enregistrements de champignons et est généralement utilisé comme ensemble de test pour les algorithmes de classification, où la tâche est de construire un classificateur qui est capable de prédire le caractère toxique / comestible des champignons sur la base des valeurs des attributs des champignons. Les groupes montrent un degré assez remarquable de pureté. Par exemple, pour une partie semi-supervisée de 10%, nous avons obtenu les groupes suivants: Cluster instances edib./pois. Pour cent du numéro de groupe dominant 1 4225 3575/650 84,615 2 165 0/165 100 3 3055 0/3055 100 4 394 393/1 99,746 5 2 0/2 100 6 55 48/7 87,273 7 36 0/36 100 8 192 192 / 0 100 Il est tout à fait remarquable que cinq des huit groupes obtenus de cette manière sont pures et les grappes restantes ont un degré élevé de pureté. Pour d'autres dimensions de l'échantillon sous surveillance, nous avons obtenu les résultats suivants: RNTI - 1 RNTI-E-3 194 Dan Simovici et Namita Singla 0,08 0,09 0,1 0,11 0,12 0,13 0,14 0,15 0,16 5 10 15 20 25 30 pourcentage d'impuretés de l'échantillon Impureté par rapport à l'échantillon Taille des données champignons? ? ? ? ? ? ? Fig. 2 - Impureté Diminuer Taille de l'échantillon pour les champignons base de données de champignons Pourcentage Nombre de grappes Temps Impureté supervisé (ms) 5% 8 0,15362536 2443 10% 8 0,1371508 2454 15% 8 0,12705285 2444 20% 8 0,10735634 2374 25% 9 0,09911141 3545 30% 9 0.0816238 3415 la dépendance de la mesure d'impureté sur la fraction de l'échantillon surveillée est représenté sur la figure 2. Un analogue, mais une amélioration plus lente de la qualité de la grappe peut être observée pour ZOO, un autre ensemble de données catégoriques de UCI (CL Blake et CJ Merz, 1998): RNTI - 1 RNTI-E-3195 pourcentage semi-supervisée Clustering 0,11 0,115 0,12 0,105 0,125 0,13 0,135 0,14 0,145 0,15 5 10 15 20 25 30 impuretés de l'échantillon par rapport à l'impureté Taille de l'échantillon solaire Fusées? ? ? ? ? ? ? Fig. 3 - Impuretés Diminuer Taille de l'échantillon pour SOLAR FLARES base de données Zoo Pourcentage Nombre de grappes Temps Impureté supervisé (ms) 5% 3 0,39802246 110 10% 4 0,37841779 90 15% 4 0,37841779 110 20% 6 0,28431165 130 25% 7 0,33374854 141 30 % 7 0.33981398 114 la variation des impuretés moyennes pour cinq expériences avec chaque taille de l'échantillon pour la base de données SOLAR_FLARES est représenté sur la figure 3. 5 Conclusion et travaux futurs semi-supervisée regroupement incrémentale est un algorithme de classification efficace des données tegorical ca- qui génère presque grappes homogènes par rapport aux classifications basées sur des valeurs d'attribut. Une idée naturelle pour le développement de l'approche semi-supervisée serait d'utiliser un modèle renforcé (Freund, 1995) du regroupement progressif semi-supervisé où plusieurs petits échantillons de formation seraient utilisés pour générer clusterings; un objet serait alors classés en fonction de ses positions par rapport à l'ensemble des clust ers. RNTI - 1 RNTI-E-3 196 Dan Simovici et Namita Singla Nous explorerons le regroupement progressif semi-supervisé dans le contexte de flux regroupement des objets, qui est un type de données importantes dans le secteur minier Internet et la sécurité du réseau. L'ordre des objets est hors de propos dans ce domaine puisque les objets doivent être traités à leur arrivée. Références J.P. et B. Leclerc Barthélemy, la procédure médiane pour les partitions, dans Cloisonnement ensembles de données, pages 3-34, Providence, 1995, American Mathematical Society. J.P. Barthélemy, sur les Remarques Métriques des ensembles Propriétés ordonnés, Math. Sci. . Hum, 61: 39-60, 1978. M. Bilenko, S. Basu, et RJ Mooney, intégration des contraintes et de l'apprentissage métrique dans le regroupement semi-supervisé, à la Conférence internationale sur l'apprentissage machine, Banff, Canada, 2004. G. Birkhoff , Théorie Lattice, American Mathematical Society, Providence, 1973. CL Blake et CJ Merz, UCI Repository des bases de données d'apprentissage machine, Université de Californie, Irvine, Département des sciences de l'information et de l'ordinateur, http: //www.ics.uci.edu /~mlearn/MLRepository.html, 1998. F. Can, EA Fox, CD Snavely, et RK France, regroupement incrémentiel pour les bases de données de documents très importants: Une première expérience de MARIAN, Inf. Science, 84:. 101-114, 1995. F. Can, regroupement incrémental pour le traitement de l'information dynamique, ACM Transaction pour les systèmes d'information, 11: 143-164, 1993. G. Carpenter et S. Grossberg, Art3: recherche hiérarchique utilisant chimique émetteurs dans les architectures de reconnaissance des formes d'auto-organisation, réseaux de neurones, 3: 129-152, 1990. M. Charikar, C. Chekuri, T. Feder, et R. Motwani, regroupement incrémental et la recherche d'information dynamique, en STOC, pages 626- 635, 1997. H. Cheung et DY Yeung, localement linéaire adaptation métrique pour semi-supervisé Tering assorti-, à la Conférence internationale sur l'apprentissage machine, Banff, Canada, 2004. D. Cohn, R. Caruana, et A. McCallum, semi le regroupement -supervised avec les commentaires des utilisateurs, rapport technique, 2003. A. Demiriz, KP Bennett, et ME Embrechts, semi-supervisé le regroupement en utilisant des algorithmes gé- Netic, rapport technique Math 9901, Rensselaer Institut polytechnique, Troy, New York, 1999. M . Ester, HP Kriegel, J. Sander, M. Wimmer, et X. Xu, Incre regroupement mentale pour l'exploitation minière dans un environnement d'entreposage de données, VLDB, pages 323-333, 1998. D. Fisher, l'acquisition des connaissances par le regroupement conceptuel incrémental, machine Mémorisation en, 2: 139-172, 1987. Y. Freund, un Dynamiser faible algorithme d'apprentissage à la majorité, l'information et la mise Compu, 121: 256-285, 1995. JA Hartigan, Clustering algorithmes, John Wiley, New York, 1975. RNTI - 1 RNTI-E-3197 semi-Supervisé Clustering T. Langford, CG Giraud-Carrier, et J. Magee, détection des éclosions de maladies infectieuses dans les hôpitaux à travers le regroupement progressif, dans les Actes de la 8e Conférence sur la grippe aviaire en médecine (AIME), pages 30-39. Springer, 2001. J. Lin, M. Vlachos, EJ Keogh, et D. Gunopulos, regroupement progressif de séries itératives de temps, dans EDBT, pages 106-122, 2004. B. MONJARDET, métriques sur des ensembles ordonnés - une enquête, Mathématiques discrètes, 35: 173-184, 1981. J. Roure et Luis Talavera, regroupement progressif robuste avec de mauvais exemple ordonnancements: Une nouvelle stratégie, en IBERAMIA, pages 136-147, 1998. Z. Zhu, Y. Pilpel, et GM Eglise, l'identification numérique des sites de liaison de facteur de transcription via un algorithme en cluster centré sur la transcription du facteur (TFCC), Journal of Molecular Biology, 318: 71-81, 2002. Une preuve de l'inégalité (1) Soit π, σ deux partitions de l'ensemble fini S, de telle sorte que π = {B1,. . . , Bm} et σ = {C1,. . . ,} Cn. Il est connu (voir (Birkhoff 1973), par exemple) que π ∧ σ est constitué par tous les ensembles de la forme Bi ∩ Cj tel que Bi ∩ Cj 6 = ∅. D'un autre côté, π ∨ σ a une description plus compliquée; à savoir, x, y ∈ S appartiennent au même bloc D de π ∨ σ s'il existe une suite d'éléments de S, z0,. . . , Zk tel que x = z0, zk = y et pour chaque paire (zp, zp + 1) il y a un bloc Bi de π ou un bloc Cj de σ telle que les deux zp et ZP + 1 appartiennent à Bi ou à Cj pour 1 ≤ p ≤ k - 1. Considérons le graphe biparti Gπ, σ dont l'ensemble des sommets est constitué par les blocs de π et les blocs de σ . Un bord (Bi, Cj) existe uniquement si Bi ∩ Cj 6 = ∅. Si K est une composante connexe de ce graphique, il est facile de voir que ⋃ {Bi ∈ π | Bi ∈ K} = {⋃ Cj ∈ σ | Cj ∈ K}. En outre, chaque bloc D de π ∨ σ est égal à l'union des blocs de π (ou les blocs de σ) qui appartiennent à une composante connexe K de Gπ, σ. Exemple 5.1 Soit S = {ai | 1 ≤ i ≤ 12} et laisser π = {Bi | 1 ≤ i ≤ 5} et {σ = Cj | 1 ≤ j ≤ 4}, où B1 = {a1, a2}, C1 = {a2, a4}, B2 = {a3, a4, a5}, C2 = {a1, a3, a5, a6, a7}, B3 = {a6, a7}, C3 = {a8, a11}, B4 = {a8, a9, a10}, C4 = {a9, a10, a12}, B5 = {a11, a12}. Le graphique Gπ, σ représenté sur la figure 4 comporte deux éléments connectés qui correspondent aux blocs D1 = {a1, a2, a3, a4, a5, a6, a7} = B1 ∪B2 ∪B3 = C1 ∪ C2, D2 = {a8 , a9, a10, a11, a12} = B4 = ∪B5 C3 ∪ C4. RNTI - 1 RNTI-E-3 198 Dan Simovici et Namita Singla a11, a12 a8, a9, a10 a6, a7 a3, a4, a5 a1, a2 a9, a10, a12 a8, a11 a1, a3, a5, a6, a7 a2, a4 B1 B2 B3 B4 B5 C1 C2 C3 C4 Fig. 4 - le graphique Gπ, σ de la partition π ∨ σ. La partition π ∧ σ est composé de 9 blocs qui correspondent aux arêtes du graphe: B1 ∩ C1 = {a2}, B1 ∩ C2 = {a1}, B2 ∩ C1 = {a4}, B2 ∩ C2 = {a3, a5 }, B3 ∩ C2 = {a6, a7}, B4 ∩ C3 = {} a8, B4 ∩ C4 = {a9, a10}, B5 ∩ C3 = {} a11, B5 ∩ C4 = {a12}. Laissez D1,. . . , Le Dr être les blocs de la partition π ∨ σ. Pour un bloc Dk définir les ensembles Ik ⊆ {1,. . . , M} et {1 Jk ⊆,. . . , N} où Ik = {i | Bi ∩ Dk 6 = ∅} et {Jk = j | RNTI - 1 RNTI-E-3199 Semi-Clustering Bi ∩Dk supervisé 6 = ∅}. Notez que v (π ∨ σ) = rΣ k = 1 | Dk | 2, v (π ∧ σ) = rΣ k = 1 Σ Σ i∈Ik j∈Jk | Bi ∩ Cj | 2, v (π) = rΣ k = 1 Σ i∈Ik | Bi | 2 = rΣ k = 1 Σ i∈Ik   Σ j∈Jk | Bi ∩ Cj |   2, v (σ) = rΣ k = 1 Σ j∈Jk | Cj | 2 = rΣ k = 1 Σ j∈Jk (Σ i∈Ik | Bi ∩ Cj |) 2. Il est immédiat de vérifier l'inégalité: (Σ i∈Ik Σ j∈Jk | Bi ∩ Cj |) 2 + Σ Σ i∈Ik j∈Jk | Bi ∩ Cj | 2 ≥ Σi∈Ik (Σ j∈Jk | Bi ∩ Cj |) 2 + Σ j∈Jk (Σ i∈Ik | Bi ∩ Cj |) 2 Ceci équivaut à: | Dk | 2 + Σ Σ i∈Ik j∈Jk | Bi ∩ Cj | 2 ≥ Σi ∈Ik (Σ j∈Jk | Bi ∩ Cj |) 2 + Σ j∈Jk (Σ i∈Ik | Bi ∩ Cj |) 2 Ajout les inégalités similaires pour 1 ≤ k ≤ r on a l'inégalité voulue: v ( π ∨ σ) + v (π ∧ σ) ≥ v (π) + v (σ)., Résumé regroupement semi-supervisé combinats supervisé et apprentissage non supervisé aux pro- Duce mieux clusterings. Dans la phase initiale surveillée de l'algorithme proposé un ensemble d'entraînement est généré par l'échantillonnage. On suppose que les exemples de l'ensemble de la formation sont marquées par un attribut de classe. Ensuite, un algorithme incrémental développé pour les données catégorielles est utilisé pour produire un ensemble de clusters purs (tels que les instances de chaque groupe ont la même étiquette) qui servent de « grappes d'ensemencement » pour la deuxième phase unsu- pervised. Dans cette phase, l'algorithme incrémental est appliqué aux données non marqué. La qualité du regroupement est évaluée par l'indice de Gini moyen des grappes. Les expériences montrent que de très bonnes clusterings peuvent être obtenus avec des ensembles de formation relativement faible. RNTI - 1 RNTI-E-3 200