 Classification topologique probabiliste pour des données catégorielles Nicoleta Rogovschi et Mohamed Nadif LIPADE Université Paris Descartes 45 rue des Saints Pères 75006 Paris France Prénom Nom parisdescartes fr Résumé Cet article présente une carte auto organisatrice probabiliste pour l’ana lyse et la classification topologique des données catégorielles En considérant un modèle de mélanges parcimonieux nous introduisons une nouvelle carte auto organisatrice SOM probabiliste L’estimation des paramètres de notre modèle est réalisée à l’aide de l’algorithme EM classique Contrairement à SOM l’al gorithme d’apprentissage proposé optimise une fonction objective Ces perfor mances ont été évaluées sur des données réelles et les résultats obtenus sont encourageants et prometteurs à la fois pour la classification et pour la modélisa tion 1 Introduction Data visualization is an important step in the exploratory phase of data analysis This step is more difficult when it involves binary data and categorical variables Andreopoulos et al 2006 Saund 1995 Self organizing maps are being increasingly used as tools for visualiza tion as they allow projection over small areas that are generally two dimensional The basic model proposed by Kohonen 2001 was only designed for numerical data but it has been successfully applied to treating textual data Kaski et al 1998 This algorithm has also been applied to binary data following transformation of the original data Ibbou et Cottrell 1995 Lebbah et al 2000 Developing generative models of the Kohonen map has long been an important goal These models vary in the form of the interactions and they assume the hid den generators may follow in generating the observations Some extensions and reformulations of the Kohonen model have been described in the literature They include probabilistic self organizing maps Anouar et al 1997 which define a map as a gaussian mixture and use the maximum likelihood approach to define an iterative algorithm In Verbeek et al 2005 the authors propose a probabilistic generalization of Kohonen’s SOM which maximizes the variational free energy that sums data log likelihood and Kullback Leibler divergence between a normalized neighbourhood function and the posterior distribu tion on the given data for the components We have also Soft topographic vector quantization STVQ which uses some measure of divergence between data items and cells to minimize a new error function Heskes 2001 Graepel et al 1998 Another model often presented as the probabilistic version of the self organizing map is the Generative Topographic Map GTM Bishop et al 1998 Kaban et Girolami 2001 However the manner in which GTM achieves 359 Classification topologique probabiliste pour des données catégorielles the topographic organization is quite different from those used in the SOM models In GTM mixture components are parameterized by a linear combination of nonlinear functions of the locations of the components in the latent space The GTM was developed for continuous data A specific GTM model was subsequently developed for binary data by adopting a variational approximation to the binomial likelihood Graepel et al 1998 Also in Kaban et al 2004 the authors concentrate on modelling binary coded data where only the presence or absence of a variable is of interest In contrast to other approaches the model is linear The model is seen as a Bernoulli analogue of the multinomial decomposition model In Jollois et Nadif 2007 the main of the proposed method is to speed up convergence of EM and second to yield same results or not so far than traditional EM using categorical data Others similar techniques have been developed to cluster large data sets Kostiainen et Lampinen 2002 Hofmann 2001 Here we concentrate on modelling qualitative data using binary coding This model in volves use of the probabilistic formalism of the topological map used in Anouar et al 1997 therefore it consists of estimating the parameters of the model by maximizing the likelihood of the data set The learning algorithm that we propose is an application of the EM standard algorithm McLachlan et Krishman 1997 Some variants are proposed to speed up EM in reducing the time spent in the E step in the case of categorical data Jollois et Nadif 2007 In this paper we proposed a new method called WeCSOM Weighted Categorical Self Organizing Map which combine the benefits of SOMs K mode Huang 1998 algorithm and mixture models to design a new mixture for categorical data This approach is based on the model of the self organizing maps and uses a parsimonious mixture models which has the advantage of being directly applicable to the categorical data without using a specific encoding a priori The proposed learning algorithm is an application of the classical EM algorithm that allows to weight the variables considering the number of modes of each one during the learning process thus achieving an optimized classification of the data The rest of this paper is organized as follows we present the principle of probabilistic map and categorical data in section 2 Our proposed approach is presented in sections 2 1 and 2 2 In sections 3 we present different results and finally the paper ends with a conclusion and some future works for the proposed methods 2 Categorical data and Probabilistic self organizing map As with a traditional self organizing map we assume that the lattice C has a discrete topo logy discrete output space defined by an undirect graph Usually this graph is a regular grid in one or two dimensions We denote the number of cells in C as Ncell For each pair of cells c r on the map the distance δ c r is defined as the length of the shortest chain linking cells r and c 2 1 General probabilistic formalism To define the model of topological maps based on mixture models we associate to each cell c of the map C a density function fc x = p x|θc whose parameters are denoted by θ Follo wing the bayesian formalism presented in Luttrel 1994 Anouar et al 1997 we assume that 360 Rogovschi et al each observation x is generated by the following process We start by associating to each cell c ∈ C a probability p x|c where x is a vector in the data space Next we pick a cell c from C according to the prior probability p c For each cell c we select an associated cell c ∈ C following the conditional probability p c|c All cells c ∈ C contribute to the generation of x with p x|c according to the proximity to c described by the probability p c|c Thus a high proximity to c implies a high probability p c|c and therefore the contribution of c to the generation of x is high Due to the "Markov" property p x|c c = p x|c the probability distribution of the ob servations generated by a cell c of C is a mixture pc x|c of probabilities completely defined from the map as pc x|c = ∑ c∈C p c|c p x|c The generative model considers the mixture of probabilities given by p x = ∑ c c ∈C p c c x = ∑ c c ∈C p x|c p c|c p c = ∑ c ∈C p c pc x 1 with pc x = p x|c = ∑ c∈C p c|c p x|c 2 where the conditional probability p c|c is assumed to be known To introduce the self organizing process in the mixture model learning we assume that p c|c can be defined as p c|c = K T δ c c ∑ r∈CK T δ r c where KT is a neighbourhood function depending on the parameter T called temperature KT δ = K δ T where K is a particular kernel function which is positive and symmetric lim |x|→∞ K x = 0 ThusK defines for each cell c a neighbourhood region in C The parameter T allows control of the size of the neighbourhood influencing a given cell on the map As with the Kohonen algorithm we decrease the value of T between two values Tmax and Tmin 2 2 The proposed model In the following let we focus on categorical data Let be a set of N instances x1 xN described by n categorical attributes x1 xn The data matrix is noted x and defined by x = { xji i = 1 N k = 1 n} Each instance i is represented as [x1i xni ] and for each attribute xj we note cj the number of categories We consider a restricted latent class model [16] then the conditional distribution in p xi|c is now given as the product of univariate single distributions p xi|c = fc xi|wc �c = n∏ k=1 fc x k i |wkc �kc where wc = w1c w n c represents the vector of categories and �c = ε 1 c ε n c is a vector of probabilities Taking fc x k i |wkc εkc = 1− εkc 1−d x k i w k c εkc ck − 1 d xki wkc 361 Classification topologique probabiliste pour des données catégorielles where d a b = 1 if a = b and 0 otherwise we define a parsimonious model where the parame ter c consists of wc �c with wc is the mode of the the component and �c is a k dimensional vector of probabilities indicating the degree of heterogeneity The density fc xi|wc �c ex presses that for c the attribute xk takes category wkc with the greatest probability 1− �kc and takes each other category with the same probability � k c ck−1 Note that setting the clustering problem under the classification maximum likelihood approach the authors in [16] have defi ned a generalization of the kmodes criterion and proposed better fit criteria In our situation we can assume that the parameter εkc depends only on a cell c ∈ C Then the model mixture generator becomes p x = ∑ c ∈C p c ∑ c∈C p c|c fc x wc �c 3 Therefore the parameters θ = θC ∪ θC which define the model mixture generator 3 are constituted of the parameters θC = {θc c = 1 Ncell} where θc = wc �c and all the prior probabilities also called mixing coefficients θC = {θc c = 1 Ncell} where θc = p c The difficulty now is to define the cost function and the learning algorithm for estimating all these parameters dedicated to categorical data Our WeCSOM algorithm was inspired by a probabilistic SOM model proposed by Anouar et al 1997 and represents a generalization of the model proposed by Lebbah et al 2007 2 3 Cost function and optimization algorithm The learning algorithm is based on maximizing the likelihood of the observations by ap plying the EM algorithm Dempster et al 1977 Learning is facilitated by introducing N hidden variables Ξ = ξ1 ξN each hidden variable ξ = c c indicates which of the cell pairs c and c generate the corresponding data observation x We introduce the hidden variable ξ = c c in expression 3 p x = ∑ ξ∈C×C p x ξ = ∑ c c ∈C p c p c|c fc x wc εc 4 We define a binary indicator variable α c c i which indicates the hidden generator that may follow in generating the observation xi as α c c i = { 1 for ξi = c c 0 otherwise Using expres sion 4 and the binary indicator α c c i we can define the classification likelihood of the observations using the hidden variables as follows LT Ξ θ = N∏ i=1 ∏ c ∈C ∏ c∈C [ θc p c|c fc x wc �c ]α c c i The log likelihood becomes lnLT Ξ θ = N∑ i=1 ∑ c c ∈C α c c i [ ln θc + ln KT δ c c Tc + ln fc x wc �c ] 362 Rogovschi et al where Tc = ∑ r∈CK T δ r c The application of the EM algorithm [7] for the maximiza tion of log likelihood requires QT θt θt−1 to be maximised for a fixed temperature T defined as QT θt θt−1 = E [ lnLT Ξ θt | θt−1 ] where θt is the set of the parameters estimated at the tth step of the learning algorithm However the E step calculates the expectation of log likelihood with respect to the hidden variable while maintaining the established parameter θt−1 During the M step after upda ting QT θt θt−1 from the previous step we maximize the QT θt θt−1 with respect to θt θt = argmaxθ QT θ θt−1 The two steps increase the function likelihood The function QT θt θt−1 is defined as QT θt θt−1 = N∑ i=1 ∑ c ∈C ∑ c∈C E α c c i |xi θt−1 × [ ln θc + ln KT δ c c Tc + ln fc x wc �c ] where E α c c i |xi θt−1 = p α c c i = 1|xi θt−1 = p c c |xi θt−1 with p c c |xi θt−1 = p c p c|c p x|c p x The function QT θt θt−1 breaks into three terms QT θt θt−1 = QT1 θ C θt−1 +QT2 θ C θt−1 +QT3 θ t−1 5 where QT1 θ C θt−1 = n∑ k=1 N∑ i=1 ∑ c∈C ∑ c ∈C p c c |xi θt−1 ln fc xk wkc εkc QT2 θ C θt−1 = N∑ i=1 ∑ c ∈C ∑ c∈C p c c |xi θt−1 ln θc QT3 θ t−1 = N∑ i=1 ∑ c ∈C ∑ c∈C p c c |xi θt−1 ln KT δ c c Tc The parameters θC and θC indicate the parameters estimated at the tth step The first term QT1 θ C θt−1 depends on θc k = wkc ε k c the second term Q T 2 θ C θt−1 depends on θc and the third term is constant Maximizing QT θt θt−1 with respect to θc and θc can be performed separately including the parameter wc and �c The maximization of QT θt θt−1 leads to the updates that are calculated using the parameters estimated at the t− 1th step The expressions are defined as follows θc = p c = ∑ xi∈A p c |xi θt−1 N 6 363 Classification topologique probabiliste pour des données catégorielles where p c |xi θt−1 = ∑ c∈C p c c |xi θt−1 and p c|xi θt−1 = ∑ c ∈C p c c |xi θt−1 Each component of wc = w1c w k c w n c and �c = ε 1 c ε 2 c ε k c ε n c is then compu ted as follows wkc =e=1 ck N∑ i=1 p c|xi θt−1 d xki wkc 7 and εkc = ∑N i=1 p c|xi θt−1 d xki wkc ∑N i=1 p c|i θt−1 8 The application of EM for the maximization gives rise to the iterative algorihtm of WeCSOM The version of the WeCSOM algorithm for a fixed T parameter is presented in the following way Algorithm 1 Principal stages of the learning algorithm WeCSOM 1 Initialization iteration t = 0 Choose the initial parameters θ0 and the number of iterations Niter 2 Basic Iteration at a constant T iteration t ≥ 1 Calculate all the parameters θt = {θc wc �c} from the previous parameters θt−1 associated with each cell c and c by applying the formulas 6 7 and 8 3 Repeat the basic iteration until t > Niter The WeCSOM learning algorithm allows us to estimate the parameters maximizing the log likelihood function for a fixed T As in the SOM algorithm we decrease the value of T between two values Tmax and Tmin to control the size of the neighbourhood influencing a given cell on the map For each T value we get a likelihood function LT and therefore the expression varies with T When decreasing T the learning algorithm of WeCSOM is defined in the Algorithm 2 Algorithm 2 Algorithm WeCSOM varying T 1 Initialization Phase iteration t = 0 Choose Tmax Tmin and Niter Apply the princi pal stages of WeCSOM algorithm described above for the value of T fixed to Tmax 2 Iterative step We assume that the previous parameter θt are known Compute the new value of T by applying the following formula T = Tmax Tmin Tmax t Niter−1 For fixed value of the parameter T apply the basic iteration described in the principal stages which estimates the new parameter θt+1 using the formulas 6 7 and 8 3 Repeat the Iterative step while t ≤ Niter We can define two steps in the operating of the algorithm – The first step corresponds to high T values In this case the influencing neighbourhood of each cell c on the map is important and corresponds to higher values of KT δ c r 364 Rogovschi et al Formulas 6 7 and 8 use a high number of observations to estimate model parame ters This step provides the topological order – The second step corresponds to small T values The number of observations in formulas 6 7 and 8 is limited Therefore the adaptation is very local The parameters are accurately computed from the local density of the data 3 Experimentations and validations To evaluate the quality of clustering we adopt the approach of comparing the results to a "ground truth" We use the clustering accuracy for measuring the clustering results This is a common approach in the general area of data clustering This procedure is defined by Jain et Dubes 1988 as "validating clustering by extrinsic classification" and has been followed in many other studies Andreopoulos et al 2006 Khan et Kant 2007 Thus to adopt this approach we need labeled data sets where the external extrinsic know ledge is the class information provided by labels Hence if the WeCSOM finds significant clusters in the data these will be reflected by the distribution of classes Therefore we operate a vote step for clusters and compare them to the behavior methods from the literature The so called vote step consists in the following For each cluster c ∈ C – Count the number of observation of each class l call it Ncl – Count the total number of observation assigned to the cell c call it Nc – Compute the proportion of observations of each class call it Scl = Ncl|Nc – Assign to the cluster the label of the most represented class l = argmaxl Scl A cluster c for which Scl = 1 for some class labeled l is usually termed a "pure" cluster and a purity measure can be expressed as the percentage of elements of the assigned class in a cluster The experimental results are then expressed as the fraction of observations falling in clusters which are labeled with a class different from that of the observation This quantity is expressed as a percentage and termed "purity percentage" indicated as Purity% in the results To test the performance of our approach we used many publics data sets extracted from the UCI repository Asuncion et Newman 2007 The table 1 summarizes a short description of these data sets TAB 1 – – Description of the used datasets for the validations Data set Size nb of classes Zoo 101× 16 7 Congressional vote 435× 16 2 Wisconsis B C 699× 9 2 Nursery 12960× 8 2 Car 1728× 6 4 Post Operative 90× 8 3 To conduct experimental comparison and to verify the efficacy of our proposed model we compare our method with the RTC Relational Topological Clustering Labiod et al 2010 365 Classification topologique probabiliste pour des données catégorielles We choose this method because it is based on the same principle of the Kohonens model conservation of the data topological order and uses the Relational Analysis formalism by optimizing a cost function defined by analogy with Condorcet criterion One disavantage of the RTC method is that this approach treats all the features equally We use the same categori cal data sets obtained from UCI repository Asuncion et Newman 2007 and used in Labiod et al 2010 For each dataset we learned a map of different sizes from 5x5 to 10x10 and we indicate in the table 2 the purity of clustering for RTC technique and WeCSOM The results illustrate that the proposed technique increase the purity index compared to the RTC and also presents the advantage to treat directly the categorical data without using the binary coding We compared also the performance of our method with the result provided in Khan et Kant 2007 that used a version of K modes clustering method dedicated to categorical data Table 3 lists the classification error obtained with different methods We compute the fraction of observations falling in clusters which are labeled with a class different from that of the ob servation We can observe that our results are much better then the results provided by K modes Khan et Kant 2007 Also we improve the error rate compared to BinBatch algorithm which represents the classical SOM approach dedicated to binary data using Hamming distance TAB 2 – – Comparison between RTC et WeCSOM using purity index RTC Relational Topo logical Clustering dedicated to categorical data using the Relational Analysis formalism Purity % Size map RTC WeCSOM Zoo 5× 5 97 84 98 13 Nursery 6× 6 78 69 81 52 Car 10× 10 80 17 82 19 Post Operative 5× 5 78 21 81 34 TAB 3 – – Comparison of the classification performances reached by K modes BinBatch and WeCSOM clustering algorithms Error rate % K modes BinBatch WeCSOM Wisconsis B C 13 2 3 87 2 34 Zoo 16 6 2 97 1 87 Congressional vote 13 2 5 91 5 77 4 Conclusion This study reports the development of a computationally efficient EM approach to maximize the likelihood of the data set to estimate the parameters of a probabilistic self organizing map model dedicated to categorical variables This algorithm has the advantage of providing a pro totype with the same coding as the input data The extention of the proposed method to the co clustering will be an interesting future work for dealing with large scale problems 366 Rogovschi et al Références Andreopoulos B A An et X Wang 2006 Bi level clustering of mixed categorical and numerical biomedical data International Journal of Data Mining and Bioinformatics 1 1 19 – 56 Anouar F F Badran et S Thiria 1997 Self organizing map a probabilistic approach In Proceedings of WSOM’97 Workshop on Self Organizing Maps Espoo Finland June 4 6 pp 339–344 Asuncion A et D Newman 2007 UCI machine learning repository ics uci edu ∼mlearn MLRepository html Bishop C M M Svensén et C K I Williams 1998 GTM The generative topographic mapping Neural Comput 10 1 215–234 Dempster A N Laird et D Rubin 1977 Maximum likelihood from incomplete data via the em algorithm Roy Statist Soc 39 1 1–38 Graepel T M Burger et K Obermayer 1998 Self organizing maps generalizations and new optimization techniques Neurocomputing 21 173–190 Heskes T 2001 Self organizing maps vector quantization and mixture modeling IEEE Trans Neural Networks 12 1299–1305 Hofmann T 2001 Unsupervised learning by probabilistic latent semantic analysis Machine Learning 42 177–196 Huang Z 1998 Extensions to the k means algorithm for clustering large data sets with categorical values In Data Mining and Knowledge Discovery 2 Ibbou S et M Cottrell 1995 Multiple correspondance analysis crosstabulation matrix using the kohonen algorithm In Verlaeysen M Editor proc of ESANN’95 pp 27–32 Dfacto Bruxelles Jain A K et R C Dubes 1988 Algorithms for clustering data Upper Saddle River NJ USA Prentice Hall Inc Jollois F X et M Nadif 2007 Speed up for the expectation maximization algorithm for clustering categorical data Journal of Global Optimization 37 4 513–525 Kaban A E Bingham et T Hirsimäki 2004 Learning to read between the lines The aspect bernoulli model In Proceedings of the Fourth SIAM International Conference on Data Mining Lake Buena Vista Florida USA Kaban A et M Girolami 2001 A combined latent class and trait model for the analysis and visualization of discrete data IEEE Trans Pattern Anal Mach Intell 23 859–872 Kaski S T Honkela K Lagus et T Kohonen 1998 Websom–self organizing maps of document collections Neurocomputing 21 101–117 Khan S S et S Kant 2007 Computation of initial modes for k modes clustering algorithm using evidence accumulation In IJCAI pp 2784–2789 Kohonen T 2001 Self organizing Maps Springer Berlin Kostiainen T et J Lampinen 2002 On the generative probability density model in the self organizing map Neurocomputing 48 217–228 Labiod L G Nistor et B Younes 2010 Relational topographic clustering rtc Internatio nal Joint Conference on Neural Networks IJCNN’10 367 Classification topologique probabiliste pour des données catégorielles Lebbah M N Rogovschi et Y Bennani 2007 Besom Bernoulli on self organizing map In IJCNN pp 631–636 IEEE Lebbah M S Thiria et F Badran 2000 Topological map for binary data In Proceedings European Symposium on Artificial Neural Networks ESANN 2000 Bruges April 26 27 28 pp 267–272 Luttrel S P 1994 A bayesian analysis of self organizing maps Neural Computing 6 767 – 794 McLachlan G et T Krishman 1997 The EM algorithm and Extensions Wiley New York Saund E 1995 A multiple cause mixture model for unsupervised learning Neural Com put 7 1 51–71 Verbeek J N Vlassis et B Krose 2005 Self organizing mixture models Neurocompu ting 63 99–123 Summary This paper introduces a probabilistic self organizing map for topographic clustering anal ysis of categorical data By considering a parsimonious mixture model we present a new probabilistic Self Organizing Map SOM The estimation of parameters is performed by the EM algorithm Contrary to SOM our proposed learning algorithm optimizes an objective function Its performance is evaluated on real datasets 368 