classification topologique probabiliste données catégorielles nicoleta rogovschi mohamed nadif lipade université paris descartes saints pères 75006 paris france prénom parisdescartes résumé article présente carte organisatrice probabiliste classification topologique données catégorielles considérant modèle mélanges parcimonieux introduisons nouvelle carte organisatrice probabiliste estimation paramètres notre modèle réalisée algorithme classique contrairement gorithme apprentissage proposé optimise fonction objective perfor mances évaluées données réelles résultats obtenus encourageants prometteurs classification modélisa introduction visualization important exploratory phase analysis difficult involves binary categorical variables andreopoulos saund organizing being increasingly tools visualiza allow projection small areas generally dimensional basic model proposed kohonen designed numerical successfully applied treating textual kaski algorithm applied binary following transformation original ibbou cottrell lebbah developing generative models kohonen important these models interactions assume generators follow generating observations extensions reformulations kohonen model described literature include probabilistic organizing anouar which define gaussian mixture maximum likelihood approach define iterative algorithm verbeek authors propose probabilistic generalization kohonen which maximizes variational energy likelihood kullback leibler divergence between normalized neighbourhood function posterior distribu given components topographic vector quantization which measure divergence between items cells minimize error function heskes graepel another model often presented probabilistic version organizing generative topographic bishop kaban girolami however manner which achieves classification topologique probabiliste données catégorielles topographic organization quite different those models mixture components parameterized linear combination nonlinear functions locations components latent space developed continuous specific model subsequently developed binary adopting variational approximation binomial likelihood graepel kaban authors concentrate modelling binary coded where presence absence variable interest contrast other approaches model linear model bernoulli analogue multinomial decomposition model jollois nadif proposed method speed convergence second yield results traditional using categorical others similar techniques developed cluster large kostiainen lampinen hofmann concentrate modelling qualitative using binary coding model volves probabilistic formalism topological anouar therefore consists estimating parameters model maximizing likelihood learning algorithm propose application standard algorithm mclachlan krishman variants proposed speed reducing spent categorical jollois nadif paper proposed method called wecsom weighted categorical organizing which combine benefits huang algorithm mixture models design mixture categorical approach based model organizing parsimonious mixture models which advantage being directly applicable categorical without using specific encoding priori proposed learning algorithm application classical algorithm allows weight variables considering number modes during learning process achieving optimized classification paper organized follows present principle probabilistic categorical section proposed approach presented sections sections present different results finally paper conclusion future works proposed methods categorical probabilistic organizing traditional organizing assume lattice discrete discrete output space defined undirect graph usually graph regular dimensions denote number cells ncell cells distance defined length shortest chain linking cells general probabilistic formalism define model topological based mixture models associate density function whose parameters denoted follo bayesian formalism presented luttrel anouar assume rogovschi observation generated following process start associating probability where vector space according prior probability select associated following conditional probability cells contribute generation according proximity described probability proximity implies probability therefore contribution generation markov property probability distribution servations generated mixture probabilities completely defined generative model considers mixture probabilities given where conditional probability assumed known introduce organizing process mixture model learning assume defined where neighbourhood function depending parameter called temperature where particular kernel function which positive symmetric thusk defines neighbourhood region parameter allows control neighbourhood influencing given kohonen algorithm decrease value between values proposed model following focus categorical instances described categorical attributes matrix noted defined instance represented attribute number categories consider restricted latent class model conditional distribution given product univariate single distributions where represents vector categories vector probabilities taking classification topologique probabiliste données catégorielles where otherwise define parsimonious model where parame consists component dimensional vector probabilities indicating degree heterogeneity density presses attribute takes category greatest probability takes other category probability setting clustering problem under classification maximum likelihood approach authors generalization kmodes criterion proposed better criteria situation assume parameter depends model mixture generator becomes therefore parameters which define model mixture generator constituted parameters ncell where prior probabilities called mixing coefficients ncell where difficulty define function learning algorithm estimating these parameters dedicated categorical wecsom algorithm inspired probabilistic model proposed anouar represents generalization model proposed lebbah function optimization algorithm learning algorithm based maximizing likelihood observations plying algorithm dempster learning facilitated introducing hidden variables hidden variable indicates which pairs generate corresponding observation introduce hidden variable expression define binary indicator variable which indicates hidden generator follow generating observation otherwise using expres binary indicator define classification likelihood observations using hidden variables follows likelihood becomes rogovschi where application algorithm maximiza likelihood requires maximised fixed temperature defined where parameters estimated learning algorithm however calculates expectation likelihood respect hidden variable while maintaining established parameter during after previous maximize respect argmaxθ steps increase function likelihood function defined where function breaks three terms where parameters indicate parameters estimated first depends second depends third constant maximizing respect performed separately including parameter maximization leads updates calculated using parameters estimated expressions defined follows classification topologique probabiliste données catégorielles where component compu follows application maximization gives iterative algorihtm wecsom version wecsom algorithm fixed parameter presented following algorithm principal stages learning algorithm wecsom initialization iteration choose initial parameters number iterations niter basic iteration constant iteration calculate parameters previous parameters associated applying formulas repeat basic iteration until niter wecsom learning algorithm allows estimate parameters maximizing likelihood function fixed algorithm decrease value between values control neighbourhood influencing given value likelihood function therefore expression varies decreasing learning algorithm wecsom defined algorithm algorithm algorithm wecsom varying initialization phase iteration choose niter apply princi stages wecsom algorithm described above value fixed iterative assume previous parameter known compute value applying following formula niter fixed value parameter apply basic iteration described principal stages which estimates parameter using formulas repeat iterative while niter define steps operating algorithm first corresponds values influencing neighbourhood important corresponds higher values rogovschi formulas number observations estimate model parame provides topological order second corresponds small values number observations formulas limited therefore adaptation local parameters accurately computed local density experimentations validations evaluate quality clustering adopt approach comparing results ground truth clustering accuracy measuring clustering results common approach general clustering procedure defined dubes validating clustering extrinsic classification followed other studies andreopoulos adopt approach labeled where external extrinsic ledge class information provided labels hence wecsom finds significant clusters these reflected distribution classes therefore operate clusters compare behavior methods literature called consists following cluster count number observation class count total number observation assigned compute proportion observations class assign cluster label represented class argmaxl cluster which class labeled usually termed cluster purity measure expressed percentage elements assigned class cluster experimental results expressed fraction observations falling clusters which labeled class different observation quantity expressed percentage termed purity percentage indicated purity results performance approach publics extracted repository asuncion newman table summarizes short description these description datasets validations classes congressional wisconsis nursery 12960 operative conduct experimental comparison verify efficacy proposed model compare method relational topological clustering labiod classification topologique probabiliste données catégorielles choose method because based principle kohonens model conservation topological order relational analysis formalism optimizing function defined analogy condorcet criterion disavantage method approach treats features equally categori obtained repository asuncion newman labiod dataset learned different sizes 10x10 indicate table purity clustering technique wecsom results illustrate proposed technique increase purity index compared presents advantage treat directly categorical without using binary coding compared performance method result provided version modes clustering method dedicated categorical table lists classification error obtained different methods compute fraction observations falling clusters which labeled class different servation observe results better results provided modes improve error compared binbatch algorithm which represents classical approach dedicated binary using hamming distance comparison between wecsom using purity index relational logical clustering dedicated categorical using relational analysis formalism purity wecsom nursery operative comparison classification performances reached modes binbatch wecsom clustering algorithms error modes binbatch wecsom wisconsis congressional conclusion study reports development computationally efficient approach maximize likelihood estimate parameters probabilistic organizing model dedicated categorical variables algorithm advantage providing totype coding input extention proposed method clustering interesting future dealing large scale problems rogovschi références andreopoulos level clustering mixed categorical numerical biomedical international journal mining bioinformatics anouar badran thiria organizing probabilistic approach proceedings workshop organizing espoo finland asuncion newman machine learning repository mlearn mlrepository bishop svensén williams generative topographic mapping neural comput dempster laird rubin maximum likelihood incomplete algorithm statist graepel burger obermayer organizing generalizations optimization techniques neurocomputing heskes organizing vector quantization mixture modeling trans neural networks hofmann unsupervised learning probabilistic latent semantic analysis machine learning huang extensions means algorithm clustering large categorical values mining knowledge discovery ibbou cottrell multiple correspondance analysis crosstabulation matrix using kohonen algorithm verlaeysen editor esann dfacto bruxelles dubes algorithms clustering upper saddle river prentice jollois nadif speed expectation maximization algorithm clustering categorical journal global optimization kaban bingham hirsimäki learning between lines aspect bernoulli model proceedings fourth international conference mining buena vista florida kaban girolami combined latent class trait model analysis visualization discrete trans pattern intell kaski honkela lagus kohonen websom organizing document collections neurocomputing computation initial modes modes clustering algorithm using evidence accumulation ijcai kohonen organizing springer berlin kostiainen lampinen generative probability density model organizing neurocomputing labiod nistor younes relational topographic clustering internatio joint conference neural networks ijcnn classification topologique probabiliste données catégorielles lebbah rogovschi bennani besom bernoulli organizing ijcnn lebbah thiria badran topological binary proceedings european symposium artificial neural networks esann bruges april luttrel bayesian analysis organizing neural computing mclachlan krishman algorithm extensions wiley saund multiple cause mixture model unsupervised learning neural verbeek vlassis krose organizing mixture models neurocompu summary paper introduces probabilistic organizing topographic clustering categorical considering parsimonious mixture model present probabilistic organizing estimation parameters performed algorithm contrary proposed learning algorithm optimizes objective function performance evaluated datasets