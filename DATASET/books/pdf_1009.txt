 Carte auto organisatrice probabiliste sur données binaires Rodolphe Priam Mohamed Nadif LITA Université de Metz Ile du Saulcy 57045 Metz Résumé Les méthodes factorielles d’analyse exploratoire statistique définissent des directions orthogonales informatives à partir d’un ensemble de données Elles conduisent par exemple à expliquer les proximités entre individus à l’aide d’un groupe de variables caractéristiques Dans le contexte du datamining lorsque les tableaux de données sont de grande taille une méthode de cartographie syn thétique s’avère intéressante Ainsi une carte auto organisatrice SOM est une méthode de partitionnement munie d’une structure de graphe de voisinage sur les classes le plus souvent planaire Des travaux récents sont développés pour étendre le SOM probabiliste Generative Topographic Mapping GTM aux mo dèles de mélanges classiques pour données discrètes Dans ce papier nous pré sentons et étudions un modèle génératif symétrique de carte auto organisatrice pour données binaires que nous appelons Bernoulli Aspect Topological Model BATM Nous introduisons un nouveau lissage et accélérons la convergence de l’estimation par une initialisation originale des probabilités en jeu 1 Introduction La visualisation des corrélations et similarités principales dans un échantillon de données est l’objectif des méthodes factorielles Lebart et al 1984 Ces méthodes cherchent souvent des directions informatives orthogonales dans un nuage de données Ces directions concentrent l’essentiel de la variance projetée car l’inertie est porteuse de sens Une décomposition perti nente de l’inertie sur des plans de projection révèle quels individus sont similaires et quelles variables sont dépendantes Bien que ces méthodes soient très pertinentes les grands échan tillons de données demandent de nouvelles méthodes efficaces pour leur analyse Dans ce contexte les cartes de Kohonen 1997 sont connues dans le domaine de l’analyse explora toire des données pour généraliser les méthodes factorielles telles que la méthode d’Analyse en Composantes Principales ou ACP Lebart et al 1984 pour les données continues Plus généralement les cartes auto organisatrices ou SOM Kohonen 1997 sont des méthodes de classification avec une contrainte de voisinage sur les classes conférant un sens topologique à la partition finale Le GTM ou Generative Topographic Mapping Bishop et al 1998 est une carte auto organisatrice probabiliste avec des contraintes sur les moyennes d’un mélange gaus sien pour données continues mais ce modèle est inopérant pour des données catégorielles ou binaires Des modèles récents Girolami 2001 Kabán et Girolami 2001 Tipping 1999 ont été proposés pour étendre le GTM aux modèles de mélanges classiques pour données discrètes Hofmann et Puzicha 1998 ont par contre proposé l’approche du modèle symétrique à aspects 445 RNTI E 6 Carte auto organisatrice probabiliste sur données binaires qui traite de la classification simultanée des lignes et colonnes d’un tableau de contingence Cette approche est bénéfique dans plusieurs domaines tels que le textmining et la segmentation d’image Dans ce papier nous nous intéressons aux données binaires et nous étudions un mo dèle original en présentant un nouveau lissage de carte auto organisatrice et une initialisation adaptée pour accélérer la convergence des algorithmes d’estimation des paramètres du modèle Les probabilités sont paramétrées de façon adéquate comme le GTM afin d’induire une auto organisation des facteurs latents ce qui nous amène à une nouvelle méthode pour visualiser les données discrètes ou vecteurs multidimensionnels de composantes 1 0 Ce papier est organisé comme suit Dans la section 2 nous décrivons notre modèle et abor dons le problème d’estimation des paramètres par la maximisation de la vraisemblance Dans la section 3 nous réalisons des expériences numériques pour valider notre modèle Enfin la section 4 résume les points principaux et présente les travaux en cours 2 Le modèle BATM Le modèle proposé repose sur l’hypothèse d’indépendance des I × J cellules xij ∈ {0 1} d’un tableau binaire en modélisant chaque probabilité unidimensionnelle d’observer xij comme un mélange de K lois de Bernoulli Pr xij = 1 = E xij = ∑ k πkiajk avec πki les propor tions des K composants telles que ∑ k πki = 1 Ce modèle génératif correspond à sélectionner pour chaque ligne xi = xi1 xi2 · · · xiJ fixée une distribution discrète πi de composantes πki puis pour chaque j ième composante xij sélectionner un état k avec la probabilité πki afin de lui attribuer une valeur binaire selon la loi de Bernoulli de paramètre ajk Un modèle com parable pour la classification sans contrainte a été récemment proposé La log vraisemblance de D = {xi}i=Ii=1 s’écrit alors L θ|D = ∑ i j log [ ∑ k πkia xij jk 1 − ajk 1−xij ] Afin d’induire une auto organisation topologique des probabilités nous considérons les K co ordonnées {sk}k=Kk=1 d’une grille bidimensionnelle régulière qui modélise un plan discrétisé sur lequel l’ensemble des données est disposé par le BATM La grille est projetée non linéaire ment dans un espace de plus grande dimension L par une transformation non linéaire consti tuée de L bases fonctionnelles φ` et telle que ξk = φ1 sk φ2 sk · · · φL sk T on note la matrice Φ = [ξ1|ξ2| · · · |ξK ]T Les ajk forment alors les noeuds d’une surface non linéaire discrète les probabilités de Bernoulli sont paramétrées par des fonctions ajk = σ wTj ξk où σ u = eu 1 + eu est une fonction sigmoïde et wj un paramètre inconnu appartenant à RL La log vraisemblance devient L θ|D = ∑ i j log [ ∑ k πkiσ w T j ξk xij 1 − σ wTj ξk 1−xij ] Ce modèle s’interprète comme une version binaire cartographique du LSA probabiliste ou pLSA de Hofmann et Puzicha 1998 Les paramètres inconnus sont estimés dans la section suivante 446 RNTI E 6 R Priam et M Nadif 2 1 Estimation par GEM L’inférence de notre modèle se réalise en maximisant la log vraisemblance par une mé thode itérative une solution analytique exacte n’existe pas en raison des non linéarités du mélange et des fonctions sigmoïdes Donc nous étudions l’approche de l’algorithme de mon tée de gradient par EM Dempster et al 1977 généralisé GEM de McLachlan et Peel 2000 Cette approche suppose la vraisemblance complétée par la connaissance de la parti tion Z = {Z1 Z2 · · · ZK} L θ Z|D = ∑ i j log [ πzii a xij jzi 1 − ajzi 1−xij ] avec zi les variables latentes ayant pour support {1 2 · · · K} L’algorithme EM ou Expectation Maximisation repose sur la maximisation de l’espérance conditionnelle sachant les données et les paramètres de l’itération précédente Ayant P t Z|D du pas t précédent nous maximi sons au pas t + 1 Q θ|θ t = EP t Z|D [L θ Z|D ] = ∑ i j k P t k|i j xij { log πki + xijξ T k wj − log 1 + exp ξ T k wj } avec Pk|i j xij ∝ πkia xij jk 1 − ajk 1−xij la probabilité a posteriori que xij soit générée pas le composant k Un calcul direct donne π t+1 ki = argmaxπkiQ θ|θ t = ∑ j P t k|i j xij J Pour résoudre w t+1 = argmaxwQ θ|θ t nous effectuons des dérivations élémentaires du critère qui aboutissent au gradient Q t j et au bloc de la hessienne H t j Le pas de Newton Raphson suivant augmente alors localement la log vraisemblance w t+1 j = w t j − H t j −1 Q t j A la convergence du GEM nous obtenons un estimateur au maximum de vraisemblance noté θ̂ Pour éviter un surapprentissage et une instabilité numérique nous ajoutons à la fonction Q un paramètre de régularisation bayésien MacKay 1992 −α ∑ j w T j wj Cette correction ajoute −αwj au gradient Qj et −α à la diagonale de la hessienne Hj La valeur de l’hyperparamètre α est choisie manuellement comme la plupart du temps dans la littérature ici nous avons pris α = 0 01 2 2 Formulation IRLS Nous écrivons l’algorithme de Newton sous une forme matricielle qui est proche d’un pas d’Iteratively Reweighted Least Squares ou IRLS McCullagh et Nelder 1983 pour la régression logistique Pour j de 1 à J Q t j = Φ T [ R t j Aj − G t j a t j ] − 0 01w t j H t j = −Φ T G t j F t j Φ − 0 01IL 447 RNTI E 6 Carte auto organisatrice probabiliste sur données binaires Nous avons R t j la matrice de taille K×I qui compte pour cellules les probabilités a posteriori P t k|i j xij la matrice diagonale G t j a pour éléments non nuls ∑ i P t k|i j xij Aj est le vecteur de i ième composante aij a t j est un vecteur colonne avec a t jk pour k ième composante F t j est la matrice diagonale avec a t jk 1 − a t jk sur sa diagonale et enfin IL est la matrice identité de taille L Pour accélérer numériquement l’algorithme l’approche de Bohning 1993 remplace la matrice exacte relativement lourde à calculer en pratique par une matrice alternative fixe Par exemple la matrice B = − I4Φ T Φ− 0 01IL qui est telle que H t j � B i e H t j −B est non négative symétrique ce qui permet de maximiser la vraisemblance Comme la convergence est lente nous proposons un algorithme de type variationnel alternatif 2 3 Estimation variationnelle En suivant la borne1 Saul et al 1996 sur log 1+ exp ξTk wj nous obtenons le nouveau critère à optimiser Q̃ θ|θ t = ∑ i j k P t k|i j xij { log πki + xij − 0 5 ξ T k wj + λ �j [ ξ T k wj 2 − �2j ] + 0 5�j − log 1 + exp �j } Avec λ �j = −tanh 0 5�j 4�j tel que Q θ|θ t ≥ Q̃ θ|θ t où �j est un paramètre variationnel à estimer en maximisant Q̃ En dérivant ce nouveau critère nous obtenons le pas de maximisation � t j = √ w t j T ΦT G t j Φw t j I w t+1 j = [ − 2λ � t j Φ T G t j Φ − 0 01IL ]−1 ΦT R t j A ′ j où A′j est le vecteur colonne ayant xij − 0 5 pour i ème composante Finalement trois algo rithmes et un quatrième décrit ci après sont présentés pour estimer les paramètres du modèle Ayant éliminé la solution du gradient simple mais inefficace on constate que l’algorithme IRLS donne la meilleure vraisemblance dans notre cas comme le montre les expériences dans la section suivante 3 Simulations Dans cette section nous abordons tout d’abord deux éléments complémentaires à la mé thode proposée l’initialisation des paramètres du modèle et l’auto organisation des probabi lités sur les lignes afin d’obtenir la meilleure carte projective finale possible Nous décrivons alors les résultats numériques de nos simulations sur des données binaires réelles 1log σ u ≥ u 2 + λ � u2 − �2 + log σ � − � 2 pour des raisons de concavité 448 RNTI E 6 R Priam et M Nadif 3 1 Initialisation du modèle Des tirages aléatoires répétés des paramètres initiaux sont une solution aux minima locaux que rencontrent les algorithmes basés sur le gradient Pour obtenir la meilleure convergence possible on procède à une ”bonne initialisation” Puisque les cartes de Kohonen sont des gé néralisations de l’ACP le premier plan de cette méthode fournit une intéressante première position Elemento 1999 des centres de classes de la carte Notons Xci Y c i les coordonnées sur le premier plan factoriel de l’ACP Jolliffe 2002 AFC Benzécri 1992 LSA Deer wester et al 1990 ou même celles obtenues suite à une projection non linéaire telle qu’un MDS Sammon 1969 Alors une grille régulière est dessinée sur cette première projection et chaque cellule de la grille correspond à un facteur du modèle BATM xi est affectée à la z 0 i ième classe correspondant à la cellule dans laquelle ses coordonnées X c i Y c i de pro jection tombent sur le plan d’initialisation Nous initialisons les probabilités de mélange par π 0 ki ∝ h k z 0 i pour une fonction de lissage telle que celle de voisinage des cartes de Koho nen i e h k z 0 i ∝ exp −||sk − sz 0 i ||2 σ pour σ bien choisi Alors on pose a 0 jk = ∑ i π 0 ki xij + α ∑ i π 0 ki + Iα où α > 0 bien choisi a pour rôle de régulariser les paramètres ajk qui correspondent à des cellules vides Finalement LP 0 J est la matrice de taille K × J dont les cellules ont pour valeurs log[a 0 jk 1− a 0 jk ] Cette matrice nous permet d’évaluer une matrice W 0 J qui a pour colonnes les paramètres initiaux w 0 j des fonctions logistiques La solution au problème de régression associé s’écrit alors W 0 J = [w 0 1 |w 0 2 | · · · |w 0 J ] = Φ T Φ −1ΦT LP 0 J Nous construisons les centres en effectuant un pas de l’algorithme non séquentiel de Koho nen pour les affectations évaluées sur le plan de projection initiale l’affectation s’effectue en attribuant le label z 0 i du noeud le plus proche de X c i Y c i pour un treillis régulier des siné sur le nuage bidimensionnel des projetés afin de discrétiser celui ci Cette approche doit également induire par ailleurs un niveau d’entropie plus élevé de la classification initiale des données comparativement à la simple classification dure sur le plan initial et facilite ainsi une convergence des paramètres vers une solution encore meilleure 3 2 Lissage des paramètres sur les lignes Il peut être intéressant d’ajouter une contrainte topologique sur les paramètres partitionnant les lignes afin d’accélérer la convergence de l’algorithme et améliorer la carte finale Comme une solution par un soft max Bishop 1995 nous apparaît relativement lourde nous proposons une solution alternative en ajoutant un simple lissage par un terme de pénalisation issu de l’approche du TNEM Priam 2003 Brièvement il s’agit de classer les vecteurs de données avec un lissage spatial sur les composantes πki du modèle BATM à la manière d’un champ de Markov caché Zhang 1992 Celeux et al 2003 Ambroise et Govaert 1998 On pose Qβ θ|θ t = Q θ|θ t + β 2 ∑ i πTi Vπi 449 RNTI E 6 Carte auto organisatrice probabiliste sur données binaires où πi est le vecteur de composantes πki et V est soit la matrice des fonctions de voisinage de la carte auto organisatrice i e Vk` = h k ` soit la matrice binaire d’adjacence du treillis correspondant à notre carte probabiliste i e Vk` = 1 ssi le k ième noeud est voisin du ` ième Le pas complémentaire associé s’écrit π t+1 ki = ∑ j P t k|i j xij + β π t+1 ki ∑ ` Vk` π t+1 `i J + β π t+1 i T Vπ t+1 i et se résout en itérant l’égalité et en réinjectant dans le membre de droite les anciennes valeurs courantes des π t+1 ki jusqu’à ce que la stabilisation de leurs valeurs soit atteinte Nous retrou vons évidemment le pas d’estimation non contrainte en annulant β Nous avons éliminé le terme additif du TNEM qui porte sur les entropies des πi donc nous obtenons un nouvel algo rithme appelé TNEM2 plus général que le TNEM original puisqu’il s’applique à des probabili tés non forcément a posteriori Une alternative au TNEM est une estimation des πki paramétrés comme le GTM La fonction à optimiser s’écrit alors QI θ|θ t = ∑ i j k P t k|i j xij { ξTk wi − log ∑ ` exp ξ T ` wi } où les wi sont les inconnues à déterminer Leur estimation s’effectue comme précédemment par une montée de gradient en réalisant une boucle sur l’indice i des lignes de 1 à I et en calculant les gradients Q t i et les matrices hessiennes H t i Enfin les pa ramètres w 0 i s’initialisent à l’aide d’une régression sur la nouvelle matrice LP 0 I qui a pour cellules les logarithmes des π 0 ki Nous proposons finalement le quatrième algorithme d’esti mation noté IRLS+TNEM2 qui associe une maximisation sur les paramètres des colonnes par l’IRLS à un lissage des probabilités des lignes par le TNEM2 Nous expliquons dans la suite comment ce lissage se comporte en pratique 3 3 Post processing de la carte finale La carte finale montre une grille de centres de classes bien organisées à chacune on affecte les données dont elle est le plus proche Pour les cartes auto organisatrices classiques on utilise la distance euclidienne entre le vecteur centre et le vecteur donnée Ici le modèle permet une alternative probabiliste puisque nous avons la probabilité de génération d’une donnée par un composant k du mélange Donc chacun des vecteurs xi est affecté à un centre par un maximum a posteriori MAP i e ẑi = argmaxk π̂ki De la même manière la j ième variable est affectée au centre de label ẑj = argmaxk âjk Le MAP aboutit aux positions bidimensionnelles pi = sẑi et pj = sẑj pour les lignes et colonnes de la matrice projetée Une seconde manière de projeter chaque donnée est par sa position moyenne Bishop et al 1998 au lieu du MAP précédent i e les positions p̃i = ∑ k π̂kisk et p̃j = ∑ k âjk ∑ ` âj` sk 3 4 Expériences Nous expérimentons notre modèle sur plusieurs échantillons de données pour valider notre approche par exemple sur l’échantillon Zoo2 qui compte 101 animaux avec sept classes et 21 caractéristiques binaires Notre méthode BATM converge vers une carte bien organisée où 2ftp ftp ics uci edu pub machine learning databases zoo zoo names 450 RNTI E 6 R Priam et M Nadif l’on reconnaît les sept classes que l’algorithme a projetées La segmentation de la grille sur la figure 1 s’effectue à l’aide d’une procédure automatique consistant Vesanto et Alhoniemi 2000 en une classification ascendante hiérarchique avec agrégation par le diamètre complete linkage associée à une distance du χ2 sur la matrice des π̂ki qui donne les meilleurs résultats en pratique En remarque la classe contenant les reptiles est peu homogène d’après nos expé riences car ces animaux se regroupent mal L’évolution de la log vraisemblance par les quatre algorithmes est présentée à la figure 2 pour le tableau du Zoo démontrant la supériorité de l’algorithme IRLS comparativement à des algorithmes plus récents alternatifs Cependant un surapprentissage peut amener à une solution non suffisamment lissée et nous préférons l’ap proche IRLS+TNEM2 à cause de son efficacité malgré sa vraisemblance moins élevée Cette valeur plus faible s’explique par le terme de pénalisation qui aide à une plus rapide et meilleure auto organisation des lignes comme vérifiée ici puisque cet algorithme s’arrête bien plus tôt que les trois autres pour un critère d’arrêt identique log vraisemblance relative inférieure au seuil 10e 5 Notre initialisation originale par une régression adéquate se base sur le premier plan principal d’une Analyse des Correspondances AFC Celle ci s’illustre sur la figure 3 démontrant l’intérêt d’une carte auto organisatrice alors que la méthode factorielle linéaire n’est pas capable de montrer les sept classes sur ce premier plan notre carte par BATM extrait ces sept classes et trouve leur lien statistique grâce à la propriété de voisinage Un ensemble de données textuelles est également projeté Cette base est un échantillon du fichier Classic3 Dhillon et al 2003 qui compte trois classes MEDLINE CISI CRANFIELD Nous avons tiré aléatoirement tirage équiprobable sans remise 450 documents de ce fichier en prenant 150 documents dans chaque classe Nous avons sélectionné les termes dont la fréquence to tale est supérieure à 30 sur le corpus entier et pour l’ensemble du vocabulaire de 4303 termes Nous aboutissons en éliminant les textes vides à une matrice de taille aléatoire 450 par 170 environ Nous montrons les positions moyennes des labels des documents correspondants et projetés sur la figure 4 pour l’une de ces matrices Nous sommes en mesure de visualiser assez distinctement les trois classes séparées par notre projection non linéaire 4 Conclusion et discussion Nous avons présenté une nouvelle méthode de carte auto organisatrice récapitulée sur la figure 5 pour données binaires comme on peut en trouver dans le domaine du traitement de l’image et du texte De nouveaux résultats pour l’initialisation d’une méthode de projection probabiliste de données qualitatives a également été introduit Une perspective de nos travaux est la construction de biplots non linéaires par carte topolo gique Nous travaillons actuellement à la projection de matrices de taille plus importante ainsi que sur des ensembles d’images binaires qui donnent des résultats encourageants Ensuite des variantes au modèle BATM se posent en remplaçant E xij par une hypothèse alternative i e un mélange de lois différent tel que par exemple E xij = ∑ k πkπi|kb xij jk 1 − bjk 1−xij ou bien E xij = ∑ k πka xij ik 1 − aik 1−xij b xij jk 1 − bjk 1−xij Le modèle BATM s’étend également à d’autres types de données comme il est proposé dans le paragraphe suivant L’esti mation peut encore être améliorée en déterminant notamment le meilleur hyperparamètre β En conclusion le récent modèle du Block Clustering Govaert et Nadif 2003 2005 effectue une classification simultanée des lignes et colonnes d’un tableau numérique en étendant le modèle 451 RNTI E 6 Carte auto organisatrice probabiliste sur données binaires stingray bass catfish chub dogfish herring pike piranha tuna seawasp carp dolphin seal haddock porpoise seahorse sole octopus clam pitviper sealion starfish seasnake slowworm crayfish crab frog platypus mink lobster scorpion frog slug newt worm toad tuatara aardvark boar bear cheetah mole leopard opossum lion lynx mongoose polecat puma raccoon wolf flea tortoise hare antelope gnat vole buffalo honeybee deer housefly elephant ladybird giraffe moth oryx termite wasp crow kiwi ostrich gorilla cavy calf duck penguin rhea squirrel goat gull swan wallaby hamster hawk pony skimmer pussycat skua reindeer chicken fruitbat girl dove vampire flamingo lark parakeet pheasant sparrow vulture wren FIG 1 – Une carte de taille 8×8 pour l’échantillon Zoo par BATM segmentée en sept macro classes représentées visuellement par sept niveaux de gris au niveau des cellules agrégées 452 RNTI E 6 R Priam et M Nadif 0 100 200 300 400 500 600 −1100 −1000 −900 −800 −700 −600 −500 −400 −300 −200 IRLS IRLS+TNEM2 Bohning Variationnel FIG 2 – Les courbes de la log vraisemblance du BATM obtenues par les quatre algorithmes présentés IRLS IRLS+TNEM2 Bohning incomplète et variationnel pour les 101 animaux aardvark antelope bass bearboar buffalo calf carp catfish cavy cheetah chicken chub clam crab crayfish crow deer dogfish dolphin dove duck elephant flamingo flea frog frog fruitbat giraffe girl gnat goat gorilla gull haddock hamster hare hawk herring honeybee housefly kiwi ladybird lark leopardlion lobster lynx mink molemongoose moth newt octopus opossum oryx ostrich parakeet penguin pheasant pike piranha pitviper platypus polecat pony porpoise puma pussycat raccoon reindeer rhea scorpion seahorse seal sealion seasnake seawasp skimmerskua slowworm slug sole sparrow squirrel starfish stingray swan termite toad tortoise tuatara tuna vampire vole vulture wallaby wasp wolf worm wren FIG 3 – Initialisation de la carte BATM pour les 101 animaux 0 0 0 0 0 0 0 0 0 0 0 00 00 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 111 111 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 11 1 1 1 1 11 1 1 1 1 1 11 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 111 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 11 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 22 2 2 2 2 22 2 2 2 2 2 2 2 2 2 2 222 2 22 2 2 2 2 22 22 2 2 2 2 2 2 2 2 2 2 2 22 2 2 2 2 22 2 2 2 2 2 22 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 22 2 2 2 2 2 2 2 2 2 2 2 2 22 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 22 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 FIG 4 – Projections moyennes de l’échantillon des 450 documents dans Classic3 453 RNTI E 6 Carte auto organisatrice probabiliste sur données binaires Initialisation de θ 0 = {a 0 jk } {π 0 ki } pour LP 0 I LP 0 J Pas Estimation de Q θ|θ t ⇒ Évaluation des P t k|i j xij pour θ t = {a t jk } {π t ki } Pas Maximisation de Q θ|θ t ⇒ Évaluation GEM IRLS ou variationnelle des paramètres a t+1 jk ⇒ Évaluation TNEM2 ou GEM des paramètres π t+1 ki Post processing final de la carte de paramètres θ̂ ⇒ Tableau segmenté par CAH projections moyennes biplot FIG 5 – Schéma récapitulatif de la méthode BATM de mélange classique à un modèle de mélange croisé Celui ci est un modèle génératif flexible qui s’avère une alternative efficace et prometteuse au modèle à aspects Il serait intéressant de l’étendre en lui ajoutant une propriété d’auto organisation Annexe paramétrisation probabiliste alternative au soft max Lorsque la matrice de données est un tableau de contingence la loi de Bernoulli n’est plus valable et une hypothèse de loi multinomiale est généralement supposée Un paramétrage soft max est alors introduit pour le cas de probabilités contraintes en régression et classification notamment On écrit dans notre cas pj|k = e wTj ξk ∑ j′ e wT j′ ξk avec ∑ j pj|k = 1 Donc cette paramétrisation implique l’inversion d’une matrice hessienne pleine pour procéder à l’optimi sation Nous proposons un moyen alternatif plus efficace L’idée principale est d’aboutir à de nouveaux paramètres sans la contrainte de somme à l’unité classique pour une multinomiale en écrivant pj|k comme une loi jointe de variables de loi de Bernoulli de paramètres inconnus Il s’agit d’écrire la loi jointe de la j ième colonne associée en mettant à l’unité la compo sante qui nous intéresse et à zéro les autres puis en supposant des lois de Bernoulli sur l’en semble des composantes prises indépendantes pour le vecteur binaire résultant L’expression pj|k = pjk ∏ j′ 6=j 1 − pj′k ' pjk avec pjk ∈ [0 1] donne une solution valide au maximum de vraisemblance d’une loi multinomiale pour des probabilités assez petites éventuellement par l’ajout de composantes artificielles supplémentaires pour diminuer les valeurs i e p t+1 jk optimum global annule la dérivée sans contrainte donc sans lagrangien de ∑ i ∑ j ∑ k p t kijxij log[pj|k] = ∑ i ∑ j ∑ k p t kijxij log [ pjk ∏ j′ 6=j 1 − pj′k ] Nous retrouvons l’expression classique de l’estimation des paramètres de la loi multinomiale p t+1 jk = P i p t kij xij P i P j′ p t kij′ xij′ pour des probabilités a posteriori p t kij et induisant leur normalisation 454 RNTI E 6 R Priam et M Nadif automatique Comme aucune contrainte ne devient nécessaire sur les pjk la paramétrisation par des sigmoïdes pjk = σ ξTk wj est licite pour une auto organisation des valeurs recher chées sans paramètre soft max Nous aboutissons à une formulation IRLS ou variationnelle très générale pour la loi multinomiale d’où une nouvelle expression du vecteur gradient et de la matrice hessienne pour l’estimation du modèle BATM sur données catégorielles Références Ambroise C et G Govaert 1998 Convergence of an em type algorithm for spatial clustering Pattern Recogn Lett 19 10 919–927 Benzécri J P 1992 Correspondence Analysis Handbook New York Dekker Bishop C M 1995 Neural Networks for Pattern Recognition Clarendon Press Bishop C M M Svensén et C K I Williams 1998 Developpements of generative topo graphic mapping Neurocomputing 21 203–224 Bohning D 1993 Construction of reliable maximum likelihood algorithms with application to logistic and cox regression Handbook of Statistics 9 409–422 Celeux G F Forbes et N Peyrard 2003 Em procedures using mean field like approxima tions for markov model based image segmentation Pattern Recognition 36 131–144 Deerwester S S T Dumais G W Furnas T Landauer et R Harshman 1990 Indexing by latent semantic analysis Journal of the American Society for Information Science 41 6 391–407 Dempster A N Laird et D Rubin 1977 Maximum likelihood from incomplete data via the EM algorithm with discussion Journal of the Royal Statistical Society Series B 39 1–38 Dhillon I S S Mallela et D S Modha 2003 Information theoretic co clustering In Proceedings of The Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 2003 pp 89–98 Elemento O 1999 Initialisation convergence et validation de cartes topologiques de koho nen in french Master’s thesis Rapport de DEA INRIA Yves Lechevallier Girolami M 2001 Document representation based on generative multivariate bernoulli latent topics models In U of Cambridge Ed BCS IRSG 22nd Annual Colloquium on Information Retrieval Research pp 194–201 Govaert G et M Nadif 2003 Clustering with block mixture models Pattern Recogni tion 36 2 463–473 Govaert G et M Nadif 2005 An EM algorithm for the block mixture model IEEE Tran sactions on Pattern Analysis and Machine Intelligence 27 4 643–647 Hofmann T et J Puzicha 1998 Statistical models for co occurrence data Technical Report AIM 1625 MIT Jolliffe I 2002 Principal Component Analysis Springer Verlag Kabán A et M Girolami 2001 A combined latent class and trait model for analysis and visualisation of discrete data IEEE Transactions on Pattern Analysis and Machine Intelli 455 RNTI E 6 Carte auto organisatrice probabiliste sur données binaires gence 23 8 859–872 Kohonen T 1997 Self organizing maps Springer Lebart L A Morineau et K Warwick 1984 Multivariate Descriptive Statistical Analysis J Wiley MacKay D J C 1992 Bayesian interpolation Neural Computation 4 3 415–447 McCullagh P et J Nelder 1983 Generalized linear models London Chapman and Hall McLachlan G J et D Peel 2000 Finite Mixture Models New York John Wiley and Sons Priam R 2003 Méthodes de carte auto organisatrice par mélange de lois contraintes Application à l’exploration dans les tableaux de contingence textuels in french Ph D thesis Université de Rennes 1 Sammon J 1969 A nonlinear mapping for data structure analysis IEEE Transactions on Computers 5 18C 401–409 Saul L K T Jaakkola et M I Jordan 1996 Mean field theory for sigmoid belief networks Journal of Artificial Intelligence Research 4 61–76 Tipping M E 1999 Probabilistic visualisation of high dimensional binary data Advances in Neural Information Processing Systems 592–598 Vesanto J et E Alhoniemi 2000 Clustering of the self organizing map IEEE Trans on Neural Networks 11 3 586–600 Zhang J 1992 The mean field theory in EM procedures for markov random fields IEEE Transactions on Signal Processing 10 40 2570–2583 Summary The mixture models behave very well to cluster large samples of continuous or categorical data Adding a vicinity constraint permits them to project data like factorial methods but in a nonlinear way In this paper we present a new model called Bernoulli Aspect Topological Mapping BATM a generative self organizing map to deal with binary data by an automatic map smoothing and an original initialization 456 RNTI E 6