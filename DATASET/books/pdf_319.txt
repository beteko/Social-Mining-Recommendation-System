nouvelles pondérations adaptées classification petits volumes données textuelles flavien bouillot pascal poncelet mathieu roche lirmm montpellier france itesoft aimargues france tetis cirad irstea agroparitech france prenom lirmm résumé défis actuels domaine classification supervisée documents pouvoir produire modèle fiable partir faible volume données volume conséquent données classifieurs fournissent résultats satisfaisants performances dégradées lorsque celui diminue proposons article nouvelles méthodes pondéra tions résistant diminution volume données efficacité évaluée utilisant algorithmes classification supervisés existants naive bayes class feature centroid corpus différents supérieure celle autres algorithmes lorsque nombre descripteurs diminue avons parallèle paramètres influençant différentes approches telles nombre classes documents descripteurs introduction classification supervisée documents déterminer catégories potentielles document partir contenu termes composant cadre supervisé processus décompose généralement phases phase apprentissage créer modèle partir ensemble exemples étiquetés documents classe connue phase classification déterminer catégories document classe inconnue application modèle entendu qualité modèle dépend qualité nombre exemples disponibles ainsi exemples observations seront fiables modèle précis efficace cependant avérer intéressant pouvoir élaborer modèle classification fiable partir faible nombre descripteurs forman cohen exemple dévelop pement réseaux sociaux nombre important messages temps taille limitée comme tweet limité caractères implique disposition outils capables classer rapidement volume restreint données contexte extraction descripteurs pertinents discriminants représente nouvelles pondérations adaptées petits volumes données textuelles intéressant arriver nombre exemples utiles création modèle phase apprentissage limité déterminer classe document opération longue généralement nécessite expert domaine taines approches fondent nombre restreint exemples approches classification supervisées utilisent documents étiquetées compléter apprentissage pervisé encore apprentissage actif consiste construire ensemble apprentissage modèle manière itérative interaction expert humain certaines méthodes appliquées faible nombre exemples présentées elles impliquent avoir grand nombre documents disposition améliorer modèle toujours possible plupart algorithmes actuels classification supervisée documents nécessitent nombre suffisant exemples créer classifieur performant performances croissent temps nombre exemples diminue notre principale contribution proposer nouvelles pondérations descripteurs textuels traiter exemples petite taille méthodes pondérations intégrées approches classiques classification class feature centroid naive bayes article organisé manière suivante section discutons térêt nouvelles pondérations mesures permettant extraire descripteurs pertinents classe présentées section intégration mesures gorithmes classification apprentissage supervisé décrite section résul expérimentaux comprenant notamment comparaison autres types approches proposés section enfin section conclue présente quelques perspectives proposition classification supervisée documents cherche déterminer catégorie classe document partir contenu considérons ensemble classes ensemble documents chaque document rattaché classe notons document classe ensemble documents classe documents représentés selon modèle words salton salton mcgill termes documents forment dictionnaire ensemble termes apparaissant moins collection documents dictionnaire contient termes terme unique dictionnaire chaque document représenté vecteur pondéré termes indication position document représentation vectorielle document poids fréquence booléen terme document poids terme dépend poids intra classe poids inter classes poids intra classe permet évaluer importance terme classe terme représentatif classe alors poids inter classes mesure terme pouvoir discriminant classe rapport autres classes terme représente toutes classes méthodes pondérations présentées zhang pondérations dérivées reposent fréquence terme documents classe aussi présence bouillot absence terme autres classes pensons situations posent problèmes présence classes composées petit nombre documents impact quence document importante présence classes sémantiquement proches puisqu terme considéré comme représentant classe apparaît seule classe nombre occurrences terme classes compte pondérations appliquées approche class feature centroid quand pensons elles peuvent pertinentes autres approches classification apprentissage supervisé section suivante proposons nouvelles pondérations répondre problèmes nouvelles pondérations présentons cette section nouvelles méthodes pondérations intra classes section inter classes section dérivées commençons peler principes principes principe donner poids important termes spéci fiques document salton mcgill méthode pondération éprouvée efficacité démontrée nombreuses reprises cette mesure repose produit entre fréquence terme frequency fréquence inverse cument inverse document frequency fréquence terme correspond nombre occurrences terme document représente poids terme aussi appelé poids intra document document terme fréquence terme calculée correspond nombre terme apparait document dénominateur correspond nombre total termes document fréquence inverse document mesure importance terme corpus objectif donner poids important termes apparaissent documents poids inter documents calculé considérant logarithme inverse fréquence documents contiennent terme corpus nombre documents corpus nombre documents contiennent terme obtenu multipliant poids intra document inter documents terme document nouvelles pondérations adaptées petits volumes données textuelles évaluer représentativité terme classe document proposons mesure adaptée poids intra classes inter classes terme poids intra classe premier temps proposons méthode pondération fondée quence documents terme classe comme décrit appelé inner weightdf calculons poids inner weightdfij terme classe selon formule considérons terme représentatif classe cessairement terme fréquemment utilisé classe celui utilisé grand nombre documents classe telle mesure révéler particuliè rement pertinente traitement documents longueurs déséquilibrées effet descripteurs linguistiques présents documents grandes tailles auront impact similaire termes présents petits documents inner weightdfij nombre documents contenant terme classe nombre documents néanmoins inner weightdf confronté mêmes limites celles présentées section classes composées faible nombre documents savoir impact fréquence documents classes moins pourvues disproportionné rapport classes fournies ainsi proposons autre méthode pondération fondée terme plutôt document considérons fréquence terme plutôt fréquence document cette méthode appelée inner weighttf définissons poids inner weighttfij terme classe selon formule inner weighttfij nombre occurrences terme classe nombre termes total classe cette section avons redéfini pondérations intra classes appelées inner weighttf inner weightdf section suivante intéressons pondération inter classes poids inter classes proposons première méthode nommons inter weightclass définie formule cette méthode fondée nombre classes contiennent terme utilisée inter weightclassij bouillot nombre classes nombre classes contenant terme cependant estimons cette approche consiste considérer présence sence terme classe prendre compte fréquence restrictive comme exemple dessous présence classes nombre classes diminue influence poids inter classes importante poids final présence classes sémantiquement proches classes proches sémantiquement partager grand nombre termes commun fréquences potentiellement différentes présence grand nombre termes classe grand nombre documents documents longs nombre termes important proba bilité terme apparaisse moins classe forte suite observations suggérons considérer absence présence terme documents place classes seuls documents autres classes doivent compte effet représentativité terme classe étudiée prise compte pondération intra classe considérer ensemble documents corpus consisterait prendre doublement compte classe étudiée définissons poids inter weightdoc selon formule inter weightdocij nombre documents appartenant classe nombre documents appartenant classe contient nombre documents ensemble classes nombre documents classe nombre documents ensemble classes contenant terme nombre documents classe contient ajoutant permet prévenir uniquement utilisé quand évaluer propositions pondérations avons intégrées méthodes prentissage supervisé détaillées section utilisation mesures contexte apprentissage supervisé support vector machine naive bayes considérés parmi algorithmes classification supervisée performants cependant toujours adaptés nouvelles pondérations adaptées petits volumes données textuelles présence faibles volumes données nouvelles méthodes dération peuvent utilisées approches naives bayes class features centroid présentent avantages suivants facilité œuvre rendent classification automatique documents elles toutes fondées descripteurs pondérés niveau classes mesures adaptées algorithme enfin modèles obtenus faciles interpréter valider utilisateur final nouvelles mesures classification class feature centroid approche class feature centroid modèle récent présenté chaque classe considérée selon modèle vectoriel salton salton mcgill représentation chacune classes représentée vecteur termes représentation classe poids terme classe phase classification document étiqueté document aussi considéré comme vecteur termes distance similarité cosinus entre vecteur document chacun vecteurs classe calculée abord calculons représentation vectorielle chaque classe différentes combinaisons poids intra classes inter classes définis précédemment classij inner weight inter weightclass classij inner weight inter weightclass docij inner weight inter weightdoc docij inner weight inter weightdoc ensuite appliquons modèle class feature centroid appelons cfctf class expérimentation visant utiliser poids classij approche class feature centroid cfcdf class cfctf cfcdf comme pondération vecteur document étiqueté choisissons valeur booléenne indiquant absence présence terme document calculons produit scalaire entre vecteur document vecteurs classes nouvelles mesures naive bayes avons aussi intégré pondérations approche naive bayes classifieur naive bayes classifieur probabiliste défini lewis utilisé classification texte donne résultats malgré hypothèse rarement vérifiée indépendance conditionnelle classe descripteurs probabilité document étiqueté composé termes appartienne classe donnée booléen préféré impacté classes grandes termes classes majoritaires présents document étiqueté seraient estimés bouillot après avoir calculé poids terme classe estimons probabilité document étiqueté appartienne classe ajouter permet prévenir terme apparait classe quand probabilité expérimentations appelées class class suite document algorithmes comparaison section suivante section comparons méthodes classification supervisée pondérations chacun algorithmes différents algorithmes implé mentés implémentations utilise noyau polynomial platt libsvm utilise noyau linéaire chang implémentation naïve bayes arbre décision ladtree holmes autres expérimentations réalisées toujours naivebayes langley naivebayes multinomial mccallum libsvm fonction radial libsvm noyau polynomial chang reptree quinlan résultats étant moins satisfaisants présentés article chacun algorithmes testé validation croisée cross validation résultats indiqués après moyenne itérations nombre itérations retenu avoir nombre suffisant données apprentissage différentes expérimentations fondées approches détaillées section suivante expérimentations protocole expérimental avons évalué propositions corpus différents corpus reuters 21578 fréquemment utilisé communauté évaluer qualité modèles ensemble dépêches écrites anglais mises disposition agence reuters regroupées différentes catégories comme exemple sucre huile documents étiquetés manuellement corpus tweet composé tweets langue française durant campagnes présidentielle législative française avons collecté messages cours projet polop corpus composé million tweets provenant utilisateurs considérons corpus parti politique classe document ensemble tweets utilisateur chaque corpus avons supprimé outils stopwords termes inférieurs caractères raisons performance fiabilité interprétation résul avons conservé classes composées minima documents documents waikato disponibles adresse lirmm bouillot weipond reuters reuters lirmm bouillot polop nouvelles pondérations adaptées petits volumes données textuelles corpus corpus classes documents termes termes distincts reuters 21578 tweet expérimentation expérimentations corpus tweet itération termes moyenne termes termes distincts apprentissage représente catégories corpus reuters 21578 principaux partis politiques français parti socialise modem front gauche corpus tweet tableau présente caractéristiques corpus après traitement résultats qualité modèles évaluée précision rappel mesure moyenne harmonique précision rappel avons évalué micro moyenne calcul global rappel précision ensemble classes macro moyenne calcul rappel précision chaque classe calcul moyenne classes nakache metais étudier comportement pondérations approches naive bayes class feature centroid comparer résultats autres algorithmes classifica avons réalisé plusieurs expérimentations corpus reuters 21578 tweet avons étudié impact nombre termes nombre documents nombre classes résultats classification conséquences nombre termes classification abord corpus tweet avons nombre classes documents décidé supprimer aléatoirement termes diminuer nombre termes document avons réalisé expérimentations résumées tableau présentons évolution mesure fonction expérimentations bleau macro moyenne micro moyenne suivant tendance repro duisons micro moyenne expérimentations permettent tirer enseigne ments nouvelles pondérations intégrées approches naive bayes class feature centroid donnent résultats meilleurs algorithmes quand nombre front national faible nombre actifs twitter était représenté corpus bouillot expérimentation évolution mesure micro moyenne ladtree libsvm cfcdf class cfcdf cfctf class cfctf class class expérimentation expérimentations corpus reuters 21578 itération classes documents moyen documents classe termes distincts termes faible expérimentations elles donnent meilleurs résultats algorithmes ladtree libsvm conséquences nombre classes classification ensuite corpus reuters 21578 intéressons impact nombre classes fixons nombre documents classe supprimons classes passer classes avons réalisé expérimentations présentées bleau comme pouvions supposer algorithmes meilleur comportement sence nombre limité classes algorithmes suivent tendance reproduire détail résultats pouvons préciser ladtree impacté grand nombre classes autres nouvelles pondérations intégrées approches naive bayes class feature centroid donnent résultats gèrement meilleurs autres approches observations similaires présentées après aussi intéressant noter résultats similaires corpus langue française tweet corpus langue anglaise reuters 21578 résultats disponibles adresse lirmm bouillot weipond nouvelles pondérations adaptées petits volumes données textuelles expérimentation expérimentations corpus reuters 21578 itération classes documents moyen documents classe termes distincts 62808 expérimentation évolution mesure micro moyenne ladtree libsvm cfcdf class cfcdf cfctf class cfctf class class conséquences nombre documents classification troisième expérimentation concentrons étude impact documents classes fixons nombre classes diminuons nombre documents classes expérimentations réalisées elles résumées tableau comme classes équilibrées micro moyenne macro moyenne similaires décidons présenter évolution mesure micro moyenne tableau partir expérimentations pouvons conclure méthodes pondérations proposées article intégrées approches naive bayes class feature centroid légèrement meilleures autres algorithmes seulement devancé ladtree résistantes plupart algorithmes quand nombre documents dispo nibles apprentissage diminue fortement conclusions perspectives classification faible volume documents textuels reste problématique tualité effet nombre documents cesse croître existe domaines applications devons rapidement partir faible volume capable classer exemple tweets nécessitent suivis pouvoir classer bouillot temps information disponible attendre avoir nombre suffisant éléments toujours possible décideur souhaite avoir rapidement documents classés article avons proposé nouvelles mesures particulièrement adaptées faibles volumes données faible nombre documents faible nombre descripteurs avons également montré comment mesures pouvaient prises compte cadre supervisé notamment naive bayes approche basée centroides expérimentations menées corpus tweets benchmark tradi tionnel permis montrer nouvelles mesures meilleur comportement autres approches manipulation faible volume documents simples mettre place elles adaptées manipulation données évoluant rapidement comme tweets notre proposition fondée extension mesure actuels évaluent possibilité prendre compte autres mesures comme exemple okapibm25 robertson mieux appréhender impact seulement petits volumes également documents courts contexte classification proximité sémantique classes difficile prendre compte propositions notamment mesures inter intra classes avons montré elles adaptées faibles volumes souhaitons présent proposer utilisateur pouvoir mieux pondérer mesures fonction distribution volume termes références chang libsvm library support vector machines trans intell technol forman cohen learning little comparison classifiers given little proceedings european conference principles practice ledge discovery databases springer verlag class feature centroid classifier categorization proceedings international conference world holmes pfahringer kirkby frank multiclass alternating decision trees proceedings european conference machine learning london springer verlag langley estimating continuous distributions bayesian classifiers proceedings eleventh conference uncertainty artificial intelligence morgan kaufmann publishers myaeng effective techniques naive bayes classification knowledge engineering transactions lewis naive bayes forty independence assumption information retrieval springer verlag cohen supervised classification network using labels proceedings international conference advances social networks nouvelles pondérations adaptées petits volumes données textuelles analysis mining asonam washington computer society mccallum nigam comparison event models naive bayes classification workshop learning categorization volume citeseer nakache metais evaluation nouvelle approche juges inforsid xxiii congrès grenoble platt advances kernel methods chapter training support vector machines using sequential minimal optimization cambridge press quinlan programs machine learning francisco morgan kaufmann publishers robertson walker beaulieu willett okapi automatic filtering interactive track salton mcgill introduction modern information retrieval mcgraw zhang matwin discriminative parameter learning networks proceedings international conference machine learning clustering based classification requiring minimal labeled proceedings third interna tional conference mining washington computer society zhang liang class based feature weighting method classification journal computational information systems summary classification tasks provide classifier documents learning quite small paper evaluate performance traditional classification methods better evaluate their limitation dealing small amount documents during learning phase extend weighting features taking account specificities weighting methods evaluated class feature centroid naive bayes approaches different datasets demon strate efficiency approach relative other supervised learning algorithms content number features investigate parameters influencing algorithms numbers classes documents words