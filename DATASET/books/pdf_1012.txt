 Classification d’un tableau de contingence et modèle probabiliste Gérard Govaert Mohamed Nadif Heudiasyc UMR CNRS 6599 Université de Technologie de Compiègne BP 20529 60205 Compiègne Cedex France gerard govaert utc fr LITA Université de Metz Ile du Saulcy 57045 Metz Cedex France mohamed nadif univ metz fr Résumé Ces dernières années la classification croisée ou classification par blocs c’est à dire la recherche simultanée d’une partition des lignes et d’une partition des colonnes d’un tableau de données est devenue un outil très utilisé en fouille de données Dans ce domaine l’information se présente souvent sous forme de tableaux de contingence ou tableaux de co occurrence croisant les mo dalités de deux variables qualitatives Dans cet article nous étudions le problème de la classification croisée de ce type de données en nous appuyant sur un mo dèle de mélange probabiliste En utilisant l’approche vraisemblance classifiante nous proposons un algorithme de classification croisée basé sur la maximisation alternée de la vraisemblance associée à deux mélanges multinomiaux classiques et nous montrons alors que sous certaines contraintes restrictives on retrouve les critères du Chi2 et de l’information mutuelle Des résultats sur des données simulées et des données réelles illustrent et confirment l’efficacité et l’intérêt de cette approche 1 Introduction La classification automatique comme la plupart des méthodes d’analyse de données peut être considérée comme une méthode de réduction et de simplification des données Dans le cas où les données mettent en jeu deux ensembles I et J ce qui est le cas le plus fréquent la classification automatique en ne faisant porter la structure recherchée que sur un seul des deux ensembles agit de façon dissymétrique et privilégie un des deux ensembles contrairement par exemple à l’analyse factorielle des correspondances qui obtient simultanément des résultats sur les deux ensembles il est alors intéressant de rechercher simultanément une partition des deux ensembles Ce type d’approche a suscité récemment beaucoup d’intérêt dans divers do maines tels que celui des biopuces où l’objectif est de caractériser des groupes de gènes par des groupes de conditions expérimentales ou encore celui de l’analyse textuelle où l’objectif est de caractériser des classes de documents par des classes de mots Notons que dans ce domaine les données se présentent généralement sous forme d’un tableau de contingence où chaque cellule correspond au nombre d’occurrences d’un mot dans un document 457 RNTI E 6 Classification d’un tableau de contingence et modèle probabiliste Par ailleurs les modèles de mélange de lois de probabilité McLachlan et Peel 2000 qui supposent que l’échantillon est formé de sous populations caractérisées chacune par une distribution de probabilité sont des modèles très intéressants en classification permettant d’une part de donner un sens probabiliste à divers critères classiques et d’autre part de proposer de nouveaux algorithmes généralisant par exemple l’algorithme classique des k means Dans le cadre de la classification croisée on a pu ainsi montrer que l’algorithme Crobin Govaert 1983 adapté aux données binaires peut être vu comme une version classifiante de l’algorithme block EM Govaert et Nadif 2005 dans un cas particulièrement simple de mélange de lois de Bernoulli Dans ce papier nous proposons d’étendre ce travail à la classification croisée d’un tableau de contingence Dans la section 2 nous définirons le modèle de mélange croisé adapté à ces données La section 3 sera consacrée à la présentation de l’algorithme Cemcroki2 dont l’ob jectif est la maximisation de la vraisemblance classifiante associée au modèle précédent Nous montrerons dans la section 4 les liens de cet algorithme avec les critères du Chi2 et de l’in formation mutuelle Dans la section 5 des résultats sur des données simulées et des données réelles confirmeront l’efficacité de cet algorithme et l’intérêt de notre approche qui peut être considérée comme une approche complémentaire de l’analyse des correspondances qui s’ap puie sur la même représentation des données Notations Dans tout ce texte on notera x = xij le tableau de contingence construit sur les deux ensembles I et J ayant respectivement r et s éléments n = ∑ i j xij la somme des éléments du tableau et xi = ∑ j xij et x j = ∑ i xij ses marges On utilisera aussi le tableau des fréquences relatives fij = xij n fi = ∑ j fij et f j = ∑ i fij ses marges et les profils en ligne f iJ = fi1 fi fir fi Une partition en g classes de l’ensemble I sera notée z = z11 zik zng où zik = 1 si i est dans la classe k et zik = 0 sinon Nous adoptons les mêmes notations pour la partition w en m classes de l’ensemble J Par ailleurs pour simplifier la présentation les sommes et les produits portant sur I J z ou w seront indicés respectivement par les lettres i j et k et � sans indiquer les bornes de variation qui seront donc implicites Ainsi la somme ∑ i j k � portera sur toutes les lignes i allant de 1 à r les colonnes j allant de 1 à s les classes en ligne k allant de 1 à g et les classes en colonne � de 1 à m 2 Modèle de mélange croisé Pour aborder le problème de la classification croisée sous l’aspect modèle de mélange nous avons proposé Govaert et Nadif 2003 un modèle dont la densité s’écrit sous la forme ∑ z w ∈Z×W ∏ i pzi ∏ j qwj ∏ i j ϕziwj xij α où les densités ϕk� appartiennent à la même famille de densités de probabilité de R π = π1 πg et ρ = ρ1 ρm sont les proportions des classes k et � et α est un paramètre qui dépendra de la situation étudiée Z et W représentent respectivement les ensembles de partitions de I en g classes et de J en m classes 458 RNTI E 6 G Govaert et M Nadif Pour adapter ce modèle aux tables de contingence on suppose que chaque valeur observée xij dans un bloc k� de la table est la réalisation d’une variable aléatoire suivant une loi de Poisson de paramètre αiβjδk� où les deux premiers termes expriment les effets en ligne et en colonne et le dernier correspond à l’effet du bloc k� La recherche d’une partition s’appuyant sur ce modèle consiste à maximiser la vraisem blance classifiante associée à notre modèle Pour assurer l’identifiabilité du modèle nous avons ajouté les conditions ∑ � β�δk� = 1 et ∑ k αkδk� = 1 où αk = ∑ i k zikαi et β� = ∑ j � wj�βj Le problème de classification alors posé est de trouver les partitions z et w et le paramètre du modèle maximisant le critère Lc z w θ = ∑ i k zik log πk + ∑ j � wj� log ρ� + ∑ i j k � zikwj�xij log δk� où θ = π ρ δ11 δgm avec ∑ � x �δk� = 1 et ∑ k xk δk� = 1 3 Algorithme de classification croisée Pour maximiser Lc z w θ nous proposons de maximiser alternativement cette fonction en fixant w et ρ puis z et π En posant ui� = ∑ j wj�xij u � = ∑ i ui� et γk� = u �δk� on peut montrer que Lc z w θ se décompose en deux termes Lc z w θ = Lc z θ w +g x w ρ où le premier correspond à une log vraisemblance conditionnelle associée à un mélange de distributions multinomiales appliquées sur les échantillons u1 ur et le second terme ne dépend pas de z On peut alors utiliser l’algorithme CEM classique Celeux et Govaert 1992 pour obtenir la partition z En faisant un travail analogue pour la recherche de la partition w on obtient finalement l’algorithme Cemcroki2 suivant 1 Choix d’une position initiale z 0 w 0 θ 0 2 Répéter le calcul de z c+1 w c+1 θ c+1 à partir de z c w c θ c jusqu’à la convergence a Calcul de z c+1 π c+1 δ c+ 1 2 en utilisant l’algorithme CEM sur les données u1 ur à partir de z c π c δ c b Calcul de w c+1 ρ c+1 δ c+1 en utilisant l’algorithme CEM sur les données v1 vs à partir de w c ρ c δ c+ 1 2 Les expressions des estimations des paramètres du modèle associés à chaque bloc k� sont données par πk = zk r ρ� = w� s et δk� = xk� xk x � = n fk� fk f � où représente le cardinal d’un ensemble 459 RNTI E 6 Classification d’un tableau de contingence et modèle probabiliste 4 Liens avec le Chi2 et l’information mutuelle Après l’étape de maximisation le critère s’écrit Lc z w θ = ∑ k zk log πk + ∑ � w� log ρ� + n ∑ k � fk� log fk� fk f � + cste où ∑ k � fk� log fk� fk f � est l’information mutuelle associée au couple de partitions z et w De plus en utilisant l’approximation 2x log x ≈ x2 − 1 on obtient aussi Lc z w θ ≈ ∑ k zk log πk + ∑ � w� log ρ� + n 2 χ2 z w + cste On peut ainsi observer que lorsque les proportions sont fixées la maximisation de L c z w θ est équivalente à la maximisation de l’information mutuelle I z w et approximativement équivalente à la maximisation du critère χ2 z w la maximisation du critère du χ2 z w utilisé par exemple dans l’algorithme Croki2 Govaert 1983 ou de l’information mutuelle utilisée par exemple par Dhillon et al 2003 supposent donc implicitement que les données sont issues d’un mélange croisé de distributions de Poisson avec des proportions égales et que l’algorithme que nous proposons peut être considéré comme une généralisation de ces algorithmes 5 Expérimentations numériques 5 1 Données simulées Pour illustrer le comportement de notre algorithme Cemcroki2 et le comparer à l’algo rithme Croki2 nous avons étudié leurs performances sur des données simulées Nous avons sélectionné 48 types de données provenant d’un mélange croisé de Poisson à 3 classes en ligne et 2 en colonne nous avons retenu deux situations proportions égales p 1 = p2 = p3 et q1 = q2 ou non p1 = 0 70 p2 = 0 20 p3 = 0 10 et q1 = q2 et nous avons fait va rier le degré de mélange 5% 11% 16% 20% 27% 34% et la taille des données r × s = 30 × 20 50× 20 100× 20 500× 20 Pour chacun de ces 48 types de données nous avons généré 30 échantillons et pour chaque échantillon nous avons lancé les algorithmes Cemcroki2 et Croki2 30 fois à partir de situations initiales aléatoires et sélectionné la meilleure solution Afin de résumer le comportement des 2 algorithmes nous avons utilisé le taux d’erreur de classification entre les partitions simulées et les partitions obtenues Pour chaque algorithme et pour quelques exemples de degré de mélange nous avons reporté dans les figures 1 et 2 les moyennes des taux d’erreur obtenus avec les 30 échantillons Ces premières expériences montrent que dans toutes les situations et en particulier pour des tailles d’échantillon suffisamment grandes l’algorithme Cemcroki2 donne de très bons résultats Pour l’algorithme Croki2 on obtient de bons résultats uniquement pour des proportions égales 460 RNTI E 6 G Govaert et M Nadif 30 50 100 500 0 054 0 056 0 058 0 06 0 062 0 064 0 066 0 068 0 07 0 072 0 074 Taille de l’échantillon T au x d’ er re ur Degré de mélange 5% 30 50 100 500 0 15 0 16 0 17 0 18 0 19 0 2 0 21 0 22 0 23 Taille de l’échantillon T au x d’ er re ur Degré de mélange 16% 30 50 100 500 0 18 0 2 0 22 0 24 0 26 0 28 0 3 0 32 0 34 0 36 Taille de l’échantillon T au x d’ er re ur Degré de mélange 27% FIG 1 – Moyennes des taux d’erreur pour Cemcroki2 ligne continue et Croki2 ligne poin tillée pour p1 = p2 = p3 et q1 = q2 30 50 100 500 0 04 0 06 0 08 0 1 0 12 0 14 0 16 Taille de l’échantillon T au x d’ er re ur Degré de mélange 5% 30 50 100 500 0 2 0 25 0 3 0 35 0 4 0 45 0 5 Taille de l’échantillon T au x d’ er re ur Degré de mélange 16% 30 50 100 500 0 25 0 3 0 35 0 4 0 45 0 5 0 55 0 6 0 65 Taille de l’échantillon T au x d’ er re ur Degré de mélange 27% FIG 2 – Moyennes des taux d’erreur pour Cemcroki2 ligne continue et Croki2 ligne poin tillée pour p = 70 20 10 et q1 = q2 5 2 Données réelles Pour illustrer l’algorithme Cemcroki2 sur des donnés réelles nous avons choisi les données SMART ftp cs cornell edu pub smart Ces données sont définies à partir de 1033 résumés issus de la base Medline de 1460 résumés issus de la base CISI et de 1400 résumés issus de la base CRANFIELD En sélectionnant alors 2000 mots intéressants Dhillon 2001 définit ainsi les données Classic3 Nous avons alors comparé les résultats obtenus par Dhillon 2001 et Dhillon et al 2003 à l’aide de 2 algorithmes de classification croisée que nous noterons A2001 et A2003 avec ceux obtenus par notre algorithme Cemcroki2 La table 1 montre les matrices de confusion obtenues respectivement par Cemcroki2 A2001 et A2003 Il apparaît clairement que Cemcroki2 fournit les meilleurs résultats avec un nombre de documents mal classés de 49 contre 70 et 64 pour les algorithmes A2001 et A2003 Med Cis Cra z1 1008 23 2 z2 2 1453 6 z3 4 12 1383 Med Cis Cra 965 0 0 65 1458 0 3 2 1390 Med Cis Cra 977 22 34 1 1444 16 0 15 1384 TAB 1 – Cemcroki2 vs A2001 et A2003 461 RNTI E 6 Classification d’un tableau de contingence et modèle probabiliste 6 Conclusion En utilisant un modèle de mélange croisé de distributions de Poisson nous avons proposé l’algorithme Cemcroki2 et montré qu’il pouvait être vu comme une extension de Croki2 Ceci permet d’interpréter cet algorithme Croki2 et d’en déduire par exemple que l’utilisation du χ2 ou de l’information mutuelle supposent implicitement l’égalité des proportions des classes Cette approche permet alors de prendre en considération de nouvelles situations comme celles où les proportions des classes sont très différentes Les premières expériences sur des données simulées et réelles montrent que ce nouvel algorithme apparaît clairement meilleur que Croki2 dans cette situation Références Celeux G et G Govaert 1992 A classification EM algorithm for clustering and two sto chastic versions Computational Statistics and Data Analysis 14 3 315–332 Dhillon I 2001 Co clustering documents and words using bipartite spectral graph partitio ning In Seventh ACM SIGKDD Conference San Francisco California USA pp 269–274 Dhillon I S Mallela et D Modha 2003 Information theoretic co clustering In Procee dings of The Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 2003 pp 89–98 Govaert G 1983 Classification croisée Thèse d’état Université Paris 6 France Govaert G et M Nadif 2003 Clustering with block mixture models Pattern Recognition 36 463–473 Govaert G et M Nadif 2005 An EM algorithm for the block mixture model IEEE Tran sactions on Pattern Analysis and Machine Intelligence 27 643–647 McLachlan G J et D Peel 2000 Finite Mixture Models New York Wiley Summary Most of methods of statistical analysis are concerned with understanding relationships among variables With categorical variables these relationships are usually studied from data that has been summarized by a contingency table giving the frequencies of observations cross classified by two variables To classify the rows and the columns simultaneously of this con tingency table we can use Croki2 which can be employed jointly with the correspondence analysis In this paper using a Poisson block mixture model we have proposed the Cemcroki2 algorithm which can be viewed as an extension of Croki2 In this setting the probabilistic interpretation of Croki2 constitutes an interesting support to consider various situations and avoids the development of ad hoc methods for example it allows one to take into account situations in which the proportions of clusters are different by applying Cemcroki2 whereas the χ2 and the mutual information criteria assume equal proportions implicitly From our ex periments the new algorithm appears clearly better than Croki2 in real situations when the proportions are not necessary equal 462 RNTI E 6