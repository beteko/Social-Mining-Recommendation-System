 actes_non_num\351rotes pdf Probabilistic Multi classifier by SVMs from voting rule to voting features Anh Phuc TRINH David BUFFONI Patrick GALLINARI Laboratoire d’Informatique de Paris 6 104 avenue du Président Kennedy 75016 Paris {anh phuc trinh david buffoni patrick gallinari} lip6 fr 1 Probabilistic multi classifier by SVMs Definition of the posterior probabilities for multiclass problem Let S = { x1 y1 x2 y2 xm ym } be a set of m training examples We assume that each example xi is drawn from a domain X ∈ R n and each class yi is an integer from the set Y = {1 k} with k > 2 The posterior probabilities of multiclass problem is a conditional probability of each class y ∈ Y given an instance x P y = i|x = pi 1 subject to k∑ i=1 pi = 1 pi > 0 ∀i 2 There are two approaches either one vs one or one vs rest in solving the multi class pro blem by SVMs Following the setting of the one vs one approach we have the voting method proposed by Tax 2002 using decision values fij x of SVMs to estimate the posterior proba bilities Another method of Wu T F 2004 obtains pi from the pairwise probability of Platt 2000 2 From voting rule to voting features Definition of the voting features Suppose that S = { x1 y1 x2 y2 xm ym } is the set of m training examples drawn from an independent and identical distribution A voting feature representation Θ C fij x ×Y → B d is a function Θ that maps a configuration of decision values c fij x ⊂ C fij x and a class yi ∈ Y to a d dimensional feature vector thus the set of voting features is denoted by VF The posterior probabilities definied on the set of voting features VF pi = P y = i|x λ = exp Pd l=1 λl×Θl x y=i P k y=1 exp P d l=1 λl×Θl x y is estimated in maximizing the logarithm of the conditional likeli hood Nigam et McCallum 1999 and is solved by unconstrained optimization problem Probabilistic Multi classifier by SVMs FIG 1 – Accuracy rates of three different methods on seven UCI and deft08 test datasets are obtained using the polynominal kernel The voting rule our and Wu’s methods are figured respectively by violet red and yellow columns 3 Experiments To compare the performance of our method with others we selected seven datasets from the UCI learning data repository 1 and the DEFT08 dataset 2 Références Nigam K J L et A McCallum 1999 Using maximum entropy for text classification IJCAI 99 Workshop on Machine Learning for Information Filtering 1 61–67 Platt J 2000 Probabilistic outputs for support vector machines and comparisons to regular ized likelihood methods Advances in Large Margin Classifiers 14 61–74 Tax D R P W D 2002 Using two class classifiers for multiclass classification Interna tional Conference on Pattern Recognition 2 124– 127 Wu T F Chih Jen Lin R C W 2004 Probability estimates for multi class classification by pairwise coupling International Conference on Pattern Recognition 5 975–1005 1 mlearn ics uci edu MLSummary html 2 deft08 limsi fr 