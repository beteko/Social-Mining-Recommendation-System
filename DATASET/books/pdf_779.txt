(Actes_non_num \ 351rotes.pdf) probabilistes multi-classificateur par SVM de la règle de vote au vote dispose Anh Phuc TRINH, David BUFFONI, Patrick Gallinari * * Laboratoire d'Informatique de Paris 6 104, avenue du Président Kennedy, 75016 Paris. {Anh-phuc.trinh, david.buffoni, patrick.gallinari} @ lip6.fr, 1 multi-classificateur probabilistes par SVM Définition des probabilités a posteriori pour le problème Soit multiclassent S = {(x1, y1), (x2, y2) ,. . . , (Xm, YM)} un ensemble d'exemples de formation m. Nous supposons que chaque exemple xi est tracé à partir d'un domaine X ∈ R n et chaque classe yi est un nombre entier de la série Y = {1,. . . , K} k> 2. Les probabilités postérieures du problème est multiclassent une probabilité conditionnelle de chaque classe y ∈ Y donné une instance x P (y = i | x) = pi (1) sous réserve kΣ i = 1 pi = 1 pi> 0 ∀i (2) Il existe deux approches, soit un contre-un ou un contre-repos, dans la résolution de la pro- blème multi-classe par SVM. Suite à la mise de l'approche d'un contre-un, nous avons la méthode de vote proposée par (Taxes, 2002) à l'aide de valeurs de décision (x la) de SVM pour estimer si les probabilités a posteriori. Un autre procédé de (Wu T-F, 2004) obtient pi de la probabilité de paires de (Platt, 2000). 2 De règle de vote à vote dispose Définition du vote comporte suppose que S = {(x1, y1), (x2, y2),. . . , (Xm, ym)} est l'ensemble des m exemples de formation élaborés à partir d'une distribution indépendante et identique. Une représentation de la fonction de vote Θ: C (fij (x)) × Y → B d est une Θ fonction qui fait correspondre une configuration de valeurs de décision c (fij (x)) ⊂ C (fij (x)) et une classe yi ∈ Y pour un vecteur caractéristique de dimension d, donc l'ensemble des fonctions de vote est notée VF. Les probabilités postérieures DÉFINIES sur l'ensemble des caractéristiques vote VF pi = P (y = i | x, λ) = exp (Pd l = 1 λl × Θl (x, y = i)) P ky = 1 exp (P dl = 1 × λl Θl (x, y)) est estimé à maximiser le logarithme de la hotte likeli- conditionnelle (Nigam et McCallum, 1999) et est résolu par problème d'optimisation non contrainte. Probabilistes multi-classificateur par SVM Fig. 1 - taux de précision de trois méthodes différentes sur sept jeux de données de test UCI et deft08 sont obtenus en utilisant le noyau polynomiale; La règle de vote, notre et les méthodes de Wu sont figurés respectivement par le violet, les colonnes rouge et jaune 3 expériences pour comparer les performances de notre méthode avec les autres, nous avons sélectionné sept ensembles de données à partir du référentiel de données d'apprentissage UCI 1, et l'ensemble de données DEFT08 2. Nigam Références, J. K. L. et A. McCallum (1999). En utilisant l'entropie maximale pour la classification texte. IJCAI-99 Atelier sur l'apprentissage automatique d'information sur le filtrage 1, 61-67. Platt, J. (2000). sorties probabilistes pour machines à vecteurs supports et comparaisons régulière- Ized méthodes de vraisemblance. Les progrès de la grande marge classificateurs 14, 61-74. Fiscale, D. R. P. W. D. (2002). En utilisant classificateurs à deux classes pour la classification multiclassent. Conférence Inter- tional sur la reconnaissance des formes 2, 124- 127. Wu T-F, Chih-Jen Lin, R. C. W. (2004). Les estimations de probabilité de classification multi-classe par couplage par paires. Conférence internationale sur la reconnaissance des formes 5, 975-1005. 1http: //mlearn.ics.uci.edu/MLSummary.html 2http: //deft08.limsi.fr/