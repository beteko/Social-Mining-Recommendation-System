articles assemblage pdfConstruction de noyaux pour l’apprentissage supervisé à partir d’arbres aléatoires Vincent Pisetta Pierre Emmanuel Jouve Djamel A Zighed RITHME 59 bd Vivier Merle 69003 Lyon vpisetta rithme eu FENICS 59 bd Vivier Merle 69003 Lyon pjouve fenics com Laboratoire ERIC 5 av Pierre Mendès France 69500 Bron abdelkader zighed univ lyon2 fr Résumé Nous montrons qu’un ensemble d’arbres de décision avec une compo sante aléatoire permet de construire un noyau efficace destiné à l’apprentissage supervisé Nous étudions théoriquement les propriétés d’un tel noyau et mon trons que sous des conditions très souvent rencontrées en pratique il existe une séparabilité linéaire entre exemples de classes distinctes dans l’espace induit par celui ci Parallèlement nous observons également que le classique vote à la ma jorité d’un ensemble d’arbres est un hyperplan sans garantie d’optimalité dans l’espace induit par le noyau Enfin comme le montrent nos expérimentations l’utilisation conjointe d’un ensemble d’arbres et d’un séparateur à vaste marge SVM aboutit à des résultats extrêmement encourageants 1 Introduction Parmi les techniques d’apprentissage statistique les plus performantes se trouvent les mé thodes à base de noyaux dont le représentant le plus célèbre est très certainement le Séparateur à Vaste Marge Vapnik 1996 Son emploi motivé initialement par les premiers résultats théo riques de l’apprentissage statistique s’est encore plus largement répandu suite aux nombreux succès empiriques apportés par cet apprenant Pavlidis et al 2004 Markowska Kaczmar et Kubacki 2005 Polat et Günes 2007 L’une des particularités du SVM est d’utiliser la fa meuse astuce du noyau pour introduire de la non linéarité dans la frontière de décision sans augmenter la complexité algorithmique de l’apprentissage Ainsi le choix du noyau est sans doute le paramètre le plus important de l’algorithme différents noyaux pouvant aboutir à des résultats très différents Plusieurs indicateurs ont été proposés dans la littérature afin d’évaluer la qualité d’un noyau a priori autrement dit avant même l’exécution de l’apprentissage Le plus connu est probable ment le Kernel Target Alignment KTA Cristianini et al 2002 bien que d’autres tels que FSM Nguyen et Ho 2008 ou la polarisation Baram 2005 aient également montré des ré sultats intéressants L’intérêt de ces indicateurs est qu’ils permettent d’évaluer la pertinence RNTI E 19 525 Noyaux et arbres aléatoires d’un noyau de manière relativement rapide avec une complexité en O n2 où n est le nombre d’exemples et ce sans recourir au calcul du modèle Outre l’évaluation l’intérêt pratique de ces mesures est qu’elles premettent de construire de manière supervisée un noyau adapté à un jeu de données spécifique Le principal paradigme utilisant ce principe est le Multiple Kernel Learning MKL basé sur l’hypothèse que la combinaison de plusieurs noyaux peut aboutir à un résultat plus intéressant que la seule sélection du meilleur Ainsi l’objectif du MKL est de chercher une combinaison linéaire à coefficients positifs de plusieurs noyaux distincts de manière à maximiser KTA ou n’importe quel autre indicateur approprié via par exemple la programmation semidéfinie Lanckriet et al 2004 ou l’optimisation séquentielle Bach et al 2004 D’autres méthodes Sonnenburg et al 2006 Rakotomamonjy et al 2008 in corporent le problème d’optimisation du noyau directement dans le problème d’optimisation global du SVM Toutes ces stratégies ont montré d’excellents résultats et une capacité d’ap prentissage très élevée Cependant elles souffrent de plusieurs problèmes Tout d’abord elles ne résolvent pas le problème du choix des noyaux initiaux ceux qui doivent être combinés Généralement des noyaux gaussiens sont utilisés bien qu’aucune justification théorique n’ait été apportée à ce choix Ensuite elles souffrent toutes d’une complexité algorithmique élevée par exemple la méthode de Lanckriet et al 2004 a une compexité en O n6 Enfin elles supposent toutes que les descripteurs utilisés pour l’apprentissage sont continus alors qu’il ar rive fréquemment que des variables catégorielles soient présentes dans les données réelles Nous proposons dans cet article d’utiliser des arbres de décision aléatoires afin de générer des noyaux efficaces pour l’apprentissage Les célèbres Forêts aléatoires Breiman 2001 sont un cas particulier de cette méthodologie et se révèlent en fait être des constructrices de noyaux très performantes En effet elles ont au fond une motivation similaire au principe du MKL à savoir l’utilisation d’un ensemble de classifieurs plutôt que la recherche du meilleur En sec tion 2 après avoir brièvement rappelé quelques généralités sur l’apprentissage par SVM nous décrivons un cadre théorique permettant d’obtenir un bon noyau pour l’apprentissage Après avoir rappelé le mécanisme des forêts aléatoires nous verrons comment construire des noyaux à partir de celles ci section 3 Nous montrerons théoriquement que sous des conditions très faibles le noyau construit par un tel ensemble induit un espace contenant un séparateur li néaire entre exemples de classes distinctes section 3 et 4 Nous montrerons également que la célèbre décision du vote à la majorité n’est rien d’autre qu’un hyperplan dans l’espace généré par la Forêt section 4 Toutes ces observations suggèrent que l’utilisation d’un SVM dans l’espace généré par un ensemble d’arbres aléatoires peut se révéler intéressante En section 5 nous montrons expérimentalement le bien fondé de cette hypothèse Enfin en section 6 nous concluons 2 Séparateur à vaste marge et qualité d’un noyau 2 1 Cadre général du SVM Nous considérons dans cet article le cadre de la classification supervisée à deux classes Formellement étant donné n exemples x1 xn décrits dans un espace à d dimensions noté X et le vecteur correspondant des réponses ou classes étiquettes y = {y1 yn} avec yi = {−1 +1} l’objectif de la classification supervisée est trouver une fonction f telle qu’une RNTI E 19 526 V Pisetta et al fonction dite fonction de perte L y f x soit minimale Généralement cette dernière est choisie comme la fonction des moindres carrés L yi f xi = yi − f xi 2 ou la fonction hinge loss L yi f xi = max 0 1− yif xi Un spectre très large de méthodes est dédié à la résolution de ce problème Parmi celles ci le SVM fait partie des plus performantes Il consiste à chercher la fonction f prédéfinie comme f x = signe 〈w ϕ x 〉+ b en trouvant la solution optimale au problème suivant P1 min w∈ϕ x b∈ ξ∈ n 0 5 〈w w〉+ C n∑ i=1 ξi s c yi 〈w ϕ xi 〉+ b ≥ 1− ξi ξi ≥ 0 où C > 0 est le paramètre de régularisation et ϕ x est obtenue à partir du mapping ϕ X → est classiquement dénomé espace des caractéristiques et est un espace de Hilbert muni du produit scalaire 〈 〉 Schölkopf et Smola 2001 Il est à noter que peut être de dimension infinie c’est pourquoi l’obtention de la solution du programme P1 passe généralement par sa forme duale P2 min α∈ n 0 5 n∑ i=1 n∑ j=1 αiαjyiyjK̃ xi xj − n∑ i=1 αi s c 0 ≤ αi ≤ C∑n i=1 αiyi = 0 K̃ représente le noyau défini comme K̃ xi xj = 〈ϕ xi ϕ xj 〉 Le passage par la forme duale permet de réécrire la fonction de décision f qui devient f x = signe n∑ i=1 yiαiK̃ x xi + b Cette écriture utilise uniquement le noyau K̃ sans avoir besoin de calculer explicitement ϕ x On parle alors d’astuce du noyau Sa principale utilité est de pouvoir plonger les exemples dans des espaces de grande dimension en calculant uniquement des produits scalaires Cela produit un gain d’espace mémoire et de rapidité de calcul considérable En contrepartie le modèle perd toute interprétabilité puisque la fonction de décision ne dépend plus des variables initiales Signalons enfin que le noyau doit impérativement être un produit scalaire valide dans Pour cela il doit respecter les conditions de Mercer stipulant qu’un K̃ symétrique est un produit scalaire valide si et seulement si sa matrice de Gram K définie par Ki j = K̃ xi xj est toujours semi définie positive 2 2 Qualité d’un noyau Dans ce contexte le succès du SVM dépend complètement du noyau utilisé Une pratique largement répandue consiste à faire apprendre le SVM à partir d’un noyau souvent choisi au hasard en l’absence de connaissances préalables puis de modifier petit à petit ses paramètres pour finalement sélectionner celui donnant les meilleurs résultats Partant du constat que la RNTI E 19 527 Noyaux et arbres aléatoires complexité algorithmique d’un SVM varie entre O n2 et O n3 sans compter la construction de la matrice noyau qui a un coût de O n2 l’évaluation d’un nombre important de paramé trages peut rapidement devenir un lourd handicap De cette problématique est né le besoin de pouvoir évaluer un noyau en amont de l’apprentissage Pour répondre à ce besoin Cristia nini et al 2002 ont créé un indicateur de qualité d’un noyau appelé Kernel Target Alignment KTA défini sur un échantillon S comme suit A S K yty = 〈K yty〉 F√ 〈K K〉F 〈yty yty〉F = ∑n i j=1 yiyjK xi xj n √∑n i j=1 K xi xj 2 où 〈 〉F représente le produit scalaire de Frobenius La matrice yty est habituellement ap pelée matrice idéale ou matrice cible Chaque cellule i j de yty a une valeur de 1 si xi et xj appartiennent à la même classe −1 sinon KTA a une valeur comprise entre −1 et 1 et est d’autant plus élevé que le noyau est considéré comme performant Vu autrement KTA calcule simplement la somme des "similarités" induites par le noyau entre individus de même classe et y soustrait la somme des similarités des individus de classes différentes Le dénominateur permet de normaliser cette valeur entre −1 et 1 Une caractéristique intéressante de KTA est son relatif faible coût algorithmique Au delà de la simple évaluation le critère KTA peut aussi être utilisé pour apprendre directement le noyau Le Multiple Kernel Learning est sans doute le principe le plus ap proprié dans cette optique Il consiste à supposer que la somme de plusieurs noyaux peut conduire à un noyau plus performant que la sélection du meilleur noyau individuellement Plus techniquement on désire trouver un vecteur μ = μ1 μp μi ≥ 0 ∀i tel que le noyau K ′ xi xj = ∑p t=1 μtKt xi xj ait un alignement maximal avec la matrice idéale y ty La contrainte de positivité des coefficients μi est essentielle pour assurer que K ′ soit semi définie positive En l’absence d’informations préalables les noyaux initiaux Kt ainsi que leur nombre sont choisis plus ou moins au hasard L’intérêt de la combinaison linéaire est double Tout d’abord la mise en forme de la fonc tion objective sous cette forme permet le recours à un arsenal important de techniques d’optimi sation Mais plus que par l’aspect pratique le choix de l’agrégation de noyaux par combinaison linéaire se justifie grâce à une possible réécriture de KTA En effet l’alignement de la somme de deux matrices noyaux K1 et K2 sur un échantillon S s’écrit A S K1 +K2 y ty = ‖K1‖F ‖K1+K2‖F A S K1 y ty + ‖K2‖F ‖K1+K2‖F A S K2 y ty On voit alors clairement que combiner deux noyaux et par extension p noyaux peut être avantageux La situation est particulièrement intéressante si les deux noyaux ont un alignement individuel élevé et s’ils sont indépendants la notion d’indépendance renvoie ici au produit de Frobenius entre les deux matrices K1 et K2 En effet dans le cas de deux noyaux iden tiques nous avons ‖K1‖F + ‖K2‖F = ‖K1 +K2‖F et par conséquent A K1 +K2 yty = A K1 y ty = A K2 y ty Dans la section suivante nous montrons que le paradigme des forêts aléatoires Breiman 2001 possède de fortes connexions avec celui de la maximisation de KTA et peut par consé quent être utilisé pour construire des noyaux performants RNTI E 19 528 V Pisetta et al 3 Forêts aléatoires et espace induit 3 1 Principe des forêts aléatoires L’algorithme des forêts aléatoires est un des apprenants les plus connus et les plus per formants mis au point à ce jour La méthode s’est même révélée comme la plus performante d’une série de classifieurs dans une récente étude à très grande échelle Caruana et al 2008 La procédure générique des forêts aléatoires consiste à répéter M fois de façon indépendante le processus suivant – Echantillonner l’ensemble d’apprentissage initial S pour obtenir S′ – Induire un arbre de décision sur S′ en recherchant pour chaque noeud le meilleur écla tement à partir d’un sous ensemble aléatoire de tous les éclatements possibles – Développer l’arbre jusqu’à l’obtention des feuilles les plus pures possibles pas d’éla gage La prédiction de la classe d’un nouvel exemple x′ s’effectue en lui attribuant la classe la plus fréquemment votée par les M arbres Les forêts aléatoires introduisent une double randomi zation par l’échantillonnage d’une part et d’autre part par une recherche non exhaustive du meilleur éclatement à chaque noeud de l’arbre De nombreuses techniques différentes de forêts ont été mises au point chacune gérant de façon différente l’introduction de processus aléatoires dans la procédure Dans Breiman 2001 l’auteur pratique un échantillonnage par le biais du bootstrap et recherche le meilleur éclatement en ne testant que sur un nombre restreint d′ des d variables initiales Geurts et al 2006 n’utilisent pas d’échantillonnage mais augmentent le degré de randomization en ne sélectionnant qu’un point de coupe au hasard sur chacune des d′ variables préalablement sélectionnées D’après les auteurs en adaptant correctement d′ on obtient un gain de temps de calcul important sans détérioration de performance par rapport aux forêts de Breiman Un cas particulier des forêts aléatoires est le Bagging Breiman 1996 qui consiste à échantillonner selon le bootstrap sans introduire de randomization dans le recherche du meilleur éclatement L’utilisation de mécanismes aléatoires s’explique par le fait qu’une forêt admet l’existence d’une borne théorique de l’erreur en généralisation EG En effet d’après Breiman 2001 EG ≤ ρ 1 − s2 s2 où ρ représente la corrélation moyenne entre les prédictions des clas sifieurs et s est la force de prédiction moyenne autrement dit une fonction de la moyenne du taux de bon classement Nous renvoyons le lecteur intéressé à Breiman 2001 pour plus d’informations sur ces deux paramètres Cette borne de l’erreur est indépendante de la façon dont on injecte de l’aléatoire pour construire la forêt On remarque aisément que d’après cette écriture la situation idéale consiste à ce que les classifieurs soient non corrélés et aient indi viduellement la plus faible erreur possible Ainsi les divers algorithmes de forêts aléatoires ont été spécifiquement mis au point pour tenter de trouver un bon compromis entre pertinence individuelle des classifieurs et faible corrélation Nous avons vu dans la section précédente que le critère KTA suggère que la combinaison de noyaux indépendants et performants individuellement est une stratégie intéressante pour construire un noyau global plus performant Nous pouvons remarquer d’emblée la connexion RNTI E 19 529 Noyaux et arbres aléatoires entre le cadre décrit par les forêts aléatoires et celui décrit par KTA En conséquence si nous trouvons un moyen de construire un noyau représentant un arbre nous pourrions alors re présenter la forêt comme une somme de noyaux individuellement pertinents et peu corrélés paradigme idéal selon KTA puis tenter d’appliquer des algorithmes de décision tels que les SVM dans l’espace généré par l’ensemble d’arbres Dans la section suivante nous montrons comment dériver un noyau à partir d’un arbre puis à partir d’un ensemble d’arbres Nous ver rons de plus que ce dernier a des propriétés intéressantes et nous paraît donc bien adapté pour l’apprentissage 3 2 Construire un noyau à partir d’une forêt d’arbres La particularité d’un arbre est de pouvoir découper l’espace en plusieurs zones dénommées feuilles A la fin du développement chaque exemple appartient à une et une seule feuille Un exemple xi peut alors être représenté par un vecteur de taille l où l est le nombre de feuilles de l’arbre La jeme composante de ce vecteur vaut 1 si xi appartient à la feuille j 0 sinon Dans ce cas un vecteur représentant un exemple ne contient qu’une seule valeur égale à 1 et l − 1 égales à 0 On peut voir cette représentation comme un nouvel espace de description de dimension l pour les exemples Il est alors aisé de remarquer que le produit scalaire 〈xi xj〉 de deux individus xi et xj dans ce nouvel espace vaut 1 si xi et xj sont dans la même feuille 0 sinon Ainsi le noyau K xi xj = 1 si xi et xj sont dans la même feuille 0 sinon est bien le noyau de l’espace induit par l’arbre et K est bien semi définie positive Dans le cas où M arbres différents ont été induits il est également aisé d’en dériver un noyau Nous pouvons considérer cette fois qu’un exemple peut être représenté par un vecteur T = T11 Tij TMlM où Tij représente la j eme feuille de l’arbre i T est par consé quent de dimension D = ∑M i=1 li où li est le nombre de feuilles du i eme arbre Dans ce cas la matrice des produits scalaires de l’ensemble Kens s’écrit Kens x x′ = ∑M i=1 Ki x x ′ où Ki est la matrice des produits scalaires du ieme arbre Kens est donc semi définie positive et correspond au nombre de fois où les exemples x et x′ se sont retrouvés dans la même feuille Breiman 2001 avait déjà évoqué l’intérêt d’une matrice presque équivalente pour l’analyse de similarité La matrice en question est équivalente à Kens mais est en plus normalisée par le nombre d’arbres M pour ramener chaque valeur entre 0 et 1 Notons la forte connexion entre la construction du noyau par les forêts et l’écriture de KTA dans le cas de la somme de noyaux En introduisant une part d’aléatoire dans l’algorithme des forêts l’objectif est d’augmenter l’indépendance entre les arbres et donc les noyaux issus de ces arbres tout en assurant que chaque arbre soit assez pertinent et ait donc un noyau avec un alignement assez élevé Dans son article fondateur Breiman 2001 développait les arbres jusqu’à l’obtention de feuilles pures Cela signifie que les noeuds terminaux des arbres ne contenaient que des individus de même classe Empiriquement Breiman 2001 a remarqué que cette façon de procéder donnait les meilleurs résultats Dans l’hypothèse où l’obtention de feuilles pures est possible c’est à dire qu’il n’existe pas d’exemples de classes distinctes ayant toutes les variables avec les mêmes valeurs nous pouvons alors démontrer que l’espace induit par l’ensemble de M arbres admet l’existence d’un séparateur linéaire dès que M = 1 Nous démontrons cela pour M = 1 RNTI E 19 530 V Pisetta et al Proposition 1 Arbre parfait et séparabilité linéaire Un arbre développé sur un échantillon S jusqu’à avoir uniquement des feuilles pures induit un espace dans lequel il existe au moins un séparateur linéaire entre les individus de classes distinctes de S Preuve L’espace généré par l’arbre est de dimension l où l est égal au nombre de feuilles de l’arbre Les feuilles étant pures les exemples sont alors représentés par l points chacun représentant un ensemble d’exemples de même classe dans un espace de dimension l Une fonction linéaire dans un espace de dimension p ayant une VC dimension de p+ 1 il est donc toujours possible de dichotomiser les exemples de classes distinctes Il est maintenant facile de démontrer l’existence de la séparabilité pour M ≥ 1 Proposition 2 Ensemble d’arbres et séparabilité linéaire Un ensemble d’arbres dont l’un au moins est développé jusqu’à avoir uniquement des feuilles pures sur un échantillon S in duit un espace dans lequel il existe au moins un séparateur linéaire entre les individus de classes distinctes de S Preuve D’après la Proposition 1 on sait qu’il suffit d’un arbre à feuilles pures pour induire un espace dans lequel on peut séparer linéairement les individus de classes distinctes Ainsi l’ajout d’arbres est équivalent à ajouter des dimensions à l’espace induit par l’arbre à feuille pure Par conséquent il est toujours possible de séparer linéairement les exemples de classes distinctes dans ce nouvel espace Remarquons que même si aucun arbre n’est développé jusqu’à l’obtention de feuilles pures il est néanmoins tout à fait possible que l’espace généré par un ensemble de M arbres contienne un séparateur linéaire Une condition suffisante serait que tous les couples possibles d’exemples de classes distinctes ne tombent pas dans la même feuille au moins une fois dé coule de la Proposition 2 et du principe de VC dimension En pratique bien d’autres situa tions sont susceptibles d’aboutir à un espace pouvant séparer linéairement les exemples ayant une étiquette différente Les forêts aléatoires utilisant un échantillonnage avant la construction des arbres n’entrent pas dans le cadre décrit par les Propositions précedentes puisqu’une partie des exemples ne sera pas bien classée avec certitude Dans la section suivante nous étudions la procédure de classement par vote à la majorité et montrons que celle ci est un hyperplan dans l’espace induit par l’ensemble d’arbres Ce résultat va également nous permettre de démontrer l’existence d’une séparabilité linéaire pour les types de forêts où il existe un échantillonage 4 Forêt vote et séparabilité associée Lorsqu’une nouvelle instance x′ se présente l’algorithme des forêts aléatoires effectue une prédiction en associant à cette instance l’étiquette la plus fréquemment attribuée par les M arbres Cette procédure simple de classement est communément appelée vote à la majorité Plusieurs travaux ont souligné que d’autres schémas de vote pouvaient apporter des amélio rations très significatives au reclassement Tsymbal et al 2006 Robnik Sikojna 2004 La plupart des autres schéma de vote utilisent un système de pondération donnant un poids à RNTI E 19 531 Noyaux et arbres aléatoires chaque votant chaque arbre proportionnel à sa pertinence Cette dernière est généralement évaluée grâce à l’ensemble out of bag OOB de chaque arbre L’ensemble OOB désigne sim plement les exemples qui n’ont pas été sélectionnés par l’échantillonnage lors de la construc tion du ieme arbre Par exemple Tsymbal et al 2006 utilise l’ensemble OOB pour estimer le taux de bon classement d’un arbre et lui attribuer un poids en fonction de ce taux Nous ap pelons cette dernière technique d’agrégation vote pondéré Nous pouvons facilement montrer la proposition suivante Proposition 3 Votes d’un ensemble Les critères de vote à la majorité et de vote pondéré sont des hyperplans dans l’espace induit par l’ensemble d’arbres Preuve Rappelons que dans l’espace induit par un ensemble d’arbres l’exemple x est re présenté par un vecteur T = T11 Tij TMlM où Tij représente la j eme feuille de l’arbre i Par simplicité d’écriture nous omettons l’indice relatif à l’arbre dans la suite de la preuve Il suffit d’observer que la décision relative au vote à la majorité peut s’écrire f x = signe ∑D i=1 αiTi où αi = 1 si la majorité des exemples de la feuille i ces exemples vé rifient alors Ti = 1 appartiennent à la classe {1} et −1 sinon Le vote pondéré consiste simplement à choisir les αi dans un ensemble potentiellement différent de {−1 1} Le fait d’utiliser un hyperplan comme fonction de décision a des garanties intéressantes concernant l’erreur en généralisation Vapnik 1996 Ceci donne une autre vision de la robus tesse de la décision d’une forêt d’arbres Une forêt peut ainsi être vue comme un construc teur de noyau pouvant induire un espace dans lequel il existe une séparabilité linéaire entre exemples de classes distinctes La procédure de vote consiste alors à trouver un hyperplan dans cet espace Ceci rappelle bien entendu le principe du SVM qui fonctionne sur un mode si milaire Le résultat de la Proposition 3 nous permet également de généraliser l’existence d’une séparabilité même en présence d’un échantillonnage Proposition 4 Ensemble d’arbres et séparabilité linéaire Soit un ensemble de M arbres construits chacun en utilisant une proportion p des n exemples de l’échantillon S sélectionnés aléatoirement Sous l’hypothèse que tous les arbres soient construits jusqu’à avoir uniquement des feuilles pures alors la probabilité θ qu’il existe un séparateur linéaire dans l’espace induit par l’ensemble d’arbres est minorée θ ≥ ⎛⎝ M∑ i>M 2 M i pi 1− p M−i ⎞⎠n Preuve Si les arbres sont développés jusqu’à contenir uniquement des feuilles pures alors la probabilité qu’un exemple xi choisi au hasard dans l’échantillon S soit bien classé par l’arbre Aj est minorée par la probabilité qu’il soit tiré au sort par Aj et est donc supérieure ou égale à p Lorsque M arbres sont construits la probabilité qu’un exemple xi soit bien classé par la règle du vote à la majorité est supérieure à la probabilité P que xi soit tiré au sort strictement plus de M 2 fois ce qui correspond à la probabilité qu’une loi binomiale B M p soit stric tement supérieure à M 2 Ainsi P = ∑M i>M 2 M i pi 1− p M−i et la probabilité pour RNTI E 19 532 V Pisetta et al que les n exemples soient bien classés est minorée par θ = ∑M i>M 2 M i pi 1− p M−i n Le vote à la majorité étant un hyperplan dans l’espace induit par l’ensemble d’arbres il est donc possible de séparer linéairement les exemples de classes distinctes avec une probabilité au moins égale à θ Corollaire 1 4 Sous les hypothèses de la Proposition 4 de p > 0 5 et de n fini alors θ → 1 lorsque M →∞ Preuve D’après Condorcet 1785 ∑M i>M 2 M i pi 1− p M−i tend vers 1 lorsque M tend vers l’infini et p > 0 5 Ainsi dans le cas n fini nous avons bien le corollaire Le corollaire précedent montre donc qu’il est toujours possible de générer M arbres indui sant un espace dans lequel il existe une séparabilité linéaire entre exemples de classes dis tinctes pourvu que l’échantillonnage en amont de chaque arbre sélectionne plus de la moitié des exemples de S et que M soit suffisamment grand Notons que la probabilité θ est une borne pessimiste de la probabilité réelle Dans la réalité la séparabilité devrait exister avec relative ment moins d’arbres que suggérés par la Proposition 4 dans la mesure où les arbres classent toujours quelques exemples de l’ensemble OOB correctement et qu’il n’y a objectivement au cune raison de penser qu’il soit nécessaire que le vote à la majorité classe correctement tous les individus pour que la séparabilité linéaire existe Le cas particulier d’un échantillonnage boots trap n’est pas explicitement traité On remarquera simplement que dans ce cas la proportion d’individus p distincts tirés au sort vaut approximativement 0 63 On peut donc légitimement s’attendre à ce que la Propostion 4 se vérifie Comme nous l’avons vu le critère du vote à la majorité et du vote pondéré sont deux pos sibilités de classement à partir d’un ensemble d’arbres Ils peuvent également être vu comme des hyperplans dans l’espace induit par l’ensemble Le vote pondéré offre parfois des perfor mances plus intéressantes que le vote à la majorité en accordant des poids plus important à certains arbres Devant toutes ces constatations il nous paraît légitime d’utiliser un SVM dans l’espace induit par l’ensemble d’arbres L’algorithme consiste alors à résoudre le programme P2 avec le noyau défini en section 3 2 On peut voir cet algorithme comme la recherche d’une pondération optimale des règles de décision issues des arbres l’optimalité objective étant le compromis entre maximisation de la marge et du taux de bon classement Il est impor tant de noter que le noyau utilisé correspondant au produit scalaire des individus dans l’espace généré par les indicatrices des règles les poids calculés par le SVM sont implicitement attri bués aux feuilles et donc aux règles réprésentées par ces feuilles et non aux arbres comme c’est le cas du vote pondéré Finalement utiliser les forêts d’arbres pour générer un noyau spécifiquement dédié au SVM est une pratique intéressante puisque le paradigme des forêts aléatoires est intimement lié à ce lui de la recherche d’un bon noyau Parallèlement la question de la pondération optimale des règles issues d’une forêt peut être résolue via un SVM Ainsi les deux problématiques ont une part de symétrie Dans la section suivante nous réalisons plusieurs expérimentations associant SVM et noyaux issus des forêts RNTI E 19 533 Noyaux et arbres aléatoires 5 Expérimentations Nous comparons les taux de bon reclassement de trois techniques d’apprentissage sur 12 jeux de données de l’UCI tableau 1 Les trois techniques testées sont le Multiple Kernel Lear ning MKL le vote à la majorité d’une forêt aléatoire et le SVM construit avec un noyau issu de la même forêt que celle utilisée pour le vote à la majorité Les résultats du MKL sont issus de Varma et Babu 2009 ce qui explique que 7 bases de données n’aient pas de résultat pour cette méthode Le même protocole de test que dans Varma et Babu 2009 a été employé pour évaluer les trois algorithmes Nous avons partitionné chaque jeu de données en deux parties 70 pourcents des individus pour l’apprentissage et les 30 pourcents restants pour le test Ce processus a été répété 20 fois et la moyenne des taux de bons reclassement a été calculée La forêt a été construite en utilisant 300 arbres développés chacun sur un échantillon bootstrap et jusqu’à l’obtention des feuilles les plus pures possible Le meilleur éclatement a été recher ché en restreignant la recherche sur un nombre de variables égal à l’entier le plus proche de la racine du nombre total d’attributs Enfin concernant le SVM quatre valeurs possibles du paramètre de régularisation C ont été utilisées C = 1 C = 10 C = 100 et C = 10000 L’ensemble d’apprentissage a été partitionné en 2 parties 70 pourcents pour apprendre et 30 pourcents pour la validation Nous avons alors retenu le paramétrage donnant les meilleurs résultats sur l’échantillon de validation Jeu de données n Attributs MKL Vote SVM + Noyau Forêt Sonar 208 60 0 823 0 846 0 894* Liver 345 6 0 727 0 725 0 736 Parkinsons 195 22 0 926 0 923 0 926 Pima 768 8 0 772 0 754 0 809* Ionosphere 351 34 0 93 0 94 0 94 German 1000 20 0 752 0 777 Wdbc 569 30 0 963 0 972 Australian 690 14 0 803 0 855* Heart 270 13 0 807 0 829 Spambase 4601 57 0 969 0 982* Vote 435 16 0 960 0 958 Chess 3196 36 0 994 0 996 TAB 1 – Proportions de bon classement selon les trois méthodes d’apprentissage Le tableau 1 montre plusieurs choses intéressantes Tout d’abord le vote à la majorité de la forêt aléatoire a une performance équivalente à celle du MKL Aucune différence significative test de Student au seuil de 0 05 n’a pu être trouvé entre les deux méthodes La performance issue du couplage SVM forêt est très intéressante En effet elle est significativement meilleure que le MKL sur 2 des 5 jeux de données communs une différence significative est marquée par un ’*’ Comparativement à la forêt le SVM forêt est significativement meilleur 4 fois sur 12 Notons que sur les jeux où la différence n’est pas significative le duo SVM forêt est néanmoins le plus performant des trois algorithmes excepté sur le jeu Vote ou la forêt est meilleure mais l’écart est non significatif RNTI E 19 534 V Pisetta et al Il est important de souligner qu’un apprentissage concernant le nombre optimal de va riables à sélectionner pour évaluer les éclatements pourrait améliorer les résultats issus de la forêt et du SVM forêt Parallèlement cela montre que calibrer une forêt est une tâche simple Un autre avantage de l’utilisation des forêts pour la construction de noyaux est la rapidité de calcul Cela laisse ainsi plus de temps pour la recherche du paramètre C bien que dans nos expérimentations les différences entre les diverses valeurs n’aient pas été trop importantes Remarquons de plus que l’algorithme des forêts est parallèlisable peut prendre en compte des attributs catégoriels et éventuellement des valeurs manquantes mais cette problématique n’est pas abordée ici 6 Conclusion et perspectives L’article a montré que les forêts avec une composante aléatoire étaient particulièrement efficaces pour construire un noyau adapté à l’apprentissage supervisé par SVM Ceci est dû au fait que le cadre théorique motivant leur constuction présente de fortes similarités avec celui de l’apprentissage optimal de noyaux Nous avons vu comment dériver un noyau à partir d’un ensemble d’arbres et montré que sous des conditions peu exigeantes l’espace induit par l’ensemble d’arbres admettait l’existence d’un séparateur linéaire entre exemples de classe distincte L’utilisation d’un SVM dans cet espace produit des résultats supérieurs au MKL et au vote à la majorité de l’ensemble Parallèlement le SVM peut être vu comme une fonction d’agrégation des règles induites par la forêt aléatoire Plusieurs perspectives découlent de ce travail Tout d’abord nous souhaitons analyser théo riquement et empiriquement d’autres façons de construire un noyau à partir d’un ensemble d’arbres Nous pouvons par exemple prendre en compte la topologie de l’arbre en calculant la similarité entre deux individus par le biais du noeud commun le plus proche Nous souhaitons également étudier la possibilité de construire des noyaux représentant une infinité d’arbres L’article montre que l’étude théorique des méthodes ensemblistes doit rester de premier plan et rappelle ce qui était déjà connu à savoir qu’il est souvent plus judicieux de faire coopérer des méthodes a priori concurrentes plutôt que de les opposer Références Bach F G Lanckriet et M Jordan 2004 Multiple kernel learning conic duality and the smo algorithm Proceedings of the 21st International Conference on Machine Learning 775–782 Baram Y 2005 Learning by kernel polarization Neural Computation 17 1264–1275 Breiman L 1996 Bagging predictors Machine Learning 24 123–140 Breiman L 2001 Random forests Machine Learning 45 5–32 Caruana R N Karampatziakis et A Yessenalina 2008 An empirical evaluation of super vised learning in high dimensions Proceedings of the 25th International Conference on Machine Learning 307 96–103 Condorcet J 1785 Essai sur l’application de l’analyse à la probabilité des décisions rendues à la pluralité des voix Imprimerie Royale RNTI E 19 535 Noyaux et arbres aléatoires Cristianini N J Kandola A Elisseef et J Shawe Taylor 2002 On kernel target alignment Advances in Neural Information Processing Systems 367–373 Geurts P D Ernst et L Wehenkel 2006 Extremely randomized trees Machine Learning 63 3–42 Lanckriet G N Cristianini L Ghaoui P Bartlett et M Jordan 2004 Learning the kernel matrix with semi definite programming Journal of Machine Learning Reasearch 5 27–72 Markowska Kaczmar U et P Kubacki 2005 Support vector machines in handwritten dig its classification Proceedings of the 5th International Conference on Intelligent Systems Design and Applications 406–411 Nguyen C et T Ho 2008 An efficient kernel matrix evaluation measure Pattern Recogni tion 41 3366–3372 Pavlidis P I Wapinski et W Noble 2004 Support vector machine classification on the web Bioinformatics 20 586–587 Polat K et S Günes 2007 Breast cancer diagnosis using least square support vector ma chine Digital Signal Processing 694–701 Rakotomamonjy A F Bach Y Grandvalet et S Canu 2008 Simplemkl Journal of Ma chine Learning Research 9 2491–2521 Robnik Sikojna M 2004 Improving random forests Proceedings of the 15th European Conference on Machine Learning 3201 359–370 Schölkopf B et A Smola 2001 Learning with Kernels Support Vector Machines Regu larization Optimization and Beyond Cambridge MA USA MIT Press Sonnenburg S G Rötsch B Schölkopf et G Schäfer 2006 Large scale multiple kernel learning Journal of Machine Learning Research 7 1531–1565 Tsymbal A M Pechenizkiy et P Cunningham 2006 Dynamic integration with random forests Proceedings of the 17th European Conference on Machine Learning 4212 2006 56–68 Vapnik V 1996 The Nature of Statistical Learning Theory New York Springer Verlag Varma M et B Babu 2009 More generality in efficient kernel learning ACM International Conference Proceedings Series 382 2491–2521 Summary We show that an ensemble of decision trees with a randomness component allows the construction of powerful kernels adapted to supervised learning We study theoretically such kernels ans show that under weak conditions it is possible to find a linear separator in the space induced by these kernels We also observe that the classical majority voting is an hyperplane in the space induced by these last We demonstrate empirically that combining a Support Vector Machine and ensemble of decision trees produces excellent results RNTI E 19 536 