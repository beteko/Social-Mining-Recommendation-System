 Construction de descripteurs à partir du coclustering pour la classification supervisée de séries temporelles Dominique Gay Marc Boullé Orange Labs Lannion FRANCE prenom nom orange com Résumé Nous présentons un processus de construction de descripteurs pour la classification supervisée de séries temporelles Ce processus est libre de tout paramétrage utilisateur et se décompose en trois étapes i à partir des don nées originales nous générons de multiples nouvelles représentations simples ii sur chacune de ces représentations nous appliquons un algorithme de co clustering iii à partir des résultats de co clustering nous construisons de nou veaux descripteurs pour les séries temporelles Nous obtenons une nouvelle base de données objets attributs dont les objets identifiant les séries temporelles sont décrits par des attributs issus des diverses représentations générées Nous utili sons un classifieur Bayésien sur cette nouvelle base de données Nous montrons expérimentalement que ce processus offre de très bonnes performances prédic tives comparées à l’état de l’art 1 Introduction La classification de séries temporelles TSC est un sujet qui a été intensivement étudié durant les dernières années Le but est de prédire la classe d’un objet une série temporelle ou courbe τi = 〈 t1 x1 t2 x2 tmi xmi 〉 où xk k = 1 mi est la valeur de la courbe au temps tk étant donné un ensemble de séries temporelles labellisées d’apprentissage Les problèmes de TSC sont différents des problèmes de classification supervisée dans les bases transactionnelles puisqu’il y a une dépendance temporelle entre les attributs ainsi l’ordre des attributs importe La TSC est applicable dans de nombreux domaines dont les données sont des séries temporelles e g pour le diagnostic médical par exemple la classification d’élec trocardiogramme de patients mais aussi dans d’autres domaines comme la maintenance de machines industrielles la finance la météo Le grand nombre d’applications a succité de nombreuses approches toutefois la majorité de la communauté s’est attachée à suivre le pro cessus suivant Liao 2005 i choisir une nouvelle représentation des données ii choisir une mesure de similarité ou une distance pour comparer deux séries temporelles et enfin iii utiliser l’algorithme NN du plus proche voisin avec la mesure choisie sur la représentation choisie comme classifieur Ding et al 2008 propose un état de l’art des différentes représen tations et mesures ainsi qu’une étude expérimentale comparative basée sur le classifieur NN Il en ressort que le classifieur NN couplé avec une distance Euclidienne ou Dynamic Time Warping DTW présente les meilleures performances prédictives pour les problèmes de TSC Construction de descripteurs pour la classification supervisée de séries temporelles Plus récemment Bagnall et al 2012 démontre expérimentalement que les performances de certains classifieurs augmentent fortement en utilisant certaines représentations par rapport au domaine temporel original ainsi pour un classifieur donné il existe une forte variance de performance selon la transformation de données utilisée Pour pallier ce problème Bagnall et al 2012 proposent une méthode ensembliste basée sur trois représentations ainsi que sur les données originales les résultats expérimentaux démontrent i l’importance de la repré sentation dans les problèmes de TSC et ii qu’une simple combinaison ensembliste de plu sieurs représentations permet d’atteindre des performances prédictives très compétitives Nous adhérons à cette conclusion sur l’importance des représentations toutefois une des faiblesses de certains classifieurs ensemblistes est la perte en interprétabilité due à la combinaison par pondération des classifeurs Un exemple illustratif les graphiques de la figure 1 confirment l’intérêt du changement de représentation si à partir des données originales a il n’est pas évident de différencier les deux classes bleu rouge de simples transformations ici l’intégrale cumulative b et la double intégrale cumulative c facilitent la discrimination des classes En effet après trans formation par double intégrale cumulative les courbes séries ayant des valeurs supérieures à 100 sont bleues et les courbes avec des valeurs inférieures à 100 sont rouges Sur cet exemple jouet extrait de la base TwoPatterns de la base de l’UCR Keogh et al 2011 une transfor mation et deux descripteurs nous permettent de caractériser les deux classes de courbes 0 20 40 60 80 100 120 −2 −1 5 −1 −0 5 0 0 5 1 1 5 2 TwoPatterns data sample original values a 0 20 40 60 80 100 120 −30 −20 −10 0 10 20 30 TwoPatterns data sample cumulative integral values b 0 20 40 60 80 100 120 −1000 −500 0 500 1000 TwoPatterns data sample cumulative double integral values c FIG 1 – Extrait de quelques séries temporelles pour deux classes de la base TwoPatterns représentation originale intégrale cumulative et double intégrale cumulative D Gay M Boullé Dans cet article nous proposons un processus de construction de descripteurs interprétables pour le problème de TSC Notre contribution est donc essentiellement méthodologique La section suivante décrit les différentes étapes de notre processus i une étape de transformation des données originales en de nouvelles représentations ii une étape de coclustering iii l’exploitation des résultats du coclustering pour construire de nouveaux descripteurs et ainsi une nouvelle base de données et enfin le classifieur utilisé La section 3 rapporte la validation expérimentale de notre processus 2 Processus de construction de descripteurs Notations Pour le problème de classification supervisée de séries temporelles TSC une série temporelle est définie par une paire τi yi où τi est un ensemble d’observations or données τi = 〈 t1 x1 t2 x2 tmi xmi 〉 de longueur mi et yi une valeur de classe Une base de données de séries temporelles D est définie comme un ensemble de paires D = { τ1 y1 τn yn } où chaque série temporelle peut avoir un nombre d’observations dif férent donc une longueur différente 1 Le but est de construire un classifieur à partir de D pour prédire la classe de nouvelles séries temporelles τn+1 τn+2 Pour ce faire nous appliquons le processus de construction de descripteurs décrit par la figure 2 dont chaque étape est détaillée dans la suite Data transformation Data1 Data2 … Data grid based coclustering co n st ru ct io n Recoding data FFFForig … FFFFDp CCCC a1 a2 a3 b1 b2 p1 p2 Cid1 Time series data … … Datap F e a tu re co n st ru ct io n Transformed data Cid2 … Cidn Enriched data FIG 2 – Processus de construction de descripteurs Etape 1 Transformation des données en de multiples nouvelles représentations Etape 2 Coclustering sur chacune des représenta tions Etape 3 Construction d’un ensemble de descripteurs pour chaque résultat de cocluste ring pour la construction d’une nouvelle base de données réunissant les différents ensembles de descripteurs construits 1 Notons aussi que les séries d’une base peuvent avoir des valeurs différentes pour tk k = 1 mi Construction de descripteurs pour la classification supervisée de séries temporelles 2 1 Transformations Représentations De nombreuses méthodes de transformation ont été proposées dans la littérature pour re présenter les séries temporelles par exemple les transformations polynomiales symboliques spectrales en ondelettes voir Ding et al 2008 pour une vue synthétique sur les repré sentations Pour notre processus nous utilisons les données originales ainsi que six représen tations Les dérivées DV et DDV Nous utilisons les dérivées et dérivées doubles des séries tem porelles i e les différences et différences doubles locales entre les valeurs au temps t et t−1 DV τi = 〈 t1 0 t2 x2 − x1 t2 − t1 tmi xmi − xmi−1 tmi − tmi−1 〉 DDV τi = DV DV τi Ces transformations nous permettent de représenter l’évolution locale croissante décroissante accélération décélération des séries Les intégrales cumulatives IV et IIV Nous utilisons aussi les intégrales cumulatives simples et doubles des séries temporelles calculées via l’approximation par la méthode des trapèzes IV τi = 〈 t1 0 t2 t2 − t1 · x1 + x2 2 tmi IVmi−1 τi + tmi − tmi−1 · xmi−1 + xmi 2 〉 IIV τi = IV IV τi Ces transformations nous permettent de représenter l’évolution globale accumulée des séries Le spectre de puissance PS Une série temporelle peut être décomposée en une combi naison linéaire de sinusoïdes d’amplitudes p q et de phase w Ainsi τi t = k=mi∑ k=1 pk cos 2πwkt + qk sin 2πwkt On appelle transformée de Fourier la série de paires τi FT = 〈 p1 q1 pmi qmi 〉 Et τif le spectre de puissance PS est obtenu par la somme des carrés des coefficients de Fourier τif = 〈 f1 a1 fmi ami 〉 où ak = p2i +q2i k = 1 mi Les fk représentent la fréquence et les ak la puissance du signal Cette transformation nous permet de représenter la série dans le domaine de fréquence La fonction d’auto corrélation ACF La transformation par fonction d’auto corrélation ACF est τiρ = 〈 t1 ρ1 tmi ρmi 〉 où ρk = ∑j=mi−k j=1 xj − x̄ · xj+k − x̄ m · s2 D Gay M Boullé et où x̄ et s2 sont la moyenne et la variance de la série originale L’ACF décrit comment les valeurs originales séparées par une certaine durée évoluent ensemble L’ACF permet de détecter des structures d’autocorrélation dans les séries temporelles Ainsi pour une base de données de séries temporelles Dorig nous construisons six nou velles bases de données DDV DDDV DIV DIIV DPS DACF suivant la transformation utilisée Dans la suite par souci de généralisation un objet d’une de ces représentations sera appelé “courbe” au lieu de série temporelle puisque DPS n’est plus dans le domaine temporel 2 2 Coclustering Une courbe peut être vue comme un ensemble de points X Y décrits par ses valeurs en abscisses et en ordonnées Un ensemble de courbes peut être vu comme un ensemble de points Cid X Y où Cid est l’identifiant de courbe Cette représentation tridimensionnelle une variable catégorielle et deux variables numériques d’une base de courbes est nécessaire à l’application de méthodes de coclustering En effet le but est de partitionner la variable ca tégorielle et discrétiser les variables numériques afin d’obtenir des clusters de courbes et des intervalles pour X et Y Le résultat final est une grille tridimensionnelle dont chaque cellule est définie par un groupe de courbes un intervalle de X et un intervalle de Y Pour ce faire nous utilisons la méthode de coclustering KHC de Boullé 2012 utilisable via le logiciel KHIOPS 2 Originalement développée pour le cas général des données fonction nelles Ramsay et Silverman 2005 elle s’adapte bien pour le cas particulier des courbes comme définies ci dessus KHC est libre de tout paramétrage utilisateur robuste évite le sur apprentissage supporte des bases de données de courbes de plusieurs millions de points et sa complexité en temps est de Θ N √ N logN où N est le nombre de points de la base c’est donc une méthode adaptée à notre problématique KHC est une méthode basée sur l’estimation de densité constante par morceaux et suit l’ap proche MODL Boullé 2006 similaire à une approche Bayésienne MAP Maximum A Pos teriori Le modèle optimal M i e la grille optimale est obtenue par optimisation gloutonne bottom up d’un critère Bayésien qui mise sur un compromis entre précision et robustesse du modèle cost M = − log p M | D ︸ ︷︷ ︸ posterior = − log p M ︸ ︷︷ ︸ prior × p D |M ︸ ︷︷ ︸ vraisemblance La grille obtenue constitue un estimateur non paramétrique de la densité jointe des courbes et dimensions des points Du point de vue de la théorie de l’information selon Shannon 1948 les logarithmes négatifs de probabilités s’interprètent comme des longueurs de codage Ainsi le critère cost peut être interprété comme la longueur de codage du modèle la grille plus la longueur des données D connaissant le modèle M selon le principe de Minimum Description Length MDL Rissanen 1978 Un exemple de visualisation de résultat de coclustering La figure 3 présente un exemple de visualisation de deux clusters de courbes de la grille optimale obtenue pour la base de don nées TwoPatterns transformée par IIV Le graphique a resp b présente un cluster dont les courbes sont majoritairement de classe c1 bleu dans l’exemple introductif de la figure 1 2 khiops com Construction de descripteurs pour la classification supervisée de séries temporelles a b FIG 3 – Représentation de l’information mutuelle des cellules pour deux clusters de courbes obtenus par la méthode KHC sur la base TwoPatterns entière transformée par IIV a cluster dont les courbes sont majoritairement de classe c1 b cluster dont les courbes sont majori tairement de classe c2 Plus les couleurs sont vives plus la différence de distribution de points entre la cellule courante donc du cluster et le reste des données est significative resp c2 rouge dans l’exemple introductif La grille optimale obtenue par KHC est composée de 133 clusters de courbes 7 intervalles pour X et 22 intervalles pour IIV L’estimateur de densité jointe obtenue i e la grille optimale est plus fin que nécessite le problème de départ en effet la base TwoPatterns est un problème de classification à 4 classes or nous obtenons 133 clusters de courbes ce qui nous donne un potentiel de caractérisation fine des classes du problème lorsque la représentation s’y prête 2 3 Construction de descripteurs A partir de chaque résultat de coclustering obtenus sur chacune des représentations utilisées Dorig DDV DDDV DIV DIIV DPS DACF nous créons un ensemble de descripteurs i e Forig FDV FDDV FIV FIIV FPS FACF Ces descripteurs sont les attributs de notre nouvelle base de données dont les objets sont les courbes Pour chaque représentation nous créons trois types de descripteurs définis comme suit Soit Drep une représentation parmi celles décrites plus haut Soit Mrep = KHC Drep la grille tridimensionnelle optimale obtenue par coclustering KHC sur Drep On note kC le nombre de clusters de Mrep et kY le nombre d’intervalles de Mrep pour la dimension Y Nous créons les attributs suivants – kC attributs numériques un pour chaque cluster C de courbes issu de Mrep dont la valeur pour une courbe cid est la distance définie par d cid C = cost Mrep cid∪C − cost Mrep i e la différence de coût entre le modèle optimal Mrep et Mrep cid∪C la grille optimale dans laquelle on a intégré la courbe cid au cluster de courbes C Intuiti vement la distance d mesure la perturbation qu’apporte l’intégration d’une courbe à un cluster de la grille optimale D Gay M Boullé – Un attribut catégoriel indiquant l’index du cluster de courbes iC le plus proche d’un objet courbe cid selon la distance définie ci dessus i e au sens du critère cost utilisé pour l’optimisation de la grille – kY attributs numériques un pour chaque intervalle iY de Y issu deMrep dont la valeur pour une courbe cid est le nombre de points de cid dans l’intervalle iY Ainsi pour une courbe donnée cid nous avons les informations suivantes fournies par les des cripteurs pour chaque représentation i la distance de cid à tous les clusters de courbes ii l’index du cluster de courbes le plus proche et iii le nombre de points de cid dans chaque intervalle de Y 2 4 Classification supervisée Nous avons vu que notre processus de construction de descripteurs peut générer des cen taines de descripteurs par représentation Ainsi l’ensemble total d’attributs générés Ftot peut contenir plusieurs milliers d’attributs Le classifieur en fin de processus doit pouvoir supporter un grand nombre d’attributs et doit être capable de sélectionner les attributs pertinents pour la tâche de classification supervisée Nous choisissons le classifieur sélectif Naive Bayes SNB Boullé 2007 qui répond à ces attentes Notons aussi que le prédicteur SNB exploite des prétraitements de type MODL de variables numériques par discrétisation et de variables ca tégorielles par groupement de valeurs en utilisant des estimateurs de densité conditionnelle robustes Ainsi les varibles construites profitent de ces prétraitements et offrent un potentiel d’interprétabilité voir section 3 De plus le SNB est libre de tout paramétrage utilisateur ce qui facilite l’utilisation de l’ensemble du processus 3 Validation expérimentale L’implémentation de notre processus utilise des outils déjà existants KHC pour le cocluste ring et SNB pour la classification supervisée disponibles sur khiops com Le branchement entre ces outils est encore au stade de prototype et a été réalisé avec MATLAB Protocole Pour valider notre processus nous utilisons 26 bases de données de classifica tion de séries temporelles 17 bases de l’UCR Keogh et al 2011 et 9 nouvelles bases intro duites dans Bagnall et al 2012 Une description succinte des caractéristiques de ces données est présentée dans la table 1 Cet ensemble de données présente une grande variété de bases tant en terme d’applications qu’en terme de nombre d’instances de classes et en longueur de série Les expériences sont menées en suivant un protocole train test prédéfini pour chaque base Nous comparons notre processus de classification qu’on appellera ici MODL TSC avec i DTW NN un classifieur basé sur le plus proche voisin et la distance Dynamic Time Warping considéré par la littérature comme difficile à battre ii TSC ENSEMBLE Bagnall et al 2012 qui exploite de multiples représentations via une méthode ensembliste et l’agorithme du plus proche voisin NN Notons que depuis 2012 il existe d’autres bases de données répertoriées par l’UCR pour la TSC Toutefois nous nous limitons à ces 26 bases pour nos expérimentations comparatives car les performances prédictives de nos concurrents rapportées de Bagnall et al 2012 ne Construction de descripteurs pour la classification supervisée de séries temporelles Bases Train Test Longueur Classes Ligthing2 60 61 637 2 Lighting7 70 73 319 7 ECG200 100 100 96 2 Adiac 390 391 176 37 FaceFour 24 88 350 4 50words 450 455 270 50 CBF 30 900 128 3 Fish 175 175 463 7 GunPoint 50 150 150 2 OSULeaf 200 242 427 6 SwedishLeaf 500 625 128 15 SyntheticControl 300 300 60 6 Trace 100 100 275 6 TwoPatterns 1000 4000 128 4 Wafer 1000 6174 152 2 Yoga 300 3000 426 2 FaceAll 560 1690 131 14 Beef 30 30 470 5 Coffee 28 28 286 2 OliveOil 30 30 570 4 Earthquakes 322 139 512 2 HandOutlines 1000 300 2709 2 FordA 3571 1320 500 2 FordB 3601 810 500 2 ElectricDevices 8953 7745 96 7 ARSim 2000 2000 500 2 TAB 1 – Description des bases de données de séries temporelles sont accessibles que pour ces bases D’autre part les bases de séries de l’UCR sont un cas par ticulier du cadre général dans lequel nous nous plaçons puisque pour une base donnée toutes les séries sont de la même longueur et utilisent le même domaine temporel i e les tk sont identiques Résultats Les résultats en terme de taux d’erreur sont reportés dans la table 2 Le meilleur résultat pour chaque base est mis en gras Premièrement les résultats globaux Taux d’erreur moyen nombre de victoires et rang moyen indiquent que MODL TSC est très compétitif par rapport aux deux méthodes de l’état de l’art Même si nous avons l’avantage numérique cet ensemble de données ne nous permet pas de montrer qu’il y a une différence significative de performance entre les trois méthodes En ef fet nous avons procédé au test de Friedman Demsar 2006 et ne pouvons rejeter l’hypothèse nulle Notons les performances remarquables de MODL TSC sur les bases OSULeaf FordA Elec tricDevices et ARSim Sur ces bases la différence de performance est d’au moins 0 1 i e 10% par rapport à ses concurrents Ici l’apport des représentations via les nouveaux des cripteurs est certainement à l’oeuvre dans notre processus alors que TSC ENSEMBLE n’ex ploite que 3 représentations et que DTW NN se base sur les données originales A l’inverse les performances de MODL TSC sont dramatiques pour les bases ECG200 Coffee et OliveOil La différence de performance tourne en notre défaveur au moins 0 1 par rapport aux deux concur rents Nous pensons que cette différence peut être dûe à deux raisons i les bases d’appren tissage de ECG200 OliveOil et Coffee sont très petites quelques dizaines de courbes ce qui rend l’apprentissage difficile ii nous n’avons pas encore trouvé la bonne représentation qui D Gay M Boullé Attributs Bases DTW NN TSC ENSEMBLE MODL TSC V DV DDV IV IIV PS ACF Ligthing2 0 1311 0 2295 0 2623 74 21 20 49 45 24 52 Lighting7 0 2877 0 3014 0 2603 56 21 21 38 36 22 39 ECG200 0 12 0 11 0 21 17 14 14 21 28 15 22 Adiac 0 3887 0 3555 0 3581 27 30 29 32 34 31 63 FaceFour 0 1818 0 1364 0 1023 16 12 29 22 34 11 18 50words 0 3297 0 3516 0 3143 194 179 77 118 89 55 129 CBF 0 0111 0 1722 0 0344 15 3 3 18 22 7 15 Fish 0 2171 0 2171 0 2286 25 36 26 31 44 48 47 GunPoint 0 0867 0 0467 0 02 29 15 13 20 25 23 20 OSULeaf 0 3843 0 4215 0 2562 104 107 30 105 102 31 64 SwedishLeaf 0 1776 0 152 0 0944 26 29 32 33 46 19 30 SyntheticControl 0 0233 0 09 0 0233 19 14 14 25 36 13 22 Trace 0 01 0 19 0 0 32 13 5 35 39 22 TwoPatterns 0 0007 0 1 0 0118 765 42 40 234 156 17 38 Wafer 0 0045 0 0044 0 0 149 183 178 52 74 23 47 Yoga 0 1653 0 1633 0 2667 50 54 54 72 84 38 41 FaceAll 0 1923 0 2941 0 2953 33 29 21 42 65 24 24 Beef 0 3667 0 4 0 4667 25 14 8 29 34 18 25 Coffee 0 0714 0 1786 0 3571 15 11 13 21 22 15 22 OliveOil 0 1667 0 2 0 6 45 29 16 40 56 26 46 Earthquakes 0 2734 0 2662 0 2878 8 17 21 64 54 16 27 HandOutlines 0 16 0 1243 0 1271 76 365 303 94 118 126 83 FordA 0 267 0 1515 0 0136 185 87 49 233 259 156 78 FordB 0 3812 0 2654 0 3272 107 63 64 226 272 147 70 ElectricDevices 0 35 0 3779 0 2608 124 62 80 163 109 126 137 ARSim 0 4426 0 3215 0 0005 645 23 56 676 293 30 830 Moyenne ER 0 19965 0 21620 0 19918 Victoires 9 7 12 Moyenne Rang 2 2 03846 1 88461 TAB 2 – Comparaisons des performances prédictives en terme de taux d’erreur ER pour les méthodes DTW NN TSC ENSEMBLE et notre processus MODL TSC sur 26 bases La dernière colonne rapporte le nombre de descripteurs construits par MODL TSC pour chaque représen tation Construction de descripteurs pour la classification supervisée de séries temporelles permet de construire de bons descripteurs C’est le cas pour OliveOil où l’étape de coclus tering de chacune des six représentations ne génère qu’un seul cluster de courbes Pour les autres ECG200 et Coffee même si l’étape de coclustering produit des clusters donc fournit des descripteurs la majorité d’entre eux ne sont pas considérés comme pertinents par le SNB Ces expériences rappellent l’importance des représentations pour la TSC d’une manière gé nérale et en particulier dans notre processus Nous pouvons donc espérer une amélioration des performances prédictives en rajoutant des représentations de la littérature dans notre processus Interprétation un exemple Pour la représentation IV de la base TwoPatterns la grille optimale obtenue par KHC est composée de 224 clusters de courbes 11 intervalles pour X et 9 intervalles pour YIV Les deux variables les plus pertinentes parmi toutes les variables générées à partir de toutes les représentations selon les prétraitements MODL du SNB sont issues de la représentation IV et sont 1 v1 le nombre de points dans l’intervalle IYIV =]−∞ −3 9082] 2 v2 l’index du cluster le plus proche Le groupement de valeurs pour v2 et la discrétisation pour v1 fournissent les tables de contin gence suivantes en apprentissage cf tables 3 et 4 p = points ∈ IYIV c1 c2 c3 c4 0 ≤ p ≤ 7 100% 0 00% 0 00% 0 00% 7 < p ≤ 12 3 17% 17 46% 79 37% 0 00% 12 < p ≤ 26 0 97% 51 21% 45 17% 2 66% 26 < p ≤ 29 0 00% 25 58% 25 58% 48 84% 29 < p 0 46% 1 39% 0 93% 97 22% TAB 3 – Table de contingence de la variable v1 nombre de points dans l’intervalle IYIV = ]−∞ −3 9082] issue de la représentation IV Groupes d’index de clusters c1 c2 c3 c4 G1 0 39% 3 53% 3 53% 92 55% G2 1 26% 0 42% 97 91% 0 42% G3 3 42% 94 44% 0 00% 2 14% G4 95 22% 2 21% 2 57% 0 00% TAB 4 – Table de contingence de la variable v2 index du cluster le plus proche issue de la représentation IV Nous observons table 3 que le nombre de points p d’une courbe dans IYIV i e le nombre de points dont la valeur est inférieure à 3 9082 est pertinent pour caractériser sa classe En effet en apprentissage les courbes telles que p ≤ 7 sont de classe c1 lorsque p > 29 elles sont très majoritairement de classe c4 et lorsque 7 < p ≤ 12 elles sont majoritairement de classe c3 Dans la table 4 nous observons tout d’abord que le prétraitement supervisé par groupement de valeurs MODL sur la variable v2 “index du cluster de courbes le plus proche” produit 4 D Gay M Boullé groupes G1 resp G2 G3 et G4 constitués de 56 resp 53 53 et 62 index de clusters qui sont majoritairement de classe c4 resp c3 c2 c1 La forme diagonale de la table de contingence table 4 indique la pertinence de l’attribut v2 pour caractériser la classe d’une courbe En effet par exemple si iC l’index du cluster de courbes le plus proche d’une courbe cid appartient au groupe G2 i e iC ∈ G2 alors cid est considéré comme très similaire aux courbes de classe c3 De plus la variable “index du cluster de courbes le plus proche” est un indicateur de la pertinence de la représentation pour notre processus dans le problème courant de TSC Dans cet exemple la variable v2 à elle seule permet de caractériser environ 95% de la base donc la représentation IV est très pertinente pour caractériser les classes de la base TwoPatterns A l’inverse pour la représentation originale DV la grille optimale générée par KHC est composée de 255 clusters de courbes mais le prétraitement indique que la variable “index du cluster de courbes le plus proche” n’est pas pertinente pour caractériser les classes de la base TwoPatterns 4 Conclusion Perspectives Nous avons proposé MODL TSC un processus générique de construction de descripteurs pour le problème de la classification supervisée de séries temporelles TSC Ce processus est libre de tout paramétrage utilisateur et donc simple d’utilisation Il se décompose en trois étapes i génération de multiples représentations des données par le biais de transforma tions ii application d’une technique de coclustering sur chacune des représentations iii construction de descripteurs à partir des résultats du coclustering La nouvelle base de données objets attributs – dont les objets identifiant les séries temporelles sont décrits par des attri buts issus des diverses représentations générées– est notre base d’apprentissage Pour classer de nouvelles séries nous utilisons un classifieur naïf Bayésien sélectif Les résultats expéri mentaux ont montré que les performances prédictives de MODL TSC sont très compétitives et comparables aux meilleures approches de la littérature Les premiers résultats expérimentaux sont prometteurs et confirment l’importance des transformations dans la TSC En effet selon les applications certaines transformations fa ciliteront la découverte de motifs caractérisant les classes de séries temporelles De plus la combinaison de plusieurs représentations par le biais de notre processus MODL TSC permet d’atteindre des performances prédictives très compétitives Nous avons utilisé quelques re présentations simples dans ces travaux préliminaires pour démontrer le bien fondé de ce pro cessus de construction de descripteurs – ce qui nous laisse un potentiel d’amélioration des performances pour les données où MODL TSC est moins performant que ses concurrents pour peu qu’on trouve la bonne représentation “Chercher la ou les bonnes représentations via une transformation” est certainement la principale perspective à ce travail et ce que nous pouvons recommander à ceux qui s’intéressent à la TSC La littérature sur la TSC regorge de représen tations voir Wang et al 2012 pour une vue d’ensemble et trouver une bonne représentation pour la TSC est toujours un sujet d’actualité e g Lines et al 2012 D’autre part une perspective pratique sera d’identifier la ou les bonnes représentations pour un domaine d’application spécifique e g ECG consommation électrique Construction de descripteurs pour la classification supervisée de séries temporelles Références Bagnall A L M Davis J Hills et J Lines 2012 Transformation based ensembles for time series classification In SIAM DM’12 pp 307–318 Boullé M 2006 MODL A bayes optimal discretization method for continuous attributes Machine Learning 65 1 131–165 Boullé M 2007 Compression based averaging of selective naive Bayes classifiers Journal of Machine Learning Research 8 1659–1685 Boullé M 2012 Functional data clustering via piecewise constant nonparametric density estimation Pattern Recognition 45 12 4389–4401 Demsar J 2006 Statistical comparisons of classifiers over multiple data sets Journal of Machine Learning Research 7 1–30 Ding H G Trajcevski P Scheuermann X Wang et E J Keogh 2008 Querying and mining of time series data experimental comparison of representations and distance measures PVLDB 1 2 1542–1552 Keogh E Q Zhu B Hu H Y X Xi L Wei et C A Ratanamahatana 2011 The UCR time series classification clustering cs ucr edu ~eamonn time_ series_data Liao T W 2005 Clustering of time series data a survey Pattern Recognition 38 11 1857–1874 Lines J L M Davis J Hills et A Bagnall 2012 A shapelet transform for time series classification In KDD’12 pp 289–297 Ramsay J et B Silverman 2005 Functional data analysis Springer Rissanen J 1978 Modeling by shortest data description Automatica 14 465–471 Shannon C E 1948 A mathematical theory of communication Bell System Technical Journal Wang X A Mueen H Ding G Trajcevski P Scheuermann et E Keogh 2012 Experi mental comparison of representation methods and distance measures for time series data Data Mining and Knowledge Discovery 1–35 Summary We suggest a parameter free process for feature construction for time series classification Our process is decomposed in three steps i we transform original data into several simple representations ii on each representation we apply a coclustering method iii we use co clustering results to build new features for time series It results in a new transactional data set made of time series identifiers described by features related to the various generated rep resentations We show that a Selective Naive Bayes classifier on this new data set is highly competitive when compared with state of the art times series classification methods 