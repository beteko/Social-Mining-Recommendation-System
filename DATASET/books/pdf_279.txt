Vers l'extraction de données liés De Tweets Manel Achichi *, * Dino Zohra Bellahsene Ienco **, Konstantin Todorov * * Université Montpellier 2, LIRMM firstname.lastname@lirmm.fr, ** Irstea Montpellier, UMR TETIS dino.ienco@teledetection.fr Résumé . Des millions d'utilisateurs de Twitter postent des messages tous les jours pour communiquer avec d'autres utilisateurs en matière d'information en temps réel sur les événements qui se produisent dans leur environ- nement. La plupart des études sur le contenu des tweets ont mis l'accent sur la détec- tion des sujets émergents. Cependant, au mieux de notre connaissance, aucune approche a été proposé de créer une base de connaissances et enrichir automatiquement informa- tions provenant de tweets. La solution que nous proposons est composé de quatre phases principales: identification, classification sujet de tweets, tion automatique summariza- et création d'un triplestore RDF. L'approche proposée est mise en œuvre dans un système couvrant toute la séquence de traitement étapes de la collection de tweets écrits en langue anglaise (basée à la fois confiance et les sources de la foule) à la création d'un ensemble de données RDF ancré dans l'espace de noms de DBpedia. 1 Introduction L'un des objectifs du Linked Open Data (LOD) initiative est de la structure et des données d'interconnexion sur le Web en utilisant les technologies du web sémantique, comme la description des ressources travail DE CADRE (RDF), prenant ainsi le web d'aujourd'hui jusqu'à un nouveau niveau où les deux données sont interprétables et accessibles par les humains et les machines (Bizer et al., 2009). Un effort considérable a été fait dans cette direction tout au long des deux dernières années. Cependant, de nombreuses sources d'informations précieuses sur le Web restent encore inexplorées, bien qu'ils contiennent des données utiles qui peuvent être bénéfiques pour le projet LOD. Dans cet article, nous nous concentrons sur le support social Twitter qui fournit une plate-forme pour la publication de messages courts (tweets) de longueur maximale de 140 caractères. Le réseau a connu une popularité croissante à travers les dernières années, devenant une source importante de nouvelles informations sur de nombreux événements importants, mis à disposition en temps réel, souvent même avant sa diffusion à travers les canaux traditionnels de radiodiffusion. La principale motiva- tion de notre travail est de permettre l'intégration des informations qui circulent tous les jours à travers le flux Twitter sur le Web des données. Nous vous proposons une méthode pour l'extraction des données pertinentes de Tweets, leur conversion en RDF et leur stockage dans un triplestore RDF avec l'objectif final de leur publication sous forme de données ouvertes liées. Notre approche couvre toute la chaîne de traitement suite à un flux de travail bien défini, qui sera présenté dans la section 3. - 383 - vers l'extraction de données liées De Tweets 2 Travaux connexes Une grande famille d'approches connexes se concentre sur l'extraction de la relation du texte, divisé en les basé sur la dépendance et celles basées sur l'analyse syntaxique. On peut citer des systèmes tels que la réverbération ou le dePoe multilingue de ce dernier groupe et CLAUSIE (Corro et Gemulla, 2013) (Fader et al., 2011) (Gamallo et al, 2012). - de l'ancien. Le OLLIE système (Schmitz et al., 2012) est basée sur des modèles de relation qui sont extraits par réverbération. Il existe deux grandes catégories d'approches pour extraire triplets RDF à partir du texte. La première catégorie exploite les connaissances de base de déduire des faits nouveaux de texte. Un exemple est la méthode proposée dans (Anantharangachar et al., 2013), où triplets RDF sont extraits à l'aide de dictionnaires spécifiques de domaine induits par une ontologie existante. Les méthodes appartenant à la deuxième catégorie appliquent généralement une analyse sémantique du texte (Exner et Nugues, 2012; Augenstein et al, 2012;.. Cattoni et al, 2012). Dans (Cattoni et al., 2012), une infrastructure à grande échelle à des ressources multimédias magasin et Interlink est présenté. Le système est capable de savoir l'importation et annoter sous forme de RDF, associant automatiquement les ressources à des entités et la création de nouvelles connaissances sous la forme de triplets RDF. Une particularité de ce système est l'association des informations de contexte à chaque resou géré rce. On distingue deux approches de tweets summarization - le premier VIDES pro- groupe un sac de termes comme un résumé, tandis que le second extraits représentatifs (Cataldi et al, 2013;; Benhardus et Kalita, 2013 ioudakis Math- et Koudas, 2010.) tweets. Les approches du second groupe sont plus appropriés pour nous, car ils conservent une sorte de cohérence structurelle et de concision dans le résumé dérivé. Dans (Sharifi et al., 2010), un ensemble de tweets est résumée par une phrase dérivée par une représentation graphique des mots (co) se produisent dans une collection de tweets. (Chua et Asur, 2013) récupérer le plus de tweets pertinents comme le résumé d'un tweets col- lection, basée sur des modèles thématiques. Deux autres techniques sont proposées dans (Olariu, 2013). Le premier premier se confond tous les tweets dans un graphe de mots (de façon similaire à (Chua et Asur, 2013)) et calcule ensuite une fonction de pointage pour sélectionner un chemin du graphique comme un résumé possible. Le second sélectionne les mots les plus fréquents suite (phrase). 3: vue d'ensemble de l'approche Notre chaîne de traitement couvre l'ensemble du processus de la constitution d'un corpus Twitter pour la génération d'un triplestore de RDF (Figure 1). Constituer un corpus de tweets. Tweets écrits en anglais sont d'abord recueillies auprès de sources de confiance - comptes Twitter des médias établis (par exemple, la BBC). Ces tweets sont ensuite regroupés le sujet sage, en utilisant des techniques d'identification des sujets, formant des groupes homogènes. Nous avons choisi les K-means algorithme des pour sa simplicité et son efficacité. Nous avons mis en K à la valeur la plus faible qui maintient le rapport {distance moyenne / moyenne distance inter-Clusters intra-groupe} sous un seuil donné. De plus, pour chaque sujet, nous collectons tweets bondé de comptes utilisateurs ordinaires, qui sont utilisés pour enrichir l'information contenue dans tous les sujets. Enfin, nous ne gardons que les tweets étroitement liés aux sujets que nous voulons représenter, à savoir, les tweets contenant des mots-clés extraits précédemment (le plus de mots fréquents dans chaque groupe). Filtrage et pré-traitement. Un élément de filtrage est appliquée à la fois fiable et sources de foule. L'objectif est de ne garder que les tweets qui contiennent des entités nommées, et en particulier ceux qui ont DBpedia URIs, en préparation de l'enchaînement des (pas encore) extraites triplets RDF à - 384 - M. Achichi et al. FIGUE. 1 - Le flux de travail du système. le web des données. Les tweets sont bondées en plus filtrés par rapport à leur adéquation à un sujet donné à l'aide d'un ensemble de mots clés décrivant chaque sujet. De plus, tous les tweets sont prétraitées en supprimant hashtags, urls et retweets et par lemmatizing le texte. Classification des tweets bondés. Nous avons introduit un module de classification texte, basé sur le principe de l'analyse des sentiments, afin de classer les tweets entassés dans deux catégories: (i) fait, y compris une information neutre et objective que nous voulons garder et utiliser pour enrichir les tweets de confiance, et (ii) les autres, y compris l'opinion, messages privés, etc. Nous avons utilisé l'outil coreNLP. Ici, nous partons du principe que d'un tweet qui transmet des informations factuelles (ou un morceau de nouvelles) est dépouillé à la fois un sentiment positif et négatif. Cependant, l'évaluation de la polarité d'un tweet n'a pas été suffisante pour juger de son objectivité. Comme critères de classification supplémentaires, nous vérifions l'absence de l'ensemble des caractéristiques suivantes dans chaque tweet: (i) le symbole « @ » indiquant un message privé, (ii) des lettres répétitives ou abus de ponctuation, (iii) les smileys qui reflètent une mindstate, et (iv) mal orthographié mots. Résumé automatique. Après un corpus qui, soit présentent tweets par sujet, nous procédons à générer un résumé de ce corpus, afin d'éliminer les informations redondantes et inutiles. L'algorithme que nous avons développé génère automatiquement un résumé d'un sujet sous la forme d'un ensemble cohérent et concis de tweets. Il faut deux paramètres en entrée: un ensemble de tweets correspondant à un sujet et un seuil de similarité tweets de commande (σ). Tout d'abord, l'algorithme construit une Undir pondérée ète graphique (G) lorsque les sommets sont les tweets et un bord existe entre deux sommets si la similarité du cosinus entre les deux tweets correspondant est - 385 - Vers lié Extraction des données à partir Tweets supérieure à σ. De plus, l'algorithme passe pour extraire les cliques maximales de G, en se fondant sur l'algorithme de Bron-Kerbosch bien connu. L'hypothèse ici est qu'une clique maximale représente un groupe d'informations potentiellement redondantes qui peut être représenté par un seul tweet. L'algorithme PageRank est ensuite utilisé pour attribuer aux scores des sommets au sein de chaque clique et sélectionnez celui avec le score le plus élevé comme un résumé de la clique. Si nous avons plusieurs tweets avec le même score, l'algorithme sélectionne la plus longue. Le résumé du sujet entier est donnée par l'ensemble des tweets représentant chaque clique maximale dans le graphique sujet. De tweets à RDF. Enfin, la dernière étape consiste à transformer chaque résumé en un graphe RDF. Nous avons été inspirés par l'état de l'art approche à l'extraction RDF du texte, LODifier (Augenstein et al., 2012), car elle repose sur l'idée d'ancrer les informations sémantiques extraites dans l'espace de noms de DBpedia. Notre approche consiste en quatre étapes principales: (1) L'analyse sémantique. La sémantique de chaque phrase est modélisé comme un (ensemble de) triple (s) du type <sujet, verbe, objet>. L'algorithme donné (Rusu et al., 2007) a été adoptée et adaptée en raison de son efficacité combinée à la Stanford Parser. Cet algorithme fonctionne bien avec des phrases simple clause simple mais ne parvient pas à gérer correctement des phrases de plusieurs clauses. Nous vous proposons l'algorithme itératif clause de fractionnement suivant: (1) Appliquer la dépendance analyse sur l'expression complexe. (2) Pour chaque relation de dépendance de type « nsubj » (sujet nominal), récupérer toutes les relations de ses arguments, à l'exception des relations de type « nsubj ». (3) Répéter (2) pour chaque relation récupéré jusqu'à ce qu'aucune nouvelle relation est ajoutée. (4) Pour toutes les dépendances enveloppées par une relation de type « nsubj », extrait et organiser tous les mots dans l'ordre croissant des numéros qui leur sont associés. Ces chiffres indiquent l'ordre des mots dans la phrase originale. (2) Disambiguation. Avant d'attribuer un URI DBpedia à un mot, nous choisissons son sens le plus approprié dans un contexte donné. Nous avons appliqué une méthode couramment utilisée basée sur l'identification des synset dans WordNet. (3) Affectation URI DBpedia. Nous attribuons à chaque terme son URI DBpedia correspondant. (4) génération d'un graphe RDF. Au cours de l'analyse sémantique, les phrases ont été composées de- aux clauses simples et chaque clause a été structurée comme un triple du genre <sujet, verbe, objet>. Dans cette dernière étape, le système effectue une conversion de ces triplets dans un graphe RDF. Les arguments créés au cours de l'analyse sémantique sont analysées pour identifier les tities en- nommées qui correspondent aux sujets de RDF et des objets et attribuer à chacun d'entre eux l'URI DBpedia correspondant. Pour ce faire, nous avons adapté au sujet et l'objet des triplets extraits des triplets existant dans le graphique DBpedia. 4 Prototypage et expériences sur réel Twitter données Nous utilisons la Twitter4J 1 API aux tweets virés de flux, le TextRazor 2 services aux entités Recon- de nize et les affecter successivement une URL Wikipedia, et la bibliothèque CoreNLP Stanford pour traiter le traitement et le sentiment du langage naturel une analyse. La collection de tweets de confiance est obtenue par exploration du média social le 9 Octobre 2014, une période de 24 heures. Nous avons suivi les comptes de BBC World, CNN, New York Times, New York Times du monde, et Briser News.We gardé ces tweets qui contiennent au moins une entité nommée qui correspond à un URI DBpedia. Cette première collection est composée de 1. http://twitter4j.org/en/index.html 2. https://www.textrazor.com/ - 386 - M. Achichi et al. 125 tweets. La collection de tweets bondés est récupéré le 10 Octobre 2014 en tenant compte des mots-clés extraits des tweets de confiance. Dans notre jeu de données, nous avons pu détecter 50 dif sujets férents des tweets de confiance. Voici un exemple d'un sujet composé de 5 tweets: {(1) Natation a fait Michael Phelps un athlète dominant, mais il ne pouvait pas le guider en dehors de la piscine; (2) USA Swimming annonce une suspension de 6 mois pour Michael Phelps après l'arrestation conduite avec facultés affaiblies; (3) Michael Phelps suspendu par USA Swimming; (4) Michael Phelps a reçu une suspension de six mois à compter de la natation; (5) La nageuse Michael Phelps suspendu pour six mois.}. Pour cet exemple, nous avons récupéré 1 325 messages courts provenant de sources bondées en utilisant des mots-clés de tweets de confiance. Successivement, nous classons les tweets comme nouvelles ou d'autres en appliquant les règles de classification présentés à la section 3. Le tableau 1 présente les résultats de la classification. Nous obtenons une précision de 76,22%. Nous analysons également le comportement de notre méthode en ne considérant que les nouvelles de classe. À cette fin, nous calculons de précision, rappel et F-mesure pour cette classe obtenir, respectivement, 0,628, 0,722 et 0,672. Cette évaluation souligne la qualité de notre stratégie et montre que notre méthode est capable de détecter environ 2 sur 3 tweets contenant des informations factuelles. Nouvelles Classe prédites Autres biens de classe Nouvelles 323 124Other 191 687 TAB. 1 - Matrice de confusion obtenue par notre stratégie de classement sur les tweets collectés. Sujet prédicats objet Michael Phelps Michael Phelps athlète être reçu suspension Michael Phelps Michael Phelps Baltimore arrêter la boisson d'arrêt Michael Phelps Michael Phelps prendre pause suspends la natation TAB. 2 - Des exemples de triplets RDF auto- matiquement extrait par notre cadre. Nous notons que les faibles valeurs (entre 0,3 et 0,5) du paramètre correspondent de σ à un nombre élevé de cliques maximales, alors que la tendance des valeurs comprises entre 0,5 et 0,95 est tout à fait stable. Dans le tableau 2, nous montrons plusieurs triplets produits par notre méthode avec σ égal à 0,65. Ces triplets représentent des informations qui est induite automatiquement par l'ensemble des tweets sur le sujet Michael Phelps. Au total, 10 triplets ont été extraits dont 4 triplets sont identiques. Le petit nombre de triplets extraits est dû au fait que la collecte des tweets a été réalisée pendant 24 heures seulement. Tous les triplets RDF sont ensuite stockés dans un magasin triple 3, prêt à être publié et reliés entre eux sur le web des données. 5 Conclusion L'approche que nous proposons est complète - nous prenons en entrée le flux hétérogène de tweets circulant tous les jours à travers le milieu social et nous affichons un triplestore RDF ne contenant que des informations factuelles sur les entités qui vivent dans l'espace de noms de DBpedia. Parmi les contributions originales de notre cadre, nous soulignons: i) l'utilisation de deux principales sources de données - confiance (tweets provenant des médias traditionnels établis) et la foule (tweets des utilisateurs ordinaires). Cette dernière source est utilisée pour enrichir les données recueillies auprès de l'ancien. Afin de limiter la redondance 3. Virtuoso, http://virtuoso.openlinksw.com/ - 387 - Vers Linked Extraction de données De Tweets dans les informations recueillies et préparer le corpus de tweets pour la phase d'extraction triplets RDF, ii) nous introduisons un nouvelle approche pour générer automatiquement un résumé d'un ensemble de tweets, correspondant à un sujet donné. Comme les résumés extraits peuvent être riches et structurellement complexes, iii) nous concevons une stratégie de diviser les tweets en plusieurs morceaux simples clause d'information afin d'identifier facilement les composants d'un triple RDF. Enfin, toute l'approche est mise en œuvre dans un prototype modulaire. Références Anantharangachar, R., S. Ramani et S. Rajagopalan (2013). Ontologie extraction de l'information guidée de texte non structuré. IJVeST 4 (1), 19. Augenstein, I., S. Pado, et S. Rudolph (2012). Lodifier: Génération des données liées du texte structuré ONU-. Dans l'ESWC, pp. 210-224. Benhardus, J. et J. Kalita (2013). Streaming détection de tendance sur Twitter. IJWBC 9 (1), 122-139. Bizer, C., T. Heath et T. Berners-Lee (2009). Les données liées - l'histoire jusqu'à présent. IJSWIS 5 (3), 1-22. Cataldi, M., L. Caro D. et C. Schifanella (2013). Détection sujet personnalisé émergents basé sur un terme modèle de vieillissement. ACM TIST 5 (1), 7. Cattoni, R., F. Corcoglioniti, C. Girardi, B. Magnini, L. Serafini, et R. Zanoli (2012). Le knowledgestore: un système de stockage basé sur l'entité. En LRGC, p. 3639-3646. Chua, F. C. T. et S. Asur (2013). Résumé automatique des événements de médias sociaux. En ICWSM. Corro, L. D. et R. Gemulla (2013). Clausie: extraction de l'information ouverte sur la base clause. Dans WWW, pp. 355-366. Exner, P. et P. Nugues (2012). extraction Entité: Du texte non structuré à DBpedia triplets rdf. En Wole. Fader, A., S. Soderland et O. Etzioni (2011). L'identification des relations pour l'extraction de l'information ouverte. En EMNLP, p. 1535-1545. Gamallo, P., M. Garcia et S. Fernández-Lanza (2012). Dépendance à base de l'extraction de l'information ouverte. Dans l'atelier sur Unsupervised et apprentissage semi-supervisé en PNL, pp. 10-18. Association de linguistique informatique. Mathioudakis, M. et N. Koudas (2010). Twittermonitor: détection de tendance sur le flux Twitter. En SIGMOD, p. 1155-1158. Olariu, A. (2013). classification hiérarchique dans l'amélioration des flux de summarization microblog. En CICling, pp. 424-435. Rusu, D., L. Dali, B. Fortuna, M. Grobelnik et D. Mladenic (2007). extraction Triplet de phrases. Dans Int. Multiconf. Société de l'information-IS, p. 8-12. Schmitz, M., R. Bart, S. Soderland, O. Etzioni, et al. (2012). Langue d'enseignement ouvert pour l'extraction de l'information. En EMNLP, pp. 523-534. Sharifi, B., M. Hutton, et J. K. Kalita (2010). Résumant microblogs automatiquement. Dans HLT-NAACL, pp. 685-688. - 388 - F - Analyse des Entreprises RSS vers l'extraction de données liés De Tweets Manel Achichi, Zohra Bellahsene, Konstantin Todorov, Dino Ienco