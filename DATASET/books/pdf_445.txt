 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles Rakia Jaziri Mustapha Lebbah YounËs Bennani LIPN UMR 7030 CNRS Université Paris 13 99 av J B Clément F 93430 Villetaneuse Prénom Nom lipn univ paris13 fr Institut National de l’Audiovisuel 4 av de l’Europe 94 366 Cedex Bry sur Marne rjaziri ina fr Résumé Nous proposons dans ce papier un nouvel algorithme de classifica tion non supervisée à base de modèle de mélange topologique pour des données non i i d non independently and identically distributed Ce nouveau paradigme probabiliste plonge les cartes topologiques probabilistes dans une formulation sous forme de chaînes de Markov cachées Dans cette formulation la généra tion d’une observation à un instant donné du temps est conditionnée par les états voisins au même instant du temps Ainsi une grande proximité impliquera une grande probabilité pour la contribution à la génération L’approche proposée est évaluée en utilisant des données séquentielles réelles issues des bases de don nées de l’Institut Nationale de l’Audiovisuel INA Les résultats obtenus sont très encourageants et prometteurs 1 Introduction Plusieurs techniques de classification automatique des données séquentielles ont été dé veloppées ces dernières années Elles ont été appliquées dans différents domaines tels que la reconnaissance des caractères manuscrits Prat et al 2009 la reconnaissance de la parole l’étude de la mobilité des objets dans les vidéos Buzan et al 2004 et l’analyse de séquences biologiques ADN La méthode la plus facile pour traiter ce type de données serait tout sim plement d’ignorer l’aspect temporel et de traiter les observations comme des données indé pendantes ou i i d "independent and identically distributed" Pour beaucoup d’applications l’hypothèse i i d rend les données plus pauvres en perdant l’information séquentielle Sou vent dans beaucoup d’applications le traitement est décomposé en deux étapes la première est l’étape de classification ou de partitionnement des données avec l’hypothèse i i d Dans la deuxième étape le résultat de la classification est utilisé pour construire un modèle probabiliste en relaxant la contrainte i i d et une des plus naturelles manières de faire cela est d’utiliser un modèle de Markov Les cartes topologiques Kohonen 2001 sont intéressantes de par leurs apports topolo giques à la classification non supervisée et leurs capacités à résumer de manière simple un 347 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles ensemble de données multi dimensionnelles Elles permettent d’une part de comprimer de grandes quantités de données en regroupant les individus similaires en classes et d’autre part de projeter les classes obtenues de façon non linéaire sur une carte ou un graphe Ce procédé permet donc d’effectuer une réduction de dimension permettant ainsi de visualiser la structure des données en deux dimensions tout en respectant la topologie des données de sorte que deux données proches dans l’espace multi dimensionnel de départ aient des images proches sur la carte Dans la plupart des modèles hybrides les cartes topologiques sont utilisées comme un pré traitement pour la quantification vectorielle et les HMMs Hidden Markov Models sont ensuite utilisés dans des processus de transformation plus avancés pour la prise en compte de la dynamique Les résultats de classification dépendent essentiellement du choix des deux paramètres la distribution de probabilité et la topologie de la carte d’auto organisation Mal heureusement les paradigmes de l’auto organisation ne peuvent pas être facilement transférés à des données non i i d Différentes approches ont été développées pour intégrer l’informa tion temporelle dans la carte d’auto organisation Une variété de modèles existe pour les cartes auto récurrentes la carte de Kohonen temporelle TKM la SOM récurrente RSOM la SOM récursive RecSOM et SOM pour les données structurées SOMSD Strickert et Hammer 2003 Hagenbuchner et al 2003 Nous proposons dans cet article une nouvelle approche de carte topologique probabiliste dédiée aux données séquentielles multivariées que nous appelons carte auto organisatrice probabiliste pour les données séquentielles PrSOMS Nous supposons que les données sé quentielles sont générées selon un processus markovien Les modèles de Markov cachés fi gurent parmi les meilleures approches adaptées aux traitements des séquences étant donné leur capacité à traiter des séquences de longueurs variables et leur pouvoir à modéliser la dynamique d’un phénomène décrit par des suites d’événements La modélisation graphique probabiliste motive différentes structures graphiques basées sur les HMMs Bengio et Fras coni 1994 Les modèles graphiques fournissent un formalisme général pour décrire et analy ser de telles structures Par conséquent il est très important d’avoir des algorithmes capables de déduire à partir d’un ensemble de données de séquences non seulement la probabilité de distribution mais aussi la structure topologique du modèle c’est à dire le nombre d’états et les transitions qui les inter connectent Malheureusement cette tâche est très difficile et on a que des solutions partielles Afin de surmonter les limites des HMMs des travaux récents dans Bouchaffra 2008 qui proposent un nouveau et un original paradigme appelé topolo gical HMM qui manipule les nœuds du graphe associé au HMM et ses transitions dans un espace Euclidien Cette approche modélise la structure locale d’un HMM et extrait leur forme en définissant une unité d’information comme une forme composée d’un groupe de symboles d’une séquence Un autre modèle souvent présenté comme la version probabiliste de la carte d’auto organisation nommé GTM qui a été étendu à un modèle de série chronologique uni variées GTM through time Bishop 2006 Olier et Vellido 2008 et aux données structurées Bacciu et al 2010 Récemment dans Yamaguchi 2010 l’auteur propose l’extension de l’algorithme SOMM Self Organizing Mixture Model Verbeek et al 2005 pour les séries chronologiques multivariées SOHMMs self organizing hidden Markov models Cependant la manière dont ces modèles réalisent l’organisation topographique est tout à fait différente de celles utilisées dans les algorithmes SOM et notre modèle PrSOMS Dans cet article nous nous sommes intéressés à la problématique d’analyse de données structurées en séquences qu’elles soient de longueurs fixes ou variables L’objectif de cette ap 348 Jaziri et al proche est de construire un nouveau modèle auto organisé génératif d’un ensemble de données non i i d Le modèle proposé est basé sur le formalisme probabiliste des cartes topologiques Anouar et al 1997 Lebbah et al 2007 et le modèle génératif utilisé dans les HMM Par conséquent il consiste à estimer les paramètres du modèle en maximisant la vraisemblance de l’ensemble des données séquentielles L’algorithme d’apprentissage que nous proposons est une application de l’algorithme standard EM "Espérance Maximisation" Cet article est organisé comme suit Les sections 2 et 3 présentent notre approche probabiliste pour la classi fication des données séquentielles PrSOMS La section 4 décrit le dispositif expérimental et les évaluations Enfin la section 5 propose une conclusion et des perspectives 2 Principe du modèle génératif PrSOMS Dans notre modèle nous considérons une carte topologique formant une grille qui sera vu comme une chaîne de Markov un HMM La génération d’une variable observable à un instant donné du temps est conditionnée par les états voisins au même instant du temps Ainsi une grande proximité implique une grande probabilité pour la contribution de la génération Cette proximité est quantifiée en utilisant la fonction de voisinage Le formalisme que nous présentons est valable pour toutes les structures Supposons une séquence d’observations X = {x1 x2 xn xN} tel que xn est un élément de la séquence de taille N La principale problématique est d’estimer les paramètres du modèle d’apprentissage PrSOMS On suppose que l’architecture de la carte modélisant aussi un HMM est représentée par un treillis C qui a une topologie discrète définie par un graphe non orienté On notera le nombre des cellules noeuds états de C parK Pour chaque paire de cellules c r dans le graphe la distance δ c r est définie comme la longueur de la plus courte chaîne qui lie les cellules ou les états c et r On suppose que chaque élément xn d’une séquence d’observations X est généré par le processus suivant on commence par associer à chaque cellule état c ∈ C une probabilité p xn c où xn est un vecteur dans l’espace des données Par la suite on sélectionne une cellule c de la carte C selon une probabilité a priori p c Pour chaque cellule c on sélectionne une cellule c ∈ C selon la probabilité conditionnelle p c c Toutes les cellules c ∈ C et au même instant n contribuent à la génération d’un élément xn avec la probabilité p xn c selon la proximité à la cellule c décrite par la probabilité p c c Nous introduisons notamment deux variables binaires aléatoires comme variables cachées zn et z n de dimension K dans lesquelles deux éléments particuliers znr et z nc est égaux à 1 et tous les autres éléments sont égaux à 0 Les deux composantes z nc et znr indiquent un couple d’états responsable de la génération d’un élément de l’observation Utilisant cette notation on peut réécrire la probabilité p xn c comme suit p xn c ' p xn znc = 1 ' p xn zn et p c c = p znc = 1 z nc = 1 ' p znc z nc ' p zn z n Pour introduire le processus d’auto organisation dans l’apprentissage du modèle de mélange on suppose que p znc z nsc peut être définie de la même manière que les modèles des cartes probabilistes p znc z nc = KT δ c c ∑ r∈C K T δ r c oùKT est la fonction de voisinage qui dépend du paramètre T appelé température KT δ = K δ T AinsiK définit pour chaque état de la chaîne de Markov z nc une région de voisinage 349 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles dans le graphe C Le paramètre T permet de contrôler la taille du voisinage qui influence une cellule donnée de la carte C La valeur de T varie entre deux valeurs Tmax et Tmin On note l’ensemble de toutes les variables cachées par Z et Z où chaque ligne z n et zn est associé à chaque élément de la séquence xn Chaque observation de la séquence en X est associée à un couple de variables cachées Z et Z responsables de la génération On note par {X Z Z } l’ensemble complet des données et on se réfère aux données observables X comme incomplètes Ainsi le modèle générateur d’une séquence est défini de la manière suivante p X θ = ∑ Z ∑ Z p X Z Z θ 1 Puisque la distribution p X Z Z θ ne peut pas se simplifier une caractéristique importante pour les distributions des probabilités sur des variables multiples est celle de l’indépendance conditionnelle Luttrel 1994 On suppose que la distribution conditionnelle de X sachant Z et Z ne dépend pas de la variable cachée Z Souvent cette hypothèse est utilisée pour les modèles graphiques ainsi p X Z Z = p X Z On peut réécrire la distribution marginale comme p X θ = ∑ Z p Z ∑ Z p Z Z p X Z 2 avec p X Z = ∑ Z p Z Z p X Z 3 3 Paramètres du modèle et estimation Considérant que la carte C représente un modèle de Markov ainsi la distribution à l’état z n dépend de l’état de la variable latente précédente z n−1 Cette dépendance est représentée avec la probabilité conditionnelle p z n|z n−1 Puisque les variables latentes sont des variables bi naires de dimension K cette distribution conditionnelle correspond à une table de probabilité qu’on note par A Les éléments de A sont connus comme des probabilités de transition notées par Ajk = p z nk = 1 z n−1 j = 1 avec ∑ k Ajk = 1 On peut écrire la distribution conditionnelle explicitement sous cette forme p z n z n−1 A = K∏ k=1 K∏ j=1 A z n−1 jz nk jk L’état initial z 1 est un cas particulier puisqu’il n’a pas d’état ou de cellule parente et ainsi il a une distribution marginale p z 1 représentée par un vecteur de probabilités π avec les éléments πk = p z 1k = 1 ainsi que p z 1|π = ∏K k=1 π z 1k où ∑ k πk = 1 Les paramètres du modèle sont complétés en définissant les distributions conditionnelles des variables observées p xn zn φ où φ est un ensemble de paramètres qui définissent la distribution qui est connue comme des probabilités d’émission dans le modèle HMM On peut représenter les probabilités d’émission sous la forme p xn zn φ = ∏K k=1 p xn φk znk 350 Jaziri et al La probabilité jointe des variables observables et les deux variables latentes Z et Z est expri mée par p X Z Z θ = p Z A × p Z Z × p X Z φ p X Z Z θ = [ p z 1|π N∏ n=2 p z n z n−1 A ] × [ N∏ i=1 p zi z i ] × [ N∏ m=1 p xm zm φ ] où θ = {π A φ} décrit l’ensemble des paramètres qui manipulent le modèle Il n’est pas évident de maximiser la fonction de vraisemblance à cause de la complexité de l’expres sion C’est pour cela qu’on utilise l’algorithme EM pour trouver les paramètres qui maxi misent la fonction de vraisemblance L’algorithme EM commence avec quelques sélections initiales pour les paramètres du modèle qu’on note par θold Dans l’étape E Estimation on prend les valeurs des paramètres et on trouve la distribution a posteriori des variables latentes p Z Z X θold Ensuite on utilise cette distribution a posteriori pour évaluer l’espérance du logarithme de la vraisemblance des séquences complètes des données eq 3 en fonction des paramètres θ pour obtenir la fonction objective Q θ θold définie par Q θ θold = ∑ Z ∑ Z p Z Z X θold ln p X Z Z θ Ainsi on peut réécrire la fonction Q θ θold = Q1 π θ old +Q2 A θ old +Q3 φ θ old +Q4 4 oú Q1 π θ old = ∑ Z ∑ Z K∑ k=1 p Z Z X θold z 1k lnπk Q2 A θ old = ∑ Z ∑ Z N∑ n=2 K∑ k=1 K∑ j=1 p Z Z X θold z n−1 jz n ln Ajk Q3 φ θ old = ∑ Z ∑ Z N∑ n=1 K∑ k=1 p Z Z X θold znk ln p xn φk Q4 = ∑ Z ∑ Z p Z Z X θold ln p Z Z A cette étape on va introduire quelques notations On va utiliser γ z n zn pour noter la distribution marginale a posteriori des variables latentes z n et zn et ξ z n−1 z n = p z n−1 z n X θ old pour noter la distribution a posteriori jointe des variables latentes successives tel que γ z n = ∑ z p z n zn|X θold et γ zn = ∑ z p z n zn|X θold 351 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles On observe que la fonction objective eq 4 Q θ θold est définie comme une somme de quatre termes Le premier terme Q1 π θold dépend des probabilités initiales le deuxième terme Q2 A θ old dépend des probabilités de transition A le troisième terme Q3 φ θold dépend de φ qui est l’ensemble des paramètres de la probabilité d’émission et le quatrième est une constante La maximisation de Q θ θold par rapport à θ = {π A φ} peut être effectuée séparément 1 La maximisation de Q1 π θold Les probabilités initiales De la même manière que les modèles probabilistes on utilise une forme explicite de la distribution des probabilités initiales La probabilité initiale π est ensuite obtenue de la manière suivante πk = γ z 1k ∑K j=1 γ z 1j 2 La maximisation de Q2 A θold Probabilités de transition Comme dans le cas des HMMs traditionnels notre modèle utilise un état caché de valeur discrète avec une distribution multinomiale sachant les valeurs précédentes de l’état Donc notre modèle est un modèle du premier ordre La mise à jour des paramètres est calculée de la manière suivante Ajk = ∑N n=2 ξ z n−1 j z k n ∑K l=1 ∑N n=2 ξ z n−1 j z nl 5 où ξ z n−1 j z n k = E[z n−1 jz k n ] = ∑ z γ z z n−1 jz n k 3 La maximisation de Q3 φ θold Les probabilités d’émission L’ensemble des paramètres φ dépend de la distribution utilisée Nous présentons l’ap plication en utilisant la loi gaussienne Dans le cas des probabilités d’émission avec une densité sphérique gaussienne on a p x φk = N x wk σk définie par sa “moyenne“ wk qui a la même dimension que les données d’entrée et sa matrice de covariance définie par σ2kI où σk est l’écart type et I est la matrice identité N x wk σk = 1 2πσk d 2 exp [ ||x−wk||2 2σ2k ] La maximisation de la fonction Q3 φ θold fournit les expressions connues wk = ∑N n=1 γ znk xn∑N n=1 γ znk 6 σ2k = ∑N n=1 γ znk ||xn −wk||2 d ∑N n=1 γ znk 7 où d est la dimension de l’élément x Dans le contexte particulier du modèle de Markov caché on va utiliser l’algorithme forward backward Rabiner 1989 puisqu’on utilise la structure du graphe pour organiser les données séquentielles d’une manière explicite Quelques formules sont similaires aux chaînes de Mar kov traditionnelles si on n’utilise pas la structure du graphe 352 Jaziri et al 4 Expérimentations L’analyse des séquences audiovisuels peut être vu comme un problème d’analyse des sé quences multidimensionnelles Dans cette partie nous allons discuter notre approche sur une base de données réelles issue de l’INA Nous l’avons appliqué aussi sur d’autres bases pu bliques mais vu la taille de l’article nous nous sommes contenté de l’application réelle de l’INA La validation est réalisée à l’aide de l’expert du domaine un croisement avec le guide du programme et la visualisation des segments audiovisuels Les données se composent de 10 séquences de longueurs variables 10 chaines TV Chaque séquence représente les différents segments diffusés dans une chaine de télévision pendant une journée Chaque segment est une composante multidimensionnelle caractérisée par 19 variables Ces variables sont validées par un expert du domaine à l’INA Elles contiennent des descripteurs sur le nombre de répétitions l’intervalle la durée et d’autres variables que nous ne pouvons pas divulguer dans ce papier Nous rappelons ici que le problème est de détecter automatiquement les segments de la sé quence qui sont homogènes d’une part et de reconstruire des séquences par la réunification des segments préalablement identifiés comme segment du programme d’une autre part Nous avons commencé par appliquer notre approche PrSOMS sur les données La séquence la plus probable est obtenue en utilisant l’algorithme de Viterbi Rabiner 1989 Ainsi il nous permet d’affecter chaque élément de la séquence Nous présentons dans ce qui suit l’expérimentation qui évalue le pouvoir de structuration de notre approche La figure 1 a schématise la carte PrSOMS 10× 10 des états latents qui sont représentés par des carrés mis à l’échelle en fonction de la cardinalité des éléments de toutes les séquences du modèle capturées ou affectées à l’aide de l’algorithme de Viterbi Les lignes rouges représentent le chemin de Viterbi pour chaque séquence La largeur des lignes reflète le nombre de transition entre les états La figure 1 b schématise les profils ou les prototypes associés à chaque état le centre de la distribution gaussienne Chaque cellule est représentée par un vecteur de 19 composantes La carte topologique PrSOMS nous permet de visualiser la partition Ainsi les experts du domaine pourraient utiliser ces cellules pour analyser certaines caractéristiques du flux audiovisuel En effet en faisant une analyse visuelle des segments vi déo de chaque cellule nous avons pu tirer des caractéristiques propres à chaque chaine et qui les fait distinguer des autres Ce qui est utile pour les experts du domaine Les deux figures permettent d’analyser toutes les séquences de 10 chaînes en même temps En effet la distribution de deux séquences différentes sur la carte PrSOMS devrait être sensiblement différente Nous allons illustrer cela avec une comparaison entre les chemins de Viterbi donnés par deux chaines de télévision différentes en se basant sur l’illustration des principales différences dans la visualisation de séquences Les figures 2 a et 2 b affichent respectivement la carte représentant le chemin de Viterbi calculé avec la chaine 1 et la chaine 2 La représentation correspondant à la chaine 1 figure 2 a est plus compact que celle de la chaine 2 figure 2 b étant donnée que les séquences de la chaine 1 sont moins variables que celle de la chaine 2 La chaine 1 est plus concentrée dans la partie inférieure à gauche et en haut à droite de la carte Après analyse la chaine1 montre un comportement d’une chaine généra lise et chaine 2 le comportement d’une chaine d’information Les figures 2 c et 2 d montrent le chemin de Viterbi de deux sous séquences passant par les mêmes états afin d’identifier la similarité entre les séquences Les figures 3 et 4 affichent respectivement des images extraites des deux cellules qui sont en haut à droite et en bas à gauche Nous constatons que la majorité des vidéos capturées correspondent aux programmes courts tel que météo nouvelles brèves 353 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles a b FIG 1 – a Carte PrSOMS les carrés et les lignes indiquent respectivement la cardinalité des cellules et les transitions capturées en utilisant l’algorithme de Viterbi b Les profils associés à chaque cellule ou état publicités qui est une caractéristique de la chaine 2 La figure 4 affiche des films capturés par l’état qui est en bas à gauche dans la chaine 1 chaîne généraliste Notre approche détecte la structure sous jacente du flux et les résultats montrent une précision temporelle conforme à la vérité terrain et bien meilleure que celle indiquée dans les guides de programmes TV fournis par les chaînes de télévision Afin de se rapprocher des partitionnement d’experts nous avons appliqué la classification ascendante hiérarchique CAH sur les états ou cellules de la carte PrSOMS pour obtenir 8 clusters et ceci en relaxant la contrainte non i i d La figure 6 montre la carte PrSOMS segmentée par une CAH Tout d’abord nous remarquons lors de notre expé rimentations que plus de 90 % des inter programmes Publicités Jingle météos sont diffusés au moins deux fois dans la journée dans plus de dix chaines de télévisions Deuxièmement seuls environ 30 % des programmes courts sont répétés La précision des programmes extraits a également été évaluée Le début respectivement à la fin de chaque programme a été extrait par rapport au démarrage effectif respectivement à la fin donné par des observations réelles Dans la suite nous visualisons pour chaque cluster de la carte PrSOMS un ensemble de prototypes d’images composantes Dans la figure 5 nous avons remarqué que les clusters sont composés de contenu homogène En effet il existe des clusters avec uniquement “émissions” ou d’autres où il n’y a que des “publicités” Nous avons constaté aussi que les segments ré pétés se regroupent ensemble ou dans des cellules voisines Nous remarquons aussi que les segments d’inter programme ont été classés suivant leur catégorie publicité bande annonce parrainage jingle etc Dans notre cas la classification automatique est parfaitement conforme à nos observations réelles alors que les indicateurs du guide sont assez décalés Les résul tats sont très encourageant Une difficulté majeure est que la qualité des résultats dépend de la détection de répétitions et du nombre de variables utilisées pour le clustering Les résultats obtenus montrent que notre approche a effectué une bonne segmentation du flux TV L’ana lyse des données montre que la plupart des inter programmes sont diffusés plusieurs fois Ces programmes courts partagent de nombreuse caractéristiques ce qui perturbe notre approche 354 Jaziri et al a b c d FIG 2 – a chaine 1 b chaine 2 c Sous séquence de la chaine1 d Sous séquence de la chaine 2 FIG 3 – Images des vidéos capturées en haut à droite de la carte 355 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles FIG 4 – Images des vidéos capturées sur le coin inférieur gauche de la carte FIG 5 – Quelques images décrivant chaque cluster 356 Jaziri et al FIG 6 – La carte PrSOMS segmentée par la CAH 5 Conclusion Notre approche présente une nouvelle approche pour capturer et modéliser l’information topographique présente dans les données séquentielles L’approche proposée offre également un nouvel outil de visualisation topographique pour l’ensemble des données séquentielles Elle est très bien adaptée pour les séquences multidimensionnelles observations non i i d et main tient un faible coût de calcul L’approche proposée est évaluée en utilisant des données réelles issues de l’Institut Nationale de l’Audiovisuel INA Les résultats montrent une précision temporelle conforme à la vérité terrain et bien meilleure que celle indiquée dans les guides de programmes TV fournis par les chaînes de télévision Comme perspectives nous voulons appliquer notre approche sur des séquences binaires et catégorielles et exploiter le pouvoir de visualisation de notre approche Remerciement Ce travail a été réalisé dans le cadre d’une thèse CIFRE avec l’INA Nous remercions vivement Monsieur Jean Hugues CHENOT pour ses remarques pertinentes Références Anouar F F Badran et S Thiria 1997 Self organizing map a probabilistic approach In Proceedings of WSOM’97 Workshop on Self Organizing Maps Espoo Finland June 4 6 pp 339–344 Bacciu D A Micheli et A Sperduti 2010 Compositional generative mapping of structured data In Proceedings of International Joint Conference on Neural Networks IJCNN’10 pp 1–8 Bengio Y et P Frasconi 1994 An input output hmm architecture In NIPS pp 427–434 357 Classification probabiliste non supervisÈe et visualisation des donnÈes sÈquentielles Bishop C M 2006 Pattern Recognition and Machine Learning Springer Science+Business Media LLC 233 Spring Street New York NY 10013 USA Bouchaffra D 2008 Embedding hmm’s based models in a euclidean space The topological hidden markov models In ICPR08 pp 1–4 Buzan D S Sclaroff et G Kollios 2004 Extraction and clustering of motion trajectories in video In In International Conference on Pattern Recognition pp 521–524 Hagenbuchner M R Sperduti A C Tsoi et S Member 2003 A self organizing map for adaptive processing of structured data IEEE Transactions on Neural Networks 14 491–505 Kohonen T 2001 Self organizing Maps Springer Berlin Lebbah M N Rogovschi et Y Bennani 2007 Besom Bernoulli on self organizing map Luttrel S P 1994 A bayesian analysis of self organizing maps Neural Computing 6 767 – 794 Olier I et A Vellido 2008 Advances in clustering and visualization of time series using gtm through time Neural Netw 21 904–913 Prat F A Marzal S MartÌn R Ramos garijo et M J C Bleda 2009 A template based recognition system for on line handwritten characters Journal of Information Science and Engineering 25 779–791 Rabiner L R 1989 A tutorial on hidden markov models and selected applications in speech recognition Proceedings of the IEEE 77 2 257–286 Strickert M et B Hammer 2003 Neural gas for sequences In Proceedings of the Workshop on Self Organizing Networks WSOM Kyushu Institute of Technology pp 53–57 Verbeek J N Vlassis et B Krose 2005 Self organizing mixture models Neurocompu ting 63 99–123 Yamaguchi N 2010 Self organizing hidden markov models In K Wong B Mendis et A Bouzerdoum Eds Neural Information Processing Models and Applications Volume 6444 of Lecture Notes in Computer Science pp 454–461 Springer Berlin Heidelberg Summary We present a generative approach to learn a new probabilistic Self Organizing Map Pr SOMS for non independent and non identically distributed data sets Our model defines a low dimensional manifold allowing friendly visualizations To yield the topology preserving maps our model has the SOM like learning behavior with the advantages of probabilistic models This new paradigm uses HMM Hidden Markov Models formalism and introduces topologi cal relationships between the states This allows us to take advantage of all the known classical views associated to topographic map We demonstrate our approach on the real world data is sued from French National Audiovisual Institute 358 