actes_non_num 351rotes méthode classification supervisée paramètre apprentissage grandes bases données boullé orange avenue pierre marzin 22300 lannion boulle orange ftgroup perso francetelecom boulle résumé papier présentons méthode classification super visée paramètre permettant attaquer grandes volumétries méthode basée estimateurs densités univariés optimaux bayes classifieur bayesien amélioré sélection variables moyennage modèles exploitant lissage logarithmique distribution posteriori modèles analysons particulier complexité algorith mique méthode montrons comment permet analyser bases données nettement volumineuses mémoire disponible sentons enfin résultats obtenu récent pascal large scale learning challenge notre méthode obtenu performances prédictives premier temps calcul raisonnables introduction phase préparation données particulièrement importante processus mining critique qualité résultats consomme typiquement ordre temps étude mining entreprise comme france télécom mining appliqué nombreux domaines marketing textuelles données classification trafic sociologie ergonomie données disponibles hétérogènes variables numériques catégorielles variables cibles comportant multiples classes valeurs manquantes distributions bruitées déséquilibrées nombres variables instances pouvant varier plusieurs ordres grandeurs contexte industriel impose contraintes telles potentiel données collectées systèmes information largement utilisé cette situation aggrave années après années suite vitesses évolution divergentes capacités systèmes information augmentation rapide stockage seulement rapide tement capacités modélisation méthodes apprentissage statistique progression lente disponibilité analystes données mieux constante contexte solutions actuelles impuissantes répondre demande rapidement croissante utilisation techniques mining projets surnombre abandonnés traités optimalement résoudre goulot étranglement intéressons blème automatisation phase préparation données processus mining méthode classification supervisée paramètre grandes bases données article présentons méthode automatiser phases prépara données modélisation processus mining intérêt article provient méthode utilisée publiée extensions apportées gérer grande volumétrie évaluation challenge internationnal après avoir rappelé partie principes notre méthode introduisons partie algorithmes permettant traiter bases nettement volumineuses mémoire disponible partie présente résultats obtenus large scale learning challenge enfin partie présente résumé perspectives méthode classification entièrement automatique notre méthode introduite boullé étend classifieur bayesien grâce estimation optimale probabilité conditionnelles univariées sélection variables selon approche bayesienne moyennage modèle exploitant lissage logarith mique distribution posteriori modèles discrétisation optimale classifieur bayesien démontré efficacité nombreuses applications réelles classifieur suppose variables explicatives conditionnellement indépendantes nécessite uniquement estimation probabilités condi tionnelles univariées études menées littérature démontré intérêt méthodes discrétisation évaluation probabilités conditionnelles approche boullé discrétisation supervisée posée forme problème sélection modèle résolu selon approche bayesienne espace modèles discrétisation défini prenant paramètres nombre intervalles bornes intervalles distribution classes intervalle distribution priori proposée espace modèles exploitant hiérarchie paramètres choix uniforme chaque étage hiérarchie distributions multinomiales classes tervalles supposées indépendantes entre elles meilleur modèle discrétisation choisi selon approche maximum posteriori consiste maximiser probabilité model modèle connaissant données basant définition explicite espace modélisation distribution priori modèles formule bayes obtenir expression analytique exacte évaluation probabilité posteriori modèle discrétisation heuristiques optimisation efficaces mises oeuvre rechercher meilleur modèle discrétisation variables descriptives catégorielles traité selon approche boullé basant famille modèles estimation densité conditionnelle partitionne ensemble valeurs descriptives groupes valeurs sélection variables selon approche bayesienne hypothèse naive indépendance dégrader performances prédictives respectée éviter problème variables fortement corrélées classifieur outil disponible shareware perso francetelecom boulle boullé sélectif langley exploite approche enveloppe kohavi sélectionner ensemble variables façon optimiser bonne classification cette méthode améliore notablement performances données petite taille applicable grande volumétrie données comportant centaines milliers instances milliers variables problème provient algorithme optimisation complexité quadra tique nombre variables critère évaluation sélection sujet apprentissage boullé problème apprentissage abordé suivant approche bayesienne meilleur modèle sélection variable modèle paramètres modélisation nombre variables ensemble variables sélectionnées utilisant nouveau priori hiérarchique vraisemblance conditionnelle modèles dérive probabilités conditionnelles prédites classifieur bayesien obtient alors calcul exact probabilité posteriori modèles algorithmes optimisation complexité super linéaire nombre variables instances utili heuristiques stochastiques ajout surpression variables moyennage modèles compression moyennage modèles appliqué succès bagging breiman exploite ensemble classifieurs appris ensembles échantillonnés opposé cette approche chaque classifieur poids classifieur moyenné moyennage bayesien modèles hoeting pondère classifieurs selon probabilité posteriori classifieur bayesien sélectif analyse dèles optimisés révèle distribution posteriori piquée moyennage revient pratiquement choix modèle enlève intérêt façon intermédiaire entre poids uniformes bagging poids fortement déséquilibrés moyennage bayesien nouvelle approche proposée boullé lissage logarithmique distribution posteriori modèles complexité algorithmique méthode cette partie rappelons abord complexité algorithmique méthodes taillées boullé toutes données tiennent moire présentons extension méthode grande volumétrie méthode comprend trois étapes prétraitement univarié discrétisations pements valeurs sélection variables moyennage modèles étape prétraite super linéaire temps calcul complexité nombre variables nombre instances étape sélection variables méthode alterne passes ajout suppression rapide variables toute amélioration immédiatement acceptée réordonnancements aléatoires variables processus répété plusieurs façon mieux explorer espace modélisation réduire variance causée ordre évaluation variables nombre passes variables telle façon complexité globale cette étape comparable étape prétraitement étape moyennage modèles consiste récolter ensemble modèles évalués pendant étape méthode classification supervisée paramètre grandes bases données sélection variables moyenner selon lissage logarithmique probabi posteriori impact complexité algorithmique globalement algorithme apprentissage complexité algorithmique temps espace quand données dépassent capacité mémoire complexité espace grands données tiennent mémoire peuvent traités algorithme précédant dépasser cette limite proposons amélioration algorithmes partitionnement ensemble variables temps importe distinguer temps accès mémoire disques accès séquentiel aléatoire ordinateurs modernes année ordre nanosecondes accès séquentiels disque rapides transfert environ ordre temps traitement processeur souvent facteur limitant quand opérations transcodage valeur impliquées temps accès aléatoire disque ordre millisecondes million ainsi seule traiter volumes données utiliser disques manière séquentielle taille données traiter taille mémoire dispo nible étape prétraitement notre méthode chaque variable analysée seule après avoir chargée mémoire partitionne ensemble variables parties variables telle façon étape prétraitement boucle ensembles variable chaque itération boucle gorithme données transcode uniquement variables traiter charge mémoire analyse façon inférer tables probabilités conditionnelles enfin libère mémoire correspondante étape sélection variables algorithme remplace premier temps valeur chaque variable index table probabili conditionnelles correspondante issue prétraitement autant fichiers prétraités temporaires nécessaire chaque fichier prétraité chargé mémoire intégra lement rapidement puisqu fichiers index nécessitant aucune opération transcodage algorithme sélection variables boucle ensemble fichiers ordre aléatoire chaque fichier prétraité comportant partie variables chargé mémoire analysé libéré mémoire globalement surcoût algorithmique volumétrie traduit passes lecture fichier phase prétraitement passes sélection variables résultats large scale learning challenge pascal large scale learning challenge2 organisé occasion conférence objectif permettre comparaison directe méthodes appren tissage grande volumétrie données présentés table représentent variété domaines données artificiels alpha détection visage reconnaissance caractère prédiction point coupure détection webspam contiennent jusqu plusieurs millions instances milliers largescale first fraunhofer about boullé données challenge performance notre méthode domaine instances variables aoprc aoprc alpha 500000 500000 gamma 500000 delta 500000 epsilon 500000 500000 5469800 3500000 50000000 webspam 350000 variable variables dizaines gigaoctets espace disque avons appliqué notre méthode entièrement automatique aucun ajustement paramètre utilisation ensemble validation expérimentations avons utilisé windows table présente résultats challenge critère perfor mance aoprc precision recall curve retenu organisateurs perfor mances obtenues données alpha mauvaises raison bayesien incompatible fortes corrélations variables cette artificielle excepté notre méthode obtient toujours performances prédictives compétitives trois données seconde position autres autant remarquable méthode entièrement automatique capacité limitée hypothèse sienne naive également noter quand taille données dépasse celle mémoire ordre grandeur surcôut temps apprentissage compris disque traitements processeur facteur comme attendu notre temps apprentissage uniquement nettement celui méthodes online sentiellement linéaires optimisation gradient stochastique utilisées plupart autres compétiteurs environ ordres grandeurs néanmoins prend compte processus complet apprentissage temps accès disque prépa ration données ajustement paramètres méthodes notre temps apprentissage devient compétitif environ heure gigaoctet données analysé résumé notre méthode capable traiter manière entièrement automatique grandes volumétries obtient performances prédictives particulièrement compétitives conclusion avons présenté méthode classification entièrement automatique exploite hypothèse bayesienne naive estime probabilités conditionnelles univariées moyen méthode discrétisations groupements valeurs optimaux variables numériques catégorielles recherche ensemble variables consis hypothèse bayesienne naive utilisant critère évaluation selon approche bayesienne sélection modèles heuristiques efficaces ajout supression méthode classification supervisée paramètre grandes bases données riables enfin moyenne ensemble modèles évalués exploitant lissage logarith mique distribution posteriori modèles résultats obtenus large scale learning challenge démontrent notre thode grande volumétrie construit automatiquement classifieurs performants quand taille données traiter dépasse celle mémoire disponible ordre grandeur notre méthode exploite stratégie efficace partitionnement données disque surcoût temps traitement seulement facteur travaux futurs notre objectif étendre méthode régression classification grand nombre valeurs expliquer références boullé bayes optimal approach partitioning values categorical attri butes journal machine learning research boullé bayes optimal discretization method continuous attributes machine learning boullé compression based averaging selective naive bayes classifiers journal machine learning research breiman bagging predictors machine learning idiot bayes stupid after international statistical review hoeting madigan raftery volinsky bayesian model averaging tutorial statistical science kohavi wrappers feature selection artificial intelligence langley induction selective bayesian classifiers proceedings conference uncertainty artificial intelligence morgan kaufmann hussain discretization enabling technique mining knowledge discovery preparation mining morgan kaufmann publishers francisco summary paper present parameter scalable classification method method based bayes optimal univariate conditional density estimators naive bayes classification enhanced bayesian variable selection scheme averaging models using rithmic smoothing posterior distribution focus complexity algorithms datasets larger available central memory finally report results large scale learning challenge where method obtains state performance within practicable computation