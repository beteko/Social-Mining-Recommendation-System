Microsoft Word - IRP_ROM_EGC_07_FINAL.doc optimale représentation de l'histogramme des grands ensembles de données: Fisher vs linéaire par morceaux approximation. Antonio Irpino *, Elvira Romano ** * Dipartimento di studi europei e mediterranei Université de Naples II Via del Setificio, 15 Complesso Monumentale Belvedere - San Leucio I-81020 Caserta (CE) irpino@unina.it ** Dipartimento di Matematica e STATISTICA Universita degli Studi di Napoli "Federico II" Via Cintia - Complesso Monte Sant'Angelo I-80126 Napoli elvrom@unina.it~~V~~plural~~3rd Résumé. représentation Histogramme d'un grand ensemble de données est une bonne façon de résumer et de visualiser des données et est souvent effectuée afin d'optimiser l'estimation de la requête dans SGBD. Dans cet article, nous montrons les performances et les propriétés des deux stratégies pour une construction optimale de sur un histogrammes réel sin- gle descripteur d'une valeur sur la base d'un choix préalable du nombre de ets sarrasin. La première est basée sur l'algorithme de Fisher, tandis que le second est basé sur une procédure géométrique pour l'interpolation de la fonction de distri- bution empirique par une fonction linéaire par morceaux. La qualité de l'ajustement est calculé en utilisant la métrique Wasserstein entre les distributions. Nous comparons les performances de la méthode proposée par contre certains jeux de données sur celles qui existent déjà artificiels et réels. 1 Introduction mécanisme de l'information de stockage Aujourd'hui, ne parvient pas à capturer une grande quantité de données et processus pré-les dans leur intégralité, alors que seulement un résumé est stocké. Dans cet histogramme contexte joue le rôle d'un outil pour produire une description récapitulant appropriée et répondre rapidement aux demandes d'aide à la décision. À la suite de l'expression de guidage « Une image dit plus d'un Hon- mots Dred », l'histogramme représente un outil simple et graphique intuitive pour décrire la distribution des données. Il aplanit les données à afficher la forme générale d'une distribution empirique. Le problème est qu'il peut donner une fausse impression de la forme de la distribution de jeux de données, car sa construction dépend du choix du nombre et la longueur des sous-intervalles - seaux usu- allié appelés ou bacs - des lignes réelles sur lesquelles la histogramme est basé. il pourrait idéalement avoir la situation dans laquelle des grands bacs de la nature de l'ensemble de données est bimodale et pour les petits bacs le terrain réduit à la représentation unimodale. La question en jeu ici concerne le genre de largeur de bac qui peut prendre en compte la meilleure représentation graphique du SGBD sous-jacent et comment il peut être construit avec une approximation d'erreur minimale. représentation de l'histogramme optimal des grands ensembles de données dans la communauté de base de données, et en particulier dans le cadre de l'optimisation des requêtes, la recherche d'un bon histogramme pour la représentation d'un grand ensemble de données est mieux connu comme le problème « d'estimation de sélectivité ». Les estimations peuvent être utilisées pour sélectionner le meilleur plan parmi beaucoup les concurrents. Il existe deux grandes classes de méthodes d'estimation de sélectivité: méthodes et méthodes d'échantillonnage Tical statis-; dans ce document, le deuxième type (méthodes statistiques non paramétriques) est prise en compte. Dans Sec. 2 quelques-unes des méthodes d'histogramme sont brièvement passés en revue, tandis qu'une excellente taxonomie des histogrammes se trouve dans Poosala et al. (1996). De nombreux ensembles de données ont des attributs continus valeur tels que des ensembles de données scientifiques et statistiques. Les domaines état de l'art Histogrammes traite implicitement avec la valeur d'attribut discrètes ou catégoriques dans lesquels il y a relativement peu de valeurs distinctes dans l'attribut, ces méthodes sont utilisées pour estimer rejoindre sélectivités aussi (voir Ioannidis et Poosala (1995)). En l'absence de nombreuses valeurs en double dans de nombreux ensembles de données scientifiques et statistiques, une équi-jointure entraînera efficacement dans l'ensemble vide, à l'origine de ces méthodes sont inefficaces. A partir de ce point, notre approche tente de saisir les caractéristiques variables statistiques, afin que nous puissions considérer un bas modèle statistique approche ed, étant donné que notre objectif est de rapprocher (en fonction) la fonction cumulative par polynôme par morceaux (modèle géométrique). Les méthodes proposées tentent de résoudre le calcul de l'histogramme en présence d'ensembles de données OUs presque continuités selon deux approches différentes: la première est basée sur l'algorithme de Fisher pour la partition de données ordonnées, celle-ci est basée sur la meilleure interpolation de la fonction de distribution des données. La sensibilité de l'algorithme de remplacement proposée est étudiée en utilisant plusieurs ensemble de données et de la qualité de l'approximation est calculée en proposant une qualité de mesure en forme sur la base de la métrique L2 Wasserstein entre deux fonctions de distribution. Une application sur un artificiel et sur deux ensemble de données réel est effectuée afin de confirmer notre procédure. 2 Touches de bienséances histogramme et une petite revue des techniques de Isting ex Examinons la définition de l'histogramme. Définition 1 Un histogramme sur une variable X est réalisé en divisant la distribution de données en sous-ensembles appelés seaux et rapprocher les fréquences f et les valeurs dans chaque godet d'une façon commune (Ioannidis, 1993). Dans cette définition, il n'est pas mentionné comment dessiner des classes spécifiques d'histogramme et qui sont les principaux aspects à prendre en compte pour sa construction. Il y a six principaux aspects à considérer dans la construction de l'histogramme: Règle de partition, l'algorithme de construction, fréquence d'approximation, approximation Valeur, erreur Guarantes (Ioannidis, 1995). Dans la première proposition se rapproche de la largeur des casiers sont également espacés et les propositions ont été essentiellement fondé sur le choix du nombre de bacs (Wand, 1997). Néanmoins, ces méthodes ont l'inconvénient de perdre les détails de la partition à haute densité de données. Au cours des dernières années, plusieurs types d'histogramme ont été proposées pour résoudre ce problème. Dans tous la ligne directrice commune est de trouver le meilleur emplacement des points de coupe, en plus du nombre de bind A. Irpino et E.Romano RNTI - X - largeur estimation fonction de densité (Kooi, 1980). Ce problème a reçu une attention non seulement dans les statistiques et la communauté de base de données, mais aussi dans l'analyse numérique, où la densité est approchée par une classe de polynôme de piecewise un certain degré fixe. Les régimes communs pour la construction dans histogrammes SGBD diffèrent en termes de contraintes de accu- racé de partition. La première proposition remonte à la thèse de doctorat de Kooi. Il présente un concept couramment utilisé dans la littérature statistique, la forme la plus simple de l'histogramme, dans laquelle le jeu de valeurs est divisée en intervalles de longueur égale, que l'on appelle histogramme équi-largeur. Dans ues Val- et des fréquences particulières au sein de chaque godet, sont approchées par la hauteur du godet. Comment- jamais, histogrammes equi-largeur avaient pas une bonne amélioration par rapport à la répartition uniforme AS- pour l'ensemble consommation de valeur, qui est la raison pour laquelle de nouvelles propositions ont été faites. Le soi-disant hauteur équi- ou histogrammes équi-profondeur (Piatetski-Shapiro, 1984) est l'un de ceux-ci. il con- siste en particulier de diviser l'ensemble de l'attribut dans des seaux qui ont comme approximativement le même nombre de tuples. Après ces propositions l'attention a été déplacé à l'étude de la façon dont les erreurs d'approximation initiale est maintenue dans la base de données d'estimation par ces techniques. Les histogrammes V-optimales (Ioannidis, 1995) ont été proposées pour minimiser l'erreur quadratique moyenne pour problème d'estimation de la sélectivité. Dans cette technique, la partition de la distribution des données est calculée de telle sorte que la variance d'une valeur-paramètre de la source à l'intérieur de chaque godet est minimisé. Outre les contraintes de séparation V-optimale, les méthodes d'autres, comme ce dernier, ont été mis au point ayant pour but principal de regrouper rapidement plusieurs valeurs de paramètres source dans le même seau. Parmi eux, on peut distinguer MaxDiff (Ioannidis Y., 1993), qui place les limites du godet entre la source paramètre adjacent. En plus de ces solutions pour les contraintes de séparation, numérique Solu tion pour capturer la distribution de forme n'a reçu que peu d'attention. Parmi eux, il a été proposé de trouver splines linéaires pour chaque bac par un moindre carré pro- blème de régression (Konig, 1999), mais pas beaucoup d'attention a été consacrée au nombre de paramètres à estimer et au coût de la construction efficace. Sur la base de la règle de la partition, histogrammes peuvent être classés en fonction de leurs propriétés mutuellement (orthogonalement Poosala et al., 1997). Le tableau 1 résume et en même temps décrit comment peut être colloqué les méthodes existantes. Dans ce cadre notre méthode a lieu dans le contexte des méthodes qui utilisent les valeurs de variable observée et la fréquence cumulée relative. SOURCE SORT PARAMETER paremeter propagation (S) Fréquence (F) zone (A) cumulation. Fréq. (C) Valeurs (V) Valeurs (V) Equi-Somme Equi-Somme V-Max-Optimal .diff comprimé V-Max-Optimal .diff comprimé à base Spline V-optimale Piecewise Fisher fréquence (F) V-OPTIMALE MaxDiff Zone (A) MaxDiff TAB. 1. - Carte des principales approches de la construction de l'histogramme. Les algorithmes proposés dans le présent document sont soulignés. représentation d'histogramme optimal de grands ensembles de données 3 Les techiques proposées Soit X une variable numérique dont le domaine est constitué de V ordonné de valeurs. Soit (x1, x2, ..., xn) une liste d'observation N (tuples) pour la variable X, tandis que (v1, v2, ..., vV) est l'ensemble de valeurs distinctes assumée par X dans l'ensemble de données. La fonction de masse empirique de X est définie par: {}: 1, / ij si J.-J. N xv N = # ≤ ≤ = On décrit la fonction de répartition empirique de X: 1 iijjcf = = Σ De la même manière, nous définissons la masse d'un intervalle]] (), ab ⊆ -∞ + ∞ comme:]] () {}, 1, / si abjj N axb N = # ≤ ≤ <≤ Si le domaine est divisé en seaux bêta, en supposant la distribution uniforme dans les ets sarrasin, on calcule la densité empirique de la j-ième (1 j β≤ ≤) seau comme: () () (), jjjjj jd bbfbbbb⎤ ⎤ ⎤ ⎤ = -⎦ ⎦ ⎦ ⎦. La densité peut être affichée par un histogramme, qui est un graphique à barres dans lequel la proportion dans la liste sont représentées par les zones de différentes barres. 3.1 L'algorithme Fisher L'algorithme Fisher peut être considéré comme un algorithme V-optimale (V, V). En effet, la fonction Ject ob- qui est réduite au minimum est la somme de la variance intra-godets. V-Optimal algo- rithme, étant fixé un certain nombre de godets ß diviser le domaine de valeurs, optimise une fonc- tion du paramètre de source selon la formule suivante: 1 min () HHHW VAR X = Σ β où x est la source paramètre et w est un poids du paramètre source. Si le paramètre source est le domaine des valeurs et w = 1, il correspond à la minimisation de la variance dans les seaux et conduit à la mise en œuvre de l'algorithme dynamique de partition en raison de Fisher (1958) pour les données commandées. On définit les quantités suivantes: () 1 1 2 1 1 1: [,]: [,] (1) ([,]) ([,]) où ([,]) i V i VV ii VV iiivvvivvv VAR k VAR vvfv AVG vv AVG vvfv ∈ ∈ = = = - = Σ Σ la vi borne supérieure de la nouvelle kième godet est choisie en fonction de la formulation dynamique suivante: () () () () 1 1 min () | 1 et,,, i H ijjjiijvh Arg VAR h VAR jjkvbb VAR VAR bv vb - = ⎧ ⎫⎡ ⎤ ⎡ ⎤ ⎡ ⎤- ∈ - ∈ + + ⎨ ⎬⎣ ⎦⎣ ⎦ ⎣ ⎦⎩ ⎭ Σ où 1 H k 1j β ≤ ≤ - ≤ ≤ et, j jb b⎡ ⎤⎣ ⎦ est la notation d'intervalle de la j-ième seau. Le coût de calcul est en termes d'opérations dans le pire des cas est égal à O (V2β). A. Irpino et E.Romano RNTI - X - 3,2 interpolation par morceaux de la fonction de distribution empirique Le procédé commence à partir de l'histogramme trivial -one seau histogramme ou l'uniforme ap- proximation- et à chaque pas la limite du nouveau godet est choisi sur la base de cette valeur pour laquelle on observe la distance maximale L2 entre la valeur prédite et la valeur observée. La distance c un être non pondérée (dans la version standard de l'algorithme), ou pondérée par le nombre d'observations dans les seaux. Dans le second cas, si les deux valeurs ont la même erreur, il est choisi de la valeur de la plus godet peuplée, en fonction de la motivation pratique qu'il est préférable de supprimer une erreur d'un seau qui se rapprochent d'un grand ensemble d'observation à partir d'un plus petit. Nous commençons compte tenu de l'histogramme trivial tel que: 0 (,) VTriv U vv~ où v0 est un point d'artificiel ajouté à l'ensemble de données de telle sorte que [] () 0 1 1, fvvf = Afin d'identifier le meilleur point de coupe appartenant à la vis dans des seaux k nous résolvons l'algorithme sui- vantes à chaque étape afin de trouver β points de coupure. Pour l'algorithme standard (PWST) et pour la version pondérée (PWW), nous avons: () {} () {} 2 2 * (PWST) ou () * (PWW) iiiiiivv Argmax vv Argmax fjvv- - où f (j ) est la fréquence de la j-ième seau qui comprend vi et où * iv est calculée au moyen de la fonction de quantile: () *, pour 1, ..., 1 () jjijjiijjbbvbcbcvbbjk fj - ⎡ ⎤⎡ ⎤ = + - ↔ ∈ = -⎣ ⎦ ⎣ ⎦ le coût de calcul en termes d'exploitation est, dans le pire des cas, égale à O (vp). 4 La représentation de mesure de la qualité Dans le paragraphe suivant, nous présentons une façon plus cohérente pour calculer le carré d'erreur moyenne de l'histogramme obtenu et la distribution des données selon une métrique entre la distribution. Nous développons une mesure de prise de précision en considération la somme des différences entre les carrés prédite et la valeur observée, compte tenu du (émis l'hypothèse) la nature Con- tinu du modèle par rapport aux valeurs observées discrètes. Lorsque nous utilisons une fonction continue avec un histogramme (à savoir un mélange de ß uniformes avec des supports non chevauchants) pour interpoler une fonction continue droite discrète nous mit toujours une ad- d'estimation d'erreur. Étant donné un vecteur [v1, .., vi, ..., vV] des valeurs avec la fonction de masse égale à fi, le meilleur histogramme est constitué de godets V, et peut être représentée par une fonction linéaire par morceaux, où la pièce linéaire général a i iv bornes F (()) i iv F v et 1 1 (, ()) + v +. L'histogramme est la meilleure dans le sens d'interpolation linéaire par morceaux. Notre proposition est d'évaluer la précision de la procédure au moyen d'un calcul de distance entre le modèle obtenu du mélange d'uniformes et le meilleur histogramme. représentation de l'histogramme optimal des grands ensembles de données Nous proposons d'utiliser la distance L2 Wasserstein faire la comparaison (Gibbs et su, 2002, Barrio et al., 1999, et Verde et Irpino, 2006). Il peut être considéré comme le prolongement naturel de la distance euclidienne à partir des données de point aux données de distribution, et il a des propriétés intéressantes de decompo- sition. Etant donné deux fonctions de distribution F et G, la distance L2 Wassertein peuvent être calculés selon la formule suivante: () 1 22 1 1 0 () () Wd F t G t dt - - = -∫ où F-1 et G- 1 sont les fonctions de quantiles des deux distributions. La mise à distance Compu est lourd lorsque la distribution est continue, mais Verde et Irpino (2006) est montré sa faisabilité lorsqu'ils traitent avec des histogrammes. En annexe, nous montrons que la distance proposée peut se décomposer comme la somme de la différence des carrés moyens, la différence carré des écarts-types et une partie uel resid- qui peut être pris comme une distance de forme entre les deux distributions. La décompositions tion est résumée comme suit: () () 2 22 2 (1 (,)) W fgfgfg QQ ShapeLocation Taille DCORR F Gμ μ σ σ σ σ = - + - + - Nous considérons comme une erreur maximale autorisée par interpolation des données avec l'histogramme de la distance L2 Wasserstein entre l'histogramme trivial (l'histogramme permettant à un seul godet) et celui optimal, on peut obtenir une qualité relative de l'indice en forme que le rapport entre la distance quadratique du modèle obtenu (M *) et la valeur optimale histogramme (Opt) et la distance quadratique entre le modèle trivial (Tri) et la valeur optimale. Nous appelons cette mesure comme SGFR (S Quare de qualité de l'ajustement Ratio): 2 2 (*,) (*) (,) WW d M Opt SGFR M d Tri Opt = Utilisation de la décomposition de la distance au carré, on peut évaluer la « qualité » de ness Good- de ajuster compte tenu de la quantité de la distance est influencée par un emplacement, une taille ou une différence de forme. 5 application sur un ensemble de données réelles et artificielle Nous testons les techniques proposées sur trois données. Le premier est un ensemble de données artificiel qui dérive de la génération aléatoire de 10.000 valeurs. Il dérive d'un mélange de trois répartition mal nor- f (x) = 0.33N (20,20) + 0.33N (40,10) + 0.34N (70,25). Le deuxième ensemble est constitué des 10.000 observations des dst_bytes variables de la database1 Cup KDD 99. Cet ensemble de données a été choisi pour sa caractéristique d'être un exemple d'une distribution de peaky (discontinue). Le troisième ensemble recueille la première 10,000 observation de la variable d'élévation à partir du type de couverture forestière database2. Cet ensemble de données a été choisi pour être un exemple d'une distribution lisse (continue). 1 2 http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html http://kdd.ics.uci.edu/databases/covertype/covertype.html A. Irpino et E.Romano RNTI - X -. Les valeurs Artificial dataset = 10.000 Obs = 10,000 Godets algorithme mesure 10 25 50 100 200 Temps en sec. 0,55955 0,604214 0,566546 0,58457 0,56958 MD SSE 65,58197 69,49141 75,58185 82,9155 89,14396 SGFR 1,033287 1,06364 1,109271 1,1618 1,20468 Temps en sec. 40,14913 52,06039 65,02176 82,95111 111,3314 Vopt SSE 7,71032 2,902464 0,339665 0,13697 0,02490 SGFR 0,354292 0,217371 0,074347 0,04719 0,02008 Temps en sec. 19,07574 33,5456 55,99695 97,6234 173,8116 FISHER SSE 8,211255 5,167489 0,955339 0,00252 0,00097 0,36562 SGFR 0,290044 0,124707 0,00633 0,00386 Temps en sec. 0.240339 0,23878 0,315012 0,52743 1,11027 PWST SSE 0,512781 0,048029 0,004592 0,00196 0,00052 SGFR 0,091357 0,027947 0,00862 0,00563 0,00289 Temps en sec. 0,19953 0,23803 0,31237 0,52245 1,500763 PWW SSE 0,02326 0,00285 0,00100 0,51278 0,00036 0,01942 0,00676 SGFR 0,09135 0,00396 0,00234 TAB. 2 - synoptiques des performances des cinq algorithmes en utilisant le premier ensemble de données: MD (MaxDiff (F, F)), Vopt (V-optimale (V, F), Fisher, PWST (pièce approximation rationnelle de la fonction de distribution, non pondéré) , PWW (Piece approximation Wise à la tion fonc- de distribution, pondérée par la fréquence du godet). les méthodes de comparaison en gras les meilleurs résultats sont montré. utilisés ici sont les MaxDiff (F, F), le V-optimale (V, F ) l'algorithme Fisher, la norme et la version pondérée de l'algorithme d'interpolation cumulée par morceaux. Nous avons mesuré le temps de fonctionnement en utilisant MATLABTM sur un PC (CPU Intel Cen- Trino 1,77Mhz, RAM 1024 Mo). la précision est calculée en utilisant le classique fonction d'erreur et la nouvelle mesure de précision sur la base L2 Wasserstein métrique entre les distributions. les principaux résultats sont recueillies dans les tableaux 2 à 5. Bien que l'algorithme MaxDiff (F, F) ont les meilleures performances en termes de temps passé pour l'estimation de l'histogramme il est moins précis lorsque le nombre de valeurs de t il domaine de la variable est très grande. Le algo- rithme est piecewise toujours le meilleur en termes de temps CPU et la précision. Pour illustrer l'exacti- tude améliorée du projet approche Figure 1 montre les principaux résultats pour β = 10 du artificielle Dataset. En ce qui concerne la qualité de qualité de l'ajustement (Tab. 5) plus les seaux le meilleur de tous les algorithmes (sauf pour le MaxDiff et le Voptimal (V, F) pour l'ensemble de données KDD99 biaisé) correspondent aux deux premiers moments. algorithme Fisher et les ont ANCE de mieux définies par intervalles perfor- sur les autres. En comparant la qualité de qualité de l'ajustement entre Fisher et les algorithmes de piecewise (Tab. 5), la dernière semble être plus précis dans l'estimation des deux premiers moments où le nombre de seaux augmente. Nous supposons que cela est dû au fait que les algorithmes Fisher, étant basés sur une variance c RITÈRE, permet de regrouper les données en classes sphériques, alors que les méthodes de sont basées sur morceaux le meilleur ajustement linéaire de la fonction de distribution, mettant l'accent sur implicitement la densité uniforme locale des données. représentation de l'histogramme optimal des grands ensembles de données cov Forest. Les valeurs du jeu de données = 360 obs. = 10,000 Godets algorithme mesure 10 25 50 100 200 Temps en sec. 0,00147 0,00144 0,00150 0,00152 0,00165 MD SSE 25,28642 19,70946 0,22061 0,07711 0,00861 0,14530 0,01547 SGFR 0,16422 0,00912 0,00281 Temps en sec. 0,24917 0,44977 0,98323 0,11676 1,37233 Vopt SSE 0,07816 0,07039 0,05140 0,85188 0,02625 0,03025 0,00849 0,00801 SGFR 0,00689 0,00481 Temps en sec. 15,11523 30,80159 51,00386 92,46458 165,41046 FISHER SSE 0,22606 0,08548 0,04502 1,91894 0,02175 0,04506 0,01500 0,00875 SGFR 0,00603 0,00368 Temps en sec. 0,24187 0,32029 0,53857 0,19329 1,47643 0,44978 0,08213 PWST SSE 0,03813 0,01217 0,00153 0,02136 0,00902 0,00596 SGFR 0,00323 0,00105 Temps en sec. 0,24211 0,50761 0,55726 0,19092 1,20335 PWW SSE 0,09306 0,04329 0,01098 0,44978 0,00153 0,00959 0,00628 SGFR 0,02136 0,00306 0,00106 TAB. 3 - synoptiques des performances des cinq algorithmes à l'aide du jeu de données de type de couverture Forest: MD (MaxDiff (F, F)), Vopt (V-optimale (V, F), Fisher, PWST (Piece approximation rationnelle de la fonction de distribution, non pondérée), PWW (Piece approximation rationnelle de la fonction de distribution, pondérée par la fréquence du godet). en gras les meilleurs résultats sont montré. KDD99 dataset Valeurs = 2096 obs. = 10.000 Godets algorithme mesures 10 25 50 100 200 Temps en sec. 0,03753 0,07955 0,02525 0,02647 0,02837 MD SSE 1.2998E + 10 + 09 1.2516E 1.2516E + 09 + 08 2.6199E 1.5893E + 08 SGFR 0,24139 0,24139 0,10965 0,78485 0,08537 Temps sec. 3,63551 6,94830 11,83113 18,52914 33,68945 Vopt SSE 8.3791E + 09 5.7991E + 09 5.7991E + 09 + 09 1.7037E 1.5893E + 08 SGFR 0,52340 0,52340 0,28197 0,62976 0,08537 Temps sec. 29,57838 73,82461 130,39616 232,45977 309,87441 FISHER SSE 1.1452E + 09 + 07 8.0053E 4.6098E + 04 9.8253E + 03 + 03 1.8049E SGFR 0,23301 0,06158 0,00154 0,00068 0,00027 Temps sec. 0,19037 0,24140 0,34018 0 0,56922 1,34964 PWST SSE 2.0296E + 06 + 05 3.0148E 1.9798E + 04 4.5786E + 03 + 02 4.9934E SGFR 0,00965 0,00370 0,00098 0,00046 0,00015 Temps en sec. 0,24546 0,33983 0,56360 0,19179 1,16070 PWW SSE 2.4614E + 05 3.8040E + 04 8.3669E + 03 + 03 1.8198E 221,99776 SGFR 0,00128 0,00052 0,00026 0,00342 0,00009 TAB. 4 - synoptiques des performances des cinq algorithmes utilisant l'ensemble de données KDD 99: MD (MaxDiff (F, F)), Vopt (V-optimale (V, F), Fisher, PWST (Piece approximation rationnelle de la fonction de distribution, non pondéré .), PWW (Piece approximation rationnelle de la fonc- tion de distribution, pondérée par la fréquence du godet) en gras les meilleurs résultats sont montré A. Irpino et E.Romano RNTI - mesures β = 10 Forêt artificielle Kdd 99 Alg -. X. β = β = 200 10 = 200 β β = β = 10 200 d2 (M, Opt) 65,58186 89,14386 25,23811 0,00737 1.296E + 10 1.533E + 08% μ MD,% σ, s% 41,5% 32,5% 26,0% 71,5% 10,5% 18,0% 1,3% 0,8% 97,9% 1,6% 0,2% 98,2% 47,2% 47,0% 5,7% 1,3% 76,2% 22,5% d2 (M, Opt) 7,71016 0,02479 0,85628 0,02167 8.342E + 09 1.533E + 08% μ Vopt, σ% s% 1,1% 27,0% 71,8% 0,0% 2,0% 98,0% 0,1% 1,4% 98,5% 0,2% 0,1% 99,7% 31,8%. 61,5% 6,8% 1,3% 76,2% 22,5% d2 (M, Opt) 8,21110 0,00092 1,90010 0,01265 1.142E + 09 1.565E + 03% μ FISHER,% σ, s% 2,7% 4,7% 92,6% 12,0% 0,8% 87,2% 0,6% 0,3% 99,1% 0,4% 0,0% 99,6% 75,1% 10,6% 14,3% 5,2% 0,9% 93,8% T d2 (M, Opt) 0,51266 0,00051 0,42695 0,00103 1,960 E + 06 5.026E + 02% μ PWST,% σ, s% 1,8% 15,5% 82,7% 0,3% 1,2% 98,4% 14,5% 6,4% 79,1% 0,8% 0,0% 99,1% 38,7% 1,1% 60,2% 3,0% 0,3% 96,7% d2 (M, Opt) 0,51266 0,00034 0,42695 0,00105 2.465E + 05 1.830E + 02% PWW μ, σ%, s% 1,8% 15,5% 82,7% 0,2% 1,0% 98,8% 14,5% 6,4% 79,1% 0,8% 0,2 % 99,0% 5,8% 10,9% 83,3% 0,7% 0,0% 99,2% TAB. 5 - Synopse s: la qualité de la bonté adéquation entre le modèle et le meilleur histogramme, AC- Cording à la décomposition proposée de la distance au carré Wasserstein. FIGUE. 1 - Histogramme de représentation pour l'ensemble de données artificielle et l'illustration de l'approximation de la fonction de répartition empirique pour les différentes méthodes. 6 Conclusions et perspectives Dans le présent document, plusieurs algorithmes bien établis pour la construction de histo- grammes de données ont montré à l'échec de la précision lorsque les données contenues dans la base de données sont quasi-continue, par exemple lorsque les valeurs assumées par le domaine de la une variable ne sont pas si peu MD Vopt Fisher PWW optimale représentation de l'histogramme des grands ensembles de données par rapport aux tuples stockés. Les techniques proposées semble plus capable de faire face à ce problème. En outre, compte tenu de la qualité de l'ajustement au meilleur modèle, la décomposition de la L2 Wasserstein métrique permet de découvrir la qualité de l'approximation d'un histogramme des données, ce qui explique la distance en termes de qualité de l'ajustement des deux premiers instants et la une partie de la distance en raison de seulement un facteur de forme. Dans le présent document la construction de l'histogramme multivariée n'a pas été considéré comme ce sera notre prochaine étape, naturellement par rapport aux techniques existantes qui semble souffrir le problème « malédiction de la dimensionnalité ». A Deeper besoins de perspicacité à donner afin de tester les techniques proposées dans un cadre de flux de données pour l'étude de leurs propriétés dans le cas des fenêtres en mouvement ou dans le histogrammes cas des mises à jour continues des modèles d'histogramme. Références Barrio, E., Matran, C., Rodriguez Rodriguez, J. et Cuesta-Albertos, J.A. (1999). Tests de qualité de l'ajustement en fonction de la distance L2-Wasserstein. Annales de la statistique (1999), 27, 1230-1239. Fisher, W. D., (1958). Le regroupement pour l'homogénéité maximale. Jorn. American Stat. Ass., 53, 789-798. Gibbs, et Su A.L., F.E. (2002). Le choix et de délimitation des mesures de probabilité, Inter- national Statistical Review, 70, 419. Ioannidis, Y., P. V. (mai 1995). Équilibre entre optimalité de l'histogramme et l'aspect pratique pour la requête estimation de la taille du résultat. Proc. de ACMSIGMOD, 233-244. Ioannidis, Y. (1993). Universalité de série Histogrammes. Actes de VLDB, terre Dublin IRE, pages 256-277. Ioannidis, Y. et Poosala, V. (1995). Équilibre entre optimalité de l'histogramme et l'aspect pratique pour la requête estimation de la taille du résultat. Dans SIGMOD, 233-244, San Jose, CA. Konig, A., W. G. (1999). d'ajustement de courbe paramétrique pour la mation de requête entraînée par rétroaction résultat dimension. VLDB Conf., 423-434. Kooi R. (1980). L'optimisation des requêtes dans les bases de données relationnelles. Thèse de doctorat, Université Case Western Reserve Piatetski-Shapiro, G., (1984). Une estimation précise du nombre de tuples satisfaisant une condi- tion. Proc. de SIGMOD, 256-276 Poosala V., Ganti V., Ioannidis Y.E. (1999): Question approximative Réponse à l'aide de grammes histopathologie. Les données IEEE Eng. Taureau. 22 (4): 5-14. Poosala, Y., Ioannidis, Y., Haas, et Shekita P.J., E.J. (1996). histogrammes améliorés pour l'estimation de la sélectivité des prédicats de gamme. Dans SIGMOD, 294-305, Montréal, Canada. A. Irpino et E.Romano RNTI - X - Vert, R., Irpino, A. (2006). Une nouvelle distance sur la base de la Wasserstein ing cluster- hiérarchique de l'histogramme des données symboliques. Données Science et classification (Eds. Batanjeli, Bock, Ferligoj, Žiberna), Springer, Berlin, p. 185-192. Baguette, M. P. (1997). choix de la largeur de bac d'histogramme à base de données. Annexe Preuve de la décomposition de la distance Wasserstein. () () () 1 22 1 1 0 2 2 (() ()): () () 2 (1 (() ())) W ijijijijij QQ ShapeLocation Taille d F x F x F t F t dt Corr Y i Y jμ μ σ σ σ σ - - = - = = - + - + - ∫ (1) observons deux fonctions de densité fi (x) et fj (x) ayant les deux premiers moments finis. Pour chaque fonction de densité peut être associée à la distribution f onctions Fi (x) et Fj (x), les moyens de μi et uJ, les écarts-types σi et σj où: 1 1 0 () () ii ix fx dx F t dtμ + ∞ - = -∞ = ⋅ ∫ ∫ ( 2) En effet () () xf dx x xdf x + ∞ + ∞ -∞ -∞ = ∫ ∫ si t = F (x) et en considérant que 1 1 (()) () x FF F x t- - = = par substitution, on obtient 1 1 0 () F t dtμ - = ∫ (3) où: () 1 22 2 2 1 2 0 xxfx dx F t dtσ μ () () () () () μ + ∞ - -∞ = - = -∫ ∫ (4) pour les mêmes substitutions ci-dessus adoptées maintenant supposons pour centrer les deux distributions en utilisant leurs moyens tels que: 1 1 1 c ci iii iz x F F tz t () () () F tμ μ - - - = - = = - (5) Barrio et al. (1999) est prouvé que () () () () () () () 22 2,:, c cW ijij W i jd F x F xd F x F xμ μ = - + (6) où () ( ) () () 1 22 1 1 0,: () () ccc cW iji jd F x F x F t F t dt - - = -∫ (7) développement du carré, nous obtenons une représentation d'histogramme optimal de grands ensembles de données () () () () () () () 1 1 1 2 22 1 1 1 1 0 0 0 1 1 1 2 21 1 1 1 0 0 0 1 2 2 1 1 0 () () () (), : () () 2 () () () () 2 () () 2 () () cccccc W ijijijiijjiijjijiijjd F x F x F t dt F t dt F t F t dt F t dt F t dt F t F t dt F t F t dt μ μ μ μ σ σ μ μ - - - - - - - - - - = + - = = - + - - - - = = + - - - ∫ ∫ ∫ ∫ ∫ ∫ ∫ (8) considérons la quantité () () () () () () () () 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 2 2 2 21 1 1 1 0 0 0 0 () () () () () () () () () () ccijiijjiijjijcc ijiijj F t F t dt F t F t dt F t F t dt QQ F t dt F t dt F t dt F t dt μ μ μ μ σ σ μ μ ρ - - - - - - - - - - - - - - - - ∫ ∫ ∫ = = = ∫ ∫ ∫ ∫ (9) on peut considérer que la corrélation des deux séries de données où chaque couple de observa- tions est représenté respectivement par la t-ième quantile de la première distribution, et la t-ième tuile quan- du second. Dans ce sens, on peut considérer comme la corrélation entre les fonc- tions quantile représentées par la courbe des points de quantile infinies dans une parcelle de QQ. Il convient de noter que 0 1QQρ <≤ différemment de la gamme classique de variation de l'indice de corrélation de Bravais-Pearson le (-1, + 1). L'équation (8) peut être réécrite sous la forme () () () () () 1 2 2 2 1 1 2 2 0, 2 () () 2c cW ijijiijjij QQ i jd F x F x F t F t dtσ σ μ μ σ σ ρ σ σ - - = + - - - + = -∫ (10) Addition et soustraction de 2 i σ jσ on obtient () () () () () 22 2 2, 2 2 2 2 1c cW ijijijij QQ ijijij QQD F x F x σ σ σ σ σ σ ρ σ σ σ σ σ σ ρ = + - + - = - + - (11) On peut remplacer ce résultat en (6) à obtenir: () () () () () () () () () () 2 2 22 2,:, 2 1c cW ijij W ijijijij QQD F x F xd F x F xμ μ μ μ σ σ σ σ ρ = - + = - + - + - QED La CV d'représentation d'un grand histogramme ensemble de l'Est Une bonne Données Manière et verser visualiseur des résumer et Données is d'fréquemment Optimiseur exécutée l'AFIN luation tion de requests Dans le Système de gestion de bases de données. En this article, nous Mon- trons les performances et les Deux Propriétés de construction juin verser stratégies des histogrammes sur optimale un à facts REELLES descripteur sur la base de d'un apriori du choix de Intervalles Élémentaires Nombre. Le premier sur l'EST basons de Fisher Algorithme, le second Alors Qué is sur un basons procédé Pour L'interpolation géométrique de la fonction de par pirique em- répartition par fonction Une morceaux linéaire. La qualité de l'calculee en is ajustement le Wasserstein Utilisant les distributions Entre métrique. Nous les comparons des methods exécutions contre proposées sur to vary Quelques des Celles ensembles de artificialisation Données et CIELS Réels.