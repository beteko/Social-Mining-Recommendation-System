 Representative training sets for classification and the variability of empirical distributions Saaid Baraty Dan Simovici University of Massachusetts Boston sbaraty cs umb edu dsim cs umb edu Abstract We propose a novel approach for the estimation of the size of training sets that are needed for constructing valid models in machine learning and data mining We aim to provide a good representation of the underlying population without making any distributional assumptions Our technique is based on the computation of the standard deviation of the χ2 statistics of a series of samples When successive statistics are relatively close we assume that the samples produced represent adequately the true underlying distribution of the population and the models learned from these samples will behave almost as well as models learned on the entire population We validate our results by experiments involving classifiers of various levels of complexity and learning capabilities 1 Introduction Estimating a sample size that allows the inference of a good model is an important part of the learning process We seek to determine the minimum size of a sample which is very likely to be a “fair” representative of the underlying population Models learned from these samples will behave almost as well as models learned on the entire population and any increase in the size of the sample would result in insignificant increases in the quality of the models Our goal is to determine sample sizes that are sufficient to ensure that these samples ade quately represent the underlying population These samples are used as training sets for con structing models comparable in performance with those inferred from the entire population but are cheaper to build Sections 2 and 3 describe in detail our approach for finding the size of a sample from a data set and a population respectively The experimental work is presented in Section 4 2 Estimating the size of a sample from data Let U = {u1 u2 un} be a set of attributes The set of possible states for attribute ui Dom ui is assumed to be finite and is commonly referred to as domain of ui The notion of domain of attributes is extended to sets of attributes by defining the set Dom V for the set of attributes V ⊆ U as Dom V = ∏v∈V Dom v 299 Estimating sizes of training sets A dataset of U is a multi set D of tuples t ∈ Dom U The multiplicity of a member t of D is the numberMD t which equals the number of occurrences of tuple t in D The size of D is |D| = ∑t∈dom U MD t Let PU t denote the unknown joint probability distribution of U where t ∈ Dom U Define the λ active domain of U to be the set AdomU λ = {t ∈ Dom U | PU t ≥ λ} where 0 ≤ λ < 1 is a user specified parameter which we refer to as the outlier threshold Definition 2 1 NλS = MS t1 MS tk is the extracted frequency vector of data sam ple S for λ If ELEM D −AdomDU λ 6= ∅ we add an extra tuple to account for those tuples consid ered as outliers that is we set k = m+ 1 andMS tm+1 = |S| − ∑m i=1MS ti otherwise we set k = m Since the tuples of sample S are i i d we can regard the frequency vector NλS for an arbitrary sample S of fixed size q from D as a random vector with distribution NλS ∼ Multinomial q MD t1 |D| MD tk |D| 1 where q= ∑k i=1MS ti and MD tk = |D|− ∑k−1 i=1 MD ti Define the χ2 statistics of sample S for outlier threshold λwith respect to target probability distribution p = p1 pk as X 2S λ p = ∑k i=1 MS ti −qpi 2 qpi We refer to X 2S λ p as χ2 statistics because if NλS ∼ Multinomial q p1 pk then as q → ∞ the distribution of the random variable X 2S λ p converges in distribution to χ2 distribution with k− 1 degree of freedom Pearson 1900 We use X 2S λ p as a measure of how close NλS is in representing the target distribution p As we increase the sample size q by the strong law of large numbers X 2S λ p becomes smaller Our aim is to estimate q the size of a sample from data such that the extracted frequency vectors of the samples of size q are likely to closely represent the empirical distribution of D for those tuples that are not λ outliers Therefore we specify the target distribution to be the empirical distribution of the data and define χ2 statistics of data sample S for outlier threshold λ with respect to empirical distribution of data set D to be X 2S λ D = k∑ i=1 MS ti − qMD ti |D| 2 qMD ti |D| = |D| q k∑ i=1 M2S ti MD ti − q Let S1 Sz be repeatedly drawn z samples of a fixed size q from D Given a threshold λ we compute X 2Si λ D for each Si followed by This process is summarized in Algorithm 1 where σ̂q is the standard deviation among values X 2Si λ D for different i 3 An iterative estimation of the size of a sample from a pop ulation We apply our approach to estimate the size of a fair sample from a population without having a data set at hand Since PU t is unknown we assume that AdomU λ = {t1 tm} for some outlier threshold λ 300 S Baraty et D Simovici Algorithm 1 The pseudocode for finding the size of a sufficient training set from data set D foreach sample size q from smallest to largest do draw data samples of size q S1 Sz with replacement from data set D compute the standard deviation σ̂q of sequence X 2S1 λ D X 2Sz λ D output sample size q such that for any sample size v ≥ q we have σ̂q ≈ σ̂v Definition 3 1 The extracted frequency vector of a population sample S for outlier threshold λ is MλS = MS t1 MS tk where as in Definition 2 1 we have two cases 1 if Dom U −AdomU λ 6= ∅ we add an extra tuple to account for those tuples considered as outliers that is we set k = m+ 1 andMS tm+1 = |S| − ∑m i=1MS ti and 2 otherwise that is if Dom U = AdomU λ we set k = m Informally we consider a sample of size q as a λ fair representative of the population if MλS q closely approximates the population’s true distribution vector of the tuples in AdomU λ Similar to previous section we treat MλS for arbitrary population sample S of size q as a random vector MλS ∼ Multinomial q PU t1 PU tk where PU tk = 1 −∑k−1i=1 PU ti However the probabilities PU ti for 1 ≤ i ≤ k are unknown Hence we define the random probability vector p = p1 pk to represent the occurrence of a k dimensional probability distribution as the true underlying distribution of the population Then p is represented by the probability space Ω P Ω f of k dimensional probability dis tribution vectors where the sample space Ω is a standard k − 1 simplex Note that X 2S λ p is a random variable itself with values in R≥0 We approximate the χ2 statistics of a population sample S with respect to the true underlying distribution by the conditional expected value of X 2S λ p given that we have another sample of the same size from the same population at hand This conditioned sample approximates the shape of the probability distribution of p the second order distribution if it is large enough to unbiasedly represent the underlying distribution of the population Let S1 S2z be a sequence of inde pendent samples of size q drawn uniformly at random with replacement from the underlying population We compute the conditional expected value of χ2 statistics CECS statistics E[X 2Si λ p |Sz+i] for 1 ≤ i ≤ z This statistics is used as a substitute for the actual χ2 statistics of Si with respect to target distribution PU Next we we compute the standard deviation among the CECS statistics of Si given Sz+i for 1 ≤ i ≤ z If q is large enough then the probability distributions captured by the frequencies extracted from Si and Sz+i would be similar to PU and thus similar to each other Therefore the variation in CECS statistics is expected to be small Next observe that P S`|p = ∏k j=1 p MS` tj j If we further assume the prior p ∼ Dirichlet µ1 µk for µ1 µk > 0 then it can be shown that f p|S` follows the Dirichlet distribution Dirichlet α1 αk of order k ≥ 2 We draw with replacement simple random samples S1 S2z of size q from OP where |OP| is a domain dependent multiple of q and the larger the size of the observation pool is relative to q the more reliable is the conclusion of the process E[X 2Si λ p |Sz+i] is evaluated for each i If the standard deviation among conditional expectations is sufficiently small and stabilizes at a certain value of q then we choose this value of q as the threshold of the size 301 Estimating sizes of training sets of fair samples or adequate training evaluation sets Otherwise we increase q and repeat the process In this iterative process we may need to expand the observation pool to make sure it is a sub stantial multiple of q As we add new observations to our pool we need to update Adom OP U λ the set of tuples to be considered according to outlier threshold λ and subsequently k the num ber of dimensions of the probability space Observe that as we expand the observation pool OP Adom OP U λ becomes a closer approximation of the set AdomU λ The following pseu docode explains the process of finding the size of a fair sample from a population as explained in this section Algorithm 2 The pseudocode for finding size of a sufficient training set from a popula tion foreach sample size q from smallest to largest do if ¬ |OP| >> q then expand the OP such that |OP| � q evaluate Adom OP U λ and find k based on this set draw independent samples of size q S1 S2 S2z with replacement from OP compute the standard deviation σ̂q of sequence E[X 2S1 λ p |T1] E[X 2Sz λ p |Tz] output sample size q such that for any size v ≥ q we have σ̂q ≈ σ̂v 4 Experimental results In the first experiment we employed the Algorithm 1 to estimate the size of a data sample from the Bank Marketing Data Set Moro et al 2011 which contains 45 211 records see Figure 1 For λ = 0 the standard deviation drops to its minimal level when q is around 5 000 so a sample of size 5 000 is very likely to fairly represent the entire data which is of size 45 211 For λ = 0 00039 a training sample of size 2 000 is suitable In the next experiment we evaluated our approach for determining the size of a represen tative sample from a population as summarized in Algorithm 2 for � = 0 005 We simulated the process of gathering observations from a population in order to expand the observation pool by synthetically generating tuples of four attributes using a multinomial distribution with randomly selected parameters |Dom U | = 24 and λ = 0 and we executed the Algorithm 2 with z = 1000 For each q we generated one hundred samples of size q from a synthetic data set and used WEKA to learn a k nearest neighbor k NN classifier from each sample Then we evaluated the prediction performance of the classifiers using a fixed test set of size 10 000 which is large enough to represent unbiasedly the underlying distribution of the domain The average and standard deviation of the percentage of correctly classified instances CCI are shown in Figure 2 Similar results were obtained for Bayesian Networks On the other hand experiments with naive Bayes classifiers yield quite different results shown in Figure 3 The improvement in average percentage of CCI as a result of increasing 302 S Baraty et D Simovici FIG 1 – Standard deviation of χ2 statistics of data samples with respect to changes in sam ple size q for Bank Marketing data Each curve corresponds to a particular value of outlier threshold λ listed in the right hand side FIG 2 – Average and STD for 20 NN the sample size q is much smaller than in the previous cases and the average percentage of CCI reaches its peak at sample size q = 2 000 and then slightly decreases to a constant level afterwards Finally the standard deviation of the percentage of CCI converges to zero slower than previous two cases These differences are due to the fact that naive Bayes classifiers are less dependent on the global joint probability distribution than k NN classifiers and Bayesian networks because of the naive independence assumption The experimental results show that it does not make sense to go beyond the size that we determine here because the improvement we gain in the performance is insignificant or inex istent If the evaluated size of the training set is prohibitively large then we may be able to reduce the sample size approximation by analyzing it in the context of a specific classifier References Pearson K 1900 On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to 303 Estimating sizes of training sets FIG 3 – The average and standard deviation of the percentage of correctly classified instances for naive Bayes classifiers have arisen from random sampling Philosophical Magazine Vol 50 no 302 ser 5 pp 157–175 Moro S Laureano R and Cortez P 2005 Using Data Mining for Bank Direct Marketing An Application of the CRISP DM Methodology Proceedings of the European Simulation and Modelling Conference pp 117 121 Portugal Schmidtmann I Hammer G Sariyar M and Gerhold Ay A 2009 Evaluation des Kreb sregisters NRW Schwerpunkt Record Linkage Technical Report IMBEI Résumé Nous proposons une nouvelle approche pour l’estimation de la taille des ensembles d’ap prentissage qui sont nécessaires pour construire des modèles valides dans l’extraction de connais saices Nous visons à fournir une bonne représentation de l’ensemble de données sans faire des hypothèses de répartition Notre technique est basée sur le calcul de l’écart type des χ2 statistiques d’une série d’échan tillons Lorsque les statistiques successives sont relativement proches nous supposons que les échantillons produits représentent adéquatement la vraie distribution sous jacente de la popula tion et les modèles tirés de ces échantillons se comportent presque aussi bien que les modèles appris sur l’ensemble de la population Nous validons nos résultats par des travaux expérimentaux impliquant une variété des clas sificateurs 304 