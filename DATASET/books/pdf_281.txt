 Un algorithme EM pour une version parcimonieuse de l’analyse en composantes principales probabiliste Charles Bouveyron Julien Jacques Université Paris Descartes Laboratoire MAP5 charles bouveyron parisdescartes fr w3 mi parisdescartes fr ∼cbouveyr Université Lumière Lyon 2 Laboratoire ERIC julien jacques univ lyon2 fr eric univ lyon2 fr ∼jjacques Résumé Nous considérons une version parcimonieuse de l’analyse en compo santes principales probabiliste La pénalité `1 imposée sur les composantes prin cipales rend leur interprétation plus aisée en ne faisant dépendre ces dernières que d’un nombre restreint de variables initiales Un algorithme EM simple de mise en œuvre est proposé pour l’estimation des paramètres du modèle La mé thode de l’heuristique de pente est finalement utilisée pour choisir le coefficient de pénalisation 1 Introduction L’analyse en composantes principales ACP Jolliffe 1986 est une des méthodes si ce n’est la méthode d’analyse exploratoire les plus couramment utilisées Elle a été ré interprétée sous un formalisme probabiliste par Tipping et Bishop 1999 montrant que les composantes principales pouvaient être estimées par maximum de vraisemblance dans le cadre d’un modèle à variables latentes Avec l’avénement des données de grande dimension la problématique consistant à sélectionner un petit nombre de variables d’intérêt parmi l’ensemble des variables disponibles est devenue primordiale Un des soucis majeurs de l’ACP dans cette optique est que les composantes principales sont définies comme une combinaison linéaire de l’ensemble des variables initiales Des versions parcimonieuses de l’ACP Zou et al 2004 ainsi que de sa version probabiliste Guan et Dy 2009 ont été proposées récemment La version parci monieuse de Zou et al 2004 repose sur l’ajout d’une pénalisation de type `1 au problème des moindres carrés qui nécessite le choix du coefficient de pénalisation de façon heuristique Dans Guan et Dy 2009 une version sparse bayésienne de l’ACP probabiliste est proposée Nous proposons dans ce travail une alternative fréquentiste utilisant un algorithme EM pour l’inférence La procédure d’estimation obtenue à l’avantage d’être particulièrement simple et ne nécessite pas le choix de loi a priori Elle offre en outre la possibilité de considérer le problème du choix de la pénalité comme un problème de choix de modèles 149 Un algorithme EM pour sparse PPCA 2 Analyse en composantes principales probabiliste Soit y un vecteur aléatoire observé de dimension p et x un vecteur aléatoire latent non observé de dimension d relié à y par l’équation suivante y = Wx+ µ+ � 1 où W est une matrice p × d µ est le vecteur moyenne supposé nul dans la suite µ = 0 et � ∼ N 0 σ2I Conditionnellement au vecteur x la distribution des vecteurs observés est y|x ∼ N Wx σ2I 2 En supposant x ∼ N 0 I la distribution marginale du vecteur observé est y ∼ N 0 WWt + σ2I 3 Ce modèle est un modèle de type "factor analysis" Bartholomew et al 2011 popularisé par Tipping et Bishop 1999 sous le nom d’analyse en composantes principales probabiliste Probabilistic Principal Component Analysis PPCA En effet une estimation par maximum de vraisemblance des paramètres du modèle à l’aide d’un algorithme EM Dempster et al 1977 considérant le vecteur latent x comme manquant conduit à estimer les colonnes de W par les vecteurs propres de la matrice de covariance empirique vecteurs qui ne sont rien d’autres que les axes principaux classiques Soit y1 yn un échantillon i i d de vecteurs observés L’algorithme EM consiste à maximiser de façon itérative la log vraisemblance complétée par les données non observées x1 xn `c = n∑ i=1 −p 2 ln 2πσ2 − 1 2σ2 yi −Wxi t yi −Wxi − d 2 ln 2π − 1 2 xtixi 4 3 Une version parcimonieuse de l’analyse en composantes principales probabiliste Dans ce travail nous considérons une version parcimonieuse de l’analyse en composantes principales probabiliste L’objectif est d’obtenir des axes principaux déterminés uniquement grâce à un nombre restreint de variables initiales et ainsi faciliter leur interprétation De plus comme nous le verrons par la suite l’approche probabiliste de l’ACP permet de sélectionner le paramètre de pénalité par des méthodes classiques de sélection de modèles Dans l’optique d’introduire de la parcimonie au sein de axes principaux nous considérons une pénalité `1 sur les colonnes de la matrice W La vraisemblance complétée à maximiser est alors la suivante `penc = `c − λ d∑ `=1 ||w`||1 5 où w` = w1` wp` t est la ` ème colonne de W et λ > 0 est le paramètre de pénalisation L’algorithme EM est un algorithme itératif qui alterne deux étapes E et M décrites ci après 150 C Bouveyron et J Jacques La q ème itération de l’étape E consiste à calculer l’espérance de `penc sous la loi p x|y θ q où θ q = W q σ2 q est la valeur courante de l’estimation des paramètres du modèles E[`penc θ |θ q ] = − n∑ i=1 p 2 lnσ2 + 1 2σ2 ytiyi − 1 σ2 etiW tyi + 1 2 tr Si + 1 2σ2 tr WtWSi − λ p∑ j=1 d∑ `=1 |wj`|+ c 6 où c = −n2 p+ d ln 2π est une constante indépendante des paramètres du modèle et où ei = M −1W q t yi et Si = σ2 q M−1 + eieti 7 avec M = W q t W q + σ2 q I L’étape M consiste alors à maximiser E[`penc θ |θ q ] en fonction de θ De sorte à faciliter la maximisation nous considérons l’approximation de la norme `1 par la forme quadratique suivante Fan et Li 2001 |wj`| ' |w q j` |+ 1 2 sign w q j` |w q j` | w2j` − w q j` 2 8 qui est valide lorsque w q j` ' wj` La maximisation de Q θ θ q en fonction de W n’ayant pas de solution analytique nous utilisons une approche alternative consistant à maximiser Q θ θ q en fonction de chaque éléments de la matrice W successivement On obtient alors en dérivant Q θ θ q par rapport à wj` et en égalant à 0 w q+1 j` = n∑ i=1  ei`yij − 1 2 ∑ k 6=` si`kw q jk   σ2 q λ sign w q j` |w q j` | + n∑ i=1 si`` 9 Cette dérivation élément par élément ne conduit pas nécessairement au maximum deQ θ θ q mais suffit pour faire augmenter la log vraisemblance à chaque étape de l’algorithme On ob tient alors un algorithme GEM Generalized EM qui conserve les mêmes propriétés de conver gence qu’un algorithme EM classique L’estimateur de la variance résiduelle est quant à lui σ2 q+1 = 1 Np n∑ i=1 { ytiyi − 2etiW q+1 t yi + tr SiW q+1 tW q+1 } 10 et s’avère être identique à celui de la version non sparse de l’analyse en composantes princi pales probabiliste Tipping et Bishop 1999 151 Un algorithme EM pour sparse PPCA 4 Sélection de λ par l’heuristique de pente Dans Zou et al 2004 le choix de la pénalité λ est réalisé de manière heuristique en se basant sur l’éboulis des valeurs propres L’idée de la stratégie que nous proposons est d’es timer le modèle sur une grille de valeurs de λ et d’utiliser un outil de sélection de modèles pour choisir le meilleur modèle Les outils classiques de sélection de modèles sont par exemple les critères AIC Akaike 1974 et BIC Schwarz 1978 qui pénalisent la log vraisemblance ` θ̂ de la façon suivante AIC = ` θ̂ − γ et BIC = ` θ̂ − γ2 log n où γ est le nombre de paramètres libres du modèles et n le nombre d’observations La valeur de γ dépend direc tement de la valeur de λ puisqu’elle est égale au nombre d’éléments non nuls dans W plus un pour la variance résiduelle Même si ces critères sont largement utilisés et asymptotique ment consistants ils sont aussi connus pour être plus efficaces sur simulations que sur données réelles Pour surmonter ce problème Birgé et Massart 2007 ont récemment proposé une mé thode dirigée par les données pour calibrer la pénalité des critères pénalisés connue sous le nom d’heuristique de pente L’heuristique de pente a été proposé initialement dans un cadre d’un modèle de régression gaussien homoscédastique mais a ensuite été étendue à d’autres situations Birgé et Massart 2007 ont démontré qu’il existait une pénalité minimale et que de considérer une pénalité égale au double de la pénalité minimale permettait d’approcher le modèle oracle en terme de risque La pénalité minimale est en pratique estimée par la pente de la partie linéaire de la log vraisemblance `penc θ̂ exprimée en fonction de la complexité du modèle Le critère associé est alors défini par SHC = ` θ̂ − 2ŝγ 11 où ŝ est l’estimation de la pente de la partie linéaire de `penc θ̂ Une revue détaillée et des conseils d’implémentation sont donnés dans Baudry et al 2012 5 Illustrations numériques Nous choisissons pour illustrer notre méthodologie un jeu de données classique issu de l’UCI machine learning repository le jeux de données USPS Le jeu original contient 7291 images représentant des chiffres manuscrits de 0 à 9 Chaque chiffre est une image en niveaux de gris de taille 16 × 16 représentée par un vecteur de dimension 256 Pour cette expérience nous avons extrait un sous ensemble de 1756 images correspondant aux chiffres 3 5 et 8 Nous réalisons sur ces données une ACP ainsi que l’ACP parcimonieuse que nous proposons Pour cette dernière nous fixons le nombre maximum d’itérations de l’algorithme EM à 500 et le seuil de convergence à 10−6 et nous considérons une grille de valeurs de λ de 0 à 150 avec un pas de 1 La méthode de l’heuristique de pente figure 1 conduit à choisir λ = 126 Dans un but illustratif nous discutons ici les résultats concernant les deux premières composantes principales La figure 2 représente la projection des 1756 images dans le premier plan prin cipal de l’ACP ainsi que les deux premières composantes principales tandis que la figure 3 propose la même représentation pour l’ACP parcimonieuse Nous pouvons noter que les deux méthodes définissent un premier plan principal relativement discriminant vis à vis des trois types d’images Tout l’intérêt de l’ACP parcimonieuse est que les composantes principales 152 C Bouveyron et J Jacques 100 200 300 400 500− 44 00 00 −4 30 00 0 −4 20 00 0 −4 10 00 0 −4 00 00 0 Nb parameters log lik eli ho od 0 50 100 150 −8 e+ 05 −7 e+ 05 −6 e+ 05 −5 e+ 05 lambda Pe na liz ed lo gli ke lih oo d FIG 1 – Heuristique de pente log vraisemblance en fonction du nombre de paramètres gauche et log vraisemblance pénalisée en fonction de λ −5 0 5 −6 −4 −2 0 2 4 6 Comp 1 Com p 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 333 3 3 3 3 3 3 33 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 333 3 33 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 33 3 3 33 3 3 3 33 3 3 3 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 3 333 333 3 3 3 3 3 33 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 333 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 33 3 3 33 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 555 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 555 5 5 55 5 5 5 555 5 55 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 55 5 5 5 5 5 5 55 55 5 5 5 55 5 5 5 5 5 5 5 5 55 5 55 5 5 5 5 5 5 55 55 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 55 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 55 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 888 8 88888 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 88 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 −2 −1 0 1 −2 0 2 4 Comp 1 Com p 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 33 3 3 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 33 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 55 5 5 5 5 5 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 888 8 8 8 8 8 8 8 8 8 8 88 88 8 8 8 8 8 88 8 8 88 8 8 88 8 8 8 888 8 8 8 88 8 8 8 8 88 8 8 8 8 8 8 88 8 888 8 8 8 8 8 8 88 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 888 8 8 8 8 8 8 88 8 8 8 8 888 8 888 8 8 88 8 8 8 88 8 8 8 8 8 8 8 88 8 88 88 888 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 88 8 8 8 8 8 88 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 8 88 8 88 8 8 8 8 88 8 8 8 88 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 88 8 8 8 88 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 88 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 888 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 Comp 1 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 Comp 2 FIG 2 – Représentation des 1756 images dans le premier plan principal gauche et deux pre mières composantes principales droite obtenues par l’analyse en composantes principales obtenues captent un signal semblable à celles de l’ACP tout en étant très parcimonieuses puis qu’elles ne dépendent que de peu de variables initiales 21 pour la première composante et 19 pour la seconde sur les 256 initialement disponibles Références Akaike H 1974 A new look at the statistical model identification IEEE Transactions on Automatic Control 19 6 716–723 Bartholomew D M Knott et I Moustaki 2011 Latent Variable Models and Factor Analy sis A Unified Approach Wiley Series in Probability and Statistics Wiley 153 Un algorithme EM pour sparse PPCA −5 0 5 −6 −4 −2 0 2 4 6 Comp 1 Com p 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 333 3 3 3 3 3 3 33 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 333 3 33 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 33 3 3 33 3 3 3 33 3 3 3 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 3 333 333 3 3 3 3 3 33 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 333 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 33 3 3 33 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 555 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 555 5 5 55 5 5 5 555 5 55 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 55 5 5 5 5 5 5 55 55 5 5 5 55 5 5 5 5 5 5 5 5 55 5 55 5 5 5 5 5 5 55 55 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 55 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 55 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 888 8 88888 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 88 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 −2 −1 0 1 −2 0 2 4 Comp 1 Com p 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 33 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 33 3 3 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 33 3 33 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 55 55 5 5 5 5 5 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 888 8 8 8 8 8 8 8 8 8 8 88 88 8 8 8 8 8 88 8 8 88 8 8 88 8 8 8 888 8 8 8 88 8 8 8 8 88 8 8 8 8 8 8 88 8 888 8 8 8 8 8 8 88 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 888 8 8 8 8 8 8 88 8 8 8 8 888 8 888 8 8 88 8 8 8 88 8 8 8 8 8 8 8 88 8 88 88 888 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 88 8 8 8 8 8 88 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 8 88 8 88 8 8 8 8 88 8 8 8 88 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 88 8 8 8 8 8 8 8 88 8 8 8 8 88 8 8 8 88 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 88 88 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 888 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 88 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 88 8 8 8 8 8 8 8 8 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 Comp 1 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 Comp 2 FIG 3 – Représentation des 1756 images dans le premier plan principal gauche et deux premières composantes principales droite obtenues par l’analyse en composantes principales parcimonieuse Baudry J P C Maugis et B Michel 2012 Slope heuristics overview and implementation Statistics and Computing 22 2 455–470 Birgé L et P Massart 2007 Minimal penalties for gaussian model selection Probability theory and related fields 138 1 2 33–73 Dempster A N Laird et D Rubin 1977 Maximum likelihood from incomplete data via the EM algorithm Journal of the Royal Statistical Society 39 1 1–38 Fan J et R Li 2001 Variable selection via nonconcave penalized likelihood and its oracle properties Journal of the American Statistical Association 96 456 1348–1360 Guan Y et J Dy 2009 Sparse probabilistic principal component analysis In In Proc AISTATS’2009 JMLR W CP pp 185–192 Jolliffe I T 1986 Principal Component Analysis Springer Verlag Schwarz G 1978 Estimating the dimension of a model Ann Statist 6 461–464 Tipping M et C Bishop 1999 Probabilistic principal component analysis Journal of the Royal Statistical Society Series B 61 611–622 Zou H T Hastie et R Tibshirani 2004 Sparse principal component analysis Journal of Computational and Graphical Statistics 15 265–286 Summary We consider a parsimonious version of probabilistic principal component analysis and we proposed an EM algorithm for model inference The `1 penalty imposed on the principal components makes their interpretation easier by linking them with only a limited number of original variables The EM algorithm easy to implement is proposed to estimate the model parameters The slope heuristic is used to select the intensity of the penalty 154 B Classification Clustering Similarité Un algorithme EM pour une version parcimonieuse de l’analyse en composante principale probabiliste Charles Bouveyron Julien Jacques