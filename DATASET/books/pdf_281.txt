algorithme version parcimonieuse analyse composantes principales probabiliste charles bouveyron julien jacques université paris descartes laboratoire charles bouveyron parisdescartes parisdescartes cbouveyr université lumière laboratoire julien jacques lyon2 lyon2 jjacques résumé considérons version parcimonieuse analyse compo santes principales probabiliste pénalité imposée composantes cipales interprétation aisée faisant dépendre dernières nombre restreint variables initiales algorithme simple œuvre proposé estimation paramètres modèle thode heuristique pente finalement utilisée choisir coefficient pénalisation introduction analyse composantes principales jolliffe méthodes méthode analyse exploratoire couramment utilisées interprétée formalisme probabiliste tipping bishop montrant composantes principales pouvaient estimées maximum vraisemblance cadre modèle variables latentes avénement données grande dimension problématique consistant sélectionner petit nombre variables intérêt parmi ensemble variables disponibles devenue primordiale soucis majeurs cette optique composantes principales définies comme combinaison linéaire ensemble variables initiales versions parcimonieuses ainsi version probabiliste proposées récemment version parci monieuse repose ajout pénalisation problème moindres carrés nécessite choix coefficient pénalisation façon heuristique version sparse bayésienne probabiliste proposée proposons travail alternative fréquentiste utilisant algorithme inférence procédure estimation obtenue avantage particulièrement simple nécessite choix priori offre outre possibilité considérer problème choix pénalité comme problème choix modèles algorithme sparse analyse composantes principales probabiliste vecteur aléatoire observé dimension vecteur aléatoire latent observé dimension relié équation suivante matrice vecteur moyenne supposé suite conditionnellement vecteur distribution vecteurs observés supposant distribution marginale vecteur observé modèle modèle factor analysis bartholomew popularisé tipping bishop analyse composantes principales probabiliste probabilistic principal component analysis effet estimation maximum vraisemblance paramètres modèle algorithme dempster considérant vecteur latent comme manquant conduit estimer colonnes vecteurs propres matrice covariance empirique vecteurs autres principaux classiques échantillon vecteurs observés algorithme consiste maximiser façon itérative vraisemblance complétée données observées xtixi version parcimonieuse analyse composantes principales probabiliste travail considérons version parcimonieuse analyse composantes principales probabiliste objectif obtenir principaux déterminés uniquement grâce nombre restreint variables initiales ainsi faciliter interprétation comme verrons suite approche probabiliste permet sélectionner paramètre pénalité méthodes classiques sélection modèles optique introduire parcimonie principaux considérons pénalité colonnes matrice vraisemblance complétée maximiser alors suivante colonne paramètre pénalisation algorithme algorithme itératif alterne étapes décrites après bouveyron jacques itération étape consiste calculer espérance valeur courante estimation paramètres modèles ytiyi wtwsi constante indépendante paramètres modèle eieti étape consiste alors maximiser fonction sorte faciliter maximisation considérons approximation norme forme quadratique suivante valide lorsque maximisation fonction ayant solution analytique utilisons approche alternative consistant maximiser fonction chaque éléments matrice successivement obtient alors dérivant rapport égalant cette dérivation élément élément conduit nécessairement maximum suffit faire augmenter vraisemblance chaque étape algorithme tient alors algorithme generalized conserve mêmes propriétés conver gence algorithme classique estimateur variance résiduelle quant ytiyi 2etiw avère identique celui version sparse analyse composantes princi pales probabiliste tipping bishop algorithme sparse sélection heuristique pente choix pénalité réalisé manière heuristique basant éboulis valeurs propres stratégie proposons timer modèle grille valeurs utiliser outil sélection modèles choisir meilleur modèle outils classiques sélection modèles exemple critères akaike schwarz pénalisent vraisemblance façon suivante nombre paramètres libres modèles nombre observations valeur dépend direc tement valeur puisqu égale nombre éléments variance résiduelle critères largement utilisés asymptotique consistants aussi connus efficaces simulations données réelles surmonter problème birgé massart récemment proposé thode dirigée données calibrer pénalité critères pénalisés connue heuristique pente heuristique pente proposé initialement cadre modèle régression gaussien homoscédastique ensuite étendue autres situations birgé massart démontré existait pénalité minimale considérer pénalité égale double pénalité minimale permettait approcher modèle oracle terme risque pénalité minimale pratique estimée pente partie linéaire vraisemblance exprimée fonction complexité modèle critère associé alors défini estimation pente partie linéaire revue détaillée conseils implémentation donnés baudry illustrations numériques choisissons illustrer notre méthodologie données classique machine learning repository données original contient images représentant chiffres manuscrits chaque chiffre image niveaux taille représentée vecteur dimension cette expérience avons extrait ensemble images correspondant chiffres réalisons données ainsi parcimonieuse proposons cette dernière fixons nombre maximum itérations algorithme seuil convergence considérons grille valeurs méthode heuristique pente figure conduit choisir illustratif discutons résultats concernant premières composantes principales figure représente projection images premier cipal ainsi premières composantes principales tandis figure propose représentation parcimonieuse pouvons noter méthodes définissent premier principal relativement discriminant trois types images intérêt parcimonieuse composantes principales bouveyron jacques parameters lambda heuristique pente vraisemblance fonction nombre paramètres gauche vraisemblance pénalisée fonction 88888 représentation images premier principal gauche mières composantes principales droite obtenues analyse composantes principales obtenues captent signal semblable celles étant parcimonieuses elles dépendent variables initiales première composante seconde initialement disponibles références akaike statistical model identification transactions automatic control bartholomew knott moustaki latent variable models factor analy unified approach wiley series probability statistics wiley algorithme sparse 88888 représentation images premier principal gauche premières composantes principales droite obtenues analyse composantes principales parcimonieuse baudry maugis michel slope heuristics overview implementation statistics computing birgé massart minimal penalties gaussian model selection probability theory related fields dempster laird rubin maximum likelihood incomplete algorithm journal royal statistical society variable selection nonconcave penalized likelihood oracle properties journal american statistical association sparse probabilistic principal component analysis aistats jolliffe principal component analysis springer verlag schwarz estimating dimension model statist tipping bishop probabilistic principal component analysis journal royal statistical society series hastie tibshirani sparse principal component analysis journal computational graphical statistics summary consider parsimonious version probabilistic principal component analysis proposed algorithm model inference penalty imposed principal components makes their interpretation easier linking limited number original variables algorithm implement proposed estimate model parameters slope heuristic select intensity penalty classification clustering similarité algorithme version parcimonieuse analyse composante principale probabiliste charles bouveyron julien jacques