 Classification d’un tableau de contingence et modèle probabiliste Gérard Govaert Mohamed Nadif Heudiasyc UMR CNRS 6599 Université de Technologie de Compiègne BP 20529 60205 Compiègne Cedex France gerard govaert utc fr IUT de Metz LITA Université de Metz Ile du Saulcy 57045 Metz Cedex France nadif iut univ metz fr Résumé Les modèles de mélange qui supposent que l’échantillon est formé de sous populations caractérisées par une distribution de probabi lité constitue un support théorique intéressant pour étudier la classifica tion automatique On peut ainsi montrer que l’algorithme des k means peut être vu comme une version classifiante de l’algorithme d’estimation EM dans un cas particulièrement simple de mélange de lois normales Lorsque l’on cherche à classifier les lignes ou les colonnes d’un tableau de contingence il est possible d’utiliser une variante de l’algorithme des k means appelé Mndki2 en s’appuyant sur la notion de profil et sur la distance du khi 2 On obtient ainsi une méthode simple et efficace pou vant s’utiliser conjointement à l’analyse factorielle des correspondances qui s’appuie sur la même représentation des données Malheureusement et contrairement à l’algorithme des k means classique les liens qui existent entre les modèles de mélange et la classification ne s’appliquent pas directement à cette situation Dans ce travail nous mon trons que l’algorithme Mndki2 peut être associé à une approximation près à un modèle de mélange de lois multinomiales 1 Introduction Les modèles de mélange qui supposent que l’échantillon est formé de sous popu lations caractérisées par une distribution de probabilité sont des modèles très souples permettant de prendre en compte des situations variées comme la présence de popu lations hétérogènes ou d’éléments atypiques Grâce à l’algorithme d’estimation EM particulièrement adapté à cette situation les modèles de mélange ont fait l’objet de nombreux développements en statistique et en particulier en classification automatique On peut ainsi montrer que l’algorithme des k means peut être vu comme une version classifiante de l’algorithme EM appelé CEM dans un cas particulièrement simple de mélange de lois normales Dans ce travail on étudie comment ces propriétés peuvent être étendues aux tableaux de contingence Rappelons qu’un tableau de contingence est obtenu à partir du croisement de 2 variables qualitatives par exemple si on note I et J les ensembles de r et s modalités de chaque variable chaque élément xij de la matrice de données contiendra le nombre RNTI E 3213 Classification d’un tableau de contingence et modèle probabiliste d’éléments prenant respectivement les modalités i et j pour chacune des 2 variables qualitatives Les tableaux de contingence sont quelquefois directement obtenus à partir de la saisie des données on retrouve par exemple ce type de tableaux en analyse de données textuelles où la matrice de données comptabilise le nombre d’occurrences d’un ensemble de mots et d’un ensemble de documents On parle alors quelquefois de tableau d’occurrences En outre la plupart des méthodes d’analyse de tels tableaux s’appliquent généralement aussi à des tableaux possédant des propriétés équivalentes Benzécri 1973 comme les tableaux de variables quantitatives avec des variables homogènes et positives quantités de même nature comme des longueurs ou des poids ou même les tableaux binaires La section 2 sera consacrée à l’algorithme Mndki2 algorithme permettant de clas sifier les lignes ou les colonnes d’un tableau de contingence Dans la section 3 nous définirons un modèle de mélange de lois de probabilité adapté à un tel type de don nées et la section 4 sera consacrée à l’application de l’algorithme d’estimation EM à ce modèle La version classifiante de cet algorithme EM baptisée Cemki2 sera étu diée dans la section 6 On montrera en particulier qu’on obtient ainsi un algorithme maximisant un critère approximant celui utilisé par Mndki2 et fournissant des résultats quasi identiques Notations Dans tout ce texte on notera x = xij le tableau de contingence construit sur les deux ensembles I et J ayant respectivement r et s éléments n = ∑ i j xij la somme des éléments du tableau et xi = ∑ j xij et x j = ∑ i xij ses marges On utilisera aussi le tableau des fréquences relatives fij = xij n fi = ∑ j fij et f j = ∑ i fij ses marges et les profils en ligne f iJ = fi1 fi fir fi Une partition en g classes de l’ensemble I sera notée z = z1 zr où zi ∈ {1 g} indique la classe de l’objet i zi = k lui même pourra aussi être représenté par le vecteur zi1 zig avec zik = 1 si i ∈ zk et zik = 0 sinon Dans ce dernier cas la classification sera représentée par une matrice z de n vecteurs de Rg vérifiant zik ∈ {0 1} et ∑ k zik = 1 Ainsi la classe k correspond à l’ensemble des objets i tel que zi = k ou encore zik = 1 et zi� = 0 ∀� �= k Par ailleurs pour simplifier la présentation les sommes et les produits portant sur les lignes les colonnes ou les classes seront indicés respectivement par les lettres i j et k sans indiquer les bornes de variation qui seront donc implicites Ainsi la somme ∑ i portera sur tous les lignes i allant de 1 à r 2 Algorithme Mndki2 2 1 L’objectif Pour mesurer l’information apportée par un tableau de contingence c’est à dire les liens existant entre deux ensembles I et J mis en correspondance dans le tableau de données il existe plusieurs mesures dont l’une des plus courantes est le χ2 de contin gence Ce critère utilisé par exemple dans l’analyse factorielle des correspondances est RNTI 1 RNTI E 3 214 Gérard Govaert et Mohamed Nadif défini de la manière suivante χ2 I J = ∑ i j xij − xi x jn 2 xi x j n = n ∑ i j fij − fi f j 2 fi f j Cette quantité représente l’écart entre les fréquences théoriques fi f j que l’on aurait s’il y avait indépendance entre les deux ensembles I et J et les fréquences observées fij une grande valeur correspondra à une forte dépendance alors qu’une valeur nulle correspondra à une indépendance entre I et J Cette mesure d’information peut également être utilisée pour évaluer la qualité d’une partition z de l’ensemble I pour ceci on associera à cette partition z le χ2 du tableau de contingence à g lignes et r colonnes obtenu à partir du tableau initial en faisant la somme des éléments de chaque classe xkj = ∑ i|zi=k xij ∀k j et qui sera noté χ2 z J On établira plus loin la relation χ2 I J ≥ χ2 z J qui montre que le regroupement des valeurs du tableau de contingence conduit nécessairement à une perte d’information L’objectif de la classification sera donc de trouver la partition z qui minimise cette perte c’est à dire qui maximise le critère χ2 z J Remarquons que dans le cas idéal où les profils en ligne sont égaux à l’intérieur de chaque classe alors χ2 z J = χ2 I J et il n’y a donc pas de perte d’information Par ailleurs le problème que l’on vient de définir n’a de sens que pour un nombre fixé de classes sinon la partition optimale est simplement la partition où chaque élément de I forme une classe 2 2 L’algorithme L’algorithme Mndki2 repose sur la représentation géométrique d’un tableau de contingence utilisée dans l’analyse factorielle des correspondances Cette représentation est justifiée pour plusieurs raisons en particulier pour les rôles analogues dévolus à cha cune des deux dimensions du tableau analysé Dans cette représentation à l’ensemble des lignes est associé dans Rs le nuage N I des r vecteurs des profils f iJ munis des masses fi La métrique utilisée dans cet espace est la métrique quadratique définie par la matrice diagonale diag 1f 1 1 f s appelée métrique du χ2 et notée dχ2 Avec cette représentation la relation classique de décomposition de l’inertie en une somme d’iner tie intraclasse et d’inertie interclasse s’écrit simplement χ2 I J = n W z + χ2 z J où W z = ∑ k ∑ i|zi=k fi d 2 χ2 f i J gk avec gk centre de gravité de la classe k En conséquence puisque la quantité χ2 I J ne dépend pas de la partition z la recherche de la partition maximisant le critère χ2 z J est équivalente à la recherche de la partition minimisant le critère W z Pour minimiser ce critère d’inertie d’inertie intraclasse il est alors possible d’appliquer la méthode des k means sur le nuage des profils avec la métrique du χ2 On obtient ainsi un algorithme itératif appelé Mndki2 maximisant localement le critère χ2 z J Malheureusement ce critère contrairement à celui des k means ne vérifie pas les conditions sous lesquelles un critère métrique de classification est équivalent à un critère de vraisemblance classifiante associé à un modèle de mélange Govaert 1989 Il est toutefois possible de montrer que ce critère est lié au moins approximativement au modèle de mélange de lois multinomiales RNTI 1 RNTI E 3215 Classification d’un tableau de contingence et modèle probabiliste 3 Le modèle de mélange de lois multinomiales Le modèle de mélange proposé consiste à considérer que chaque ligne xi du tableau de contingence est générée suivant le mécanisme suivant – la marge xi est simulée suivant une loi discrète ψ à valeur entière quelconque Poisson binomiale négative – la classe k est tirée au hasard suivant les probabilités π1 πg – le vecteur xi = xi1 xir est simulé suivant la distribution multinomiale de paramètres xi αk1 αkr Plus formellement si on note θ = π1 πg α11 αgr le paramètre du modèle et ϕ est la densité de la loi multinomiale la densité du modèle s’écrit f x θ = ∏ i f xi θ = ∏ i ψ xi ∑ k πkϕ xi xi αk1 αks = ∏ i ψ xi ∑ k πk xi xi1 xis αxi1k1 α xis ks = A ∏ i ∑ k πkα xi1 k1 α xis ks où A = ∏ i ψ xi ∏ i xi xi1 xis est un terme qui ne dépend pas du paramètre θ On notera L θ x = ∑ i log ∑ k πkα xi1 k1 α xis ks la log vraisemblance associée et L θ x z =∑ i k zik lnπk + ∑ j xij logαkj la log vraisemblance des données complétées Le problème généralement posé est alors l’estimation du paramètre θ à partir de l’échantillon Il s’agit d’un problème classique d’estimation statistique L’utilisation en classification automatique de ce modèle de mélange conduit en réalité à un autre problème retrouver le composant dont est issu chaque élément de l’échantillon Nous verrons plus loin comment utiliser le modèle de mélange pour atteindre cet objectif 4 Estimation des paramètres L’estimation du paramètre θ peut être obtenue en maximisant la log vraisemblance L θ x à l’aide de l’algorithme EM Sous certaines conditions de régularité il a été établi que l’algorithme EM assure une convergence vers un maximum local de la vrai semblance Il a un bon comportement pratique mais peut être toutefois assez lent dans certaines situations c’est le cas par exemple si les classes sont très mélangées Cet al gorithme proposé par Dempster Laird et Rubin 1977 dans un papier célèbre souvent simple à mettre en place est devenu populaire et a fait l’objet de nombreux travaux que l’on pourra trouver dans l’ouvrage très complet de McLachlan et Krishnan 1997 Partant d’un paramètre initial θ 0 une itération de l’algorithme EM consiste à maximiser l’espérance de la log vraisemblance des données complétées conditionnelle ment à l’estimation courante θ c et aux données x qui s’écrit Q θ θ c = ∑ i k t c ik  log πk +∑ j xij logαkj   RNTI 1 RNTI E 3 216 Gérard Govaert et Mohamed Nadif où t c ik est la probabilité d’appartenance de xi à la classe k conditionnellement à θ c Chaque itération se décompose en 2 étapes l’étape E calcule les probabilités t c ik t c ik = πkα xi1 k1 α xir kr� � π�α xi1 �1 α xir �r l’étape M détermine le paramètre θ maximisant Q θ θ c On montre facilement que cette maximisation conduit pour notre modèle aux relations π c+1 k = � i t c ik n et α c+1 kj = � i t c ik xij� i t c ik xi 5 Algorithme Cemki2 L’utilisation du modèle de mélange pour obtenir une partition des données initiales peut alors se faire en rangeant chaque individu dans la classe maximisant la probabilité a posteriori tik calculée à partir des paramètres estimés Une autre solution consiste à rechercher une partition de l’échantillon de telle sorte que chaque classe k soit assimi lable à un sous échantillon issue de la loi f αk Il s’agit donc d’estimer simultanément les paramètres du modèle et la partition recherchée en maximisant la vraisemblance complétée L θ x z définie précédemment Cette maximisation peut être obtenue par l’algorithme CEM classification EM Celeux et Govaert 1992 version classifiante de l’algorithme EM obtenue en lui ajou tant une étape de classification Chaque itération se décompose maintenant en 3 étapes l’étape E calcule les t c ik comme dans l’algorithme EM l’étape C détermine la parti tion z c+1 en rangeant chaque xi dans la classe maximisant t c ik l’étape M maxi mise la vraisemblance conditionnellement aux z c+1 ik les estimations du maximum de vraisemblance des πk et des αk sont obtenues en utilisant les classes de la partition z c+1 comme sous échantillons On montre facilement que cette maximisation conduit à π c+1 k = nk n et α c+1 kj = xkj xk où nk est le cardinal de la classe k de la partition z c+1 xkj = ∑ i k z c+1 ik xij et xk = ∑ j xkj Après la maximisation en θ et en notant fkj = xkj n le critère s’exprime L θ x z = ∑ k nk lnπk + ∑ k j xkj log xkj xk = ∑ k nk lnπk + n ∑ k j fkj log fkj fk f j + n ∑ j f j log f j En utilisant l’approximation 2x log x ≈ x2−1 cette quantité peut être approximée par ∑ k nk lnπk + n 2 ∑ k j fkj − fk f j 2 fk f j + n ∑ j f j log f j On retrouve alors à une constante près et lorsque les proportions sont fixées le critère maximisé par l’algorithme Mndki2 la maximisation de la vraisemblance classifiante est donc approximativement équivalente à la maximisation du χ2 de contingence et utiliser ce critère revient à supposer que les données sont issues d’un modèle de mélange de lois multinomiales En pratique les deux algorithmes Mndki2 et Cemki2 fournissent généralement les mêmes partitions RNTI 1 RNTI E 3217 Classification d’un tableau de contingence et modèle probabiliste 6 Conclusion L’interprétation probabiliste de l’algorithme Mndki2 constitue un support intéres sant pour traiter différentes situations qui sinon auraient nécessité le développement de méthodes ad hoc par exemple elle permet de prendre en compte des situations où les classes sont très mélangées en appliquant l’algorithme EM de prendre en compte des classes de proportions très différentes alors que le critère du χ2 suppose implicitement des proportions égales de traiter le problème du nombre de classes en s’appuyant sur les outils statistiques de choix de modèle comme l’utilisation des critères de complexité BIC ou ICL Par ailleurs l’interprétation géométrique du modèle de mélange de lois multinomiales permet en utilisant la représentation factorielle de l’analyse des corres pondances de visualiser les résultats fournis par les algorithmes EM ou CEM appliqués au modèle de mélange Références Benzécri J P 1973 L’analyse des données tome 2 l’analyse des correspondances Dunod Paris Celeux G et Govaert G 1992 A classification EM algorithm for clustering and two stochastic versions Computational Statistics and Data Analysis 14 3 pp 315 332 Dempster A P Laird N M et Rubin D B 1977 Maximum likelihood from incom plete data via the EM algorithm with discussion Journal of the Royal Statistical Society B 39 pp 1 38 Govaert G 1989 Clustering model and metric with continuous data In Diday Y editor Data Analysis Learning Symbolic and Numeric Knowledge Nova Science Publishers New York pp 95 102 McLachlan G J et Krishnan K 1997 The EM Algorithm Wiley New York Summary Mixture models which suppose that the sample is made of subpopulations charac terized by a probability distribution constitutes an interesting theoretical support to study clustering One can thus show that the k means algorithm can be seen like a clas sifying version of the EM algorithm in a particularly simple case of mixture of normal distributions When one seeks to classify the lines or the columns of a contingency table it is possible to use an alternative of the k means algorithm called Mndki2 based on the concept of profile and the khi 2 distance We obtain a simple and effective me thod which can be used jointly with the factorial correspondence analysis based on the same representation of the data Unfortunately and contrary to the traditional k means algorithm the links which exist between mixture models and clustering does not apply directly to this situation In this work we show that the Mndki2 algorithm can be associated with an approximation with a mixture model of multinomial distributions RNTI 1 RNTI E 3 218